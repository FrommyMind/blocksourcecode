{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/Yilia/source/main.0cf68a.css","path":"main.0cf68a.css","modified":1,"renderable":1},{"_id":"themes/Yilia/source/slider.e37972.js","path":"slider.e37972.js","modified":1,"renderable":1},{"_id":"source/img/1539920764009.png","path":"img/1539920764009.png","modified":1,"renderable":0},{"_id":"source/img/1540286924463.png","path":"img/1540286924463.png","modified":1,"renderable":0},{"_id":"source/img/DF2947BB-B48F-4C4E-B27A-C259D57EDA86.jpg","path":"img/DF2947BB-B48F-4C4E-B27A-C259D57EDA86.jpg","modified":1,"renderable":0},{"_id":"themes/Yilia/source/main.0cf68a.js","path":"main.0cf68a.js","modified":1,"renderable":1},{"_id":"themes/Yilia/source/mobile.992cbe.js","path":"mobile.992cbe.js","modified":1,"renderable":1},{"_id":"source/img/1540285662271.png","path":"img/1540285662271.png","modified":1,"renderable":0},{"_id":"source/img/1540285688689.png","path":"img/1540285688689.png","modified":1,"renderable":0},{"_id":"source/img/1540285620938.png","path":"img/1540285620938.png","modified":1,"renderable":0},{"_id":"source/img/1540285723630.png","path":"img/1540285723630.png","modified":1,"renderable":0},{"_id":"source/img/1540286887142.png","path":"img/1540286887142.png","modified":1,"renderable":0},{"_id":"source/img/1540286907954.png","path":"img/1540286907954.png","modified":1,"renderable":0},{"_id":"source/img/1540287176183.png","path":"img/1540287176183.png","modified":1,"renderable":0},{"_id":"themes/Yilia/source/fonts/default-skin.b257fa.svg","path":"fonts/default-skin.b257fa.svg","modified":1,"renderable":1},{"_id":"themes/Yilia/source/fonts/iconfont.16acc2.ttf","path":"fonts/iconfont.16acc2.ttf","modified":1,"renderable":1},{"_id":"themes/Yilia/source/fonts/iconfont.b322fa.eot","path":"fonts/iconfont.b322fa.eot","modified":1,"renderable":1},{"_id":"themes/Yilia/source/fonts/iconfont.45d7ee.svg","path":"fonts/iconfont.45d7ee.svg","modified":1,"renderable":1},{"_id":"themes/Yilia/source/fonts/tooltip.4004ff.svg","path":"fonts/tooltip.4004ff.svg","modified":1,"renderable":1},{"_id":"themes/Yilia/source/fonts/iconfont.8c627f.woff","path":"fonts/iconfont.8c627f.woff","modified":1,"renderable":1},{"_id":"themes/Yilia/source/img/preloader.gif","path":"img/preloader.gif","modified":1,"renderable":1},{"_id":"themes/Yilia/source/img/default-skin.png","path":"img/default-skin.png","modified":1,"renderable":1},{"_id":"themes/Yilia/source/img/scrollbar_arrow.png","path":"img/scrollbar_arrow.png","modified":1,"renderable":1},{"_id":"source/assets/img/QR Code.jpeg","path":"assets/img/QR Code.jpeg","modified":1,"renderable":0},{"_id":"themes/Yilia/source/img/QR Code.jpeg","path":"img/QR Code.jpeg","modified":1,"renderable":1},{"_id":"source/img/1539939380054.png","path":"img/1539939380054.png","modified":1,"renderable":0},{"_id":"source/img/1539939416172.png","path":"img/1539939416172.png","modified":1,"renderable":0},{"_id":"source/img/1539939421998.png","path":"img/1539939421998.png","modified":1,"renderable":0},{"_id":"source/img/1539939461605.png","path":"img/1539939461605.png","modified":1,"renderable":0},{"_id":"source/img/1539939371522.png","path":"img/1539939371522.png","modified":1,"renderable":0},{"_id":"source/img/kerberos.png","path":"img/kerberos.png","modified":1,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"71142ec70363a25c560829bf5bd91f1711797148","modified":1543356263968},{"_id":"themes/Yilia/.editorconfig","hash":"da6d022b8f4d9c961e2f8f80677e92af8de0db4d","modified":1526173414621},{"_id":"themes/Yilia/.eslintignore","hash":"df0a50b13cc00acb749226fee3cee6e0351fb1d9","modified":1526173414621},{"_id":"themes/Yilia/.babelrc","hash":"b1b76475ac17dc9e2fa50af96c9e31eea2d0f2b4","modified":1526173414620},{"_id":"themes/Yilia/.gitignore","hash":"9c4b7d27a1e3e5efa0c8ed143a032a85d586b03b","modified":1526173414621},{"_id":"themes/Yilia/.gitattributes","hash":"e0f24dceeb1e6878a1dd9b01a2b9df1bc037a867","modified":1526173414621},{"_id":"themes/Yilia/README.md","hash":"1bf755806af9d8874bd22e1abbdaaa24328ef4dc","modified":1526173414621},{"_id":"themes/Yilia/.eslintrc.js","hash":"5696ae049de010ed3786768b0c359f14c05b5ec6","modified":1526173414621},{"_id":"themes/Yilia/_config.yml","hash":"c4567ce5488093b1030299df3517a3f112100e64","modified":1543356823677},{"_id":"themes/Yilia/package.json","hash":"367cb9579d35968a942c243ab248a5f5ebfaf462","modified":1526173414631},{"_id":"themes/Yilia/webpack.config.js","hash":"05ba46a4ae744272f5312e684928910dccad3755","modified":1526173414652},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1526181061624},{"_id":"source/CCAH131/index.md","hash":"7f251cb1fd4441fa27a0735719dfb47b249ad608","modified":1526176008145},{"_id":"source/_posts/CCAH-131-Configure.md","hash":"0fb7613a548c187a23350312b17dd7631445f62f","modified":1536887933799},{"_id":"source/_posts/CCAH-131-Install.md","hash":"27c949aefcca7c92369c95a4d8a8b5a105835d31","modified":1536887817039},{"_id":"source/_posts/CCAH-131-Secure.md","hash":"b4f413ae56615fad7fd58b39dcdd9ea0c77709c0","modified":1536887973192},{"_id":"source/_posts/CCAH-131-Required-Skills.md","hash":"8013dac72f37d5863fa1b41a3c1bd8f547d2598c","modified":1536887794443},{"_id":"source/_posts/CCAH-131-Manage.md","hash":"a1282e2a3920b3c20c136398ff371034156eeb59","modified":1536887954214},{"_id":"source/_posts/CCAH-131-Test.md","hash":"88b33ad9668573a1797581e7d96478830e7f6dd3","modified":1536887991101},{"_id":"source/_posts/Cloudera-Manager-API.md","hash":"8edfc94a00be668c3f79f9d9c441b273933c2a7f","modified":1540289643464},{"_id":"source/_posts/CCAH-131-Troubleshoot.md","hash":"3f9f1532db81e6e1122ce122fef5c6719d60bf19","modified":1536887999870},{"_id":"source/_posts/CDH-安装准备.md","hash":"e7b989eaa694a2d2380d083a9fca9c15f6e6a7b8","modified":1537521449212},{"_id":"source/_posts/CDH-POC-Env-requirement.md","hash":"52c63eb05148077d4d4323dc22a7bb18c6f69ab7","modified":1536888531794},{"_id":"source/_posts/Configuration-DbVisualize-Connect-Hive.md","hash":"278aa50dc0a251ef68c90650d65d1246c391d0ec","modified":1536888391073},{"_id":"source/_posts/CDH-User-Managerment.md","hash":"451a5de0b32b0ab35da36893b20934a784ab0f14","modified":1536888410494},{"_id":"source/_posts/Cloudera-Manager-High-Availability.md","hash":"dd37bcc5c0531a7678bf0df8a3a6579a46f69a2d","modified":1540185656025},{"_id":"source/_posts/Cloudera-Navigator.md","hash":"01034ab5d760df4ec2e0bb9bd20f647b51ad08c3","modified":1536888200090},{"_id":"source/_posts/Hive UDFs.md","hash":"9609a7a125a152b80a5ae38c8a7571bd6689d1ac","modified":1526183492716},{"_id":"source/_posts/Hive架构相关.md","hash":"7d9053874d597827a90376edcf6987d255d57ffa","modified":1536888571898},{"_id":"source/_posts/HBase-Region-Splitting-and-Merging.md","hash":"7f77b6d13bdb771c6428b2ae5472008b312f9440","modified":1543356721973},{"_id":"source/_posts/Hive窗口函数和分析函数.md","hash":"a96baa7e6e096f060a3612868311b48d013da071","modified":1536888249678},{"_id":"source/_posts/Hive-复杂数据类型.md","hash":"459b0e48b3228db6a4bc1ffb6640b07146d9fe5f","modified":1536888103961},{"_id":"source/_posts/Install-CDH6-On-CentOS6.md","hash":"1b0059870678699e772e41571569e53afee9447c","modified":1541128867762},{"_id":"source/_posts/Parquet-File-Format.md","hash":"3f4ab089f6705d80274df227912efe0de6c5045e","modified":1536888151907},{"_id":"source/_posts/Install-CDH6-on-CentOS7.md","hash":"ec0d8687894b0aba7f224a1582308f737dbb94ac","modified":1536887717423},{"_id":"source/_posts/Kerberos.md","hash":"eca59483cbc8160690373d663c47476b180da26c","modified":1540294705700},{"_id":"source/_posts/test.md","hash":"e4fa32d34267d0eca05e60d7930764058415ef1e","modified":1526174947225},{"_id":"source/_posts/Test-hive-overwrite-and-delete-target-dir-in-sqoop.md","hash":"1c6135252ef4adc0e12370b92295799f21656883","modified":1536888053876},{"_id":"source/_posts/Sqoop-导入数据库到hive.md","hash":"4badc20c394e38e8607a517db71447682c1e3846","modified":1536888041645},{"_id":"source/_posts/help.md","hash":"8a02477044e2b77f1b262da2c48c01429e4a32e4","modified":1514008396740},{"_id":"source/hive/index.md","hash":"a4f07a0af047ba774f97f98a00e23dc510e8e13f","modified":1526175777502},{"_id":"source/_posts/测试Hive权限级别.md","hash":"f8c2b10166d6f2f804a5c8286f5c18a4e5843941","modified":1526193600661},{"_id":"themes/Yilia/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1526173414615},{"_id":"themes/Yilia/.git/config","hash":"256e47c5a4386e6881318e44d2a716fdc3197863","modified":1526173414616},{"_id":"themes/Yilia/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1526173404321},{"_id":"themes/Yilia/.git/index","hash":"3a9e2ecdbfdcccfaae73e7bc0504bc3a9be8463c","modified":1540292514144},{"_id":"themes/Yilia/.git/packed-refs","hash":"83644c3638dafa38c817265c9207f098dd8aeee6","modified":1526173414613},{"_id":"themes/Yilia/languages/fr.yml","hash":"84ab164b37c6abf625473e9a0c18f6f815dd5fd9","modified":1526173414622},{"_id":"themes/Yilia/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1526173414623},{"_id":"themes/Yilia/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1526173414622},{"_id":"themes/Yilia/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1526173414622},{"_id":"themes/Yilia/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1526173414623},{"_id":"themes/Yilia/languages/zh-tw.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1526173414623},{"_id":"themes/Yilia/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1526173414622},{"_id":"themes/Yilia/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1526173414629},{"_id":"themes/Yilia/layout/index.ejs","hash":"a35dc900203f9d8dd863ea4c1722198d6d457ec8","modified":1526173414629},{"_id":"themes/Yilia/layout/layout.ejs","hash":"0a332bdbd3b86c231d690614687f5b97186b85d5","modified":1526173414630},{"_id":"themes/Yilia/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1526173414629},{"_id":"themes/Yilia/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1526173414630},{"_id":"themes/Yilia/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1526173414630},{"_id":"themes/Yilia/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1526173414630},{"_id":"themes/Yilia/source/main.0cf68a.css","hash":"ddf6e2c6b953c2c59a3c271e6070010a4cc81cf9","modified":1526173414650},{"_id":"themes/Yilia/source/slider.e37972.js","hash":"6dec4e220c89049037eebc44404abd8455d22ad7","modified":1526173414652},{"_id":"themes/Yilia/source-src/css.ejs","hash":"94dbdb02ca11849e415d54fb28546a598f2cffb1","modified":1526173414631},{"_id":"themes/Yilia/source-src/script.ejs","hash":"c21381e1317db7bb157f1d182b8c088cb7cba411","modified":1526173414646},{"_id":"source/img/1539920764009.png","hash":"09a908ebe8562d14602b26a0e1821a63d3c197d9","modified":1540183010873},{"_id":"source/img/1540286924463.png","hash":"bc25418075240a1ed256a938fc6dcf22ae01ec79","modified":1540292442074},{"_id":"source/img/DF2947BB-B48F-4C4E-B27A-C259D57EDA86.jpg","hash":"6ecd61743a593b12c736cb49a1c9f549f3e59679","modified":1541472257281},{"_id":"themes/Yilia/layout/_partial/toc.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526173414628},{"_id":"themes/Yilia/source/main.0cf68a.js","hash":"993fadeb5f6d296e9d997a49ee20dc97333ceab7","modified":1526173414650},{"_id":"themes/Yilia/source/mobile.992cbe.js","hash":"01b35e71e37aa2849664eb5daf26daede2278398","modified":1526173414651},{"_id":"source/img/1540285662271.png","hash":"9f7a5f731d9a0d253af0a108bd11a46dc8c261fe","modified":1540289736005},{"_id":"source/img/1540285688689.png","hash":"86a8a6aa337eefc361887849690dcd8a6b281ec6","modified":1540289749236},{"_id":"source/img/1540285620938.png","hash":"640302a6555a526c413e14f9f74e4d602ec8fb29","modified":1540289722490},{"_id":"source/img/1540285723630.png","hash":"4f3b8f80af32e8add29890e88e68f3b7a91929a7","modified":1540289772772},{"_id":"source/img/1540286887142.png","hash":"62185cd656c9d0836b07ccf68c68d8a763e8b2bc","modified":1540292390553},{"_id":"source/img/1540286907954.png","hash":"dcdd931738195f69aaa1cf715d9ebd05c70e87ca","modified":1540292410747},{"_id":"source/img/1540287176183.png","hash":"c89d6a4e6810a6024c2ce74c8b17159b0091b3bd","modified":1540292457884},{"_id":"themes/Yilia/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1526173404322},{"_id":"themes/Yilia/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1526173404321},{"_id":"themes/Yilia/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1526173404323},{"_id":"themes/Yilia/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1526173404323},{"_id":"themes/Yilia/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1526173404322},{"_id":"themes/Yilia/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1526173404322},{"_id":"themes/Yilia/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1526173404323},{"_id":"themes/Yilia/.git/hooks/pre-rebase.sample","hash":"18be3eb275c1decd3614e139f5a311b75f1b0ab8","modified":1526173404321},{"_id":"themes/Yilia/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1526173404322},{"_id":"themes/Yilia/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1526173404323},{"_id":"themes/Yilia/.git/info/exclude","hash":"bb5a85730dcf100facee799c05cc4f6affec0745","modified":1526173404321},{"_id":"themes/Yilia/.git/logs/HEAD","hash":"17c7fc377368583b26fe312ed60bffc79bdaebce","modified":1526173414616},{"_id":"themes/Yilia/layout/_partial/after-footer.ejs","hash":"b86b248720ad415ec1b5fee53fb583776c776f83","modified":1526173414623},{"_id":"themes/Yilia/layout/_partial/archive-post.ejs","hash":"1f7d4819b7f67602c4a1b99871808d2160b60978","modified":1526173414623},{"_id":"themes/Yilia/layout/_partial/archive.ejs","hash":"a6e94061ac55b9eb55275f87b608d62f6ea35659","modified":1526173414623},{"_id":"themes/Yilia/layout/_partial/article.ejs","hash":"1e9731ccbaf4cd4e55b03a1ea999c8f67df139f8","modified":1536972550431},{"_id":"themes/Yilia/layout/_partial/aside.ejs","hash":"8edbd7993b9b061611a193533a664e2e85eae748","modified":1526173414624},{"_id":"themes/Yilia/layout/_partial/footer.ejs","hash":"f2994e0acd1d606ebf4680afc4fa652e148ccf4e","modified":1526173414624},{"_id":"themes/Yilia/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1526173414624},{"_id":"themes/Yilia/layout/_partial/baidu-analytics.ejs","hash":"f0e6e88f9f7eb08b8fe51449a8a3016273507924","modified":1526173414624},{"_id":"themes/Yilia/layout/_partial/css.ejs","hash":"236f8a377b2e4e35754319c3029bcd4a4115431d","modified":1526173414624},{"_id":"themes/Yilia/layout/_partial/header.ejs","hash":"6387a93dad7c3d778eb91e3821852fbf6813880c","modified":1526173414625},{"_id":"themes/Yilia/layout/_partial/head.ejs","hash":"64f092186b5a744aa1603ce22bb1d44a34446add","modified":1526173414625},{"_id":"themes/Yilia/layout/_partial/left-col.ejs","hash":"183d7ca4ba8e7c80694ffdc8cf39957092238346","modified":1526173414625},{"_id":"themes/Yilia/layout/_partial/mathjax.ejs","hash":"151a1ef2173ba7b6789d349f0f8da89616cc1394","modified":1526173414625},{"_id":"themes/Yilia/layout/_partial/mobile-nav.ejs","hash":"7fbbfabf5e29525b24ada14613c21a26789132b4","modified":1526173414625},{"_id":"themes/Yilia/layout/_partial/viewer.ejs","hash":"e495790b2abe2290875817e42bd505bc611d3e26","modified":1526173414629},{"_id":"themes/Yilia/layout/_partial/tools.ejs","hash":"c41341b9618e591538e1136a2d1637587c1bbd90","modified":1526173414628},{"_id":"themes/Yilia/source/fonts/default-skin.b257fa.svg","hash":"2ac727c9e092331d35cce95af209ccfac6d4c7c7","modified":1526173414646},{"_id":"themes/Yilia/source/fonts/iconfont.16acc2.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1526173414647},{"_id":"themes/Yilia/source/fonts/iconfont.b322fa.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1526173414648},{"_id":"themes/Yilia/source/fonts/iconfont.45d7ee.svg","hash":"f9304e5714d20861be7d8f4d36687e88e86b9e1b","modified":1526173414647},{"_id":"themes/Yilia/source/fonts/tooltip.4004ff.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1526173414648},{"_id":"themes/Yilia/source/fonts/iconfont.8c627f.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1526173414648},{"_id":"themes/Yilia/source/img/preloader.gif","hash":"6342367c93c82da1b9c620e97c84a389cc43d96d","modified":1526173414649},{"_id":"themes/Yilia/source/img/default-skin.png","hash":"ed95a8e40a2c3478c5915376acb8e5f33677f24d","modified":1526173414649},{"_id":"themes/Yilia/source/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1526173414649},{"_id":"themes/Yilia/source-src/js/anm.js","hash":"4a4c5d82b09a3063f91a434388e6aa064fd7fd98","modified":1526173414643},{"_id":"themes/Yilia/source-src/js/aside.js","hash":"754f771264548a6c5a8ad842908e59ae4e7ed099","modified":1526173414644},{"_id":"themes/Yilia/source-src/js/Q.js","hash":"d011af172064b6c6e0c7051d8f9879373ddac113","modified":1526173414643},{"_id":"themes/Yilia/source-src/js/browser.js","hash":"04095b38cfd4316a23f8eb14b1ffaf95f78a4260","modified":1526173414644},{"_id":"themes/Yilia/source-src/js/fix.js","hash":"d6782d53c992e712af39c84e804eccaf38830b94","modified":1526173414644},{"_id":"themes/Yilia/source-src/js/main.js","hash":"3894e60827c817319e43c9ff3ed045fc3d7336ce","modified":1526173414644},{"_id":"themes/Yilia/source-src/js/mobile.js","hash":"4d823b039fd296d24a454eae5a798b93c44560cb","modified":1526173414644},{"_id":"themes/Yilia/source-src/js/report.js","hash":"4f1d9a18a936ce40b037f636a39127dd19175b6e","modified":1526173414645},{"_id":"themes/Yilia/source-src/js/share.js","hash":"b090f82cf80cba7da764753906d9e2cc2acdf30d","modified":1526173414645},{"_id":"themes/Yilia/source-src/js/util.js","hash":"8456e9d6b19532742582c99b2fb9d09e146e1c58","modified":1526173414646},{"_id":"themes/Yilia/source-src/js/viewer.js","hash":"2577deb6a9fe4f5436360b2ce9afcc7f9a7f0116","modified":1526173414646},{"_id":"themes/Yilia/source-src/css/_function.scss","hash":"93a50dd19a93485712da1f8d0a1672482dd1eabc","modified":1526173414632},{"_id":"themes/Yilia/source-src/css/_core.scss","hash":"24f347a2412abbf58318369152504da9538f8d3b","modified":1526173414631},{"_id":"themes/Yilia/source-src/js/slider.js","hash":"e846bcc5aac9c68b93f7b8de353df54d8d29f666","modified":1526173414645},{"_id":"themes/Yilia/source-src/css/archive.scss","hash":"7d27e22ac898e8fafec14549e940c73cbea1fba8","modified":1526173414632},{"_id":"themes/Yilia/source-src/css/article-main.scss","hash":"3fad68bd74260326f83090b0974dd80707e7bac7","modified":1526173414633},{"_id":"themes/Yilia/source-src/css/article-inner.scss","hash":"d79f2d35a06de83a2a226ca790b7a0a34789c115","modified":1526173414632},{"_id":"themes/Yilia/source-src/css/article-nav.scss","hash":"43e507f2a48504079afd9471353337e23ca47470","modified":1526173414633},{"_id":"themes/Yilia/source-src/css/aside.scss","hash":"578a67464dd0f542197f7fcee158c991db058563","modified":1526173414633},{"_id":"themes/Yilia/source-src/css/comment.scss","hash":"cafe3834017a3bf47420f37543725025225a2c89","modified":1526173414634},{"_id":"themes/Yilia/source-src/css/article.scss","hash":"0f6d61af99ed4db87f8589db1feaea7747b55963","modified":1526173414633},{"_id":"themes/Yilia/source-src/css/fonts.scss","hash":"97b8fba41c914145710b90091f400b845879577f","modified":1526173414635},{"_id":"themes/Yilia/source-src/css/global.scss","hash":"b4cb4f45a55d4250cd9056f76dab2a3c0dabcec4","modified":1526173414638},{"_id":"themes/Yilia/source-src/css/footer.scss","hash":"7c995410b25baaf61dfc5e148e22ca60330abcd3","modified":1526173414638},{"_id":"themes/Yilia/source-src/css/grid.scss","hash":"849a29fcd7150214fcf7b9715fa5dc71d1f9b896","modified":1526173414638},{"_id":"themes/Yilia/source-src/css/highlight.scss","hash":"3719994c2c9393813cc1d42b657205c368a329cb","modified":1526173414638},{"_id":"themes/Yilia/source-src/css/left.scss","hash":"0d30c0e7cdb831c3881a017006c782f2214ac195","modified":1526173414639},{"_id":"themes/Yilia/source-src/css/main.scss","hash":"2f86a014af93583caba78a563d9549826bf28294","modified":1526173414640},{"_id":"themes/Yilia/source-src/css/mobile-slider.scss","hash":"f053c609d84df0dd9eee1d11ddf0c19163a456be","modified":1526173414640},{"_id":"themes/Yilia/source-src/css/page.scss","hash":"bf206bb7f7d0967bc8b7fdf01b7ffc99aff9ba88","modified":1526173414640},{"_id":"themes/Yilia/source-src/css/reward.scss","hash":"80a4fcf9171d4a33235da96ac8a2b7dcabc45dfb","modified":1526173414641},{"_id":"themes/Yilia/source-src/css/mobile.scss","hash":"ace041d72f95b419f6a5e443191703c2b62007f4","modified":1526173414640},{"_id":"themes/Yilia/source-src/css/scroll.scss","hash":"9c8dfd1c76854ef063494ca76fac6360b391ed6d","modified":1526173414641},{"_id":"themes/Yilia/source-src/css/share.scss","hash":"150c6425f6582e7ec78a873256ce49c9930e8805","modified":1526173414641},{"_id":"themes/Yilia/source-src/css/social.scss","hash":"724162ccf3977e70a45d189abfaa20b6e2fba87b","modified":1526173414641},{"_id":"themes/Yilia/source-src/css/tags-cloud.scss","hash":"c8aa84fca93862d3caae77c552873b8610f33327","modified":1526173414642},{"_id":"themes/Yilia/source-src/css/tags.scss","hash":"ac67a3c7097849206244db9b0ba91daaba017ef5","modified":1526173414642},{"_id":"themes/Yilia/source-src/css/tools.scss","hash":"1b1aa0908e58cf942b28e3881d07c5573c4129e1","modified":1526173414642},{"_id":"themes/Yilia/source-src/css/tooltip.scss","hash":"53d5a554bc2f38e9bb3d26400a47767013c05216","modified":1526173414642},{"_id":"source/assets/img/QR Code.jpeg","hash":"2df1d0ac1c37102aff3223bd06dd23de8d3406ca","modified":1526175364957},{"_id":"themes/Yilia/layout/_partial/script.ejs","hash":"4cb685f07e89dd5175c2a576e73a1a957aec5637","modified":1526173414628},{"_id":"themes/Yilia/source/img/QR Code.jpeg","hash":"2df1d0ac1c37102aff3223bd06dd23de8d3406ca","modified":1526175158094},{"_id":"source/img/1539939380054.png","hash":"f5ff4d0dbb06db8318d99d44decde9b8e06e51c5","modified":1540185415313},{"_id":"source/img/1539939416172.png","hash":"21ab7669e8e39a977f1e36872c9db4092e25c37b","modified":1540185423680},{"_id":"source/img/1539939421998.png","hash":"aed47aaebd421b5eeffe771224b568bbdbb15535","modified":1540185419599},{"_id":"source/img/1539939461605.png","hash":"8a110c72e5cb9df0e1bf39b260916307bab3d8fd","modified":1540185403368},{"_id":"themes/Yilia/.git/objects/pack/pack-d91b96247e3dc9ea6670b71e9dcce74914f2ba78.idx","hash":"f9dabb64699353ef0ec6a2e6f6099e0500aff4b6","modified":1526173414600},{"_id":"themes/Yilia/.git/refs/heads/master","hash":"4ed77da1a2617db0e77c3e3e190a1c79c16dfb9a","modified":1526173414615},{"_id":"themes/Yilia/layout/_partial/post/changyan.ejs","hash":"5f99b55980da64a723a8e14d5a7daba0d6504647","modified":1526173414626},{"_id":"themes/Yilia/layout/_partial/post/date.ejs","hash":"ef71c4081e866a494367575c59610e7e6339ece0","modified":1526173414626},{"_id":"themes/Yilia/layout/_partial/post/category.ejs","hash":"0809a4829aabeb4e911a3ed04ec28db4df7dfe3f","modified":1526173414625},{"_id":"themes/Yilia/layout/_partial/post/duoshuo.ejs","hash":"e8399025ed3b980aedb821c92855889f5f12fd5b","modified":1526173414626},{"_id":"themes/Yilia/layout/_partial/post/livere.ejs","hash":"bbd0d944067b43ecaad4f5e1fab88bb8413ab428","modified":1536973261288},{"_id":"themes/Yilia/layout/_partial/post/share.ejs","hash":"5dccfbe165b23a101f1333cc65ed8efbd197453c","modified":1526173414626},{"_id":"themes/Yilia/layout/_partial/post/gitment.ejs","hash":"e68bbac9ffb1ad27b56837c9abad6ed6bb7daa0c","modified":1526173414626},{"_id":"themes/Yilia/layout/_partial/post/tag.ejs","hash":"2e783e68755abb852760eb0e627a3fbb50a05a55","modified":1526173414626},{"_id":"themes/Yilia/layout/_partial/post/nav.ejs","hash":"1036c8e4e1a7bc935ba173744da735a0d6ed09cd","modified":1526173414626},{"_id":"themes/Yilia/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1526173414627},{"_id":"themes/Yilia/layout/_partial/post/wangyiyun.ejs","hash":"ea41c462168d9697caef9485862e9cac718a12c1","modified":1526173414627},{"_id":"themes/Yilia/source-src/css/core/_animation.scss","hash":"63a37f26276f9207405afe0f2d65339ce295bbaf","modified":1526173414634},{"_id":"themes/Yilia/source-src/css/core/_media-queries.scss","hash":"491ab3378d5c11005ba65c607608bb36b368a9d5","modified":1526173414634},{"_id":"themes/Yilia/source-src/css/core/_mixin.scss","hash":"3bba5c77bad5981eac859fe05c9561d580ba7fa9","modified":1526173414635},{"_id":"themes/Yilia/source-src/css/core/_reset.scss","hash":"fab871fa93bd542e76a71a56428f2994a4aaf443","modified":1526173414635},{"_id":"themes/Yilia/source-src/css/core/_variables.scss","hash":"fb511c505d1309249f21dc577d4ad2bad99a764f","modified":1526173414635},{"_id":"themes/Yilia/source-src/css/fonts/iconfont.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1526173414636},{"_id":"themes/Yilia/source-src/css/fonts/iconfont.svg","hash":"f9304e5714d20861be7d8f4d36687e88e86b9e1b","modified":1526173414636},{"_id":"themes/Yilia/source-src/css/fonts/iconfont.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1526173414637},{"_id":"themes/Yilia/source-src/css/fonts/iconfont.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1526173414637},{"_id":"themes/Yilia/source-src/css/img/checkered-pattern.png","hash":"049262fa0886989d750637b264bed34ab51c23c8","modified":1526173414639},{"_id":"themes/Yilia/source-src/css/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1526173414639},{"_id":"themes/Yilia/source-src/css/img/tooltip.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1526173414639},{"_id":"source/img/1539939371522.png","hash":"ef4c70bba5319b41bf228a575406e6dd6c4b46c6","modified":1540185409937},{"_id":"themes/Yilia/.git/logs/refs/heads/master","hash":"17c7fc377368583b26fe312ed60bffc79bdaebce","modified":1526173414616},{"_id":"themes/Yilia/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1526173414614},{"_id":"source/img/kerberos.png","hash":"6fdbb65aee458acd6b89ab57476d5ea0b200fb47","modified":1540294609332},{"_id":"themes/Yilia/.git/logs/refs/remotes/origin/HEAD","hash":"17c7fc377368583b26fe312ed60bffc79bdaebce","modified":1526173414614},{"_id":"themes/Yilia/.git/objects/pack/pack-d91b96247e3dc9ea6670b71e9dcce74914f2ba78.pack","hash":"b378db191fd5ac64da3f2c9ef7274d72c5d484d8","modified":1526173414598},{"_id":"public/content.json","hash":"1f4e41de28dcd641e7527704e2e86f2b42718f0d","modified":1543356950817},{"_id":"public/CCAH131/index.html","hash":"862c20fe3fcaa6ecfa2a1c0aaa808edcebf1586a","modified":1543356951202},{"_id":"public/hive/index.html","hash":"df2af8624b6efcca85f5e63e5de40459037ce74a","modified":1543356951202},{"_id":"public/2018/11/28/HBase-Region-Splitting-and-Merging/index.html","hash":"897d4b2b8e73eacca4a7fbdd302b6db8484fbbae","modified":1543356951202},{"_id":"public/2018/10/23/Kerberos/index.html","hash":"131c1802ce165b0c2244e3a41ba9763e26a76702","modified":1543356951203},{"_id":"public/2018/10/23/Cloudera-Manager-API/index.html","hash":"8500bbf4f91d359e197fd537b0f1a0399e0300fd","modified":1543356951203},{"_id":"public/2018/10/22/Cloudera-Manager-High-Availability/index.html","hash":"d6ca27005889aabe637d51eee2ab9d4240f8cdf7","modified":1543356951203},{"_id":"public/2018/09/21/CDH-安装准备/index.html","hash":"6994f39dc91d03a343bef344a85bb6cecd519897","modified":1543356951203},{"_id":"public/2018/09/13/Install-CDH6-on-CentOS7/index.html","hash":"765829f56db0902202a628273aedb4eabe1fe598","modified":1543356951203},{"_id":"public/2018/09/10/Install-CDH6-On-CentOS6/index.html","hash":"97989a81544eabb7d24b207f136dbfa9544fad94","modified":1543356951203},{"_id":"public/2018/05/16/Test-hive-overwrite-and-delete-target-dir-in-sqoop/index.html","hash":"19206369614d75ca0260349b20405dbd59fc926d","modified":1543356951203},{"_id":"public/2018/05/13/Hive-复杂数据类型/index.html","hash":"015cb985e786e7e0ec94e94beb021d813d3ef1cc","modified":1543356951204},{"_id":"public/2018/05/13/Parquet-File-Format/index.html","hash":"4a585214000670436e7e2bfa9f920570a9ac68c4","modified":1543356951204},{"_id":"public/2018/05/13/Cloudera-Navigator/index.html","hash":"03d5198fdfda25bc4137a1185781c6584169a3b7","modified":1543356951204},{"_id":"public/2018/05/13/Hive窗口函数和分析函数/index.html","hash":"0cc55adfc428f5fc780fe7459d7e7e958e3f6972","modified":1543356951204},{"_id":"public/2018/05/13/Hive UDFs/index.html","hash":"685de29ab76139519ef0d6b14a1a1e7115ab58d9","modified":1543356951204},{"_id":"public/2018/05/13/Configuration-DbVisualize-Connect-Hive/index.html","hash":"f832f4b78572ad4bada485298d9e115c48226677","modified":1543356951205},{"_id":"public/2018/05/13/CDH-User-Managerment/index.html","hash":"2e7a708ed5673d704bff64cf659fc1a571f3a619","modified":1543356951205},{"_id":"public/2018/05/13/CDH-POC-Env-requirement/index.html","hash":"98552b71f78431ba297399b8df27410e6787930c","modified":1543356951205},{"_id":"public/2018/05/13/CCAH-131-Test/index.html","hash":"51e7a338fb1d31f0e652f214b16e3917b83fced5","modified":1543356951205},{"_id":"public/2018/05/13/CCAH-131-Troubleshoot/index.html","hash":"56a0f316edccefaac1c234b5e17a7ea942cdaf49","modified":1543356951205},{"_id":"public/2018/05/13/CCAH-131-Secure/index.html","hash":"87772d8bddf823536f29ef4e50b375e97589007c","modified":1543356951205},{"_id":"public/2018/05/13/CCAH-131-Manage/index.html","hash":"564160b6db8c25b23f46dd4b0433035cba8f2415","modified":1543356951206},{"_id":"public/2018/05/13/CCAH-131-Configure/index.html","hash":"b208da8d738209067ddf885068425e1af5b2526d","modified":1543356951206},{"_id":"public/2018/05/13/CCAH-131-Install/index.html","hash":"47ddaf5f82dcf4993ec7fea788068544ecf6d408","modified":1543356951206},{"_id":"public/2018/05/13/test/index.html","hash":"949f8629ad06bbdef4496f6360bbc8f502d05c29","modified":1543356951206},{"_id":"public/2018/05/12/Hive架构相关/index.html","hash":"a189699e608596bdc76fde98f3f706917e4cd349","modified":1543356951206},{"_id":"public/2018/05/12/测试Hive权限级别/index.html","hash":"09aa0d4f2b9149934f466e6baf813bc51c138f4d","modified":1543356951206},{"_id":"public/2018/05/12/Sqoop-导入数据库到hive/index.html","hash":"66d0ccb8235e99319e3d8652b7bd37921cf14fc4","modified":1543356951207},{"_id":"public/2018/05/12/CCAH-131-Required-Skills/index.html","hash":"f7203bda51d13e2743adfccf3b25ff8f3a59e412","modified":1543356951207},{"_id":"public/2017/12/23/help/index.html","hash":"7be9e06626c81a930fc09f3be3a36aa6dbb2a181","modified":1543356951207},{"_id":"public/archives/index.html","hash":"098cc294f0513d7dc328b7d74f1c036c5890a1d6","modified":1543356951207},{"_id":"public/archives/page/2/index.html","hash":"d4607e82c56dcf77d70d8193753ec0104d6d1187","modified":1543356951208},{"_id":"public/archives/page/3/index.html","hash":"4b48fec97275018c055b3d028fa5d82585f2b080","modified":1543356951208},{"_id":"public/archives/page/4/index.html","hash":"a44de826de8d39c602114fc3b321bf28a9e6938e","modified":1543356951208},{"_id":"public/archives/page/5/index.html","hash":"705532760f82fd659e5f49acb622ebf9b950b8d4","modified":1543356951208},{"_id":"public/archives/page/6/index.html","hash":"59251f86aa56bad78c4368b4f0b7730e00e83ba9","modified":1543356951208},{"_id":"public/archives/2017/index.html","hash":"e511fd53fde04524f98db335f98db77a14ca5705","modified":1543356951208},{"_id":"public/archives/2017/12/index.html","hash":"998dd9eeba731896438c2f1c8581cd9cba59b86b","modified":1543356951208},{"_id":"public/archives/2018/index.html","hash":"1dfa8aecd2c58042a1d75185a40d937b5244ea73","modified":1543356951209},{"_id":"public/archives/2018/page/2/index.html","hash":"1376738a585e1693da0241ceff8c162b582974c9","modified":1543356951209},{"_id":"public/archives/2018/page/3/index.html","hash":"ee66b08c1dab06980b718ec86146bbd4b5009a72","modified":1543356951209},{"_id":"public/archives/2018/page/4/index.html","hash":"0c0a7b3e17aa75568b6773ad7c403daf422b3ec3","modified":1543356951209},{"_id":"public/archives/2018/page/5/index.html","hash":"48f8b26b6383a45a79b50285d1691173edb46697","modified":1543356951209},{"_id":"public/archives/2018/page/6/index.html","hash":"a8a90b5a413b40275802ffb07cb4c3db7f04a47f","modified":1543356951209},{"_id":"public/archives/2018/05/index.html","hash":"a566d6cd6d8b2f64580f67c40d1182f310476d86","modified":1543356951209},{"_id":"public/archives/2018/05/page/2/index.html","hash":"5846d7ecd58258e4ce600f1138378ee72f4ba462","modified":1543356951210},{"_id":"public/archives/2018/05/page/3/index.html","hash":"0602b1bfa2b282a6d135092b0062404edf1b2ce0","modified":1543356951210},{"_id":"public/archives/2018/05/page/4/index.html","hash":"5cfb173891badb0634b6d9ddfb63681a348d8018","modified":1543356951210},{"_id":"public/archives/2018/09/index.html","hash":"f8e97c072a66fbd2e20cbd96191165ddca413de4","modified":1543356951210},{"_id":"public/archives/2018/10/index.html","hash":"f44d82b232b68e22d2ff828e038c30df8ca313a1","modified":1543356951210},{"_id":"public/archives/2018/11/index.html","hash":"ff8bedcb4890a2ea6967950202ed0ff9397d946f","modified":1543356951210},{"_id":"public/categories/CDH安装/index.html","hash":"56052d5b292be140033bcb518468d63320f74740","modified":1543356951210},{"_id":"public/index.html","hash":"67ae829325dea9a43dd049e5cf8e463731891d50","modified":1543356951211},{"_id":"public/page/2/index.html","hash":"4f1efe183b84a9ef4bb5912b3b683a499bbc9125","modified":1543356951211},{"_id":"public/page/3/index.html","hash":"40e09e20d10165a97c5ac3fd1f5d8085c90d7e31","modified":1543356951211},{"_id":"public/tags/CCAH-131/index.html","hash":"33f816aeb16873b8fc33b8adfcf4408223117638","modified":1543356951211},{"_id":"public/tags/CCAH-131/page/2/index.html","hash":"de8a431aaf09c6f07a74a0ad281bb84cd0ddf993","modified":1543356951211},{"_id":"public/tags/CM/index.html","hash":"c925ca97e44cde1ace3d0b6f8da1139201c148ba","modified":1543356951212},{"_id":"public/tags/CDH安装/index.html","hash":"b54d5552892f265285c7af9c55f3acf304c8a69e","modified":1543356951212},{"_id":"public/tags/CDH/index.html","hash":"7f8442d8e992ff276611e7c378aca9aebe1db6d9","modified":1543356951212},{"_id":"public/tags/CDH/page/2/index.html","hash":"c7ed5dbb0819acd645a91b1f39d80831dde2c6ab","modified":1543356951212},{"_id":"public/tags/Hive/index.html","hash":"c16f02aad67854bf48be8387fdb8cc6e6cbeac94","modified":1543356951212},{"_id":"public/tags/Hive/page/2/index.html","hash":"134c22cd5d7ee442ff14a99da2c921a052213c0d","modified":1543356951212},{"_id":"public/tags/Kerberos/index.html","hash":"53cb59f1627e2c5f54d9d9225d60e5139f76dcb5","modified":1543356951213},{"_id":"public/tags/User/index.html","hash":"c3e1cb7e9fed56e16a803c53fce8ea0ac02d4ca0","modified":1543356951214},{"_id":"public/tags/Navigator/index.html","hash":"7d7ad10ba6879b1e9be7ce00b95924b70396aa7e","modified":1543356951214},{"_id":"public/tags/Udf/index.html","hash":"2eec9d3a0d65722e08f7be8dfbacab325d6811a3","modified":1543356951214},{"_id":"public/tags/metastore/index.html","hash":"fa80bc5011867706b2e96747a9935a8d9671d8b0","modified":1543356951214},{"_id":"public/tags/函数/index.html","hash":"cda887cd4c87058ca1c4cbf1919ec19ecf79e92f","modified":1543356951214},{"_id":"public/tags/Data-Type/index.html","hash":"e865dd2ce1e2f9e8ba12407a3efa327ccf2d34a4","modified":1543356951214},{"_id":"public/tags/HBase/index.html","hash":"bdd661e26abc5a2330a050ab07d9e569b4276621","modified":1543356951214},{"_id":"public/tags/CDH6/index.html","hash":"b3eb3321625f9f5d3c2996dfc9ef3a55a259769b","modified":1543356951214},{"_id":"public/tags/Parquet/index.html","hash":"5334b4b2035fb5df03ab0ad85fbe278c35bdf529","modified":1543356951214},{"_id":"public/tags/随笔/index.html","hash":"1e622277f71dce64639ab8727725e7fdc493fbe4","modified":1543356951215},{"_id":"public/tags/sqoop/index.html","hash":"c1a9671c5e756f1f6dcb8b7bd0e4f8347a7d2c65","modified":1543356951215},{"_id":"public/fonts/default-skin.b257fa.svg","hash":"2ac727c9e092331d35cce95af209ccfac6d4c7c7","modified":1543356951220},{"_id":"public/fonts/iconfont.16acc2.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1543356951220},{"_id":"public/fonts/iconfont.b322fa.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1543356951220},{"_id":"public/fonts/tooltip.4004ff.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1543356951220},{"_id":"public/fonts/iconfont.45d7ee.svg","hash":"f9304e5714d20861be7d8f4d36687e88e86b9e1b","modified":1543356951220},{"_id":"public/fonts/iconfont.8c627f.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1543356951220},{"_id":"public/img/preloader.gif","hash":"6342367c93c82da1b9c620e97c84a389cc43d96d","modified":1543356951220},{"_id":"public/img/default-skin.png","hash":"ed95a8e40a2c3478c5915376acb8e5f33677f24d","modified":1543356951220},{"_id":"public/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1543356951220},{"_id":"public/img/1539920764009.png","hash":"09a908ebe8562d14602b26a0e1821a63d3c197d9","modified":1543356951225},{"_id":"public/img/1540286924463.png","hash":"bc25418075240a1ed256a938fc6dcf22ae01ec79","modified":1543356951225},{"_id":"public/img/DF2947BB-B48F-4C4E-B27A-C259D57EDA86.jpg","hash":"6ecd61743a593b12c736cb49a1c9f549f3e59679","modified":1543356951226},{"_id":"public/assets/img/QR Code.jpeg","hash":"2df1d0ac1c37102aff3223bd06dd23de8d3406ca","modified":1543356951226},{"_id":"public/img/QR Code.jpeg","hash":"2df1d0ac1c37102aff3223bd06dd23de8d3406ca","modified":1543356951226},{"_id":"public/main.0cf68a.css","hash":"ddf6e2c6b953c2c59a3c271e6070010a4cc81cf9","modified":1543356951231},{"_id":"public/slider.e37972.js","hash":"6dec4e220c89049037eebc44404abd8455d22ad7","modified":1543356951231},{"_id":"public/mobile.992cbe.js","hash":"01b35e71e37aa2849664eb5daf26daede2278398","modified":1543356951231},{"_id":"public/main.0cf68a.js","hash":"993fadeb5f6d296e9d997a49ee20dc97333ceab7","modified":1543356951231},{"_id":"public/img/1540285662271.png","hash":"9f7a5f731d9a0d253af0a108bd11a46dc8c261fe","modified":1543356951231},{"_id":"public/img/1540285688689.png","hash":"86a8a6aa337eefc361887849690dcd8a6b281ec6","modified":1543356951231},{"_id":"public/img/1540285620938.png","hash":"640302a6555a526c413e14f9f74e4d602ec8fb29","modified":1543356951231},{"_id":"public/img/1540286887142.png","hash":"62185cd656c9d0836b07ccf68c68d8a763e8b2bc","modified":1543356951231},{"_id":"public/img/1540285723630.png","hash":"4f3b8f80af32e8add29890e88e68f3b7a91929a7","modified":1543356951231},{"_id":"public/img/1540286907954.png","hash":"dcdd931738195f69aaa1cf715d9ebd05c70e87ca","modified":1543356951232},{"_id":"public/img/1540287176183.png","hash":"c89d6a4e6810a6024c2ce74c8b17159b0091b3bd","modified":1543356951232},{"_id":"public/img/1539939416172.png","hash":"21ab7669e8e39a977f1e36872c9db4092e25c37b","modified":1543356951236},{"_id":"public/img/1539939380054.png","hash":"f5ff4d0dbb06db8318d99d44decde9b8e06e51c5","modified":1543356951237},{"_id":"public/img/1539939421998.png","hash":"aed47aaebd421b5eeffe771224b568bbdbb15535","modified":1543356951237},{"_id":"public/img/1539939461605.png","hash":"8a110c72e5cb9df0e1bf39b260916307bab3d8fd","modified":1543356951237},{"_id":"public/img/1539939371522.png","hash":"ef4c70bba5319b41bf228a575406e6dd6c4b46c6","modified":1543356951239},{"_id":"public/img/kerberos.png","hash":"6fdbb65aee458acd6b89ab57476d5ea0b200fb47","modified":1543356951243}],"Category":[{"name":"CDH安装","_id":"cjp0avnzy0017inam98kd8ein"}],"Data":[],"Page":[{"title":"CCAH131","date":"2018-05-13T01:46:48.000Z","_content":"","source":"CCAH131/index.md","raw":"---\ntitle: CCAH131\ndate: 2018-05-13 09:46:48\n---\n","updated":"2018-05-13T01:46:48.145Z","path":"CCAH131/index.html","comments":1,"layout":"page","_id":"cjp0avnz10000inam3nvhw0so","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"hive","date":"2018-05-13T01:42:57.000Z","_content":"","source":"hive/index.md","raw":"---\ntitle: hive\ndate: 2018-05-13 09:42:57\n---\n","updated":"2018-05-13T01:42:57.502Z","path":"hive/index.html","comments":1,"layout":"page","_id":"cjp0avnz50002inamfx8x96ye","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"CCAH-131 Configure","date":"2018-05-13T00:34:26.000Z","_content":"# Configure\n\n Perform basic and advanced configuration needed to effectively administer a Hadoop cluster\n##  Configure a service using Cloudera Manager\n\n**使用Cloudera Manager配置YARN服务**\n\nHOME -> CLusters -> YARN(MR2 Included) -> Configuration 到修改配置页面\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services.png)\n\n可以在搜索框中使用模糊搜索或者在左侧的 Filters中选择相应的属性进行过滤\n\n<!-- more -->\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%202.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%203.png)\n\n输入更多的条件，过滤到相要修改的属性。修改属性值并保存。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%204.png)\n\n保存后会有重启图标 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%205.png)\n\n或者在HOME页面，也可以看到哪些服务受到影响，并且需求重启。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%206.png)\n\n点击其中一个重启，会给出需要修改的配置文件里面修改的内容，红色背景代表去掉这行，绿色背景代表添加这行。点击 Restart Stale Servers进入下一步\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%207.png)\n\n勾选 Re-deploy client configuration 并重启\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%208.png)\n\n进入重启 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%209.png)\n\n重启完成\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%2010.png)\n##  Create an HDFS user's home directory\n\n```\n[training@elephant ~]$ sudo -u hdfs hdfs dfs -ls /user  ## 查看hdfs上 /user目录下当前的文件\nFound 2 items\ndrwxrwxrwx   - mapred hadoop          0 2018-01-10 16:29 /user/history\ndrwxr-x--x   - spark  spark           0 2018-01-10 16:36 /user/spark\n[training@elephant ~]$ sudo -u hdfs hdfs dfs -mkdir  /user/training  ## 创建training文件夹\n[training@elephant ~]$ sudo -u hdfs hdfs dfs -chown training /user/training ## 修改其权限\n[training@elephant ~]$ hdfs dfs -ls /user/training \n[training@elephant ~]$ hdfs dfs -ls /user/  ## 查看新创建的文件夹\nFound 3 items\ndrwxrwxrwx   - mapred   hadoop              0 2018-01-10 16:29 /user/history\ndrwxr-x--x   - spark    spark               0 2018-01-10 16:36 /user/spark\ndrwxr-xr-x   - training supergroup          0 2018-01-10 16:57 /user/training\n[training@elephant ~]$\n```\n##  Configure NameNode HA\n\nHOME -> Cluster -> HDFS -> Actions -> Enable High Availability 点击 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%201.png)\n\n填入namespace的名称\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%202.png)\n\n选择要添加的新的namenode所在的服务器，并添加奇数个Journal Nodes。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%203.png)\n\n给定Journal Edits所在的文件夹\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%204.png)\n\n进入配置重启阶段\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%205.png)\n\n因为HDFS在之前是初始化过的，此处初始化会失败。正常就是都会失败。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%206.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%207.png)\n\n配置成功后，会提示如果Hive中在HDFS启用HA之前有数据，则需要对Hive执行 Update Hive Metastore NameNodes\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%208.png)\n\n查看配置成功后的HDFS 服务， SecondryNamenode被删除，2个Namenode一个是Active 一个是Standby，另外有3个JournalNode\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%209.png)\n##  Configure ResourceManager HA\n\nHOME -> Cluster -> YARN(MR2 Included) -> Instances -> Add Role Instances\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA.png)\n\n选择添加一个Resource Manager\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%202.png)\n\n会有红色的配置警告，将Zookeepr勾选为 Zookeeper 服务。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%203.png)\n\n会返回到YARN的实例界面，有重启选项\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%204.png)\n\n选择重启，会弹出修改的配置文件内容 红色的为删除的内容，绿色的为添加的内容\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%205.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%206.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%207.png)\n\n选择要重启的服务，因为 Spark、Hive、Oozie、Hue都依赖于YARN，如果YARN配置了HA，这些依赖的服务也要重新更新配置。 所以勾选为需要重启的服务。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%208.png)\n\n重启服务\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%209.png)\n\n重启完成\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%2010.png)\n\n查看服务实例 有2个Resource Manager 一个是Active状态一个是Standby状态。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%2011.png)\n##  Configure proxy for Hiveserver2/Impala\n\nHOME -> Cluster -> Hive -> Configuration 搜索load balance\n\n填写load balance的服务器的域名\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance1.png)\n\n添加 高级配置 \n\n```\nName: hive.server2.support.dynamic.service.discovery\nValue: true\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance2.png)\n\n停掉一个 hiveserver2 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance3.png)\n\n使用如下代码，测试连接\n\n```\nbeeline -u \"jdbc:hive2://elephant:2181,horse:2181,tiger:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2\"\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance4.png)\n\n\n\n\n\n\n### Impala Proxy\n\n**安装Haproxy**\n\n[Example of Configuring HAProxy Load Balancer for Impala](https://www.cloudera.com/documentation/enterprise/latest/topics/impala_proxy.html#tut_proxy)\n\n参考：http://blog.csdn.net/lsb2002/article/details/53843340\n\t  http://blog.csdn.net/aa168b/article/details/50372649\n\t  \n下载：haproxy：http://www.haproxy.org/download/1.7/src/haproxy-1.7.1.tar.gz\n\n解压 \n\n```\ntar -xvf haproxy-1.7.1.tar\n```\n\n编译和安装\n\n```\n  sudo make TARGET=linux31 PREFIX=/usr/local/haproxy\n  sudo make install PREFIX=/usr/local/haproxy\n  cd /usr/local/haproxy/\n  sudo mkdir -p /usr/haproxy/\n  sudo vim /etc/haproxy/haproxy.cfg\n```\n将下面的内容添加到 /etc/haproxy/haproxy.cfg\n\n```\nglobal\n    # To have these messages end up in /var/log/haproxy.log you will\n    # need to:\n    #\n    # 1) configure syslog to accept network log events.  This is done\n    #    by adding the '-r' option to the SYSLOGD_OPTIONS in\n    #    /etc/sysconfig/syslog\n    #\n    # 2) configure local2 events to go to the /var/log/haproxy.log\n    #   file. A line like the following can be added to\n    #   /etc/sysconfig/syslog\n    #\n    #    local2.*                       /var/log/haproxy.log\n    #\n    log         127.0.0.1 local0\n    log         127.0.0.1 local1 notice\n    #chroot      /usr/local/haproxy\n    pidfile     /usr/local/haproxy/logs/haproxy.pid\n    maxconn     4000\n    #uid      501\n    #gid 501\n    daemon\n\n    # turn on stats unix socket\n    #stats socket /var/lib/haproxy/stats\n\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#\n# You might need to adjust timing values to prevent timeouts.\n#---------------------------------------------------------------------\ndefaults\n    mode                    http\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n    #option forwardfor       except 127.0.0.0/8\n    option                  redispatch\n    retries                 3\n    maxconn                 3000\n    timeout connect 5000\n    timeout check 20000\n    timeout client 50000\n    timeout server 50000\n\n#\n# This sets up the admin page for HA Proxy at port 25002.\n#\nlisten stats\n    bind 0.0.0.0:25002\n    balance\n    mode http\n    stats enable\n    stats auth admin:admin\n\n# This is the setup for Impala. Impala client connect to load_balancer_host:25003.\n# HAProxy will balance connections among the list of servers listed below.\n# The list of Impalad is listening at port 21000 for beeswax (impala-shell) or original ODBC driver.\n# For JDBC or ODBC version 2.x driver, use port 21050 instead of 21000.\nlisten impala\n    bind 0.0.0.0:25002\n    mode tcp\n    option tcplog\n    balance leastconn\n\n    server symbolic_name_1 elephant:21000\n    server symbolic_name_2 tiger:21000\n    server symbolic_name_3 monkey:21000\n    server symbolic_name_4 horse:21000\n\n# Setup for Hue or other JDBC-enabled applications.\n# In particular, Hue requires sticky sessions.\n# The application connects to load_balancer_host:21051, and HAProxy balances\n# connections to the associated hosts, where Impala listens for JDBC\n# requests on port 21050.\nlisten impalajdbc\n    bind 0.0.0.0:21051\n    mode tcp\n    option tcplog\n    balance source\n    server symbolic_name_5 elephant:21050\n    server symbolic_name_6 tiger:21050\n    server symbolic_name_7 monkey:21050\n    server symbolic_name_8 horse:21050\n```\n\n启动 haproxy\n\n```\nsudo /usr/local/haproxy/sbin/haproxy -f /etc/haproxy/haproxy.cfg\n```\n\n\nimpala 访问\n\n```\n[training@elephant ~]$ beeline -d \"com.cloudera.impala.jdbc41.Driver\" -u \"jdbc:impala://elephant:21051\"\n2018-01-12 14:35:57,602 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.\nConnecting to jdbc:impala://elephant:21051\ncom.cloudera.impala.jdbc41.Driver\nBeeline version 1.1.0-cdh5.9.0 by Apache Hive\n0: jdbc:impala://elephant:21051 (closed)>\n```\n\n停掉 elephant上的impala daemon，再次测试\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/ImpalaLoadBalance1.png)\n\n```\n[training@elephant ~]$ beeline -d \"com.cloudera.impala.jdbc41.Driver\" -u \"jdbc:impala://elephant:21051\"\n2018-01-12 14:35:57,602 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.\nConnecting to jdbc:impala://elephant:21051\ncom.cloudera.impala.jdbc41.Driver\nBeeline version 1.1.0-cdh5.9.0 by Apache Hive\n0: jdbc:impala://elephant:21051 (closed)>\n```\n","source":"_posts/CCAH-131-Configure.md","raw":"---\ntitle: CCAH-131 Configure\ndate: 2018-05-13 08:34:26\ntags: CCAH-131\n---\n# Configure\n\n Perform basic and advanced configuration needed to effectively administer a Hadoop cluster\n##  Configure a service using Cloudera Manager\n\n**使用Cloudera Manager配置YARN服务**\n\nHOME -> CLusters -> YARN(MR2 Included) -> Configuration 到修改配置页面\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services.png)\n\n可以在搜索框中使用模糊搜索或者在左侧的 Filters中选择相应的属性进行过滤\n\n<!-- more -->\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%202.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%203.png)\n\n输入更多的条件，过滤到相要修改的属性。修改属性值并保存。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%204.png)\n\n保存后会有重启图标 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%205.png)\n\n或者在HOME页面，也可以看到哪些服务受到影响，并且需求重启。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%206.png)\n\n点击其中一个重启，会给出需要修改的配置文件里面修改的内容，红色背景代表去掉这行，绿色背景代表添加这行。点击 Restart Stale Servers进入下一步\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%207.png)\n\n勾选 Re-deploy client configuration 并重启\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%208.png)\n\n进入重启 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%209.png)\n\n重启完成\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%2010.png)\n##  Create an HDFS user's home directory\n\n```\n[training@elephant ~]$ sudo -u hdfs hdfs dfs -ls /user  ## 查看hdfs上 /user目录下当前的文件\nFound 2 items\ndrwxrwxrwx   - mapred hadoop          0 2018-01-10 16:29 /user/history\ndrwxr-x--x   - spark  spark           0 2018-01-10 16:36 /user/spark\n[training@elephant ~]$ sudo -u hdfs hdfs dfs -mkdir  /user/training  ## 创建training文件夹\n[training@elephant ~]$ sudo -u hdfs hdfs dfs -chown training /user/training ## 修改其权限\n[training@elephant ~]$ hdfs dfs -ls /user/training \n[training@elephant ~]$ hdfs dfs -ls /user/  ## 查看新创建的文件夹\nFound 3 items\ndrwxrwxrwx   - mapred   hadoop              0 2018-01-10 16:29 /user/history\ndrwxr-x--x   - spark    spark               0 2018-01-10 16:36 /user/spark\ndrwxr-xr-x   - training supergroup          0 2018-01-10 16:57 /user/training\n[training@elephant ~]$\n```\n##  Configure NameNode HA\n\nHOME -> Cluster -> HDFS -> Actions -> Enable High Availability 点击 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%201.png)\n\n填入namespace的名称\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%202.png)\n\n选择要添加的新的namenode所在的服务器，并添加奇数个Journal Nodes。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%203.png)\n\n给定Journal Edits所在的文件夹\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%204.png)\n\n进入配置重启阶段\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%205.png)\n\n因为HDFS在之前是初始化过的，此处初始化会失败。正常就是都会失败。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%206.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%207.png)\n\n配置成功后，会提示如果Hive中在HDFS启用HA之前有数据，则需要对Hive执行 Update Hive Metastore NameNodes\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%208.png)\n\n查看配置成功后的HDFS 服务， SecondryNamenode被删除，2个Namenode一个是Active 一个是Standby，另外有3个JournalNode\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%209.png)\n##  Configure ResourceManager HA\n\nHOME -> Cluster -> YARN(MR2 Included) -> Instances -> Add Role Instances\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA.png)\n\n选择添加一个Resource Manager\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%202.png)\n\n会有红色的配置警告，将Zookeepr勾选为 Zookeeper 服务。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%203.png)\n\n会返回到YARN的实例界面，有重启选项\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%204.png)\n\n选择重启，会弹出修改的配置文件内容 红色的为删除的内容，绿色的为添加的内容\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%205.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%206.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%207.png)\n\n选择要重启的服务，因为 Spark、Hive、Oozie、Hue都依赖于YARN，如果YARN配置了HA，这些依赖的服务也要重新更新配置。 所以勾选为需要重启的服务。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%208.png)\n\n重启服务\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%209.png)\n\n重启完成\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%2010.png)\n\n查看服务实例 有2个Resource Manager 一个是Active状态一个是Standby状态。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%2011.png)\n##  Configure proxy for Hiveserver2/Impala\n\nHOME -> Cluster -> Hive -> Configuration 搜索load balance\n\n填写load balance的服务器的域名\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance1.png)\n\n添加 高级配置 \n\n```\nName: hive.server2.support.dynamic.service.discovery\nValue: true\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance2.png)\n\n停掉一个 hiveserver2 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance3.png)\n\n使用如下代码，测试连接\n\n```\nbeeline -u \"jdbc:hive2://elephant:2181,horse:2181,tiger:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2\"\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance4.png)\n\n\n\n\n\n\n### Impala Proxy\n\n**安装Haproxy**\n\n[Example of Configuring HAProxy Load Balancer for Impala](https://www.cloudera.com/documentation/enterprise/latest/topics/impala_proxy.html#tut_proxy)\n\n参考：http://blog.csdn.net/lsb2002/article/details/53843340\n\t  http://blog.csdn.net/aa168b/article/details/50372649\n\t  \n下载：haproxy：http://www.haproxy.org/download/1.7/src/haproxy-1.7.1.tar.gz\n\n解压 \n\n```\ntar -xvf haproxy-1.7.1.tar\n```\n\n编译和安装\n\n```\n  sudo make TARGET=linux31 PREFIX=/usr/local/haproxy\n  sudo make install PREFIX=/usr/local/haproxy\n  cd /usr/local/haproxy/\n  sudo mkdir -p /usr/haproxy/\n  sudo vim /etc/haproxy/haproxy.cfg\n```\n将下面的内容添加到 /etc/haproxy/haproxy.cfg\n\n```\nglobal\n    # To have these messages end up in /var/log/haproxy.log you will\n    # need to:\n    #\n    # 1) configure syslog to accept network log events.  This is done\n    #    by adding the '-r' option to the SYSLOGD_OPTIONS in\n    #    /etc/sysconfig/syslog\n    #\n    # 2) configure local2 events to go to the /var/log/haproxy.log\n    #   file. A line like the following can be added to\n    #   /etc/sysconfig/syslog\n    #\n    #    local2.*                       /var/log/haproxy.log\n    #\n    log         127.0.0.1 local0\n    log         127.0.0.1 local1 notice\n    #chroot      /usr/local/haproxy\n    pidfile     /usr/local/haproxy/logs/haproxy.pid\n    maxconn     4000\n    #uid      501\n    #gid 501\n    daemon\n\n    # turn on stats unix socket\n    #stats socket /var/lib/haproxy/stats\n\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#\n# You might need to adjust timing values to prevent timeouts.\n#---------------------------------------------------------------------\ndefaults\n    mode                    http\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n    #option forwardfor       except 127.0.0.0/8\n    option                  redispatch\n    retries                 3\n    maxconn                 3000\n    timeout connect 5000\n    timeout check 20000\n    timeout client 50000\n    timeout server 50000\n\n#\n# This sets up the admin page for HA Proxy at port 25002.\n#\nlisten stats\n    bind 0.0.0.0:25002\n    balance\n    mode http\n    stats enable\n    stats auth admin:admin\n\n# This is the setup for Impala. Impala client connect to load_balancer_host:25003.\n# HAProxy will balance connections among the list of servers listed below.\n# The list of Impalad is listening at port 21000 for beeswax (impala-shell) or original ODBC driver.\n# For JDBC or ODBC version 2.x driver, use port 21050 instead of 21000.\nlisten impala\n    bind 0.0.0.0:25002\n    mode tcp\n    option tcplog\n    balance leastconn\n\n    server symbolic_name_1 elephant:21000\n    server symbolic_name_2 tiger:21000\n    server symbolic_name_3 monkey:21000\n    server symbolic_name_4 horse:21000\n\n# Setup for Hue or other JDBC-enabled applications.\n# In particular, Hue requires sticky sessions.\n# The application connects to load_balancer_host:21051, and HAProxy balances\n# connections to the associated hosts, where Impala listens for JDBC\n# requests on port 21050.\nlisten impalajdbc\n    bind 0.0.0.0:21051\n    mode tcp\n    option tcplog\n    balance source\n    server symbolic_name_5 elephant:21050\n    server symbolic_name_6 tiger:21050\n    server symbolic_name_7 monkey:21050\n    server symbolic_name_8 horse:21050\n```\n\n启动 haproxy\n\n```\nsudo /usr/local/haproxy/sbin/haproxy -f /etc/haproxy/haproxy.cfg\n```\n\n\nimpala 访问\n\n```\n[training@elephant ~]$ beeline -d \"com.cloudera.impala.jdbc41.Driver\" -u \"jdbc:impala://elephant:21051\"\n2018-01-12 14:35:57,602 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.\nConnecting to jdbc:impala://elephant:21051\ncom.cloudera.impala.jdbc41.Driver\nBeeline version 1.1.0-cdh5.9.0 by Apache Hive\n0: jdbc:impala://elephant:21051 (closed)>\n```\n\n停掉 elephant上的impala daemon，再次测试\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/ImpalaLoadBalance1.png)\n\n```\n[training@elephant ~]$ beeline -d \"com.cloudera.impala.jdbc41.Driver\" -u \"jdbc:impala://elephant:21051\"\n2018-01-12 14:35:57,602 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.\nConnecting to jdbc:impala://elephant:21051\ncom.cloudera.impala.jdbc41.Driver\nBeeline version 1.1.0-cdh5.9.0 by Apache Hive\n0: jdbc:impala://elephant:21051 (closed)>\n```\n","slug":"CCAH-131-Configure","published":1,"updated":"2018-09-14T01:18:53.799Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnz20001inamtbjhbohr","content":"<h1 id=\"Configure\"><a href=\"#Configure\" class=\"headerlink\" title=\"Configure\"></a>Configure</h1><p> Perform basic and advanced configuration needed to effectively administer a Hadoop cluster</p>\n<h2 id=\"Configure-a-service-using-Cloudera-Manager\"><a href=\"#Configure-a-service-using-Cloudera-Manager\" class=\"headerlink\" title=\"Configure a service using Cloudera Manager\"></a>Configure a service using Cloudera Manager</h2><p><strong>使用Cloudera Manager配置YARN服务</strong></p>\n<p>HOME -&gt; CLusters -&gt; YARN(MR2 Included) -&gt; Configuration 到修改配置页面</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services.png\" alt=\"\"></p>\n<p>可以在搜索框中使用模糊搜索或者在左侧的 Filters中选择相应的属性进行过滤</p>\n<a id=\"more\"></a>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%202.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%203.png\" alt=\"\"></p>\n<p>输入更多的条件，过滤到相要修改的属性。修改属性值并保存。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%204.png\" alt=\"\"></p>\n<p>保存后会有重启图标 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%205.png\" alt=\"\"></p>\n<p>或者在HOME页面，也可以看到哪些服务受到影响，并且需求重启。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%206.png\" alt=\"\"></p>\n<p>点击其中一个重启，会给出需要修改的配置文件里面修改的内容，红色背景代表去掉这行，绿色背景代表添加这行。点击 Restart Stale Servers进入下一步</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%207.png\" alt=\"\"></p>\n<p>勾选 Re-deploy client configuration 并重启</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%208.png\" alt=\"\"></p>\n<p>进入重启 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%209.png\" alt=\"\"></p>\n<p>重启完成</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%2010.png\" alt=\"\"></p>\n<h2 id=\"Create-an-HDFS-user’s-home-directory\"><a href=\"#Create-an-HDFS-user’s-home-directory\" class=\"headerlink\" title=\"Create an HDFS user’s home directory\"></a>Create an HDFS user’s home directory</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ sudo -u hdfs hdfs dfs -ls /user  ## 查看hdfs上 /user目录下当前的文件</span><br><span class=\"line\">Found 2 items</span><br><span class=\"line\">drwxrwxrwx   - mapred hadoop          0 2018-01-10 16:29 /user/history</span><br><span class=\"line\">drwxr-x--x   - spark  spark           0 2018-01-10 16:36 /user/spark</span><br><span class=\"line\">[training@elephant ~]$ sudo -u hdfs hdfs dfs -mkdir  /user/training  ## 创建training文件夹</span><br><span class=\"line\">[training@elephant ~]$ sudo -u hdfs hdfs dfs -chown training /user/training ## 修改其权限</span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -ls /user/training </span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -ls /user/  ## 查看新创建的文件夹</span><br><span class=\"line\">Found 3 items</span><br><span class=\"line\">drwxrwxrwx   - mapred   hadoop              0 2018-01-10 16:29 /user/history</span><br><span class=\"line\">drwxr-x--x   - spark    spark               0 2018-01-10 16:36 /user/spark</span><br><span class=\"line\">drwxr-xr-x   - training supergroup          0 2018-01-10 16:57 /user/training</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<h2 id=\"Configure-NameNode-HA\"><a href=\"#Configure-NameNode-HA\" class=\"headerlink\" title=\"Configure NameNode HA\"></a>Configure NameNode HA</h2><p>HOME -&gt; Cluster -&gt; HDFS -&gt; Actions -&gt; Enable High Availability 点击 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%201.png\" alt=\"\"></p>\n<p>填入namespace的名称</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%202.png\" alt=\"\"></p>\n<p>选择要添加的新的namenode所在的服务器，并添加奇数个Journal Nodes。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%203.png\" alt=\"\"></p>\n<p>给定Journal Edits所在的文件夹</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%204.png\" alt=\"\"></p>\n<p>进入配置重启阶段</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%205.png\" alt=\"\"></p>\n<p>因为HDFS在之前是初始化过的，此处初始化会失败。正常就是都会失败。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%206.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%207.png\" alt=\"\"></p>\n<p>配置成功后，会提示如果Hive中在HDFS启用HA之前有数据，则需要对Hive执行 Update Hive Metastore NameNodes</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%208.png\" alt=\"\"></p>\n<p>查看配置成功后的HDFS 服务， SecondryNamenode被删除，2个Namenode一个是Active 一个是Standby，另外有3个JournalNode</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%209.png\" alt=\"\"></p>\n<h2 id=\"Configure-ResourceManager-HA\"><a href=\"#Configure-ResourceManager-HA\" class=\"headerlink\" title=\"Configure ResourceManager HA\"></a>Configure ResourceManager HA</h2><p>HOME -&gt; Cluster -&gt; YARN(MR2 Included) -&gt; Instances -&gt; Add Role Instances</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA.png\" alt=\"\"></p>\n<p>选择添加一个Resource Manager</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%202.png\" alt=\"\"></p>\n<p>会有红色的配置警告，将Zookeepr勾选为 Zookeeper 服务。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%203.png\" alt=\"\"></p>\n<p>会返回到YARN的实例界面，有重启选项</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%204.png\" alt=\"\"></p>\n<p>选择重启，会弹出修改的配置文件内容 红色的为删除的内容，绿色的为添加的内容</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%205.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%206.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%207.png\" alt=\"\"></p>\n<p>选择要重启的服务，因为 Spark、Hive、Oozie、Hue都依赖于YARN，如果YARN配置了HA，这些依赖的服务也要重新更新配置。 所以勾选为需要重启的服务。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%208.png\" alt=\"\"></p>\n<p>重启服务</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%209.png\" alt=\"\"></p>\n<p>重启完成</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%2010.png\" alt=\"\"></p>\n<p>查看服务实例 有2个Resource Manager 一个是Active状态一个是Standby状态。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%2011.png\" alt=\"\"></p>\n<h2 id=\"Configure-proxy-for-Hiveserver2-Impala\"><a href=\"#Configure-proxy-for-Hiveserver2-Impala\" class=\"headerlink\" title=\"Configure proxy for Hiveserver2/Impala\"></a>Configure proxy for Hiveserver2/Impala</h2><p>HOME -&gt; Cluster -&gt; Hive -&gt; Configuration 搜索load balance</p>\n<p>填写load balance的服务器的域名</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance1.png\" alt=\"\"></p>\n<p>添加 高级配置 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Name: hive.server2.support.dynamic.service.discovery</span><br><span class=\"line\">Value: true</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance2.png\" alt=\"\"></p>\n<p>停掉一个 hiveserver2 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance3.png\" alt=\"\"></p>\n<p>使用如下代码，测试连接</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">beeline -u &quot;jdbc:hive2://elephant:2181,horse:2181,tiger:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2&quot;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance4.png\" alt=\"\"></p>\n<h3 id=\"Impala-Proxy\"><a href=\"#Impala-Proxy\" class=\"headerlink\" title=\"Impala Proxy\"></a>Impala Proxy</h3><p><strong>安装Haproxy</strong></p>\n<p><a href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/impala_proxy.html#tut_proxy\" target=\"_blank\" rel=\"noopener\">Example of Configuring HAProxy Load Balancer for Impala</a></p>\n<p>参考：<a href=\"http://blog.csdn.net/lsb2002/article/details/53843340\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/lsb2002/article/details/53843340</a><br>      <a href=\"http://blog.csdn.net/aa168b/article/details/50372649\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/aa168b/article/details/50372649</a></p>\n<p>下载：haproxy：<a href=\"http://www.haproxy.org/download/1.7/src/haproxy-1.7.1.tar.gz\" target=\"_blank\" rel=\"noopener\">http://www.haproxy.org/download/1.7/src/haproxy-1.7.1.tar.gz</a></p>\n<p>解压 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -xvf haproxy-1.7.1.tar</span><br></pre></td></tr></table></figure>\n<p>编译和安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo make TARGET=linux31 PREFIX=/usr/local/haproxy</span><br><span class=\"line\">sudo make install PREFIX=/usr/local/haproxy</span><br><span class=\"line\">cd /usr/local/haproxy/</span><br><span class=\"line\">sudo mkdir -p /usr/haproxy/</span><br><span class=\"line\">sudo vim /etc/haproxy/haproxy.cfg</span><br></pre></td></tr></table></figure>\n<p>将下面的内容添加到 /etc/haproxy/haproxy.cfg</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">global</span><br><span class=\"line\">    # To have these messages end up in /var/log/haproxy.log you will</span><br><span class=\"line\">    # need to:</span><br><span class=\"line\">    #</span><br><span class=\"line\">    # 1) configure syslog to accept network log events.  This is done</span><br><span class=\"line\">    #    by adding the &apos;-r&apos; option to the SYSLOGD_OPTIONS in</span><br><span class=\"line\">    #    /etc/sysconfig/syslog</span><br><span class=\"line\">    #</span><br><span class=\"line\">    # 2) configure local2 events to go to the /var/log/haproxy.log</span><br><span class=\"line\">    #   file. A line like the following can be added to</span><br><span class=\"line\">    #   /etc/sysconfig/syslog</span><br><span class=\"line\">    #</span><br><span class=\"line\">    #    local2.*                       /var/log/haproxy.log</span><br><span class=\"line\">    #</span><br><span class=\"line\">    log         127.0.0.1 local0</span><br><span class=\"line\">    log         127.0.0.1 local1 notice</span><br><span class=\"line\">    #chroot      /usr/local/haproxy</span><br><span class=\"line\">    pidfile     /usr/local/haproxy/logs/haproxy.pid</span><br><span class=\"line\">    maxconn     4000</span><br><span class=\"line\">    #uid      501</span><br><span class=\"line\">    #gid 501</span><br><span class=\"line\">    daemon</span><br><span class=\"line\"></span><br><span class=\"line\">    # turn on stats unix socket</span><br><span class=\"line\">    #stats socket /var/lib/haproxy/stats</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># common defaults that all the &apos;listen&apos; and &apos;backend&apos; sections will</span><br><span class=\"line\"># use if not designated in their block</span><br><span class=\"line\">#</span><br><span class=\"line\"># You might need to adjust timing values to prevent timeouts.</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\">defaults</span><br><span class=\"line\">    mode                    http</span><br><span class=\"line\">    log                     global</span><br><span class=\"line\">    option                  httplog</span><br><span class=\"line\">    option                  dontlognull</span><br><span class=\"line\">    option http-server-close</span><br><span class=\"line\">    #option forwardfor       except 127.0.0.0/8</span><br><span class=\"line\">    option                  redispatch</span><br><span class=\"line\">    retries                 3</span><br><span class=\"line\">    maxconn                 3000</span><br><span class=\"line\">    timeout connect 5000</span><br><span class=\"line\">    timeout check 20000</span><br><span class=\"line\">    timeout client 50000</span><br><span class=\"line\">    timeout server 50000</span><br><span class=\"line\"></span><br><span class=\"line\">#</span><br><span class=\"line\"># This sets up the admin page for HA Proxy at port 25002.</span><br><span class=\"line\">#</span><br><span class=\"line\">listen stats</span><br><span class=\"line\">    bind 0.0.0.0:25002</span><br><span class=\"line\">    balance</span><br><span class=\"line\">    mode http</span><br><span class=\"line\">    stats enable</span><br><span class=\"line\">    stats auth admin:admin</span><br><span class=\"line\"></span><br><span class=\"line\"># This is the setup for Impala. Impala client connect to load_balancer_host:25003.</span><br><span class=\"line\"># HAProxy will balance connections among the list of servers listed below.</span><br><span class=\"line\"># The list of Impalad is listening at port 21000 for beeswax (impala-shell) or original ODBC driver.</span><br><span class=\"line\"># For JDBC or ODBC version 2.x driver, use port 21050 instead of 21000.</span><br><span class=\"line\">listen impala</span><br><span class=\"line\">    bind 0.0.0.0:25002</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    balance leastconn</span><br><span class=\"line\"></span><br><span class=\"line\">    server symbolic_name_1 elephant:21000</span><br><span class=\"line\">    server symbolic_name_2 tiger:21000</span><br><span class=\"line\">    server symbolic_name_3 monkey:21000</span><br><span class=\"line\">    server symbolic_name_4 horse:21000</span><br><span class=\"line\"></span><br><span class=\"line\"># Setup for Hue or other JDBC-enabled applications.</span><br><span class=\"line\"># In particular, Hue requires sticky sessions.</span><br><span class=\"line\"># The application connects to load_balancer_host:21051, and HAProxy balances</span><br><span class=\"line\"># connections to the associated hosts, where Impala listens for JDBC</span><br><span class=\"line\"># requests on port 21050.</span><br><span class=\"line\">listen impalajdbc</span><br><span class=\"line\">    bind 0.0.0.0:21051</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    balance source</span><br><span class=\"line\">    server symbolic_name_5 elephant:21050</span><br><span class=\"line\">    server symbolic_name_6 tiger:21050</span><br><span class=\"line\">    server symbolic_name_7 monkey:21050</span><br><span class=\"line\">    server symbolic_name_8 horse:21050</span><br></pre></td></tr></table></figure>\n<p>启动 haproxy</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo /usr/local/haproxy/sbin/haproxy -f /etc/haproxy/haproxy.cfg</span><br></pre></td></tr></table></figure>\n<p>impala 访问</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ beeline -d &quot;com.cloudera.impala.jdbc41.Driver&quot; -u &quot;jdbc:impala://elephant:21051&quot;</span><br><span class=\"line\">2018-01-12 14:35:57,602 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.</span><br><span class=\"line\">Connecting to jdbc:impala://elephant:21051</span><br><span class=\"line\">com.cloudera.impala.jdbc41.Driver</span><br><span class=\"line\">Beeline version 1.1.0-cdh5.9.0 by Apache Hive</span><br><span class=\"line\">0: jdbc:impala://elephant:21051 (closed)&gt;</span><br></pre></td></tr></table></figure>\n<p>停掉 elephant上的impala daemon，再次测试</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/ImpalaLoadBalance1.png\" alt=\"\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ beeline -d &quot;com.cloudera.impala.jdbc41.Driver&quot; -u &quot;jdbc:impala://elephant:21051&quot;</span><br><span class=\"line\">2018-01-12 14:35:57,602 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.</span><br><span class=\"line\">Connecting to jdbc:impala://elephant:21051</span><br><span class=\"line\">com.cloudera.impala.jdbc41.Driver</span><br><span class=\"line\">Beeline version 1.1.0-cdh5.9.0 by Apache Hive</span><br><span class=\"line\">0: jdbc:impala://elephant:21051 (closed)&gt;</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<h1 id=\"Configure\"><a href=\"#Configure\" class=\"headerlink\" title=\"Configure\"></a>Configure</h1><p> Perform basic and advanced configuration needed to effectively administer a Hadoop cluster</p>\n<h2 id=\"Configure-a-service-using-Cloudera-Manager\"><a href=\"#Configure-a-service-using-Cloudera-Manager\" class=\"headerlink\" title=\"Configure a service using Cloudera Manager\"></a>Configure a service using Cloudera Manager</h2><p><strong>使用Cloudera Manager配置YARN服务</strong></p>\n<p>HOME -&gt; CLusters -&gt; YARN(MR2 Included) -&gt; Configuration 到修改配置页面</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services.png\" alt=\"\"></p>\n<p>可以在搜索框中使用模糊搜索或者在左侧的 Filters中选择相应的属性进行过滤</p>","more":"<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%202.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%203.png\" alt=\"\"></p>\n<p>输入更多的条件，过滤到相要修改的属性。修改属性值并保存。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%204.png\" alt=\"\"></p>\n<p>保存后会有重启图标 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%205.png\" alt=\"\"></p>\n<p>或者在HOME页面，也可以看到哪些服务受到影响，并且需求重启。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%206.png\" alt=\"\"></p>\n<p>点击其中一个重启，会给出需要修改的配置文件里面修改的内容，红色背景代表去掉这行，绿色背景代表添加这行。点击 Restart Stale Servers进入下一步</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%207.png\" alt=\"\"></p>\n<p>勾选 Re-deploy client configuration 并重启</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%208.png\" alt=\"\"></p>\n<p>进入重启 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%209.png\" alt=\"\"></p>\n<p>重启完成</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Configuration%20Services/configuration%20yarn%20services%2010.png\" alt=\"\"></p>\n<h2 id=\"Create-an-HDFS-user’s-home-directory\"><a href=\"#Create-an-HDFS-user’s-home-directory\" class=\"headerlink\" title=\"Create an HDFS user’s home directory\"></a>Create an HDFS user’s home directory</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ sudo -u hdfs hdfs dfs -ls /user  ## 查看hdfs上 /user目录下当前的文件</span><br><span class=\"line\">Found 2 items</span><br><span class=\"line\">drwxrwxrwx   - mapred hadoop          0 2018-01-10 16:29 /user/history</span><br><span class=\"line\">drwxr-x--x   - spark  spark           0 2018-01-10 16:36 /user/spark</span><br><span class=\"line\">[training@elephant ~]$ sudo -u hdfs hdfs dfs -mkdir  /user/training  ## 创建training文件夹</span><br><span class=\"line\">[training@elephant ~]$ sudo -u hdfs hdfs dfs -chown training /user/training ## 修改其权限</span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -ls /user/training </span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -ls /user/  ## 查看新创建的文件夹</span><br><span class=\"line\">Found 3 items</span><br><span class=\"line\">drwxrwxrwx   - mapred   hadoop              0 2018-01-10 16:29 /user/history</span><br><span class=\"line\">drwxr-x--x   - spark    spark               0 2018-01-10 16:36 /user/spark</span><br><span class=\"line\">drwxr-xr-x   - training supergroup          0 2018-01-10 16:57 /user/training</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<h2 id=\"Configure-NameNode-HA\"><a href=\"#Configure-NameNode-HA\" class=\"headerlink\" title=\"Configure NameNode HA\"></a>Configure NameNode HA</h2><p>HOME -&gt; Cluster -&gt; HDFS -&gt; Actions -&gt; Enable High Availability 点击 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%201.png\" alt=\"\"></p>\n<p>填入namespace的名称</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%202.png\" alt=\"\"></p>\n<p>选择要添加的新的namenode所在的服务器，并添加奇数个Journal Nodes。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%203.png\" alt=\"\"></p>\n<p>给定Journal Edits所在的文件夹</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%204.png\" alt=\"\"></p>\n<p>进入配置重启阶段</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%205.png\" alt=\"\"></p>\n<p>因为HDFS在之前是初始化过的，此处初始化会失败。正常就是都会失败。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%206.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%207.png\" alt=\"\"></p>\n<p>配置成功后，会提示如果Hive中在HDFS启用HA之前有数据，则需要对Hive执行 Update Hive Metastore NameNodes</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%208.png\" alt=\"\"></p>\n<p>查看配置成功后的HDFS 服务， SecondryNamenode被删除，2个Namenode一个是Active 一个是Standby，另外有3个JournalNode</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/HDFS%20HA/HDFS%20HA%209.png\" alt=\"\"></p>\n<h2 id=\"Configure-ResourceManager-HA\"><a href=\"#Configure-ResourceManager-HA\" class=\"headerlink\" title=\"Configure ResourceManager HA\"></a>Configure ResourceManager HA</h2><p>HOME -&gt; Cluster -&gt; YARN(MR2 Included) -&gt; Instances -&gt; Add Role Instances</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA.png\" alt=\"\"></p>\n<p>选择添加一个Resource Manager</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%202.png\" alt=\"\"></p>\n<p>会有红色的配置警告，将Zookeepr勾选为 Zookeeper 服务。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%203.png\" alt=\"\"></p>\n<p>会返回到YARN的实例界面，有重启选项</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%204.png\" alt=\"\"></p>\n<p>选择重启，会弹出修改的配置文件内容 红色的为删除的内容，绿色的为添加的内容</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%205.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%206.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%207.png\" alt=\"\"></p>\n<p>选择要重启的服务，因为 Spark、Hive、Oozie、Hue都依赖于YARN，如果YARN配置了HA，这些依赖的服务也要重新更新配置。 所以勾选为需要重启的服务。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%208.png\" alt=\"\"></p>\n<p>重启服务</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%209.png\" alt=\"\"></p>\n<p>重启完成</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%2010.png\" alt=\"\"></p>\n<p>查看服务实例 有2个Resource Manager 一个是Active状态一个是Standby状态。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HA/Resource%20Manager%20HA/Resource%20Manager%20HA%2011.png\" alt=\"\"></p>\n<h2 id=\"Configure-proxy-for-Hiveserver2-Impala\"><a href=\"#Configure-proxy-for-Hiveserver2-Impala\" class=\"headerlink\" title=\"Configure proxy for Hiveserver2/Impala\"></a>Configure proxy for Hiveserver2/Impala</h2><p>HOME -&gt; Cluster -&gt; Hive -&gt; Configuration 搜索load balance</p>\n<p>填写load balance的服务器的域名</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance1.png\" alt=\"\"></p>\n<p>添加 高级配置 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Name: hive.server2.support.dynamic.service.discovery</span><br><span class=\"line\">Value: true</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance2.png\" alt=\"\"></p>\n<p>停掉一个 hiveserver2 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance3.png\" alt=\"\"></p>\n<p>使用如下代码，测试连接</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">beeline -u &quot;jdbc:hive2://elephant:2181,horse:2181,tiger:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2&quot;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/HiveLoadBalance4.png\" alt=\"\"></p>\n<h3 id=\"Impala-Proxy\"><a href=\"#Impala-Proxy\" class=\"headerlink\" title=\"Impala Proxy\"></a>Impala Proxy</h3><p><strong>安装Haproxy</strong></p>\n<p><a href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/impala_proxy.html#tut_proxy\" target=\"_blank\" rel=\"noopener\">Example of Configuring HAProxy Load Balancer for Impala</a></p>\n<p>参考：<a href=\"http://blog.csdn.net/lsb2002/article/details/53843340\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/lsb2002/article/details/53843340</a><br>      <a href=\"http://blog.csdn.net/aa168b/article/details/50372649\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/aa168b/article/details/50372649</a></p>\n<p>下载：haproxy：<a href=\"http://www.haproxy.org/download/1.7/src/haproxy-1.7.1.tar.gz\" target=\"_blank\" rel=\"noopener\">http://www.haproxy.org/download/1.7/src/haproxy-1.7.1.tar.gz</a></p>\n<p>解压 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -xvf haproxy-1.7.1.tar</span><br></pre></td></tr></table></figure>\n<p>编译和安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo make TARGET=linux31 PREFIX=/usr/local/haproxy</span><br><span class=\"line\">sudo make install PREFIX=/usr/local/haproxy</span><br><span class=\"line\">cd /usr/local/haproxy/</span><br><span class=\"line\">sudo mkdir -p /usr/haproxy/</span><br><span class=\"line\">sudo vim /etc/haproxy/haproxy.cfg</span><br></pre></td></tr></table></figure>\n<p>将下面的内容添加到 /etc/haproxy/haproxy.cfg</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">global</span><br><span class=\"line\">    # To have these messages end up in /var/log/haproxy.log you will</span><br><span class=\"line\">    # need to:</span><br><span class=\"line\">    #</span><br><span class=\"line\">    # 1) configure syslog to accept network log events.  This is done</span><br><span class=\"line\">    #    by adding the &apos;-r&apos; option to the SYSLOGD_OPTIONS in</span><br><span class=\"line\">    #    /etc/sysconfig/syslog</span><br><span class=\"line\">    #</span><br><span class=\"line\">    # 2) configure local2 events to go to the /var/log/haproxy.log</span><br><span class=\"line\">    #   file. A line like the following can be added to</span><br><span class=\"line\">    #   /etc/sysconfig/syslog</span><br><span class=\"line\">    #</span><br><span class=\"line\">    #    local2.*                       /var/log/haproxy.log</span><br><span class=\"line\">    #</span><br><span class=\"line\">    log         127.0.0.1 local0</span><br><span class=\"line\">    log         127.0.0.1 local1 notice</span><br><span class=\"line\">    #chroot      /usr/local/haproxy</span><br><span class=\"line\">    pidfile     /usr/local/haproxy/logs/haproxy.pid</span><br><span class=\"line\">    maxconn     4000</span><br><span class=\"line\">    #uid      501</span><br><span class=\"line\">    #gid 501</span><br><span class=\"line\">    daemon</span><br><span class=\"line\"></span><br><span class=\"line\">    # turn on stats unix socket</span><br><span class=\"line\">    #stats socket /var/lib/haproxy/stats</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># common defaults that all the &apos;listen&apos; and &apos;backend&apos; sections will</span><br><span class=\"line\"># use if not designated in their block</span><br><span class=\"line\">#</span><br><span class=\"line\"># You might need to adjust timing values to prevent timeouts.</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\">defaults</span><br><span class=\"line\">    mode                    http</span><br><span class=\"line\">    log                     global</span><br><span class=\"line\">    option                  httplog</span><br><span class=\"line\">    option                  dontlognull</span><br><span class=\"line\">    option http-server-close</span><br><span class=\"line\">    #option forwardfor       except 127.0.0.0/8</span><br><span class=\"line\">    option                  redispatch</span><br><span class=\"line\">    retries                 3</span><br><span class=\"line\">    maxconn                 3000</span><br><span class=\"line\">    timeout connect 5000</span><br><span class=\"line\">    timeout check 20000</span><br><span class=\"line\">    timeout client 50000</span><br><span class=\"line\">    timeout server 50000</span><br><span class=\"line\"></span><br><span class=\"line\">#</span><br><span class=\"line\"># This sets up the admin page for HA Proxy at port 25002.</span><br><span class=\"line\">#</span><br><span class=\"line\">listen stats</span><br><span class=\"line\">    bind 0.0.0.0:25002</span><br><span class=\"line\">    balance</span><br><span class=\"line\">    mode http</span><br><span class=\"line\">    stats enable</span><br><span class=\"line\">    stats auth admin:admin</span><br><span class=\"line\"></span><br><span class=\"line\"># This is the setup for Impala. Impala client connect to load_balancer_host:25003.</span><br><span class=\"line\"># HAProxy will balance connections among the list of servers listed below.</span><br><span class=\"line\"># The list of Impalad is listening at port 21000 for beeswax (impala-shell) or original ODBC driver.</span><br><span class=\"line\"># For JDBC or ODBC version 2.x driver, use port 21050 instead of 21000.</span><br><span class=\"line\">listen impala</span><br><span class=\"line\">    bind 0.0.0.0:25002</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    balance leastconn</span><br><span class=\"line\"></span><br><span class=\"line\">    server symbolic_name_1 elephant:21000</span><br><span class=\"line\">    server symbolic_name_2 tiger:21000</span><br><span class=\"line\">    server symbolic_name_3 monkey:21000</span><br><span class=\"line\">    server symbolic_name_4 horse:21000</span><br><span class=\"line\"></span><br><span class=\"line\"># Setup for Hue or other JDBC-enabled applications.</span><br><span class=\"line\"># In particular, Hue requires sticky sessions.</span><br><span class=\"line\"># The application connects to load_balancer_host:21051, and HAProxy balances</span><br><span class=\"line\"># connections to the associated hosts, where Impala listens for JDBC</span><br><span class=\"line\"># requests on port 21050.</span><br><span class=\"line\">listen impalajdbc</span><br><span class=\"line\">    bind 0.0.0.0:21051</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    balance source</span><br><span class=\"line\">    server symbolic_name_5 elephant:21050</span><br><span class=\"line\">    server symbolic_name_6 tiger:21050</span><br><span class=\"line\">    server symbolic_name_7 monkey:21050</span><br><span class=\"line\">    server symbolic_name_8 horse:21050</span><br></pre></td></tr></table></figure>\n<p>启动 haproxy</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo /usr/local/haproxy/sbin/haproxy -f /etc/haproxy/haproxy.cfg</span><br></pre></td></tr></table></figure>\n<p>impala 访问</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ beeline -d &quot;com.cloudera.impala.jdbc41.Driver&quot; -u &quot;jdbc:impala://elephant:21051&quot;</span><br><span class=\"line\">2018-01-12 14:35:57,602 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.</span><br><span class=\"line\">Connecting to jdbc:impala://elephant:21051</span><br><span class=\"line\">com.cloudera.impala.jdbc41.Driver</span><br><span class=\"line\">Beeline version 1.1.0-cdh5.9.0 by Apache Hive</span><br><span class=\"line\">0: jdbc:impala://elephant:21051 (closed)&gt;</span><br></pre></td></tr></table></figure>\n<p>停掉 elephant上的impala daemon，再次测试</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HiveAndImpalaLoadBalance/ImpalaLoadBalance1.png\" alt=\"\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ beeline -d &quot;com.cloudera.impala.jdbc41.Driver&quot; -u &quot;jdbc:impala://elephant:21051&quot;</span><br><span class=\"line\">2018-01-12 14:35:57,602 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.</span><br><span class=\"line\">Connecting to jdbc:impala://elephant:21051</span><br><span class=\"line\">com.cloudera.impala.jdbc41.Driver</span><br><span class=\"line\">Beeline version 1.1.0-cdh5.9.0 by Apache Hive</span><br><span class=\"line\">0: jdbc:impala://elephant:21051 (closed)&gt;</span><br></pre></td></tr></table></figure>"},{"title":"CCAH-131 Install","date":"2018-05-13T00:33:26.000Z","_content":"\n## Install\n\nDemonstrate an understanding of the installation process for Cloudera Manager, CDH, and the ecosystem projects.\n\n### Set up a local CDH repository.  设置CDH 本地Yum源\n    \n#### 方法1：使用httpd默认的文件路径\n\n检查httpd是否安装 ，如果没有安装结果如下： \n\n```\n[daniel@tiger ~]$ sudo rpm -qa | grep httpd\n[daniel@tiger ~]$\n```\n如果已经安装则是如下结果\n\n```\n[daniel@tiger ~]$ sudo rpm -qa | grep httpd\nhttpd-tools-2.2.15-60.el6.centos.6.x86_64\nhttpd-2.2.15-60.el6.centos.6.x86_64\n[daniel@tiger ~]$\n```\n<!-- more -->\n\n**安装 httpd**\n\n```\nsudo yum install -y httpd\n```\n\n启动httpd服务\n\n```\n[daniel@tiger ~]$ sudo service httpd start\nStarting httpd: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.25.150 for ServerName\n                                                           [  OK  ]\n[daniel@tiger ~]$\n```\n\n在/var/www/html/ 下创建文件夹 cm\n\n```\n[daniel@tiger html]$ sudo mkdir /var/www/html/cm\n[daniel@tiger html]$ ll /var/www/html/\ntotal 4\ndrwxr-xr-x. 2 root root 4096 Jan  5 12:43 cm\n[daniel@tiger html]$\n```\n  \n将下载的Cloudera Manager 包放到 /var/www/html/的cm文件下 \n\nsudo mv CDH-5.9.0-1.cdh5.9.0.p0.23-el5.parcel* /var/www/html/cm\n#### 修改cm文件夹得权限\n```\n[daniel@tiger cm]$ sudo yum install -y createrepo\n[daniel@tiger cm]$ sudo createrepo /var/www/html/cm\n[daniel@tiger cm]$ sudo chmod -R 755 /var/www/html/cm\n```\n使用浏览器打开 http://hostname/cm/ 其中 hostname是ip或者服务器的hostname。\n\n#### 配置Yum源\n在/etc/yum.repo.d/创建文件 cloudera-cm.repo\n\n```\n[daniel@tiger yum.repos.d]$ cat cloudera-cm.repo\n[cm]\nname=Cloudera-Manager\nbaseurl=http://192.168.25.150/cm\ngpgkey=https://archive.cloudera.com/redhat/cdh/RPM-GPG-KEY-cloudera\ngpgcheck=1\nenabled=1\n[daniel@tiger yum.repos.d]$\n```\n配置完成后，更新本地yum的cache，并查询是否配置成功\n\n```\n[daniel@tiger yum.repos.d]$ sudo yum clean all && yum makecache\n[daniel@tiger yum.repos.d]$ sudo yum search cloudera-manager\nFailed to set locale, defaulting to C\nLoaded plugins: fastestmirror, refresh-packagekit, security\nLoading mirror speeds from cached hostfile\n * base: mirrors.shuosc.org\n  * extras: mirrors.cn99.com\n   * updates: mirrors.shuosc.org\n=================================================================== N/S Matched: cloudera-manager ====================================================================\ncloudera-manager-agent.x86_64 : The Cloudera Manager Agent\ncloudera-manager-daemons.x86_64 : Provides daemons for monitoring Hadoop and related tools.\ncloudera-manager-server.x86_64 : The Cloudera Manager Server\ncloudera-manager-server-db-2.x86_64 : Embedded database for the Cloudera Manager Server\n\n  Name and summary matches only, use \"search all\" for everything.\n[daniel@tiger yum.repos.d]$\n```\nNote: 这个方法中需要在Cloudera Manager Server安装完成后将CDH的parcels文件放到 /opt/cloudera/parcels-repo/路径下。\n#### 方法2：使用httpd配置Cloudera Manager和CDH本地源\n例如： Cloudera Manager的文件在 /home/daniel/software/clouderea-cm5 ,CDH的parcels包文件在/home/daniel/software/cloudera-cdh\n\n使用方法1中安装httpd的方法安装httpd\n\n在配置文件 /etc/httpd/conf/httpd.conf的最后添加\n\n```\n<Directory \"/home/daniel/software\">\nOptions Indexes FollowSymLinks\n  allowOverride None\n  </Directory>\n  <VirtualHost tiger:8050>\n      DocumentRoot \"/home/daniel/software/cloudera-cm5\"\n      ServerName tiger:8050\n      </VirtualHost>\n      <VirtualHost *:8000>\n          DocumentRoot /home/daniel/software/cloudera-cdh\n\t      ServerName tiger:8000\n\t      </VirtualHost>\n```\n并将监听的端口从80修改位8000和8050\n\n```\n#Listen 80\nListen 8000\nListen 8050\n```\n将欢迎页面关闭掉。\n\n```\n[daniel@tiger ~]$ sudo vim /etc/httpd/conf.d/welcome.conf\n```\n注释掉里面的内容\n\n```\n#<LocationMatch \"^/+$\">\n#    Options -Indexes\n#    ErrorDocument 403 /error/noindex.html\n#</LocationMatch>\n```\n\n\n另外需要注意对于文件所在的文件夹对other用户都有r和x的权限。\n\n```\n[daniel@tiger ~]$ ll software/\ntotal 8\ndrwxrwxr-x. 2 daniel daniel 4096 Jan  6 05:19 cloudera-cdh\ndrwxrwxr-x. 3 daniel daniel 4096 Jan  6 14:47 cloudera-cm5\n[daniel@tiger ~]$\n```\n配置完成后，重启httpd服务，并使用浏览器或者curl检查配置的结果。\n\n```\ncurl tiger:8000\ncurl tiger:8050\n```\n\n###Perform OS-level configuration for Hadoop installation\n\n**关闭防火墙**\n\n```\nsudo service iptables stop\nsudo chkconfig iptables off\n```\n\n**关闭 SELinux**\n\n```\nsudo vim /etc/sysconfig/selinux\n```\n将其中的 SELINUX设置为disabled, SELINUXTYPE 设置为targeted\n  \n```\n# This file controls the state of SELinux on the system.\n# SELINUX= can take one of these three values:\n#     enforcing - SELinux security policy is enforced.\n#     permissive - SELinux prints warnings instead of enforcing.\n#     disabled - No SELinux policy is loaded.\nSELINUX=disabled\n# SELINUXTYPE= can take one of these two values:\n#     targeted - Targeted processes are protected,\n#     mls - Multi Level Security protection.\nSELINUXTYPE=targeted\n```\n\n**关闭透明大页**\n\n因为透明大页（Transparent HugePages ） 存在一些问题：\n\na.在RAC环境下 透明大页（Transparent HugePages ）会导致异常节点重启，和性能问题；   \nb.在单机环境中，透明大页（Transparent HugePages ） 也会导致一些异常的性能问题；\n        \n将下面2个命令执行，并写入 /etc/rc.local 文件里\n\n```\nsudo echo never > /sys/kernel/mm/redhat_transparent_hugepage/defrag\nsudo echo never > /sys/kernel/mm/redhat_transparent_hugepage/defrag\n```\n\n```\n[training@lion ~]$ cat /etc/rc.local\n#!/bin/sh\n#\n# This script will be executed *after* all the other init scripts.\n# You can put your own initialization stuff in here if you don't\n# want to do the full Sys V style init stuff.\n\ntouch /var/lock/subsys/local\necho never > /sys/kernel/mm/redhat_transparent_hugepage/defrag\necho never > /sys/kernel/mm/redhat_transparent_hugepage/enabled\n[training@lion ~]$\n```\n\n**修改使用SWAP分区的优先级**\n在文件 /etc/sysctl.conf 里添加 vm.swappiness = 1\n\n```\n[training@lion ~]$ vim /etc/sysctl.conf\n```\n\n```\nvm.swappiness = 1\n```\nlinux 会使用硬盘的一部分做为SWAP分区，用来进行进程调度--进程是正在运行的程序--把当前不用的进程调成‘等待（standby）‘，甚至‘睡眠（sleep）’，一旦要用，再调成‘活动（active）’，睡眠的进程就躺到SWAP分区睡大觉，把内存空出来让给‘活动’的进程。\n　　如果内存够大，应当告诉 linux 不必太多的使用 SWAP 分区， 可以通过修改 swappiness 的数值。swappiness=0的时候表示最大限度使用物理内存，然后才是 swap空间，swappiness＝100的时候表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间里面。\n\t\t\t\t\t\t\t\t\t\t\t\t\t    　　\n\n###Install Cloudera Manager server and agents\n\n#### 安装Cloudera Manager Server\n```\n[daniel@tiger ~]$ sudo yum install -y cloudera-manager-daemons cloudera-manager-server\n```\n#### 配置使用数据库\n执行如下命令：使Cloudera Manager使用配置好的MySQL数据库\n\n```\n[daniel@tiger lib]$ sudo /usr/share/cmf/schema/scm_prepare_database.sh mysql cmserver cmserveruser password\nJAVA_HOME=/usr/lib/java\nVerifying that we can write to /etc/cloudera-scm-server\nCreating SCM configuration file in /etc/cloudera-scm-server\nExecuting:  /usr/lib/java/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/cmf/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.\n[                          main] DbCommandExecutor              INFO  Successfully connected to database.\nAll done, your SCM database is configured correctly!\n```\n等待1-2分钟，使用浏览器打开 http://localhost:7180\n\n###Install CDH using Cloudera Manager\n安装Cloudear Manager Server\n\n```\n[training@lion training]$ sudo yum install -y cloudera-manager-daemons  cloudera-manager-server\n```\n配置Cloudear Manager所使用的数据库\n\n```\n[training@lion training]$ sudo /usr/share/cmf/schema/scm_prepare_database.sh mysql cmserver cmserveruser password\n```\n启动Cloudera Manager， 并使其开机启动,并坚持其状态（TODO：解释1-6）\n\n```\n[training@lion training]$ sudo service cloudera-scm-server start\nStarting cloudera-scm-server:                              [  OK  ]\n[training@lion training]$ sudo chkconfig cloudera-scm-server on\n[training@lion training]$ sudo chkconfig --list | grep cloudera-scm-server\ncloudera-scm-server  0:off  1:off  2:on  3:on  4:on  5:on  6:off\n[training@lion training]$\n```\n\n###Add a new node to an existing cluster\n\nHOME -> Hosts -> All Hosts -> Add New Hosts to Cluster 当前只有4个节点 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode1.png)\n\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode2.png)\n\n搜索节点 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode3.png)\n\n选择节点\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode4.png)\n\n设置本地Cloudera Manager所在的http服务\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode5.png)\n\n选择用户并填写密码\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode6.png)\n\n添加Host到集群\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode7.png)\n\n成功添加Host到集群\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode8.png)\n\n分配并激活Parcels\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode9.png)\n\n成功激活Parcels\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode10.png)\n\n检测服务器\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode11.png)\n\n使用模板或者使用默认设置，使用模板可以在添加节点的时候添加和模板一样的服务到新的节点上\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode12.png)\n\n添加成功\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode13.png)\n\n查看新节点状态\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode14.png)\n\n###Add a service using Cloudera Manager\n\n**添加Hive 服务**\n\n在集群的Home页，选择下拉框，Add Service\n选择要添加的服务\n![第一步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%201.png)\n\n选择服务的依赖\n\n![第二步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%202.png)\n\n选择需要添加的角色\n\n![第三步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%203.png)\n\n选择要使用的数据库\n\n![第四步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%204.png)\n\n查看也可修改相关配置\n\n![第五步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%205.png)\n\n初始化和启动服务\n\n![第六步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%206.png)\n\n服务启动完成\n\n![第七步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%207.png)\n\n添加成功\n\n![第八步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%208.png)\n\n","source":"_posts/CCAH-131-Install.md","raw":"---\ntitle: CCAH-131 Install\ndate: 2018-05-13 08:33:26\ntags: CCAH-131\n---\n\n## Install\n\nDemonstrate an understanding of the installation process for Cloudera Manager, CDH, and the ecosystem projects.\n\n### Set up a local CDH repository.  设置CDH 本地Yum源\n    \n#### 方法1：使用httpd默认的文件路径\n\n检查httpd是否安装 ，如果没有安装结果如下： \n\n```\n[daniel@tiger ~]$ sudo rpm -qa | grep httpd\n[daniel@tiger ~]$\n```\n如果已经安装则是如下结果\n\n```\n[daniel@tiger ~]$ sudo rpm -qa | grep httpd\nhttpd-tools-2.2.15-60.el6.centos.6.x86_64\nhttpd-2.2.15-60.el6.centos.6.x86_64\n[daniel@tiger ~]$\n```\n<!-- more -->\n\n**安装 httpd**\n\n```\nsudo yum install -y httpd\n```\n\n启动httpd服务\n\n```\n[daniel@tiger ~]$ sudo service httpd start\nStarting httpd: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.25.150 for ServerName\n                                                           [  OK  ]\n[daniel@tiger ~]$\n```\n\n在/var/www/html/ 下创建文件夹 cm\n\n```\n[daniel@tiger html]$ sudo mkdir /var/www/html/cm\n[daniel@tiger html]$ ll /var/www/html/\ntotal 4\ndrwxr-xr-x. 2 root root 4096 Jan  5 12:43 cm\n[daniel@tiger html]$\n```\n  \n将下载的Cloudera Manager 包放到 /var/www/html/的cm文件下 \n\nsudo mv CDH-5.9.0-1.cdh5.9.0.p0.23-el5.parcel* /var/www/html/cm\n#### 修改cm文件夹得权限\n```\n[daniel@tiger cm]$ sudo yum install -y createrepo\n[daniel@tiger cm]$ sudo createrepo /var/www/html/cm\n[daniel@tiger cm]$ sudo chmod -R 755 /var/www/html/cm\n```\n使用浏览器打开 http://hostname/cm/ 其中 hostname是ip或者服务器的hostname。\n\n#### 配置Yum源\n在/etc/yum.repo.d/创建文件 cloudera-cm.repo\n\n```\n[daniel@tiger yum.repos.d]$ cat cloudera-cm.repo\n[cm]\nname=Cloudera-Manager\nbaseurl=http://192.168.25.150/cm\ngpgkey=https://archive.cloudera.com/redhat/cdh/RPM-GPG-KEY-cloudera\ngpgcheck=1\nenabled=1\n[daniel@tiger yum.repos.d]$\n```\n配置完成后，更新本地yum的cache，并查询是否配置成功\n\n```\n[daniel@tiger yum.repos.d]$ sudo yum clean all && yum makecache\n[daniel@tiger yum.repos.d]$ sudo yum search cloudera-manager\nFailed to set locale, defaulting to C\nLoaded plugins: fastestmirror, refresh-packagekit, security\nLoading mirror speeds from cached hostfile\n * base: mirrors.shuosc.org\n  * extras: mirrors.cn99.com\n   * updates: mirrors.shuosc.org\n=================================================================== N/S Matched: cloudera-manager ====================================================================\ncloudera-manager-agent.x86_64 : The Cloudera Manager Agent\ncloudera-manager-daemons.x86_64 : Provides daemons for monitoring Hadoop and related tools.\ncloudera-manager-server.x86_64 : The Cloudera Manager Server\ncloudera-manager-server-db-2.x86_64 : Embedded database for the Cloudera Manager Server\n\n  Name and summary matches only, use \"search all\" for everything.\n[daniel@tiger yum.repos.d]$\n```\nNote: 这个方法中需要在Cloudera Manager Server安装完成后将CDH的parcels文件放到 /opt/cloudera/parcels-repo/路径下。\n#### 方法2：使用httpd配置Cloudera Manager和CDH本地源\n例如： Cloudera Manager的文件在 /home/daniel/software/clouderea-cm5 ,CDH的parcels包文件在/home/daniel/software/cloudera-cdh\n\n使用方法1中安装httpd的方法安装httpd\n\n在配置文件 /etc/httpd/conf/httpd.conf的最后添加\n\n```\n<Directory \"/home/daniel/software\">\nOptions Indexes FollowSymLinks\n  allowOverride None\n  </Directory>\n  <VirtualHost tiger:8050>\n      DocumentRoot \"/home/daniel/software/cloudera-cm5\"\n      ServerName tiger:8050\n      </VirtualHost>\n      <VirtualHost *:8000>\n          DocumentRoot /home/daniel/software/cloudera-cdh\n\t      ServerName tiger:8000\n\t      </VirtualHost>\n```\n并将监听的端口从80修改位8000和8050\n\n```\n#Listen 80\nListen 8000\nListen 8050\n```\n将欢迎页面关闭掉。\n\n```\n[daniel@tiger ~]$ sudo vim /etc/httpd/conf.d/welcome.conf\n```\n注释掉里面的内容\n\n```\n#<LocationMatch \"^/+$\">\n#    Options -Indexes\n#    ErrorDocument 403 /error/noindex.html\n#</LocationMatch>\n```\n\n\n另外需要注意对于文件所在的文件夹对other用户都有r和x的权限。\n\n```\n[daniel@tiger ~]$ ll software/\ntotal 8\ndrwxrwxr-x. 2 daniel daniel 4096 Jan  6 05:19 cloudera-cdh\ndrwxrwxr-x. 3 daniel daniel 4096 Jan  6 14:47 cloudera-cm5\n[daniel@tiger ~]$\n```\n配置完成后，重启httpd服务，并使用浏览器或者curl检查配置的结果。\n\n```\ncurl tiger:8000\ncurl tiger:8050\n```\n\n###Perform OS-level configuration for Hadoop installation\n\n**关闭防火墙**\n\n```\nsudo service iptables stop\nsudo chkconfig iptables off\n```\n\n**关闭 SELinux**\n\n```\nsudo vim /etc/sysconfig/selinux\n```\n将其中的 SELINUX设置为disabled, SELINUXTYPE 设置为targeted\n  \n```\n# This file controls the state of SELinux on the system.\n# SELINUX= can take one of these three values:\n#     enforcing - SELinux security policy is enforced.\n#     permissive - SELinux prints warnings instead of enforcing.\n#     disabled - No SELinux policy is loaded.\nSELINUX=disabled\n# SELINUXTYPE= can take one of these two values:\n#     targeted - Targeted processes are protected,\n#     mls - Multi Level Security protection.\nSELINUXTYPE=targeted\n```\n\n**关闭透明大页**\n\n因为透明大页（Transparent HugePages ） 存在一些问题：\n\na.在RAC环境下 透明大页（Transparent HugePages ）会导致异常节点重启，和性能问题；   \nb.在单机环境中，透明大页（Transparent HugePages ） 也会导致一些异常的性能问题；\n        \n将下面2个命令执行，并写入 /etc/rc.local 文件里\n\n```\nsudo echo never > /sys/kernel/mm/redhat_transparent_hugepage/defrag\nsudo echo never > /sys/kernel/mm/redhat_transparent_hugepage/defrag\n```\n\n```\n[training@lion ~]$ cat /etc/rc.local\n#!/bin/sh\n#\n# This script will be executed *after* all the other init scripts.\n# You can put your own initialization stuff in here if you don't\n# want to do the full Sys V style init stuff.\n\ntouch /var/lock/subsys/local\necho never > /sys/kernel/mm/redhat_transparent_hugepage/defrag\necho never > /sys/kernel/mm/redhat_transparent_hugepage/enabled\n[training@lion ~]$\n```\n\n**修改使用SWAP分区的优先级**\n在文件 /etc/sysctl.conf 里添加 vm.swappiness = 1\n\n```\n[training@lion ~]$ vim /etc/sysctl.conf\n```\n\n```\nvm.swappiness = 1\n```\nlinux 会使用硬盘的一部分做为SWAP分区，用来进行进程调度--进程是正在运行的程序--把当前不用的进程调成‘等待（standby）‘，甚至‘睡眠（sleep）’，一旦要用，再调成‘活动（active）’，睡眠的进程就躺到SWAP分区睡大觉，把内存空出来让给‘活动’的进程。\n　　如果内存够大，应当告诉 linux 不必太多的使用 SWAP 分区， 可以通过修改 swappiness 的数值。swappiness=0的时候表示最大限度使用物理内存，然后才是 swap空间，swappiness＝100的时候表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间里面。\n\t\t\t\t\t\t\t\t\t\t\t\t\t    　　\n\n###Install Cloudera Manager server and agents\n\n#### 安装Cloudera Manager Server\n```\n[daniel@tiger ~]$ sudo yum install -y cloudera-manager-daemons cloudera-manager-server\n```\n#### 配置使用数据库\n执行如下命令：使Cloudera Manager使用配置好的MySQL数据库\n\n```\n[daniel@tiger lib]$ sudo /usr/share/cmf/schema/scm_prepare_database.sh mysql cmserver cmserveruser password\nJAVA_HOME=/usr/lib/java\nVerifying that we can write to /etc/cloudera-scm-server\nCreating SCM configuration file in /etc/cloudera-scm-server\nExecuting:  /usr/lib/java/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/cmf/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.\n[                          main] DbCommandExecutor              INFO  Successfully connected to database.\nAll done, your SCM database is configured correctly!\n```\n等待1-2分钟，使用浏览器打开 http://localhost:7180\n\n###Install CDH using Cloudera Manager\n安装Cloudear Manager Server\n\n```\n[training@lion training]$ sudo yum install -y cloudera-manager-daemons  cloudera-manager-server\n```\n配置Cloudear Manager所使用的数据库\n\n```\n[training@lion training]$ sudo /usr/share/cmf/schema/scm_prepare_database.sh mysql cmserver cmserveruser password\n```\n启动Cloudera Manager， 并使其开机启动,并坚持其状态（TODO：解释1-6）\n\n```\n[training@lion training]$ sudo service cloudera-scm-server start\nStarting cloudera-scm-server:                              [  OK  ]\n[training@lion training]$ sudo chkconfig cloudera-scm-server on\n[training@lion training]$ sudo chkconfig --list | grep cloudera-scm-server\ncloudera-scm-server  0:off  1:off  2:on  3:on  4:on  5:on  6:off\n[training@lion training]$\n```\n\n###Add a new node to an existing cluster\n\nHOME -> Hosts -> All Hosts -> Add New Hosts to Cluster 当前只有4个节点 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode1.png)\n\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode2.png)\n\n搜索节点 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode3.png)\n\n选择节点\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode4.png)\n\n设置本地Cloudera Manager所在的http服务\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode5.png)\n\n选择用户并填写密码\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode6.png)\n\n添加Host到集群\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode7.png)\n\n成功添加Host到集群\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode8.png)\n\n分配并激活Parcels\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode9.png)\n\n成功激活Parcels\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode10.png)\n\n检测服务器\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode11.png)\n\n使用模板或者使用默认设置，使用模板可以在添加节点的时候添加和模板一样的服务到新的节点上\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode12.png)\n\n添加成功\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode13.png)\n\n查看新节点状态\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode14.png)\n\n###Add a service using Cloudera Manager\n\n**添加Hive 服务**\n\n在集群的Home页，选择下拉框，Add Service\n选择要添加的服务\n![第一步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%201.png)\n\n选择服务的依赖\n\n![第二步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%202.png)\n\n选择需要添加的角色\n\n![第三步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%203.png)\n\n选择要使用的数据库\n\n![第四步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%204.png)\n\n查看也可修改相关配置\n\n![第五步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%205.png)\n\n初始化和启动服务\n\n![第六步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%206.png)\n\n服务启动完成\n\n![第七步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%207.png)\n\n添加成功\n\n![第八步](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%208.png)\n\n","slug":"CCAH-131-Install","published":1,"updated":"2018-09-14T01:16:57.039Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnz60003inamxxf8dsdi","content":"<h2 id=\"Install\"><a href=\"#Install\" class=\"headerlink\" title=\"Install\"></a>Install</h2><p>Demonstrate an understanding of the installation process for Cloudera Manager, CDH, and the ecosystem projects.</p>\n<h3 id=\"Set-up-a-local-CDH-repository-设置CDH-本地Yum源\"><a href=\"#Set-up-a-local-CDH-repository-设置CDH-本地Yum源\" class=\"headerlink\" title=\"Set up a local CDH repository.  设置CDH 本地Yum源\"></a>Set up a local CDH repository.  设置CDH 本地Yum源</h3><h4 id=\"方法1：使用httpd默认的文件路径\"><a href=\"#方法1：使用httpd默认的文件路径\" class=\"headerlink\" title=\"方法1：使用httpd默认的文件路径\"></a>方法1：使用httpd默认的文件路径</h4><p>检查httpd是否安装 ，如果没有安装结果如下： </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger ~]$ sudo rpm -qa | grep httpd</span><br><span class=\"line\">[daniel@tiger ~]$</span><br></pre></td></tr></table></figure>\n<p>如果已经安装则是如下结果</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger ~]$ sudo rpm -qa | grep httpd</span><br><span class=\"line\">httpd-tools-2.2.15-60.el6.centos.6.x86_64</span><br><span class=\"line\">httpd-2.2.15-60.el6.centos.6.x86_64</span><br><span class=\"line\">[daniel@tiger ~]$</span><br></pre></td></tr></table></figure>\n<a id=\"more\"></a>\n<p><strong>安装 httpd</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo yum install -y httpd</span><br></pre></td></tr></table></figure>\n<p>启动httpd服务</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger ~]$ sudo service httpd start</span><br><span class=\"line\">Starting httpd: httpd: Could not reliably determine the server&apos;s fully qualified domain name, using 192.168.25.150 for ServerName</span><br><span class=\"line\">                                                           [  OK  ]</span><br><span class=\"line\">[daniel@tiger ~]$</span><br></pre></td></tr></table></figure>\n<p>在/var/www/html/ 下创建文件夹 cm</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger html]$ sudo mkdir /var/www/html/cm</span><br><span class=\"line\">[daniel@tiger html]$ ll /var/www/html/</span><br><span class=\"line\">total 4</span><br><span class=\"line\">drwxr-xr-x. 2 root root 4096 Jan  5 12:43 cm</span><br><span class=\"line\">[daniel@tiger html]$</span><br></pre></td></tr></table></figure>\n<p>将下载的Cloudera Manager 包放到 /var/www/html/的cm文件下 </p>\n<p>sudo mv CDH-5.9.0-1.cdh5.9.0.p0.23-el5.parcel* /var/www/html/cm</p>\n<h4 id=\"修改cm文件夹得权限\"><a href=\"#修改cm文件夹得权限\" class=\"headerlink\" title=\"修改cm文件夹得权限\"></a>修改cm文件夹得权限</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger cm]$ sudo yum install -y createrepo</span><br><span class=\"line\">[daniel@tiger cm]$ sudo createrepo /var/www/html/cm</span><br><span class=\"line\">[daniel@tiger cm]$ sudo chmod -R 755 /var/www/html/cm</span><br></pre></td></tr></table></figure>\n<p>使用浏览器打开 <a href=\"http://hostname/cm/\" target=\"_blank\" rel=\"noopener\">http://hostname/cm/</a> 其中 hostname是ip或者服务器的hostname。</p>\n<h4 id=\"配置Yum源\"><a href=\"#配置Yum源\" class=\"headerlink\" title=\"配置Yum源\"></a>配置Yum源</h4><p>在/etc/yum.repo.d/创建文件 cloudera-cm.repo</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger yum.repos.d]$ cat cloudera-cm.repo</span><br><span class=\"line\">[cm]</span><br><span class=\"line\">name=Cloudera-Manager</span><br><span class=\"line\">baseurl=http://192.168.25.150/cm</span><br><span class=\"line\">gpgkey=https://archive.cloudera.com/redhat/cdh/RPM-GPG-KEY-cloudera</span><br><span class=\"line\">gpgcheck=1</span><br><span class=\"line\">enabled=1</span><br><span class=\"line\">[daniel@tiger yum.repos.d]$</span><br></pre></td></tr></table></figure>\n<p>配置完成后，更新本地yum的cache，并查询是否配置成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger yum.repos.d]$ sudo yum clean all &amp;&amp; yum makecache</span><br><span class=\"line\">[daniel@tiger yum.repos.d]$ sudo yum search cloudera-manager</span><br><span class=\"line\">Failed to set locale, defaulting to C</span><br><span class=\"line\">Loaded plugins: fastestmirror, refresh-packagekit, security</span><br><span class=\"line\">Loading mirror speeds from cached hostfile</span><br><span class=\"line\"> * base: mirrors.shuosc.org</span><br><span class=\"line\">  * extras: mirrors.cn99.com</span><br><span class=\"line\">   * updates: mirrors.shuosc.org</span><br><span class=\"line\">=================================================================== N/S Matched: cloudera-manager ====================================================================</span><br><span class=\"line\">cloudera-manager-agent.x86_64 : The Cloudera Manager Agent</span><br><span class=\"line\">cloudera-manager-daemons.x86_64 : Provides daemons for monitoring Hadoop and related tools.</span><br><span class=\"line\">cloudera-manager-server.x86_64 : The Cloudera Manager Server</span><br><span class=\"line\">cloudera-manager-server-db-2.x86_64 : Embedded database for the Cloudera Manager Server</span><br><span class=\"line\"></span><br><span class=\"line\">  Name and summary matches only, use &quot;search all&quot; for everything.</span><br><span class=\"line\">[daniel@tiger yum.repos.d]$</span><br></pre></td></tr></table></figure>\n<p>Note: 这个方法中需要在Cloudera Manager Server安装完成后将CDH的parcels文件放到 /opt/cloudera/parcels-repo/路径下。</p>\n<h4 id=\"方法2：使用httpd配置Cloudera-Manager和CDH本地源\"><a href=\"#方法2：使用httpd配置Cloudera-Manager和CDH本地源\" class=\"headerlink\" title=\"方法2：使用httpd配置Cloudera Manager和CDH本地源\"></a>方法2：使用httpd配置Cloudera Manager和CDH本地源</h4><p>例如： Cloudera Manager的文件在 /home/daniel/software/clouderea-cm5 ,CDH的parcels包文件在/home/daniel/software/cloudera-cdh</p>\n<p>使用方法1中安装httpd的方法安装httpd</p>\n<p>在配置文件 /etc/httpd/conf/httpd.conf的最后添加</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;Directory &quot;/home/daniel/software&quot;&gt;</span><br><span class=\"line\">Options Indexes FollowSymLinks</span><br><span class=\"line\">  allowOverride None</span><br><span class=\"line\">  &lt;/Directory&gt;</span><br><span class=\"line\">  &lt;VirtualHost tiger:8050&gt;</span><br><span class=\"line\">      DocumentRoot &quot;/home/daniel/software/cloudera-cm5&quot;</span><br><span class=\"line\">      ServerName tiger:8050</span><br><span class=\"line\">      &lt;/VirtualHost&gt;</span><br><span class=\"line\">      &lt;VirtualHost *:8000&gt;</span><br><span class=\"line\">          DocumentRoot /home/daniel/software/cloudera-cdh</span><br><span class=\"line\">\t      ServerName tiger:8000</span><br><span class=\"line\">\t      &lt;/VirtualHost&gt;</span><br></pre></td></tr></table></figure>\n<p>并将监听的端口从80修改位8000和8050</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#Listen 80</span><br><span class=\"line\">Listen 8000</span><br><span class=\"line\">Listen 8050</span><br></pre></td></tr></table></figure>\n<p>将欢迎页面关闭掉。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger ~]$ sudo vim /etc/httpd/conf.d/welcome.conf</span><br></pre></td></tr></table></figure>\n<p>注释掉里面的内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#&lt;LocationMatch &quot;^/+$&quot;&gt;</span><br><span class=\"line\">#    Options -Indexes</span><br><span class=\"line\">#    ErrorDocument 403 /error/noindex.html</span><br><span class=\"line\">#&lt;/LocationMatch&gt;</span><br></pre></td></tr></table></figure>\n<p>另外需要注意对于文件所在的文件夹对other用户都有r和x的权限。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger ~]$ ll software/</span><br><span class=\"line\">total 8</span><br><span class=\"line\">drwxrwxr-x. 2 daniel daniel 4096 Jan  6 05:19 cloudera-cdh</span><br><span class=\"line\">drwxrwxr-x. 3 daniel daniel 4096 Jan  6 14:47 cloudera-cm5</span><br><span class=\"line\">[daniel@tiger ~]$</span><br></pre></td></tr></table></figure>\n<p>配置完成后，重启httpd服务，并使用浏览器或者curl检查配置的结果。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl tiger:8000</span><br><span class=\"line\">curl tiger:8050</span><br></pre></td></tr></table></figure>\n<p>###Perform OS-level configuration for Hadoop installation</p>\n<p><strong>关闭防火墙</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo service iptables stop</span><br><span class=\"line\">sudo chkconfig iptables off</span><br></pre></td></tr></table></figure>\n<p><strong>关闭 SELinux</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo vim /etc/sysconfig/selinux</span><br></pre></td></tr></table></figure>\n<p>将其中的 SELINUX设置为disabled, SELINUXTYPE 设置为targeted</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># This file controls the state of SELinux on the system.</span><br><span class=\"line\"># SELINUX= can take one of these three values:</span><br><span class=\"line\">#     enforcing - SELinux security policy is enforced.</span><br><span class=\"line\">#     permissive - SELinux prints warnings instead of enforcing.</span><br><span class=\"line\">#     disabled - No SELinux policy is loaded.</span><br><span class=\"line\">SELINUX=disabled</span><br><span class=\"line\"># SELINUXTYPE= can take one of these two values:</span><br><span class=\"line\">#     targeted - Targeted processes are protected,</span><br><span class=\"line\">#     mls - Multi Level Security protection.</span><br><span class=\"line\">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure>\n<p><strong>关闭透明大页</strong></p>\n<p>因为透明大页（Transparent HugePages ） 存在一些问题：</p>\n<p>a.在RAC环境下 透明大页（Transparent HugePages ）会导致异常节点重启，和性能问题；<br>b.在单机环境中，透明大页（Transparent HugePages ） 也会导致一些异常的性能问题；</p>\n<p>将下面2个命令执行，并写入 /etc/rc.local 文件里</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag</span><br><span class=\"line\">sudo echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@lion ~]$ cat /etc/rc.local</span><br><span class=\"line\">#!/bin/sh</span><br><span class=\"line\">#</span><br><span class=\"line\"># This script will be executed *after* all the other init scripts.</span><br><span class=\"line\"># You can put your own initialization stuff in here if you don&apos;t</span><br><span class=\"line\"># want to do the full Sys V style init stuff.</span><br><span class=\"line\"></span><br><span class=\"line\">touch /var/lock/subsys/local</span><br><span class=\"line\">echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag</span><br><span class=\"line\">echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/enabled</span><br><span class=\"line\">[training@lion ~]$</span><br></pre></td></tr></table></figure>\n<p><strong>修改使用SWAP分区的优先级</strong><br>在文件 /etc/sysctl.conf 里添加 vm.swappiness = 1</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@lion ~]$ vim /etc/sysctl.conf</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vm.swappiness = 1</span><br></pre></td></tr></table></figure>\n<p>linux 会使用硬盘的一部分做为SWAP分区，用来进行进程调度–进程是正在运行的程序–把当前不用的进程调成‘等待（standby）‘，甚至‘睡眠（sleep）’，一旦要用，再调成‘活动（active）’，睡眠的进程就躺到SWAP分区睡大觉，把内存空出来让给‘活动’的进程。<br>　　如果内存够大，应当告诉 linux 不必太多的使用 SWAP 分区， 可以通过修改 swappiness 的数值。swappiness=0的时候表示最大限度使用物理内存，然后才是 swap空间，swappiness＝100的时候表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间里面。\n                                                        　　</p>\n<p>###Install Cloudera Manager server and agents</p>\n<h4 id=\"安装Cloudera-Manager-Server\"><a href=\"#安装Cloudera-Manager-Server\" class=\"headerlink\" title=\"安装Cloudera Manager Server\"></a>安装Cloudera Manager Server</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger ~]$ sudo yum install -y cloudera-manager-daemons cloudera-manager-server</span><br></pre></td></tr></table></figure>\n<h4 id=\"配置使用数据库\"><a href=\"#配置使用数据库\" class=\"headerlink\" title=\"配置使用数据库\"></a>配置使用数据库</h4><p>执行如下命令：使Cloudera Manager使用配置好的MySQL数据库</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger lib]$ sudo /usr/share/cmf/schema/scm_prepare_database.sh mysql cmserver cmserveruser password</span><br><span class=\"line\">JAVA_HOME=/usr/lib/java</span><br><span class=\"line\">Verifying that we can write to /etc/cloudera-scm-server</span><br><span class=\"line\">Creating SCM configuration file in /etc/cloudera-scm-server</span><br><span class=\"line\">Executing:  /usr/lib/java/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/cmf/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.</span><br><span class=\"line\">[                          main] DbCommandExecutor              INFO  Successfully connected to database.</span><br><span class=\"line\">All done, your SCM database is configured correctly!</span><br></pre></td></tr></table></figure>\n<p>等待1-2分钟，使用浏览器打开 <a href=\"http://localhost:7180\" target=\"_blank\" rel=\"noopener\">http://localhost:7180</a></p>\n<p>###Install CDH using Cloudera Manager<br>安装Cloudear Manager Server</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@lion training]$ sudo yum install -y cloudera-manager-daemons  cloudera-manager-server</span><br></pre></td></tr></table></figure>\n<p>配置Cloudear Manager所使用的数据库</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@lion training]$ sudo /usr/share/cmf/schema/scm_prepare_database.sh mysql cmserver cmserveruser password</span><br></pre></td></tr></table></figure>\n<p>启动Cloudera Manager， 并使其开机启动,并坚持其状态（TODO：解释1-6）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@lion training]$ sudo service cloudera-scm-server start</span><br><span class=\"line\">Starting cloudera-scm-server:                              [  OK  ]</span><br><span class=\"line\">[training@lion training]$ sudo chkconfig cloudera-scm-server on</span><br><span class=\"line\">[training@lion training]$ sudo chkconfig --list | grep cloudera-scm-server</span><br><span class=\"line\">cloudera-scm-server  0:off  1:off  2:on  3:on  4:on  5:on  6:off</span><br><span class=\"line\">[training@lion training]$</span><br></pre></td></tr></table></figure>\n<p>###Add a new node to an existing cluster</p>\n<p>HOME -&gt; Hosts -&gt; All Hosts -&gt; Add New Hosts to Cluster 当前只有4个节点 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode1.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode2.png\" alt=\"\"></p>\n<p>搜索节点 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode3.png\" alt=\"\"></p>\n<p>选择节点</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode4.png\" alt=\"\"></p>\n<p>设置本地Cloudera Manager所在的http服务</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode5.png\" alt=\"\"></p>\n<p>选择用户并填写密码</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode6.png\" alt=\"\"></p>\n<p>添加Host到集群</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode7.png\" alt=\"\"></p>\n<p>成功添加Host到集群</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode8.png\" alt=\"\"></p>\n<p>分配并激活Parcels</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode9.png\" alt=\"\"></p>\n<p>成功激活Parcels</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode10.png\" alt=\"\"></p>\n<p>检测服务器</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode11.png\" alt=\"\"></p>\n<p>使用模板或者使用默认设置，使用模板可以在添加节点的时候添加和模板一样的服务到新的节点上</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode12.png\" alt=\"\"></p>\n<p>添加成功</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode13.png\" alt=\"\"></p>\n<p>查看新节点状态</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode14.png\" alt=\"\"></p>\n<p>###Add a service using Cloudera Manager</p>\n<p><strong>添加Hive 服务</strong></p>\n<p>在集群的Home页，选择下拉框，Add Service<br>选择要添加的服务<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%201.png\" alt=\"第一步\"></p>\n<p>选择服务的依赖</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%202.png\" alt=\"第二步\"></p>\n<p>选择需要添加的角色</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%203.png\" alt=\"第三步\"></p>\n<p>选择要使用的数据库</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%204.png\" alt=\"第四步\"></p>\n<p>查看也可修改相关配置</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%205.png\" alt=\"第五步\"></p>\n<p>初始化和启动服务</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%206.png\" alt=\"第六步\"></p>\n<p>服务启动完成</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%207.png\" alt=\"第七步\"></p>\n<p>添加成功</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%208.png\" alt=\"第八步\"></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"Install\"><a href=\"#Install\" class=\"headerlink\" title=\"Install\"></a>Install</h2><p>Demonstrate an understanding of the installation process for Cloudera Manager, CDH, and the ecosystem projects.</p>\n<h3 id=\"Set-up-a-local-CDH-repository-设置CDH-本地Yum源\"><a href=\"#Set-up-a-local-CDH-repository-设置CDH-本地Yum源\" class=\"headerlink\" title=\"Set up a local CDH repository.  设置CDH 本地Yum源\"></a>Set up a local CDH repository.  设置CDH 本地Yum源</h3><h4 id=\"方法1：使用httpd默认的文件路径\"><a href=\"#方法1：使用httpd默认的文件路径\" class=\"headerlink\" title=\"方法1：使用httpd默认的文件路径\"></a>方法1：使用httpd默认的文件路径</h4><p>检查httpd是否安装 ，如果没有安装结果如下： </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger ~]$ sudo rpm -qa | grep httpd</span><br><span class=\"line\">[daniel@tiger ~]$</span><br></pre></td></tr></table></figure>\n<p>如果已经安装则是如下结果</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger ~]$ sudo rpm -qa | grep httpd</span><br><span class=\"line\">httpd-tools-2.2.15-60.el6.centos.6.x86_64</span><br><span class=\"line\">httpd-2.2.15-60.el6.centos.6.x86_64</span><br><span class=\"line\">[daniel@tiger ~]$</span><br></pre></td></tr></table></figure>","more":"<p><strong>安装 httpd</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo yum install -y httpd</span><br></pre></td></tr></table></figure>\n<p>启动httpd服务</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger ~]$ sudo service httpd start</span><br><span class=\"line\">Starting httpd: httpd: Could not reliably determine the server&apos;s fully qualified domain name, using 192.168.25.150 for ServerName</span><br><span class=\"line\">                                                           [  OK  ]</span><br><span class=\"line\">[daniel@tiger ~]$</span><br></pre></td></tr></table></figure>\n<p>在/var/www/html/ 下创建文件夹 cm</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger html]$ sudo mkdir /var/www/html/cm</span><br><span class=\"line\">[daniel@tiger html]$ ll /var/www/html/</span><br><span class=\"line\">total 4</span><br><span class=\"line\">drwxr-xr-x. 2 root root 4096 Jan  5 12:43 cm</span><br><span class=\"line\">[daniel@tiger html]$</span><br></pre></td></tr></table></figure>\n<p>将下载的Cloudera Manager 包放到 /var/www/html/的cm文件下 </p>\n<p>sudo mv CDH-5.9.0-1.cdh5.9.0.p0.23-el5.parcel* /var/www/html/cm</p>\n<h4 id=\"修改cm文件夹得权限\"><a href=\"#修改cm文件夹得权限\" class=\"headerlink\" title=\"修改cm文件夹得权限\"></a>修改cm文件夹得权限</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger cm]$ sudo yum install -y createrepo</span><br><span class=\"line\">[daniel@tiger cm]$ sudo createrepo /var/www/html/cm</span><br><span class=\"line\">[daniel@tiger cm]$ sudo chmod -R 755 /var/www/html/cm</span><br></pre></td></tr></table></figure>\n<p>使用浏览器打开 <a href=\"http://hostname/cm/\" target=\"_blank\" rel=\"noopener\">http://hostname/cm/</a> 其中 hostname是ip或者服务器的hostname。</p>\n<h4 id=\"配置Yum源\"><a href=\"#配置Yum源\" class=\"headerlink\" title=\"配置Yum源\"></a>配置Yum源</h4><p>在/etc/yum.repo.d/创建文件 cloudera-cm.repo</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger yum.repos.d]$ cat cloudera-cm.repo</span><br><span class=\"line\">[cm]</span><br><span class=\"line\">name=Cloudera-Manager</span><br><span class=\"line\">baseurl=http://192.168.25.150/cm</span><br><span class=\"line\">gpgkey=https://archive.cloudera.com/redhat/cdh/RPM-GPG-KEY-cloudera</span><br><span class=\"line\">gpgcheck=1</span><br><span class=\"line\">enabled=1</span><br><span class=\"line\">[daniel@tiger yum.repos.d]$</span><br></pre></td></tr></table></figure>\n<p>配置完成后，更新本地yum的cache，并查询是否配置成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger yum.repos.d]$ sudo yum clean all &amp;&amp; yum makecache</span><br><span class=\"line\">[daniel@tiger yum.repos.d]$ sudo yum search cloudera-manager</span><br><span class=\"line\">Failed to set locale, defaulting to C</span><br><span class=\"line\">Loaded plugins: fastestmirror, refresh-packagekit, security</span><br><span class=\"line\">Loading mirror speeds from cached hostfile</span><br><span class=\"line\"> * base: mirrors.shuosc.org</span><br><span class=\"line\">  * extras: mirrors.cn99.com</span><br><span class=\"line\">   * updates: mirrors.shuosc.org</span><br><span class=\"line\">=================================================================== N/S Matched: cloudera-manager ====================================================================</span><br><span class=\"line\">cloudera-manager-agent.x86_64 : The Cloudera Manager Agent</span><br><span class=\"line\">cloudera-manager-daemons.x86_64 : Provides daemons for monitoring Hadoop and related tools.</span><br><span class=\"line\">cloudera-manager-server.x86_64 : The Cloudera Manager Server</span><br><span class=\"line\">cloudera-manager-server-db-2.x86_64 : Embedded database for the Cloudera Manager Server</span><br><span class=\"line\"></span><br><span class=\"line\">  Name and summary matches only, use &quot;search all&quot; for everything.</span><br><span class=\"line\">[daniel@tiger yum.repos.d]$</span><br></pre></td></tr></table></figure>\n<p>Note: 这个方法中需要在Cloudera Manager Server安装完成后将CDH的parcels文件放到 /opt/cloudera/parcels-repo/路径下。</p>\n<h4 id=\"方法2：使用httpd配置Cloudera-Manager和CDH本地源\"><a href=\"#方法2：使用httpd配置Cloudera-Manager和CDH本地源\" class=\"headerlink\" title=\"方法2：使用httpd配置Cloudera Manager和CDH本地源\"></a>方法2：使用httpd配置Cloudera Manager和CDH本地源</h4><p>例如： Cloudera Manager的文件在 /home/daniel/software/clouderea-cm5 ,CDH的parcels包文件在/home/daniel/software/cloudera-cdh</p>\n<p>使用方法1中安装httpd的方法安装httpd</p>\n<p>在配置文件 /etc/httpd/conf/httpd.conf的最后添加</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;Directory &quot;/home/daniel/software&quot;&gt;</span><br><span class=\"line\">Options Indexes FollowSymLinks</span><br><span class=\"line\">  allowOverride None</span><br><span class=\"line\">  &lt;/Directory&gt;</span><br><span class=\"line\">  &lt;VirtualHost tiger:8050&gt;</span><br><span class=\"line\">      DocumentRoot &quot;/home/daniel/software/cloudera-cm5&quot;</span><br><span class=\"line\">      ServerName tiger:8050</span><br><span class=\"line\">      &lt;/VirtualHost&gt;</span><br><span class=\"line\">      &lt;VirtualHost *:8000&gt;</span><br><span class=\"line\">          DocumentRoot /home/daniel/software/cloudera-cdh</span><br><span class=\"line\">\t      ServerName tiger:8000</span><br><span class=\"line\">\t      &lt;/VirtualHost&gt;</span><br></pre></td></tr></table></figure>\n<p>并将监听的端口从80修改位8000和8050</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#Listen 80</span><br><span class=\"line\">Listen 8000</span><br><span class=\"line\">Listen 8050</span><br></pre></td></tr></table></figure>\n<p>将欢迎页面关闭掉。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger ~]$ sudo vim /etc/httpd/conf.d/welcome.conf</span><br></pre></td></tr></table></figure>\n<p>注释掉里面的内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#&lt;LocationMatch &quot;^/+$&quot;&gt;</span><br><span class=\"line\">#    Options -Indexes</span><br><span class=\"line\">#    ErrorDocument 403 /error/noindex.html</span><br><span class=\"line\">#&lt;/LocationMatch&gt;</span><br></pre></td></tr></table></figure>\n<p>另外需要注意对于文件所在的文件夹对other用户都有r和x的权限。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger ~]$ ll software/</span><br><span class=\"line\">total 8</span><br><span class=\"line\">drwxrwxr-x. 2 daniel daniel 4096 Jan  6 05:19 cloudera-cdh</span><br><span class=\"line\">drwxrwxr-x. 3 daniel daniel 4096 Jan  6 14:47 cloudera-cm5</span><br><span class=\"line\">[daniel@tiger ~]$</span><br></pre></td></tr></table></figure>\n<p>配置完成后，重启httpd服务，并使用浏览器或者curl检查配置的结果。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl tiger:8000</span><br><span class=\"line\">curl tiger:8050</span><br></pre></td></tr></table></figure>\n<p>###Perform OS-level configuration for Hadoop installation</p>\n<p><strong>关闭防火墙</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo service iptables stop</span><br><span class=\"line\">sudo chkconfig iptables off</span><br></pre></td></tr></table></figure>\n<p><strong>关闭 SELinux</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo vim /etc/sysconfig/selinux</span><br></pre></td></tr></table></figure>\n<p>将其中的 SELINUX设置为disabled, SELINUXTYPE 设置为targeted</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># This file controls the state of SELinux on the system.</span><br><span class=\"line\"># SELINUX= can take one of these three values:</span><br><span class=\"line\">#     enforcing - SELinux security policy is enforced.</span><br><span class=\"line\">#     permissive - SELinux prints warnings instead of enforcing.</span><br><span class=\"line\">#     disabled - No SELinux policy is loaded.</span><br><span class=\"line\">SELINUX=disabled</span><br><span class=\"line\"># SELINUXTYPE= can take one of these two values:</span><br><span class=\"line\">#     targeted - Targeted processes are protected,</span><br><span class=\"line\">#     mls - Multi Level Security protection.</span><br><span class=\"line\">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure>\n<p><strong>关闭透明大页</strong></p>\n<p>因为透明大页（Transparent HugePages ） 存在一些问题：</p>\n<p>a.在RAC环境下 透明大页（Transparent HugePages ）会导致异常节点重启，和性能问题；<br>b.在单机环境中，透明大页（Transparent HugePages ） 也会导致一些异常的性能问题；</p>\n<p>将下面2个命令执行，并写入 /etc/rc.local 文件里</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag</span><br><span class=\"line\">sudo echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@lion ~]$ cat /etc/rc.local</span><br><span class=\"line\">#!/bin/sh</span><br><span class=\"line\">#</span><br><span class=\"line\"># This script will be executed *after* all the other init scripts.</span><br><span class=\"line\"># You can put your own initialization stuff in here if you don&apos;t</span><br><span class=\"line\"># want to do the full Sys V style init stuff.</span><br><span class=\"line\"></span><br><span class=\"line\">touch /var/lock/subsys/local</span><br><span class=\"line\">echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag</span><br><span class=\"line\">echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/enabled</span><br><span class=\"line\">[training@lion ~]$</span><br></pre></td></tr></table></figure>\n<p><strong>修改使用SWAP分区的优先级</strong><br>在文件 /etc/sysctl.conf 里添加 vm.swappiness = 1</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@lion ~]$ vim /etc/sysctl.conf</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vm.swappiness = 1</span><br></pre></td></tr></table></figure>\n<p>linux 会使用硬盘的一部分做为SWAP分区，用来进行进程调度–进程是正在运行的程序–把当前不用的进程调成‘等待（standby）‘，甚至‘睡眠（sleep）’，一旦要用，再调成‘活动（active）’，睡眠的进程就躺到SWAP分区睡大觉，把内存空出来让给‘活动’的进程。<br>　　如果内存够大，应当告诉 linux 不必太多的使用 SWAP 分区， 可以通过修改 swappiness 的数值。swappiness=0的时候表示最大限度使用物理内存，然后才是 swap空间，swappiness＝100的时候表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间里面。\n                                                        　　</p>\n<p>###Install Cloudera Manager server and agents</p>\n<h4 id=\"安装Cloudera-Manager-Server\"><a href=\"#安装Cloudera-Manager-Server\" class=\"headerlink\" title=\"安装Cloudera Manager Server\"></a>安装Cloudera Manager Server</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger ~]$ sudo yum install -y cloudera-manager-daemons cloudera-manager-server</span><br></pre></td></tr></table></figure>\n<h4 id=\"配置使用数据库\"><a href=\"#配置使用数据库\" class=\"headerlink\" title=\"配置使用数据库\"></a>配置使用数据库</h4><p>执行如下命令：使Cloudera Manager使用配置好的MySQL数据库</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[daniel@tiger lib]$ sudo /usr/share/cmf/schema/scm_prepare_database.sh mysql cmserver cmserveruser password</span><br><span class=\"line\">JAVA_HOME=/usr/lib/java</span><br><span class=\"line\">Verifying that we can write to /etc/cloudera-scm-server</span><br><span class=\"line\">Creating SCM configuration file in /etc/cloudera-scm-server</span><br><span class=\"line\">Executing:  /usr/lib/java/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/cmf/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.</span><br><span class=\"line\">[                          main] DbCommandExecutor              INFO  Successfully connected to database.</span><br><span class=\"line\">All done, your SCM database is configured correctly!</span><br></pre></td></tr></table></figure>\n<p>等待1-2分钟，使用浏览器打开 <a href=\"http://localhost:7180\" target=\"_blank\" rel=\"noopener\">http://localhost:7180</a></p>\n<p>###Install CDH using Cloudera Manager<br>安装Cloudear Manager Server</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@lion training]$ sudo yum install -y cloudera-manager-daemons  cloudera-manager-server</span><br></pre></td></tr></table></figure>\n<p>配置Cloudear Manager所使用的数据库</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@lion training]$ sudo /usr/share/cmf/schema/scm_prepare_database.sh mysql cmserver cmserveruser password</span><br></pre></td></tr></table></figure>\n<p>启动Cloudera Manager， 并使其开机启动,并坚持其状态（TODO：解释1-6）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@lion training]$ sudo service cloudera-scm-server start</span><br><span class=\"line\">Starting cloudera-scm-server:                              [  OK  ]</span><br><span class=\"line\">[training@lion training]$ sudo chkconfig cloudera-scm-server on</span><br><span class=\"line\">[training@lion training]$ sudo chkconfig --list | grep cloudera-scm-server</span><br><span class=\"line\">cloudera-scm-server  0:off  1:off  2:on  3:on  4:on  5:on  6:off</span><br><span class=\"line\">[training@lion training]$</span><br></pre></td></tr></table></figure>\n<p>###Add a new node to an existing cluster</p>\n<p>HOME -&gt; Hosts -&gt; All Hosts -&gt; Add New Hosts to Cluster 当前只有4个节点 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode1.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode2.png\" alt=\"\"></p>\n<p>搜索节点 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode3.png\" alt=\"\"></p>\n<p>选择节点</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode4.png\" alt=\"\"></p>\n<p>设置本地Cloudera Manager所在的http服务</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode5.png\" alt=\"\"></p>\n<p>选择用户并填写密码</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode6.png\" alt=\"\"></p>\n<p>添加Host到集群</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode7.png\" alt=\"\"></p>\n<p>成功添加Host到集群</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode8.png\" alt=\"\"></p>\n<p>分配并激活Parcels</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode9.png\" alt=\"\"></p>\n<p>成功激活Parcels</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode10.png\" alt=\"\"></p>\n<p>检测服务器</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode11.png\" alt=\"\"></p>\n<p>使用模板或者使用默认设置，使用模板可以在添加节点的时候添加和模板一样的服务到新的节点上</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode12.png\" alt=\"\"></p>\n<p>添加成功</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode13.png\" alt=\"\"></p>\n<p>查看新节点状态</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/AddNewNode/AddNewNode14.png\" alt=\"\"></p>\n<p>###Add a service using Cloudera Manager</p>\n<p><strong>添加Hive 服务</strong></p>\n<p>在集群的Home页，选择下拉框，Add Service<br>选择要添加的服务<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%201.png\" alt=\"第一步\"></p>\n<p>选择服务的依赖</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%202.png\" alt=\"第二步\"></p>\n<p>选择需要添加的角色</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%203.png\" alt=\"第三步\"></p>\n<p>选择要使用的数据库</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%204.png\" alt=\"第四步\"></p>\n<p>查看也可修改相关配置</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%205.png\" alt=\"第五步\"></p>\n<p>初始化和启动服务</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%206.png\" alt=\"第六步\"></p>\n<p>服务启动完成</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%207.png\" alt=\"第七步\"></p>\n<p>添加成功</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Hive/Add%20New%20Hive%20Service%208.png\" alt=\"第八步\"></p>"},{"title":"CCAH-131 Secure","date":"2018-05-13T00:36:26.000Z","_content":"# Secure\n\nEnable relevant services and configure the cluster to meet goals defined by security policy; demonstrate knowledge of basic security practices\n\n## Configure HDFS ACLs\n[HDFS Extended ACLs](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_sg_hdfs_ext_acls.html)\n\nHOME -> Cluster -> HDFS -Configuration 搜索ACL，勾选启用 Access Control Lists\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HDFSAcl1.png)\n\n重启依赖的服务 \n\n<!-- more -->\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HDFSAcl2.png)\n\n## Install and configure Sentry\nHOME -> Cluster -> Add Services \n\n选择Sentry\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%201.png)\n\n选择服务安装的服务器\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%202.png)\n\n选择数据库\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%203.png)\n\n安装配置并重启\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%204.png)\n\n重启完成\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%205.png)\n\n安装成功\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%206.png)\n\nHive 启用 Sentry ，在Configuration配置选择中，勾选Sentry Services\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%207.png)\n\nImpala 启用 Sentry，在Configuration配置选择中，勾选Sentry Services\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%208.png)\n## Configure Hue user authorization and authentication\n\n\n## Enable/configure log and query redaction\n[Log and Query Redaction](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/sg_redaction.html#concept_i2b_zt2_5y)\n\n[Using Cloudera Navigator Data Management for Data Redaction](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/sg_redaction.html#concept_aym_vw3_fr)\n\nHOME -> Cluster -> HDFS -> Configuration 搜索redaction\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Log%20and%20Query%20Redaction%201.png)\n\n选择要添加的规则并测试\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Log%20and%20Query%20Redaction%202.png)\n\n## Create encrypted zones in HDFS\n\nCreate an encryption key for your zone as the application user that will be using the key. For example, if you are creating an encryption zone for HBase, create the key as the hbase user as follows:\n\n```\n$ sudo -u hbase hadoop key create <key_name>\n```\nCreate a new empty directory and make it an encryption zone using the key created above.\n\n```\n$ sudo -u hdfs hadoop fs -mkdir /encryption_zone\n$ sudo -u hdfs hdfs crypto -createZone -keyName <key_name> -path /encryption_zone\n```\n\nYou can verify creation of the new encryption zone by running the -listZones command. You should see the encryption zone along with its key listed as follows:\n\n```\n$ sudo -u hdfs hdfs crypto -listZones\n/encryption_zone    <key_name>\n```\nWarning: Do not delete an encryption key as long as it is still in use for an encryption zone. This results in loss of access to data in that zone.\n\n\n\n###Validate Data Encryption\nLogin or su to these users on one of the hosts in your cluster. These directions will help to verify KMS is setup to encrypt files.\n\nCreate a key and directory.\n\n```\nsu <KEY_ADMIN_USER>\nhadoop key create mykey1\nhadoop fs -mkdir /tmp/zone1\n```\n```\n[training@elephant ~]$ hadoop key create mykey1\nmykey1 has been successfully created with options Options{cipher='AES/CTR/NoPadding', bitLength=128, description='null', attributes=null}.\nKMSClientProvider[http://horse:16000/kms/v1/] has been updated.\n[training@elephant ~]$ hadoop fs -mkdir /tmp/zone1\n```\n\n\nCreate a zone and link to the key.\n\n```\nsu hdfs\nhdfs crypto -createZone -keyName mykey1 -path /tmp/zone1\n```\n\n```\n[training@elephant ~]$ sudo su - hdfs\n-bash-4.1$ hdfs crypto -createZone -keyName mykey1 -path /tmp/zone1\nAdded encryption zone /tmp/zone1\n-bash-4.1$\n```\n\nCreate a file, put it in your zone and ensure the file can be decrypted.\n\n```\nsu <KEY_ADMIN_USER>\necho \"Hello World\" > /tmp/helloWorld.txt\nhadoop fs -put /tmp/helloWorld.txt /tmp/zone1\nhadoop fs -cat /tmp/zone1/helloWorld.txt\nrm /tmp/helloWorld.txt\n```\n\n```\n[training@elephant ~]$ echo \"Hello World\" > /tmp/helloWorld.txt\n[training@elephant ~]$ hadoop fs -put /tmp/helloWorld.txt /tmp/zone1\n[training@elephant ~]$ hadoop fs -cat /tmp/zone1/helloWorld.txt\nHello World\n[training@elephant ~]$ rm /tmp/helloWorld.txt\n[training@elephant ~]$\n```\n\nEnsure the file is stored as encrypted.\n\n```\nsu hdfs\nhadoop fs -cat /.reserved/raw/tmp/zone1/helloWorld.txt\nhadoop fs -rm -R /tmp/zone1\n```\n```\n[training@elephant ~]$ hadoop fs -cat /.reserved/raw/tmp/zone1/helloWorld.txt\ncat: Access denied for user training. Superuser privilege is required\n[training@elephant ~]$\n[training@elephant ~]$ sudo -u hdfs hadoop fs -cat /.reserved/raw/tmp/zone1/helloWorld.txt\n�@*�@��\"��[training@elephant ~]$ hadoop fs -rm -R /tmp/zone1\n18/01/10 20:13:22 INFO fs.TrashPolicyDefault: Moved: 'hdfs://mycluster/tmp/zone1' to trash at: hdfs://mycluster/user/training/.Trash/Current/tmp/zone1\n[training@elephant ~]$\n\n```\n\nBy default, non-admin users cannot access any encrypted data. You must create appropriate ACLs before users can access encrypted data. See the Cloudera documentation for more information on managing KMS ACLs.\n\nModify the Advanced Configuration Snippet for ACL file: kms-acls.xml\n\n\n[Integrating Key HSM with Key Trustee Server](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/key_hsm_key_trustee.html#concept_key_hsm_key_trustee)\n\n[Installing Cloudera Navigator Key HSM](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/key_hsm_install.html#xd_583c10bfdbd326ba-590cb1d1-149e9ca9886--7a1c)\n\n[Configuring CDH Services for HDFS Encryption](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_sg_component_kms.html#xd_583c10bfdbd326ba-7dae4aa6-147c30d0933--7ba5)\n","source":"_posts/CCAH-131-Secure.md","raw":"---\ntitle: CCAH-131 Secure\ndate: 2018-05-13 08:36:26\ntags: CCAH-131\n---\n# Secure\n\nEnable relevant services and configure the cluster to meet goals defined by security policy; demonstrate knowledge of basic security practices\n\n## Configure HDFS ACLs\n[HDFS Extended ACLs](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_sg_hdfs_ext_acls.html)\n\nHOME -> Cluster -> HDFS -Configuration 搜索ACL，勾选启用 Access Control Lists\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HDFSAcl1.png)\n\n重启依赖的服务 \n\n<!-- more -->\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HDFSAcl2.png)\n\n## Install and configure Sentry\nHOME -> Cluster -> Add Services \n\n选择Sentry\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%201.png)\n\n选择服务安装的服务器\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%202.png)\n\n选择数据库\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%203.png)\n\n安装配置并重启\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%204.png)\n\n重启完成\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%205.png)\n\n安装成功\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%206.png)\n\nHive 启用 Sentry ，在Configuration配置选择中，勾选Sentry Services\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%207.png)\n\nImpala 启用 Sentry，在Configuration配置选择中，勾选Sentry Services\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%208.png)\n## Configure Hue user authorization and authentication\n\n\n## Enable/configure log and query redaction\n[Log and Query Redaction](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/sg_redaction.html#concept_i2b_zt2_5y)\n\n[Using Cloudera Navigator Data Management for Data Redaction](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/sg_redaction.html#concept_aym_vw3_fr)\n\nHOME -> Cluster -> HDFS -> Configuration 搜索redaction\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Log%20and%20Query%20Redaction%201.png)\n\n选择要添加的规则并测试\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Log%20and%20Query%20Redaction%202.png)\n\n## Create encrypted zones in HDFS\n\nCreate an encryption key for your zone as the application user that will be using the key. For example, if you are creating an encryption zone for HBase, create the key as the hbase user as follows:\n\n```\n$ sudo -u hbase hadoop key create <key_name>\n```\nCreate a new empty directory and make it an encryption zone using the key created above.\n\n```\n$ sudo -u hdfs hadoop fs -mkdir /encryption_zone\n$ sudo -u hdfs hdfs crypto -createZone -keyName <key_name> -path /encryption_zone\n```\n\nYou can verify creation of the new encryption zone by running the -listZones command. You should see the encryption zone along with its key listed as follows:\n\n```\n$ sudo -u hdfs hdfs crypto -listZones\n/encryption_zone    <key_name>\n```\nWarning: Do not delete an encryption key as long as it is still in use for an encryption zone. This results in loss of access to data in that zone.\n\n\n\n###Validate Data Encryption\nLogin or su to these users on one of the hosts in your cluster. These directions will help to verify KMS is setup to encrypt files.\n\nCreate a key and directory.\n\n```\nsu <KEY_ADMIN_USER>\nhadoop key create mykey1\nhadoop fs -mkdir /tmp/zone1\n```\n```\n[training@elephant ~]$ hadoop key create mykey1\nmykey1 has been successfully created with options Options{cipher='AES/CTR/NoPadding', bitLength=128, description='null', attributes=null}.\nKMSClientProvider[http://horse:16000/kms/v1/] has been updated.\n[training@elephant ~]$ hadoop fs -mkdir /tmp/zone1\n```\n\n\nCreate a zone and link to the key.\n\n```\nsu hdfs\nhdfs crypto -createZone -keyName mykey1 -path /tmp/zone1\n```\n\n```\n[training@elephant ~]$ sudo su - hdfs\n-bash-4.1$ hdfs crypto -createZone -keyName mykey1 -path /tmp/zone1\nAdded encryption zone /tmp/zone1\n-bash-4.1$\n```\n\nCreate a file, put it in your zone and ensure the file can be decrypted.\n\n```\nsu <KEY_ADMIN_USER>\necho \"Hello World\" > /tmp/helloWorld.txt\nhadoop fs -put /tmp/helloWorld.txt /tmp/zone1\nhadoop fs -cat /tmp/zone1/helloWorld.txt\nrm /tmp/helloWorld.txt\n```\n\n```\n[training@elephant ~]$ echo \"Hello World\" > /tmp/helloWorld.txt\n[training@elephant ~]$ hadoop fs -put /tmp/helloWorld.txt /tmp/zone1\n[training@elephant ~]$ hadoop fs -cat /tmp/zone1/helloWorld.txt\nHello World\n[training@elephant ~]$ rm /tmp/helloWorld.txt\n[training@elephant ~]$\n```\n\nEnsure the file is stored as encrypted.\n\n```\nsu hdfs\nhadoop fs -cat /.reserved/raw/tmp/zone1/helloWorld.txt\nhadoop fs -rm -R /tmp/zone1\n```\n```\n[training@elephant ~]$ hadoop fs -cat /.reserved/raw/tmp/zone1/helloWorld.txt\ncat: Access denied for user training. Superuser privilege is required\n[training@elephant ~]$\n[training@elephant ~]$ sudo -u hdfs hadoop fs -cat /.reserved/raw/tmp/zone1/helloWorld.txt\n�@*�@��\"��[training@elephant ~]$ hadoop fs -rm -R /tmp/zone1\n18/01/10 20:13:22 INFO fs.TrashPolicyDefault: Moved: 'hdfs://mycluster/tmp/zone1' to trash at: hdfs://mycluster/user/training/.Trash/Current/tmp/zone1\n[training@elephant ~]$\n\n```\n\nBy default, non-admin users cannot access any encrypted data. You must create appropriate ACLs before users can access encrypted data. See the Cloudera documentation for more information on managing KMS ACLs.\n\nModify the Advanced Configuration Snippet for ACL file: kms-acls.xml\n\n\n[Integrating Key HSM with Key Trustee Server](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/key_hsm_key_trustee.html#concept_key_hsm_key_trustee)\n\n[Installing Cloudera Navigator Key HSM](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/key_hsm_install.html#xd_583c10bfdbd326ba-590cb1d1-149e9ca9886--7a1c)\n\n[Configuring CDH Services for HDFS Encryption](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_sg_component_kms.html#xd_583c10bfdbd326ba-7dae4aa6-147c30d0933--7ba5)\n","slug":"CCAH-131-Secure","published":1,"updated":"2018-09-14T01:19:33.192Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnza0005inamxd9fq06d","content":"<h1 id=\"Secure\"><a href=\"#Secure\" class=\"headerlink\" title=\"Secure\"></a>Secure</h1><p>Enable relevant services and configure the cluster to meet goals defined by security policy; demonstrate knowledge of basic security practices</p>\n<h2 id=\"Configure-HDFS-ACLs\"><a href=\"#Configure-HDFS-ACLs\" class=\"headerlink\" title=\"Configure HDFS ACLs\"></a>Configure HDFS ACLs</h2><p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_sg_hdfs_ext_acls.html\" target=\"_blank\" rel=\"noopener\">HDFS Extended ACLs</a></p>\n<p>HOME -&gt; Cluster -&gt; HDFS -Configuration 搜索ACL，勾选启用 Access Control Lists</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HDFSAcl1.png\" alt=\"\"></p>\n<p>重启依赖的服务 </p>\n<a id=\"more\"></a>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HDFSAcl2.png\" alt=\"\"></p>\n<h2 id=\"Install-and-configure-Sentry\"><a href=\"#Install-and-configure-Sentry\" class=\"headerlink\" title=\"Install and configure Sentry\"></a>Install and configure Sentry</h2><p>HOME -&gt; Cluster -&gt; Add Services </p>\n<p>选择Sentry</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%201.png\" alt=\"\"></p>\n<p>选择服务安装的服务器</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%202.png\" alt=\"\"></p>\n<p>选择数据库</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%203.png\" alt=\"\"></p>\n<p>安装配置并重启</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%204.png\" alt=\"\"></p>\n<p>重启完成</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%205.png\" alt=\"\"></p>\n<p>安装成功</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%206.png\" alt=\"\"></p>\n<p>Hive 启用 Sentry ，在Configuration配置选择中，勾选Sentry Services</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%207.png\" alt=\"\"></p>\n<p>Impala 启用 Sentry，在Configuration配置选择中，勾选Sentry Services</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%208.png\" alt=\"\"></p>\n<h2 id=\"Configure-Hue-user-authorization-and-authentication\"><a href=\"#Configure-Hue-user-authorization-and-authentication\" class=\"headerlink\" title=\"Configure Hue user authorization and authentication\"></a>Configure Hue user authorization and authentication</h2><h2 id=\"Enable-configure-log-and-query-redaction\"><a href=\"#Enable-configure-log-and-query-redaction\" class=\"headerlink\" title=\"Enable/configure log and query redaction\"></a>Enable/configure log and query redaction</h2><p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/sg_redaction.html#concept_i2b_zt2_5y\" target=\"_blank\" rel=\"noopener\">Log and Query Redaction</a></p>\n<p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/sg_redaction.html#concept_aym_vw3_fr\" target=\"_blank\" rel=\"noopener\">Using Cloudera Navigator Data Management for Data Redaction</a></p>\n<p>HOME -&gt; Cluster -&gt; HDFS -&gt; Configuration 搜索redaction</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Log%20and%20Query%20Redaction%201.png\" alt=\"\"></p>\n<p>选择要添加的规则并测试</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Log%20and%20Query%20Redaction%202.png\" alt=\"\"></p>\n<h2 id=\"Create-encrypted-zones-in-HDFS\"><a href=\"#Create-encrypted-zones-in-HDFS\" class=\"headerlink\" title=\"Create encrypted zones in HDFS\"></a>Create encrypted zones in HDFS</h2><p>Create an encryption key for your zone as the application user that will be using the key. For example, if you are creating an encryption zone for HBase, create the key as the hbase user as follows:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo -u hbase hadoop key create &lt;key_name&gt;</span><br></pre></td></tr></table></figure>\n<p>Create a new empty directory and make it an encryption zone using the key created above.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo -u hdfs hadoop fs -mkdir /encryption_zone</span><br><span class=\"line\">$ sudo -u hdfs hdfs crypto -createZone -keyName &lt;key_name&gt; -path /encryption_zone</span><br></pre></td></tr></table></figure>\n<p>You can verify creation of the new encryption zone by running the -listZones command. You should see the encryption zone along with its key listed as follows:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo -u hdfs hdfs crypto -listZones</span><br><span class=\"line\">/encryption_zone    &lt;key_name&gt;</span><br></pre></td></tr></table></figure>\n<p>Warning: Do not delete an encryption key as long as it is still in use for an encryption zone. This results in loss of access to data in that zone.</p>\n<p>###Validate Data Encryption<br>Login or su to these users on one of the hosts in your cluster. These directions will help to verify KMS is setup to encrypt files.</p>\n<p>Create a key and directory.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">su &lt;KEY_ADMIN_USER&gt;</span><br><span class=\"line\">hadoop key create mykey1</span><br><span class=\"line\">hadoop fs -mkdir /tmp/zone1</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ hadoop key create mykey1</span><br><span class=\"line\">mykey1 has been successfully created with options Options&#123;cipher=&apos;AES/CTR/NoPadding&apos;, bitLength=128, description=&apos;null&apos;, attributes=null&#125;.</span><br><span class=\"line\">KMSClientProvider[http://horse:16000/kms/v1/] has been updated.</span><br><span class=\"line\">[training@elephant ~]$ hadoop fs -mkdir /tmp/zone1</span><br></pre></td></tr></table></figure>\n<p>Create a zone and link to the key.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">su hdfs</span><br><span class=\"line\">hdfs crypto -createZone -keyName mykey1 -path /tmp/zone1</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ sudo su - hdfs</span><br><span class=\"line\">-bash-4.1$ hdfs crypto -createZone -keyName mykey1 -path /tmp/zone1</span><br><span class=\"line\">Added encryption zone /tmp/zone1</span><br><span class=\"line\">-bash-4.1$</span><br></pre></td></tr></table></figure>\n<p>Create a file, put it in your zone and ensure the file can be decrypted.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">su &lt;KEY_ADMIN_USER&gt;</span><br><span class=\"line\">echo &quot;Hello World&quot; &gt; /tmp/helloWorld.txt</span><br><span class=\"line\">hadoop fs -put /tmp/helloWorld.txt /tmp/zone1</span><br><span class=\"line\">hadoop fs -cat /tmp/zone1/helloWorld.txt</span><br><span class=\"line\">rm /tmp/helloWorld.txt</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ echo &quot;Hello World&quot; &gt; /tmp/helloWorld.txt</span><br><span class=\"line\">[training@elephant ~]$ hadoop fs -put /tmp/helloWorld.txt /tmp/zone1</span><br><span class=\"line\">[training@elephant ~]$ hadoop fs -cat /tmp/zone1/helloWorld.txt</span><br><span class=\"line\">Hello World</span><br><span class=\"line\">[training@elephant ~]$ rm /tmp/helloWorld.txt</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<p>Ensure the file is stored as encrypted.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">su hdfs</span><br><span class=\"line\">hadoop fs -cat /.reserved/raw/tmp/zone1/helloWorld.txt</span><br><span class=\"line\">hadoop fs -rm -R /tmp/zone1</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ hadoop fs -cat /.reserved/raw/tmp/zone1/helloWorld.txt</span><br><span class=\"line\">cat: Access denied for user training. Superuser privilege is required</span><br><span class=\"line\">[training@elephant ~]$</span><br><span class=\"line\">[training@elephant ~]$ sudo -u hdfs hadoop fs -cat /.reserved/raw/tmp/zone1/helloWorld.txt</span><br><span class=\"line\">�@*�@��&quot;��[training@elephant ~]$ hadoop fs -rm -R /tmp/zone1</span><br><span class=\"line\">18/01/10 20:13:22 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://mycluster/tmp/zone1&apos; to trash at: hdfs://mycluster/user/training/.Trash/Current/tmp/zone1</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<p>By default, non-admin users cannot access any encrypted data. You must create appropriate ACLs before users can access encrypted data. See the Cloudera documentation for more information on managing KMS ACLs.</p>\n<p>Modify the Advanced Configuration Snippet for ACL file: kms-acls.xml</p>\n<p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/key_hsm_key_trustee.html#concept_key_hsm_key_trustee\" target=\"_blank\" rel=\"noopener\">Integrating Key HSM with Key Trustee Server</a></p>\n<p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/key_hsm_install.html#xd_583c10bfdbd326ba-590cb1d1-149e9ca9886--7a1c\" target=\"_blank\" rel=\"noopener\">Installing Cloudera Navigator Key HSM</a></p>\n<p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_sg_component_kms.html#xd_583c10bfdbd326ba-7dae4aa6-147c30d0933--7ba5\" target=\"_blank\" rel=\"noopener\">Configuring CDH Services for HDFS Encryption</a></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"Secure\"><a href=\"#Secure\" class=\"headerlink\" title=\"Secure\"></a>Secure</h1><p>Enable relevant services and configure the cluster to meet goals defined by security policy; demonstrate knowledge of basic security practices</p>\n<h2 id=\"Configure-HDFS-ACLs\"><a href=\"#Configure-HDFS-ACLs\" class=\"headerlink\" title=\"Configure HDFS ACLs\"></a>Configure HDFS ACLs</h2><p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_sg_hdfs_ext_acls.html\" target=\"_blank\" rel=\"noopener\">HDFS Extended ACLs</a></p>\n<p>HOME -&gt; Cluster -&gt; HDFS -Configuration 搜索ACL，勾选启用 Access Control Lists</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HDFSAcl1.png\" alt=\"\"></p>\n<p>重启依赖的服务 </p>","more":"<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/HDFSAcl2.png\" alt=\"\"></p>\n<h2 id=\"Install-and-configure-Sentry\"><a href=\"#Install-and-configure-Sentry\" class=\"headerlink\" title=\"Install and configure Sentry\"></a>Install and configure Sentry</h2><p>HOME -&gt; Cluster -&gt; Add Services </p>\n<p>选择Sentry</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%201.png\" alt=\"\"></p>\n<p>选择服务安装的服务器</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%202.png\" alt=\"\"></p>\n<p>选择数据库</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%203.png\" alt=\"\"></p>\n<p>安装配置并重启</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%204.png\" alt=\"\"></p>\n<p>重启完成</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%205.png\" alt=\"\"></p>\n<p>安装成功</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%206.png\" alt=\"\"></p>\n<p>Hive 启用 Sentry ，在Configuration配置选择中，勾选Sentry Services</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%207.png\" alt=\"\"></p>\n<p>Impala 启用 Sentry，在Configuration配置选择中，勾选Sentry Services</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20New%20Service/Add%20Sentry/Add%20Sentry%20Service%208.png\" alt=\"\"></p>\n<h2 id=\"Configure-Hue-user-authorization-and-authentication\"><a href=\"#Configure-Hue-user-authorization-and-authentication\" class=\"headerlink\" title=\"Configure Hue user authorization and authentication\"></a>Configure Hue user authorization and authentication</h2><h2 id=\"Enable-configure-log-and-query-redaction\"><a href=\"#Enable-configure-log-and-query-redaction\" class=\"headerlink\" title=\"Enable/configure log and query redaction\"></a>Enable/configure log and query redaction</h2><p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/sg_redaction.html#concept_i2b_zt2_5y\" target=\"_blank\" rel=\"noopener\">Log and Query Redaction</a></p>\n<p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/sg_redaction.html#concept_aym_vw3_fr\" target=\"_blank\" rel=\"noopener\">Using Cloudera Navigator Data Management for Data Redaction</a></p>\n<p>HOME -&gt; Cluster -&gt; HDFS -&gt; Configuration 搜索redaction</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Log%20and%20Query%20Redaction%201.png\" alt=\"\"></p>\n<p>选择要添加的规则并测试</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Log%20and%20Query%20Redaction%202.png\" alt=\"\"></p>\n<h2 id=\"Create-encrypted-zones-in-HDFS\"><a href=\"#Create-encrypted-zones-in-HDFS\" class=\"headerlink\" title=\"Create encrypted zones in HDFS\"></a>Create encrypted zones in HDFS</h2><p>Create an encryption key for your zone as the application user that will be using the key. For example, if you are creating an encryption zone for HBase, create the key as the hbase user as follows:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo -u hbase hadoop key create &lt;key_name&gt;</span><br></pre></td></tr></table></figure>\n<p>Create a new empty directory and make it an encryption zone using the key created above.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo -u hdfs hadoop fs -mkdir /encryption_zone</span><br><span class=\"line\">$ sudo -u hdfs hdfs crypto -createZone -keyName &lt;key_name&gt; -path /encryption_zone</span><br></pre></td></tr></table></figure>\n<p>You can verify creation of the new encryption zone by running the -listZones command. You should see the encryption zone along with its key listed as follows:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo -u hdfs hdfs crypto -listZones</span><br><span class=\"line\">/encryption_zone    &lt;key_name&gt;</span><br></pre></td></tr></table></figure>\n<p>Warning: Do not delete an encryption key as long as it is still in use for an encryption zone. This results in loss of access to data in that zone.</p>\n<p>###Validate Data Encryption<br>Login or su to these users on one of the hosts in your cluster. These directions will help to verify KMS is setup to encrypt files.</p>\n<p>Create a key and directory.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">su &lt;KEY_ADMIN_USER&gt;</span><br><span class=\"line\">hadoop key create mykey1</span><br><span class=\"line\">hadoop fs -mkdir /tmp/zone1</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ hadoop key create mykey1</span><br><span class=\"line\">mykey1 has been successfully created with options Options&#123;cipher=&apos;AES/CTR/NoPadding&apos;, bitLength=128, description=&apos;null&apos;, attributes=null&#125;.</span><br><span class=\"line\">KMSClientProvider[http://horse:16000/kms/v1/] has been updated.</span><br><span class=\"line\">[training@elephant ~]$ hadoop fs -mkdir /tmp/zone1</span><br></pre></td></tr></table></figure>\n<p>Create a zone and link to the key.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">su hdfs</span><br><span class=\"line\">hdfs crypto -createZone -keyName mykey1 -path /tmp/zone1</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ sudo su - hdfs</span><br><span class=\"line\">-bash-4.1$ hdfs crypto -createZone -keyName mykey1 -path /tmp/zone1</span><br><span class=\"line\">Added encryption zone /tmp/zone1</span><br><span class=\"line\">-bash-4.1$</span><br></pre></td></tr></table></figure>\n<p>Create a file, put it in your zone and ensure the file can be decrypted.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">su &lt;KEY_ADMIN_USER&gt;</span><br><span class=\"line\">echo &quot;Hello World&quot; &gt; /tmp/helloWorld.txt</span><br><span class=\"line\">hadoop fs -put /tmp/helloWorld.txt /tmp/zone1</span><br><span class=\"line\">hadoop fs -cat /tmp/zone1/helloWorld.txt</span><br><span class=\"line\">rm /tmp/helloWorld.txt</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ echo &quot;Hello World&quot; &gt; /tmp/helloWorld.txt</span><br><span class=\"line\">[training@elephant ~]$ hadoop fs -put /tmp/helloWorld.txt /tmp/zone1</span><br><span class=\"line\">[training@elephant ~]$ hadoop fs -cat /tmp/zone1/helloWorld.txt</span><br><span class=\"line\">Hello World</span><br><span class=\"line\">[training@elephant ~]$ rm /tmp/helloWorld.txt</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<p>Ensure the file is stored as encrypted.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">su hdfs</span><br><span class=\"line\">hadoop fs -cat /.reserved/raw/tmp/zone1/helloWorld.txt</span><br><span class=\"line\">hadoop fs -rm -R /tmp/zone1</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ hadoop fs -cat /.reserved/raw/tmp/zone1/helloWorld.txt</span><br><span class=\"line\">cat: Access denied for user training. Superuser privilege is required</span><br><span class=\"line\">[training@elephant ~]$</span><br><span class=\"line\">[training@elephant ~]$ sudo -u hdfs hadoop fs -cat /.reserved/raw/tmp/zone1/helloWorld.txt</span><br><span class=\"line\">�@*�@��&quot;��[training@elephant ~]$ hadoop fs -rm -R /tmp/zone1</span><br><span class=\"line\">18/01/10 20:13:22 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://mycluster/tmp/zone1&apos; to trash at: hdfs://mycluster/user/training/.Trash/Current/tmp/zone1</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<p>By default, non-admin users cannot access any encrypted data. You must create appropriate ACLs before users can access encrypted data. See the Cloudera documentation for more information on managing KMS ACLs.</p>\n<p>Modify the Advanced Configuration Snippet for ACL file: kms-acls.xml</p>\n<p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/key_hsm_key_trustee.html#concept_key_hsm_key_trustee\" target=\"_blank\" rel=\"noopener\">Integrating Key HSM with Key Trustee Server</a></p>\n<p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/key_hsm_install.html#xd_583c10bfdbd326ba-590cb1d1-149e9ca9886--7a1c\" target=\"_blank\" rel=\"noopener\">Installing Cloudera Navigator Key HSM</a></p>\n<p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_sg_component_kms.html#xd_583c10bfdbd326ba-7dae4aa6-147c30d0933--7ba5\" target=\"_blank\" rel=\"noopener\">Configuring CDH Services for HDFS Encryption</a></p>"},{"title":"CCAH-131 Required Skills （考试要求）","date":"2018-05-12T04:03:44.000Z","_content":"# Required Skills （技能）\n\n## Install （安装）\n\nDemonstrate an understanding of the installation process for Cloudera Manager, CDH, and the ecosystem projects. （熟悉Cloudera Manager、CDH和其他生态系统内的项目的安装过程）\n* Set up a local CDH repository （设置一个本地的CDH仓库）\n* Perform OS-level configuration for Hadoop installation （操作系统级别的配置准备）\n* Install Cloudera Manager server and agents （安装 Cloudera Manager 服务和代理）\n* Install CDH using Cloudera Manager （使用Cloudera Manager 安装CDH）\n* Add a new node to an existing cluster （在集群上添加一个新的节点）\n* Add a service using Cloudera Manager （使用Cloudera Manager添加一个新的服务）\n<!-- more -->\n\n## Configure （配置）\n\nPerform basic and advanced configuration needed to effectively administer a Hadoop cluster （为了高效管理集群的基础和高级配置）\n* Configure a service using Cloudera Manager（使用Cloudera Manager 配置一个集群）\n* Create an HDFS user's home directory （创建用户的HDFS home文件夹）\n* Configure NameNode HA （配置Hadoop NameNode的高可用）\n* Configure ResourceManager HA （配置Yarn的ResourceManager的高可用）\n* Configure proxy for Hiveserver2/Impala（配置HiveServer2或者Impala的代理）\n\n## Manage （管理）\n\nMaintain and modify the cluster to support day-to-day operations in the enterprise (维护或者修改集群已满足企业的日常使用）\n* Rebalance the cluster（重新平衡集群）\n* Set up alerting for excessive disk fill （设置磁盘使用预警）\n* Define and install a rack topology script （定义和安装机架拓扑）\n* Install new type of I/O compression library in cluster（在集群中安装新的读写压缩包）\n* Revise YARN resource assignment based on user feedback（根据用户的反馈配置YARN的资源）\n* Commission/decommission a node（服役/退役一个节点）\n\n## Secure （安全）\n\nEnable relevant services and configure the cluster to meet goals defined by security policy; demonstrate knowledge of basic security practices（可以根据安全要求进行服务安全配置）\n* Configure HDFS ACLs （配置HDFS的ACLs）\n* Install and configure Sentry（安装配置Sentry）\n* Configure Hue user authorization and authentication（配置Hue的用户认证）\n* Enable/configure log and query redaction（启用或者配置log和查询的修改）\n* Create encrypted zones in HDFS（在HDFS上创建加密区间）\n\n## Test （测试）\n\nBenchmark the cluster operational metrics, test system configuration for operation and efficiency（对集群的性能进行基准测试）\n* Execute file system commands via HTTPFS （通过HTTPFS执行文件系统命令）\n* Efficiently copy data within a cluster/between clusters （集群间的数据复制）\n* Create/restore a snapshot of an HDFS directory（创建或者恢复HDFS文件夹得快照）\n* Get/set ACLs for a file or directory structure（获取/设置一个文件或者文件夹的ALCs）\n* Benchmark the cluster (I/O, CPU, network)（测试集群的读写、CPU和网络性能）\n\n## Troubleshoot （定位错误）\n\nDemonstrate ability to find the root cause of a problem, optimize inefficient execution, and resolve resource contention scenarios（证明有找到问题根本错误的能力，优化慢的执行任务和解决实际问题的能了）\n* Resolve errors/warnings in Cloudera Manager （解决Cloudera Manager上的错误或者警告）\n* Resolve performance problems/errors in cluster operation（解决集群操作上性能问题）\n* Determine reason for application failure（定位应用失败的原因）\n* Configure the Fair Scheduler to resolve application delays（配置公平调度用以解决应用的提交延迟）\n","source":"_posts/CCAH-131-Required-Skills.md","raw":"---\ntitle: CCAH-131 Required Skills （考试要求）\ndate: 2018-05-12 12:03:44\ntags: \n    - CCAH-131\n---\n# Required Skills （技能）\n\n## Install （安装）\n\nDemonstrate an understanding of the installation process for Cloudera Manager, CDH, and the ecosystem projects. （熟悉Cloudera Manager、CDH和其他生态系统内的项目的安装过程）\n* Set up a local CDH repository （设置一个本地的CDH仓库）\n* Perform OS-level configuration for Hadoop installation （操作系统级别的配置准备）\n* Install Cloudera Manager server and agents （安装 Cloudera Manager 服务和代理）\n* Install CDH using Cloudera Manager （使用Cloudera Manager 安装CDH）\n* Add a new node to an existing cluster （在集群上添加一个新的节点）\n* Add a service using Cloudera Manager （使用Cloudera Manager添加一个新的服务）\n<!-- more -->\n\n## Configure （配置）\n\nPerform basic and advanced configuration needed to effectively administer a Hadoop cluster （为了高效管理集群的基础和高级配置）\n* Configure a service using Cloudera Manager（使用Cloudera Manager 配置一个集群）\n* Create an HDFS user's home directory （创建用户的HDFS home文件夹）\n* Configure NameNode HA （配置Hadoop NameNode的高可用）\n* Configure ResourceManager HA （配置Yarn的ResourceManager的高可用）\n* Configure proxy for Hiveserver2/Impala（配置HiveServer2或者Impala的代理）\n\n## Manage （管理）\n\nMaintain and modify the cluster to support day-to-day operations in the enterprise (维护或者修改集群已满足企业的日常使用）\n* Rebalance the cluster（重新平衡集群）\n* Set up alerting for excessive disk fill （设置磁盘使用预警）\n* Define and install a rack topology script （定义和安装机架拓扑）\n* Install new type of I/O compression library in cluster（在集群中安装新的读写压缩包）\n* Revise YARN resource assignment based on user feedback（根据用户的反馈配置YARN的资源）\n* Commission/decommission a node（服役/退役一个节点）\n\n## Secure （安全）\n\nEnable relevant services and configure the cluster to meet goals defined by security policy; demonstrate knowledge of basic security practices（可以根据安全要求进行服务安全配置）\n* Configure HDFS ACLs （配置HDFS的ACLs）\n* Install and configure Sentry（安装配置Sentry）\n* Configure Hue user authorization and authentication（配置Hue的用户认证）\n* Enable/configure log and query redaction（启用或者配置log和查询的修改）\n* Create encrypted zones in HDFS（在HDFS上创建加密区间）\n\n## Test （测试）\n\nBenchmark the cluster operational metrics, test system configuration for operation and efficiency（对集群的性能进行基准测试）\n* Execute file system commands via HTTPFS （通过HTTPFS执行文件系统命令）\n* Efficiently copy data within a cluster/between clusters （集群间的数据复制）\n* Create/restore a snapshot of an HDFS directory（创建或者恢复HDFS文件夹得快照）\n* Get/set ACLs for a file or directory structure（获取/设置一个文件或者文件夹的ALCs）\n* Benchmark the cluster (I/O, CPU, network)（测试集群的读写、CPU和网络性能）\n\n## Troubleshoot （定位错误）\n\nDemonstrate ability to find the root cause of a problem, optimize inefficient execution, and resolve resource contention scenarios（证明有找到问题根本错误的能力，优化慢的执行任务和解决实际问题的能了）\n* Resolve errors/warnings in Cloudera Manager （解决Cloudera Manager上的错误或者警告）\n* Resolve performance problems/errors in cluster operation（解决集群操作上性能问题）\n* Determine reason for application failure（定位应用失败的原因）\n* Configure the Fair Scheduler to resolve application delays（配置公平调度用以解决应用的提交延迟）\n","slug":"CCAH-131-Required-Skills","published":1,"updated":"2018-09-14T01:16:34.443Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzc0006inamdehsh4hp","content":"<h1 id=\"Required-Skills-（技能）\"><a href=\"#Required-Skills-（技能）\" class=\"headerlink\" title=\"Required Skills （技能）\"></a>Required Skills （技能）</h1><h2 id=\"Install-（安装）\"><a href=\"#Install-（安装）\" class=\"headerlink\" title=\"Install （安装）\"></a>Install （安装）</h2><p>Demonstrate an understanding of the installation process for Cloudera Manager, CDH, and the ecosystem projects. （熟悉Cloudera Manager、CDH和其他生态系统内的项目的安装过程）</p>\n<ul>\n<li>Set up a local CDH repository （设置一个本地的CDH仓库）</li>\n<li>Perform OS-level configuration for Hadoop installation （操作系统级别的配置准备）</li>\n<li>Install Cloudera Manager server and agents （安装 Cloudera Manager 服务和代理）</li>\n<li>Install CDH using Cloudera Manager （使用Cloudera Manager 安装CDH）</li>\n<li>Add a new node to an existing cluster （在集群上添加一个新的节点）</li>\n<li>Add a service using Cloudera Manager （使用Cloudera Manager添加一个新的服务）<a id=\"more\"></a>\n</li>\n</ul>\n<h2 id=\"Configure-（配置）\"><a href=\"#Configure-（配置）\" class=\"headerlink\" title=\"Configure （配置）\"></a>Configure （配置）</h2><p>Perform basic and advanced configuration needed to effectively administer a Hadoop cluster （为了高效管理集群的基础和高级配置）</p>\n<ul>\n<li>Configure a service using Cloudera Manager（使用Cloudera Manager 配置一个集群）</li>\n<li>Create an HDFS user’s home directory （创建用户的HDFS home文件夹）</li>\n<li>Configure NameNode HA （配置Hadoop NameNode的高可用）</li>\n<li>Configure ResourceManager HA （配置Yarn的ResourceManager的高可用）</li>\n<li>Configure proxy for Hiveserver2/Impala（配置HiveServer2或者Impala的代理）</li>\n</ul>\n<h2 id=\"Manage-（管理）\"><a href=\"#Manage-（管理）\" class=\"headerlink\" title=\"Manage （管理）\"></a>Manage （管理）</h2><p>Maintain and modify the cluster to support day-to-day operations in the enterprise (维护或者修改集群已满足企业的日常使用）</p>\n<ul>\n<li>Rebalance the cluster（重新平衡集群）</li>\n<li>Set up alerting for excessive disk fill （设置磁盘使用预警）</li>\n<li>Define and install a rack topology script （定义和安装机架拓扑）</li>\n<li>Install new type of I/O compression library in cluster（在集群中安装新的读写压缩包）</li>\n<li>Revise YARN resource assignment based on user feedback（根据用户的反馈配置YARN的资源）</li>\n<li>Commission/decommission a node（服役/退役一个节点）</li>\n</ul>\n<h2 id=\"Secure-（安全）\"><a href=\"#Secure-（安全）\" class=\"headerlink\" title=\"Secure （安全）\"></a>Secure （安全）</h2><p>Enable relevant services and configure the cluster to meet goals defined by security policy; demonstrate knowledge of basic security practices（可以根据安全要求进行服务安全配置）</p>\n<ul>\n<li>Configure HDFS ACLs （配置HDFS的ACLs）</li>\n<li>Install and configure Sentry（安装配置Sentry）</li>\n<li>Configure Hue user authorization and authentication（配置Hue的用户认证）</li>\n<li>Enable/configure log and query redaction（启用或者配置log和查询的修改）</li>\n<li>Create encrypted zones in HDFS（在HDFS上创建加密区间）</li>\n</ul>\n<h2 id=\"Test-（测试）\"><a href=\"#Test-（测试）\" class=\"headerlink\" title=\"Test （测试）\"></a>Test （测试）</h2><p>Benchmark the cluster operational metrics, test system configuration for operation and efficiency（对集群的性能进行基准测试）</p>\n<ul>\n<li>Execute file system commands via HTTPFS （通过HTTPFS执行文件系统命令）</li>\n<li>Efficiently copy data within a cluster/between clusters （集群间的数据复制）</li>\n<li>Create/restore a snapshot of an HDFS directory（创建或者恢复HDFS文件夹得快照）</li>\n<li>Get/set ACLs for a file or directory structure（获取/设置一个文件或者文件夹的ALCs）</li>\n<li>Benchmark the cluster (I/O, CPU, network)（测试集群的读写、CPU和网络性能）</li>\n</ul>\n<h2 id=\"Troubleshoot-（定位错误）\"><a href=\"#Troubleshoot-（定位错误）\" class=\"headerlink\" title=\"Troubleshoot （定位错误）\"></a>Troubleshoot （定位错误）</h2><p>Demonstrate ability to find the root cause of a problem, optimize inefficient execution, and resolve resource contention scenarios（证明有找到问题根本错误的能力，优化慢的执行任务和解决实际问题的能了）</p>\n<ul>\n<li>Resolve errors/warnings in Cloudera Manager （解决Cloudera Manager上的错误或者警告）</li>\n<li>Resolve performance problems/errors in cluster operation（解决集群操作上性能问题）</li>\n<li>Determine reason for application failure（定位应用失败的原因）</li>\n<li>Configure the Fair Scheduler to resolve application delays（配置公平调度用以解决应用的提交延迟）</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"Required-Skills-（技能）\"><a href=\"#Required-Skills-（技能）\" class=\"headerlink\" title=\"Required Skills （技能）\"></a>Required Skills （技能）</h1><h2 id=\"Install-（安装）\"><a href=\"#Install-（安装）\" class=\"headerlink\" title=\"Install （安装）\"></a>Install （安装）</h2><p>Demonstrate an understanding of the installation process for Cloudera Manager, CDH, and the ecosystem projects. （熟悉Cloudera Manager、CDH和其他生态系统内的项目的安装过程）</p>\n<ul>\n<li>Set up a local CDH repository （设置一个本地的CDH仓库）</li>\n<li>Perform OS-level configuration for Hadoop installation （操作系统级别的配置准备）</li>\n<li>Install Cloudera Manager server and agents （安装 Cloudera Manager 服务和代理）</li>\n<li>Install CDH using Cloudera Manager （使用Cloudera Manager 安装CDH）</li>\n<li>Add a new node to an existing cluster （在集群上添加一个新的节点）</li>\n<li>Add a service using Cloudera Manager （使用Cloudera Manager添加一个新的服务）","more":"</li>\n</ul>\n<h2 id=\"Configure-（配置）\"><a href=\"#Configure-（配置）\" class=\"headerlink\" title=\"Configure （配置）\"></a>Configure （配置）</h2><p>Perform basic and advanced configuration needed to effectively administer a Hadoop cluster （为了高效管理集群的基础和高级配置）</p>\n<ul>\n<li>Configure a service using Cloudera Manager（使用Cloudera Manager 配置一个集群）</li>\n<li>Create an HDFS user’s home directory （创建用户的HDFS home文件夹）</li>\n<li>Configure NameNode HA （配置Hadoop NameNode的高可用）</li>\n<li>Configure ResourceManager HA （配置Yarn的ResourceManager的高可用）</li>\n<li>Configure proxy for Hiveserver2/Impala（配置HiveServer2或者Impala的代理）</li>\n</ul>\n<h2 id=\"Manage-（管理）\"><a href=\"#Manage-（管理）\" class=\"headerlink\" title=\"Manage （管理）\"></a>Manage （管理）</h2><p>Maintain and modify the cluster to support day-to-day operations in the enterprise (维护或者修改集群已满足企业的日常使用）</p>\n<ul>\n<li>Rebalance the cluster（重新平衡集群）</li>\n<li>Set up alerting for excessive disk fill （设置磁盘使用预警）</li>\n<li>Define and install a rack topology script （定义和安装机架拓扑）</li>\n<li>Install new type of I/O compression library in cluster（在集群中安装新的读写压缩包）</li>\n<li>Revise YARN resource assignment based on user feedback（根据用户的反馈配置YARN的资源）</li>\n<li>Commission/decommission a node（服役/退役一个节点）</li>\n</ul>\n<h2 id=\"Secure-（安全）\"><a href=\"#Secure-（安全）\" class=\"headerlink\" title=\"Secure （安全）\"></a>Secure （安全）</h2><p>Enable relevant services and configure the cluster to meet goals defined by security policy; demonstrate knowledge of basic security practices（可以根据安全要求进行服务安全配置）</p>\n<ul>\n<li>Configure HDFS ACLs （配置HDFS的ACLs）</li>\n<li>Install and configure Sentry（安装配置Sentry）</li>\n<li>Configure Hue user authorization and authentication（配置Hue的用户认证）</li>\n<li>Enable/configure log and query redaction（启用或者配置log和查询的修改）</li>\n<li>Create encrypted zones in HDFS（在HDFS上创建加密区间）</li>\n</ul>\n<h2 id=\"Test-（测试）\"><a href=\"#Test-（测试）\" class=\"headerlink\" title=\"Test （测试）\"></a>Test （测试）</h2><p>Benchmark the cluster operational metrics, test system configuration for operation and efficiency（对集群的性能进行基准测试）</p>\n<ul>\n<li>Execute file system commands via HTTPFS （通过HTTPFS执行文件系统命令）</li>\n<li>Efficiently copy data within a cluster/between clusters （集群间的数据复制）</li>\n<li>Create/restore a snapshot of an HDFS directory（创建或者恢复HDFS文件夹得快照）</li>\n<li>Get/set ACLs for a file or directory structure（获取/设置一个文件或者文件夹的ALCs）</li>\n<li>Benchmark the cluster (I/O, CPU, network)（测试集群的读写、CPU和网络性能）</li>\n</ul>\n<h2 id=\"Troubleshoot-（定位错误）\"><a href=\"#Troubleshoot-（定位错误）\" class=\"headerlink\" title=\"Troubleshoot （定位错误）\"></a>Troubleshoot （定位错误）</h2><p>Demonstrate ability to find the root cause of a problem, optimize inefficient execution, and resolve resource contention scenarios（证明有找到问题根本错误的能力，优化慢的执行任务和解决实际问题的能了）</p>\n<ul>\n<li>Resolve errors/warnings in Cloudera Manager （解决Cloudera Manager上的错误或者警告）</li>\n<li>Resolve performance problems/errors in cluster operation（解决集群操作上性能问题）</li>\n<li>Determine reason for application failure（定位应用失败的原因）</li>\n<li>Configure the Fair Scheduler to resolve application delays（配置公平调度用以解决应用的提交延迟）</li>\n</ul>"},{"title":"CCAH-131 Manage","date":"2018-05-13T00:35:26.000Z","_content":"# Manage\n\nMaintain and modify the cluster to support day-to-day operations in the enterprise\n##  Rebalance the cluster\n\n[Configuring and Running the HDFS Balancer Using Cloudera Manager](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/admin_hdfs_balancer.html#cmug_topic_5_14)\n\nHOME -> Clusters -> HDFS -> Configuration 在 SCOPE 选择 Balancer，在 CATEGORY里选择main。设置Rebalancing Threshold\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%201.png)\n\n修改DataNode Advanced Configuration Snippet (Safety Valve) 的值。\n<!-- more -->\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%202.png)\n\n输入内容\n\n```\n<property>\n  <name>dfs.datanode.balance.max.concurrent.moves</name>\n  <value>50</value>\n</property>\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%202.1.png)\n\n执行 Rebalance\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%203.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%204.png)\n\nRebalance log \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%205.png)\n\n##  Set up alerting for excessive disk fill\n\n### 配置邮箱\nAdministrator -> Alerts \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%201.png)\n\n点击 Edit \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%202.png)\n\n修改 Alerts: Mail Message Recipients\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%203.png)\n\n或者搜索email进行相关设置修改\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%204.png)\n\n\n### 修改警告项配置\n\n选择一个服务进入configuration 在左侧选择 monitoring ，然后可以修改监控选项的 warning和critial值。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%205.png)\n\n\n\n##  Define and install a rack topology script\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%201.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%202.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%203.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%204.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%205.png)\n\n##  Install new type of I/O compression library in cluster\nhttp://archive.cloudera.com/gplextras5/parcels/5.9.0/\n\n[Configuring Services to Use the GPL Extras Parcel](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cm_mc_gpl_extras.html#xd_583c10bfdbd326ba--6eed2fb8-14349d04bee--7c3e)\n\nHOME -> Clusters -> HDFS -> Configuration 搜索 compression \n点击 + 添加新的压缩类。（Note: 操作系统已经支持此压缩）\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20Compression%20Codec/AddCompressionCodec1.png)\n \n##  Revise YARN resource assignment based on user feedback\n\n##  Commission/decommission a node\n\n**Decommission**\n\nHOME -> Hosts -> Roles  查看要退役节点上的服务\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode1.png)\n\n选择节点，停止上面的所有服务\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode2.png)\n\n停止上面所有的服务\n\t\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode3.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode4.png)\n\n查看节点上的服务是否已经都停止\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode5.png)\n\n将节点从集群里移除\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode6.png)\n\n确认 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode7.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode8.png)\n\n查看节点，已经没有角色了\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode9.png)\n\n退役节点\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode10.png)\n\n确认\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode11.png)\n\n成功退役\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode12.png)\n\n查看节点的服役状态，已经为 Decommission\n\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode13.png)\n\n**Commission**\n\n选择退役的节点，点击重启服役\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode1.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode2.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode3.png)\n\n查看节点的服役状态，已经为 Commission\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode4.png)\n","source":"_posts/CCAH-131-Manage.md","raw":"---\ntitle: CCAH-131 Manage\ndate: 2018-05-13 08:35:26\ntags: CCAH-131\n---\n# Manage\n\nMaintain and modify the cluster to support day-to-day operations in the enterprise\n##  Rebalance the cluster\n\n[Configuring and Running the HDFS Balancer Using Cloudera Manager](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/admin_hdfs_balancer.html#cmug_topic_5_14)\n\nHOME -> Clusters -> HDFS -> Configuration 在 SCOPE 选择 Balancer，在 CATEGORY里选择main。设置Rebalancing Threshold\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%201.png)\n\n修改DataNode Advanced Configuration Snippet (Safety Valve) 的值。\n<!-- more -->\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%202.png)\n\n输入内容\n\n```\n<property>\n  <name>dfs.datanode.balance.max.concurrent.moves</name>\n  <value>50</value>\n</property>\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%202.1.png)\n\n执行 Rebalance\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%203.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%204.png)\n\nRebalance log \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%205.png)\n\n##  Set up alerting for excessive disk fill\n\n### 配置邮箱\nAdministrator -> Alerts \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%201.png)\n\n点击 Edit \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%202.png)\n\n修改 Alerts: Mail Message Recipients\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%203.png)\n\n或者搜索email进行相关设置修改\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%204.png)\n\n\n### 修改警告项配置\n\n选择一个服务进入configuration 在左侧选择 monitoring ，然后可以修改监控选项的 warning和critial值。\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%205.png)\n\n\n\n##  Define and install a rack topology script\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%201.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%202.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%203.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%204.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%205.png)\n\n##  Install new type of I/O compression library in cluster\nhttp://archive.cloudera.com/gplextras5/parcels/5.9.0/\n\n[Configuring Services to Use the GPL Extras Parcel](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cm_mc_gpl_extras.html#xd_583c10bfdbd326ba--6eed2fb8-14349d04bee--7c3e)\n\nHOME -> Clusters -> HDFS -> Configuration 搜索 compression \n点击 + 添加新的压缩类。（Note: 操作系统已经支持此压缩）\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20Compression%20Codec/AddCompressionCodec1.png)\n \n##  Revise YARN resource assignment based on user feedback\n\n##  Commission/decommission a node\n\n**Decommission**\n\nHOME -> Hosts -> Roles  查看要退役节点上的服务\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode1.png)\n\n选择节点，停止上面的所有服务\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode2.png)\n\n停止上面所有的服务\n\t\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode3.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode4.png)\n\n查看节点上的服务是否已经都停止\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode5.png)\n\n将节点从集群里移除\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode6.png)\n\n确认 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode7.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode8.png)\n\n查看节点，已经没有角色了\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode9.png)\n\n退役节点\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode10.png)\n\n确认\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode11.png)\n\n成功退役\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode12.png)\n\n查看节点的服役状态，已经为 Decommission\n\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode13.png)\n\n**Commission**\n\n选择退役的节点，点击重启服役\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode1.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode2.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode3.png)\n\n查看节点的服役状态，已经为 Commission\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode4.png)\n","slug":"CCAH-131-Manage","published":1,"updated":"2018-09-14T01:19:14.214Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzd0007inamkwsen9pv","content":"<h1 id=\"Manage\"><a href=\"#Manage\" class=\"headerlink\" title=\"Manage\"></a>Manage</h1><p>Maintain and modify the cluster to support day-to-day operations in the enterprise</p>\n<h2 id=\"Rebalance-the-cluster\"><a href=\"#Rebalance-the-cluster\" class=\"headerlink\" title=\"Rebalance the cluster\"></a>Rebalance the cluster</h2><p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/admin_hdfs_balancer.html#cmug_topic_5_14\" target=\"_blank\" rel=\"noopener\">Configuring and Running the HDFS Balancer Using Cloudera Manager</a></p>\n<p>HOME -&gt; Clusters -&gt; HDFS -&gt; Configuration 在 SCOPE 选择 Balancer，在 CATEGORY里选择main。设置Rebalancing Threshold</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%201.png\" alt=\"\"></p>\n<p>修改DataNode Advanced Configuration Snippet (Safety Valve) 的值。<br><a id=\"more\"></a></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%202.png\" alt=\"\"></p>\n<p>输入内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;dfs.datanode.balance.max.concurrent.moves&lt;/name&gt;</span><br><span class=\"line\">  &lt;value&gt;50&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%202.1.png\" alt=\"\"></p>\n<p>执行 Rebalance</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%203.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%204.png\" alt=\"\"></p>\n<p>Rebalance log </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%205.png\" alt=\"\"></p>\n<h2 id=\"Set-up-alerting-for-excessive-disk-fill\"><a href=\"#Set-up-alerting-for-excessive-disk-fill\" class=\"headerlink\" title=\"Set up alerting for excessive disk fill\"></a>Set up alerting for excessive disk fill</h2><h3 id=\"配置邮箱\"><a href=\"#配置邮箱\" class=\"headerlink\" title=\"配置邮箱\"></a>配置邮箱</h3><p>Administrator -&gt; Alerts </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%201.png\" alt=\"\"></p>\n<p>点击 Edit </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%202.png\" alt=\"\"></p>\n<p>修改 Alerts: Mail Message Recipients</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%203.png\" alt=\"\"></p>\n<p>或者搜索email进行相关设置修改</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%204.png\" alt=\"\"></p>\n<h3 id=\"修改警告项配置\"><a href=\"#修改警告项配置\" class=\"headerlink\" title=\"修改警告项配置\"></a>修改警告项配置</h3><p>选择一个服务进入configuration 在左侧选择 monitoring ，然后可以修改监控选项的 warning和critial值。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%205.png\" alt=\"\"></p>\n<h2 id=\"Define-and-install-a-rack-topology-script\"><a href=\"#Define-and-install-a-rack-topology-script\" class=\"headerlink\" title=\"Define and install a rack topology script\"></a>Define and install a rack topology script</h2><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%201.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%202.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%203.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%204.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%205.png\" alt=\"\"></p>\n<h2 id=\"Install-new-type-of-I-O-compression-library-in-cluster\"><a href=\"#Install-new-type-of-I-O-compression-library-in-cluster\" class=\"headerlink\" title=\"Install new type of I/O compression library in cluster\"></a>Install new type of I/O compression library in cluster</h2><p><a href=\"http://archive.cloudera.com/gplextras5/parcels/5.9.0/\" target=\"_blank\" rel=\"noopener\">http://archive.cloudera.com/gplextras5/parcels/5.9.0/</a></p>\n<p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cm_mc_gpl_extras.html#xd_583c10bfdbd326ba--6eed2fb8-14349d04bee--7c3e\" target=\"_blank\" rel=\"noopener\">Configuring Services to Use the GPL Extras Parcel</a></p>\n<p>HOME -&gt; Clusters -&gt; HDFS -&gt; Configuration 搜索 compression<br>点击 + 添加新的压缩类。（Note: 操作系统已经支持此压缩）</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20Compression%20Codec/AddCompressionCodec1.png\" alt=\"\"></p>\n<h2 id=\"Revise-YARN-resource-assignment-based-on-user-feedback\"><a href=\"#Revise-YARN-resource-assignment-based-on-user-feedback\" class=\"headerlink\" title=\"Revise YARN resource assignment based on user feedback\"></a>Revise YARN resource assignment based on user feedback</h2><h2 id=\"Commission-decommission-a-node\"><a href=\"#Commission-decommission-a-node\" class=\"headerlink\" title=\"Commission/decommission a node\"></a>Commission/decommission a node</h2><p><strong>Decommission</strong></p>\n<p>HOME -&gt; Hosts -&gt; Roles  查看要退役节点上的服务</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode1.png\" alt=\"\"></p>\n<p>选择节点，停止上面的所有服务</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode2.png\" alt=\"\"></p>\n<p>停止上面所有的服务</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode3.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode4.png\" alt=\"\"></p>\n<p>查看节点上的服务是否已经都停止</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode5.png\" alt=\"\"></p>\n<p>将节点从集群里移除</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode6.png\" alt=\"\"></p>\n<p>确认 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode7.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode8.png\" alt=\"\"></p>\n<p>查看节点，已经没有角色了</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode9.png\" alt=\"\"></p>\n<p>退役节点</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode10.png\" alt=\"\"></p>\n<p>确认</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode11.png\" alt=\"\"></p>\n<p>成功退役</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode12.png\" alt=\"\"></p>\n<p>查看节点的服役状态，已经为 Decommission</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode13.png\" alt=\"\"></p>\n<p><strong>Commission</strong></p>\n<p>选择退役的节点，点击重启服役</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode1.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode2.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode3.png\" alt=\"\"></p>\n<p>查看节点的服役状态，已经为 Commission</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode4.png\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"Manage\"><a href=\"#Manage\" class=\"headerlink\" title=\"Manage\"></a>Manage</h1><p>Maintain and modify the cluster to support day-to-day operations in the enterprise</p>\n<h2 id=\"Rebalance-the-cluster\"><a href=\"#Rebalance-the-cluster\" class=\"headerlink\" title=\"Rebalance the cluster\"></a>Rebalance the cluster</h2><p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/admin_hdfs_balancer.html#cmug_topic_5_14\" target=\"_blank\" rel=\"noopener\">Configuring and Running the HDFS Balancer Using Cloudera Manager</a></p>\n<p>HOME -&gt; Clusters -&gt; HDFS -&gt; Configuration 在 SCOPE 选择 Balancer，在 CATEGORY里选择main。设置Rebalancing Threshold</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%201.png\" alt=\"\"></p>\n<p>修改DataNode Advanced Configuration Snippet (Safety Valve) 的值。<br>","more":"</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%202.png\" alt=\"\"></p>\n<p>输入内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;dfs.datanode.balance.max.concurrent.moves&lt;/name&gt;</span><br><span class=\"line\">  &lt;value&gt;50&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%202.1.png\" alt=\"\"></p>\n<p>执行 Rebalance</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%203.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%204.png\" alt=\"\"></p>\n<p>Rebalance log </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rebalance/Rebalance%205.png\" alt=\"\"></p>\n<h2 id=\"Set-up-alerting-for-excessive-disk-fill\"><a href=\"#Set-up-alerting-for-excessive-disk-fill\" class=\"headerlink\" title=\"Set up alerting for excessive disk fill\"></a>Set up alerting for excessive disk fill</h2><h3 id=\"配置邮箱\"><a href=\"#配置邮箱\" class=\"headerlink\" title=\"配置邮箱\"></a>配置邮箱</h3><p>Administrator -&gt; Alerts </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%201.png\" alt=\"\"></p>\n<p>点击 Edit </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%202.png\" alt=\"\"></p>\n<p>修改 Alerts: Mail Message Recipients</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%203.png\" alt=\"\"></p>\n<p>或者搜索email进行相关设置修改</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%204.png\" alt=\"\"></p>\n<h3 id=\"修改警告项配置\"><a href=\"#修改警告项配置\" class=\"headerlink\" title=\"修改警告项配置\"></a>修改警告项配置</h3><p>选择一个服务进入configuration 在左侧选择 monitoring ，然后可以修改监控选项的 warning和critial值。</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Alerts/Add%20Email%205.png\" alt=\"\"></p>\n<h2 id=\"Define-and-install-a-rack-topology-script\"><a href=\"#Define-and-install-a-rack-topology-script\" class=\"headerlink\" title=\"Define and install a rack topology script\"></a>Define and install a rack topology script</h2><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%201.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%202.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%203.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%204.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Rack/Rack%205.png\" alt=\"\"></p>\n<h2 id=\"Install-new-type-of-I-O-compression-library-in-cluster\"><a href=\"#Install-new-type-of-I-O-compression-library-in-cluster\" class=\"headerlink\" title=\"Install new type of I/O compression library in cluster\"></a>Install new type of I/O compression library in cluster</h2><p><a href=\"http://archive.cloudera.com/gplextras5/parcels/5.9.0/\" target=\"_blank\" rel=\"noopener\">http://archive.cloudera.com/gplextras5/parcels/5.9.0/</a></p>\n<p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cm_mc_gpl_extras.html#xd_583c10bfdbd326ba--6eed2fb8-14349d04bee--7c3e\" target=\"_blank\" rel=\"noopener\">Configuring Services to Use the GPL Extras Parcel</a></p>\n<p>HOME -&gt; Clusters -&gt; HDFS -&gt; Configuration 搜索 compression<br>点击 + 添加新的压缩类。（Note: 操作系统已经支持此压缩）</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Add%20Compression%20Codec/AddCompressionCodec1.png\" alt=\"\"></p>\n<h2 id=\"Revise-YARN-resource-assignment-based-on-user-feedback\"><a href=\"#Revise-YARN-resource-assignment-based-on-user-feedback\" class=\"headerlink\" title=\"Revise YARN resource assignment based on user feedback\"></a>Revise YARN resource assignment based on user feedback</h2><h2 id=\"Commission-decommission-a-node\"><a href=\"#Commission-decommission-a-node\" class=\"headerlink\" title=\"Commission/decommission a node\"></a>Commission/decommission a node</h2><p><strong>Decommission</strong></p>\n<p>HOME -&gt; Hosts -&gt; Roles  查看要退役节点上的服务</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode1.png\" alt=\"\"></p>\n<p>选择节点，停止上面的所有服务</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode2.png\" alt=\"\"></p>\n<p>停止上面所有的服务</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode3.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode4.png\" alt=\"\"></p>\n<p>查看节点上的服务是否已经都停止</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode5.png\" alt=\"\"></p>\n<p>将节点从集群里移除</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode6.png\" alt=\"\"></p>\n<p>确认 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode7.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode8.png\" alt=\"\"></p>\n<p>查看节点，已经没有角色了</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode9.png\" alt=\"\"></p>\n<p>退役节点</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode10.png\" alt=\"\"></p>\n<p>确认</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode11.png\" alt=\"\"></p>\n<p>成功退役</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode12.png\" alt=\"\"></p>\n<p>查看节点的服役状态，已经为 Decommission</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/decommissionNode13.png\" alt=\"\"></p>\n<p><strong>Commission</strong></p>\n<p>选择退役的节点，点击重启服役</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode1.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode2.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode3.png\" alt=\"\"></p>\n<p>查看节点的服役状态，已经为 Commission</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/DecommisionNode/recommissionNode4.png\" alt=\"\"></p>"},{"title":"CCAH-131 Test","date":"2018-05-13T00:38:26.000Z","_content":"# Test\n\nBenchmark the cluster operational metrics, test system configuration for operation and efficiency\n\n\n## Execute file system commands via HTTPFS\n\n[Using the HttpFS Server with curl](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_ig_httpfs_server_curl.html#topic_25_8)\n\n例如查看 用户training的Home目录\n\n```\n[training@elephant ~]$ curl \"http://elephant:14000/webhdfs/v1?op=gethomedirectory&user.name=training\"\n{\"Path\":\"\\/user\\/training\"}\n[training@elephant ~]$\n```\n<!-- more --> \n\n[WebHDFS REST API](https://archive.cloudera.com/cdh5/cdh/5/hadoop/hadoop-project-dist/hadoop-hdfs/WebHDFS.html)\n\n```\ncurl -i -X PUT \"http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=CREATE\n                    [&overwrite=<true|false>][&blocksize=<LONG>][&replication=<SHORT>]\n                    [&permission=<OCTAL>][&buffersize=<INT>]\"\n```\n\n查询一个目录 \n\n```\n[training@elephant ~]$ curl -i  \"http://elephant:14000/webhdfs/v1/user?op=LISTSTATUS&user.name=training\"\nHTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nSet-Cookie: hadoop.auth=\"u=training&p=training&t=simple-dt&e=1515685732115&s=9cv4wBc8rJe83+6WUg7B5xaeafE=\"; Path=/; HttpOnly\nContent-Type: application/json\nTransfer-Encoding: chunked\nDate: Thu, 11 Jan 2018 05:48:52 GMT\n\n{\"FileStatuses\":{\"FileStatus\":[{\"pathSuffix\":\"history\",\"type\":\"DIRECTORY\",\"length\":0,\"owner\":\"mapred\",\"group\":\"hadoop\",\"permission\":\"777\",\"accessTime\":0,\"modificationTime\":1515572964146,\"blockSize\":0,\"replication\":0},{\"pathSuffix\":\"hive\",\"type\":\"DIRECTORY\",\"length\":0,\"owner\":\"hive\",\"group\":\"hive\",\"permission\":\"1775\",\"accessTime\":0,\"modificationTime\":1515575604845,\"blockSize\":0,\"replication\":0},{\"pathSuffix\":\"hue\",\"type\":\"DIRECTORY\",\"length\":0,\"owner\":\"hue\",\"group\":\"hue\",\"permission\":\"775\",\"accessTime\":0,\"modificationTime\":1515575637482,\"blockSize\":0,\"replication\":0},{\"pathSuffix\":\"impala\",\"type\":\"DIRECTORY\",\"length\":0,\"owner\":\"impala\",\"group\":\"impala\",\"permission\":\"775\",\"accessTime\":0,\"modificationTime\":1515575938788,\"blockSize\":0,\"replication\":0},{\"pathSuffix\":\"oozie\",\"type\":\"DIRECTORY\",\"length\":0,\"owner\":\"oozie\",\"group\":\"oozie\",\"permission\":\"775\",\"accessTime\":0,\"modificationTime\":1515645000550,\"blockSize\":0,\"replication\":0},{\"pathSuffix\":\"spark\",\"type\":\"DIRECTORY\",\"length\":0,\"owner\":\"spark\",\"group\":\"spark\",\"permission\":\"751\",\"accessTime\":0,\"modificationTime\":1515573417601,\"blockSize\":0,\"replication\":0},{\"pathSuffix\":\"training\",\"type\":\"DIRECTORY\",\"length\":0,\"owner\":\"training\",\"group\":\"supergroup\",\"permission\":\"755\",\"accessTime\":0,\"modificationTime\":1515586402387,\"blockSize\":0,\"replication\":0}]}}\n[training@elephant ~]$\n```\n\n## Efficiently copy data within a cluster/between clusters\n\n**迁移hdfs数据至新集群**\n\n```\nhadoop distcp -update hdfs://hostname:port/path hdfs://ip:8020/path\n```\n\n**迁移Hive数据仓库**\n\n可以每次迁移一个表或者一个database。\n注意： 表的内容小文件不应过多，否则会非常慢。\n源集群metastore数据备份导出(mysql导出)\n\n```\nmysqldump -u root -p’密码’--skip-lock-tables -h xxx.xxx.xxx.xxx hive > mysql_hive.sql\n```\n\n新的集群导入metastore数据\n\n```\nmysql -u root -proot --default-character-set=utf8 hvie < mysql_hive.sql\n```\n\n升级hive内容库(如果hive版本需要升级操作，同版本不需要操作)\n\n```\nmysql -uroot -proot risk -hxxx.xxx.xxx.xxx < mysqlupgrade-0.13.0-to-0.14.0.mysql.sql\nmysql -uroot -proot risk -hxxx.xxx.xxx.xxx < mysqlupgrade-0.14.0-to-1.1.0.mysql.sql\n```\n\n版本要依据版本序列升序升级,不可跨越版本，如当前是hive0.12打算升级到0.14，需要先升级到0.13再升级到0.14\n\n修改 metastore 内容库的集群信息\n\n因为夸集群，hdfs访问的名字可能变化了，所以需要修改下hive库中的表DBS和SDS内容，除非你的集群名字或者HA的名字跟之前的一致这个就不用修改了\n登录mysql数据库，查看：\n\n```\nmysql> use hive;\nmysql> select * from DBS;\nmysql> select * from SDS;\n```\n\n## Create/restore a snapshot of an HDFS directory\n\n创建一个测试文件夹和文件\n\n```\n[training@elephant ~]$ hdfs dfs -mkdir /user/training/snapshottest\n[training@elephant ~]$ hdfs dfs -put /etc/hosts /user/training/snapshottest/\n[training@elephant ~]$ hdfs dfs -ls /user/training/snapshottest/\nFound 1 items\n-rw-r--r--   3 training supergroup        251 2018-01-11 14:03 /user/training/snapshottest/hosts\n[training@elephant ~]$\n```\nHOME -> Cluster -> HDFS -> File Browser -> /user/training/snapshottest/ 点击 Enable Snapshot\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot1.png)\n\n启用\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot3.png)\n\nTake Sanpshot 并命名 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot4.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot5.png)\n\n在 Snapshots 下可以看到已有的Sanpshots\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot6.png)\n\n或者到Hadoop 50070上查看\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot7.png)\n\n删除测试文件 \n\n```\n[training@elephant ~]$ hdfs dfs -rm /user/training/snapshottest/hosts\n18/01/11 14:09:37 INFO fs.TrashPolicyDefault: Moved: 'hdfs://mycluster/user/training/snapshottest/hosts' to trash at: hdfs://mycluster/user/training/.Trash/Current/user/training/snapshottest/hosts\n[training@elephant ~]$\n```\n\n从snapshot恢复\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot8.png)\n\n选择使用得分snapshot，并恢复\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot10.png)\n\n查看文件是否已经恢复\n\n```\n[training@elephant ~]$ hdfs dfs -ls /user/training/snapshottest/\nFound 1 items\n-rw-r--r--   3 hdfs supergroup        251 2018-01-11 14:12 /user/training/snapshottest/hosts\n[training@elephant ~]$\n```\n\n## Get/set ACLs for a file or directory structure\n\n```\n[training@elephant ~]$ hdfs dfs -ls /user/training/ ## 查看文件夹内内容\nFound 1 items\ndrwx------   - training supergroup          0 2018-01-11 08:00 /user/training/.Trash\n[training@elephant ~]$ hdfs dfs -put /etc/hosts /user/training/  ## 上传一个文件\n[training@elephant ~]$ hdfs dfs -ls /user/training/ ## 查看上传的文件的权限\nFound 2 items\ndrwx------   - training supergroup          0 2018-01-11 08:00 /user/training/.Trash\n-rw-r--r--   3 training supergroup        251 2018-01-11 13:51 /user/training/hosts\n[training@elephant ~]$ hdfs dfs -getfacl /user/training/hosts  ## 使用命令查询文件的ACL \n# file: /user/training/hosts\n# owner: training\n# group: supergroup\nuser::rw-\ngroup::r--\nother::r--\n\n[training@elephant ~]$ hdfs dfs -getfacl /user/training/  ## 查看文件夹的ACL\n# file: /user/training\n# owner: training\n# group: supergroup\nuser::rwx\ngroup::r-x\nother::r-x\n\n[training@elephant ~]$\n```\n## Benchmark the cluster (I/O, CPU, network)\n","source":"_posts/CCAH-131-Test.md","raw":"---\ntitle: CCAH-131 Test\ndate: 2018-05-13 08:38:26\ntags: CCAH-131\n---\n# Test\n\nBenchmark the cluster operational metrics, test system configuration for operation and efficiency\n\n\n## Execute file system commands via HTTPFS\n\n[Using the HttpFS Server with curl](https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_ig_httpfs_server_curl.html#topic_25_8)\n\n例如查看 用户training的Home目录\n\n```\n[training@elephant ~]$ curl \"http://elephant:14000/webhdfs/v1?op=gethomedirectory&user.name=training\"\n{\"Path\":\"\\/user\\/training\"}\n[training@elephant ~]$\n```\n<!-- more --> \n\n[WebHDFS REST API](https://archive.cloudera.com/cdh5/cdh/5/hadoop/hadoop-project-dist/hadoop-hdfs/WebHDFS.html)\n\n```\ncurl -i -X PUT \"http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=CREATE\n                    [&overwrite=<true|false>][&blocksize=<LONG>][&replication=<SHORT>]\n                    [&permission=<OCTAL>][&buffersize=<INT>]\"\n```\n\n查询一个目录 \n\n```\n[training@elephant ~]$ curl -i  \"http://elephant:14000/webhdfs/v1/user?op=LISTSTATUS&user.name=training\"\nHTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nSet-Cookie: hadoop.auth=\"u=training&p=training&t=simple-dt&e=1515685732115&s=9cv4wBc8rJe83+6WUg7B5xaeafE=\"; Path=/; HttpOnly\nContent-Type: application/json\nTransfer-Encoding: chunked\nDate: Thu, 11 Jan 2018 05:48:52 GMT\n\n{\"FileStatuses\":{\"FileStatus\":[{\"pathSuffix\":\"history\",\"type\":\"DIRECTORY\",\"length\":0,\"owner\":\"mapred\",\"group\":\"hadoop\",\"permission\":\"777\",\"accessTime\":0,\"modificationTime\":1515572964146,\"blockSize\":0,\"replication\":0},{\"pathSuffix\":\"hive\",\"type\":\"DIRECTORY\",\"length\":0,\"owner\":\"hive\",\"group\":\"hive\",\"permission\":\"1775\",\"accessTime\":0,\"modificationTime\":1515575604845,\"blockSize\":0,\"replication\":0},{\"pathSuffix\":\"hue\",\"type\":\"DIRECTORY\",\"length\":0,\"owner\":\"hue\",\"group\":\"hue\",\"permission\":\"775\",\"accessTime\":0,\"modificationTime\":1515575637482,\"blockSize\":0,\"replication\":0},{\"pathSuffix\":\"impala\",\"type\":\"DIRECTORY\",\"length\":0,\"owner\":\"impala\",\"group\":\"impala\",\"permission\":\"775\",\"accessTime\":0,\"modificationTime\":1515575938788,\"blockSize\":0,\"replication\":0},{\"pathSuffix\":\"oozie\",\"type\":\"DIRECTORY\",\"length\":0,\"owner\":\"oozie\",\"group\":\"oozie\",\"permission\":\"775\",\"accessTime\":0,\"modificationTime\":1515645000550,\"blockSize\":0,\"replication\":0},{\"pathSuffix\":\"spark\",\"type\":\"DIRECTORY\",\"length\":0,\"owner\":\"spark\",\"group\":\"spark\",\"permission\":\"751\",\"accessTime\":0,\"modificationTime\":1515573417601,\"blockSize\":0,\"replication\":0},{\"pathSuffix\":\"training\",\"type\":\"DIRECTORY\",\"length\":0,\"owner\":\"training\",\"group\":\"supergroup\",\"permission\":\"755\",\"accessTime\":0,\"modificationTime\":1515586402387,\"blockSize\":0,\"replication\":0}]}}\n[training@elephant ~]$\n```\n\n## Efficiently copy data within a cluster/between clusters\n\n**迁移hdfs数据至新集群**\n\n```\nhadoop distcp -update hdfs://hostname:port/path hdfs://ip:8020/path\n```\n\n**迁移Hive数据仓库**\n\n可以每次迁移一个表或者一个database。\n注意： 表的内容小文件不应过多，否则会非常慢。\n源集群metastore数据备份导出(mysql导出)\n\n```\nmysqldump -u root -p’密码’--skip-lock-tables -h xxx.xxx.xxx.xxx hive > mysql_hive.sql\n```\n\n新的集群导入metastore数据\n\n```\nmysql -u root -proot --default-character-set=utf8 hvie < mysql_hive.sql\n```\n\n升级hive内容库(如果hive版本需要升级操作，同版本不需要操作)\n\n```\nmysql -uroot -proot risk -hxxx.xxx.xxx.xxx < mysqlupgrade-0.13.0-to-0.14.0.mysql.sql\nmysql -uroot -proot risk -hxxx.xxx.xxx.xxx < mysqlupgrade-0.14.0-to-1.1.0.mysql.sql\n```\n\n版本要依据版本序列升序升级,不可跨越版本，如当前是hive0.12打算升级到0.14，需要先升级到0.13再升级到0.14\n\n修改 metastore 内容库的集群信息\n\n因为夸集群，hdfs访问的名字可能变化了，所以需要修改下hive库中的表DBS和SDS内容，除非你的集群名字或者HA的名字跟之前的一致这个就不用修改了\n登录mysql数据库，查看：\n\n```\nmysql> use hive;\nmysql> select * from DBS;\nmysql> select * from SDS;\n```\n\n## Create/restore a snapshot of an HDFS directory\n\n创建一个测试文件夹和文件\n\n```\n[training@elephant ~]$ hdfs dfs -mkdir /user/training/snapshottest\n[training@elephant ~]$ hdfs dfs -put /etc/hosts /user/training/snapshottest/\n[training@elephant ~]$ hdfs dfs -ls /user/training/snapshottest/\nFound 1 items\n-rw-r--r--   3 training supergroup        251 2018-01-11 14:03 /user/training/snapshottest/hosts\n[training@elephant ~]$\n```\nHOME -> Cluster -> HDFS -> File Browser -> /user/training/snapshottest/ 点击 Enable Snapshot\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot1.png)\n\n启用\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot3.png)\n\nTake Sanpshot 并命名 \n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot4.png)\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot5.png)\n\n在 Snapshots 下可以看到已有的Sanpshots\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot6.png)\n\n或者到Hadoop 50070上查看\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot7.png)\n\n删除测试文件 \n\n```\n[training@elephant ~]$ hdfs dfs -rm /user/training/snapshottest/hosts\n18/01/11 14:09:37 INFO fs.TrashPolicyDefault: Moved: 'hdfs://mycluster/user/training/snapshottest/hosts' to trash at: hdfs://mycluster/user/training/.Trash/Current/user/training/snapshottest/hosts\n[training@elephant ~]$\n```\n\n从snapshot恢复\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot8.png)\n\n选择使用得分snapshot，并恢复\n\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot10.png)\n\n查看文件是否已经恢复\n\n```\n[training@elephant ~]$ hdfs dfs -ls /user/training/snapshottest/\nFound 1 items\n-rw-r--r--   3 hdfs supergroup        251 2018-01-11 14:12 /user/training/snapshottest/hosts\n[training@elephant ~]$\n```\n\n## Get/set ACLs for a file or directory structure\n\n```\n[training@elephant ~]$ hdfs dfs -ls /user/training/ ## 查看文件夹内内容\nFound 1 items\ndrwx------   - training supergroup          0 2018-01-11 08:00 /user/training/.Trash\n[training@elephant ~]$ hdfs dfs -put /etc/hosts /user/training/  ## 上传一个文件\n[training@elephant ~]$ hdfs dfs -ls /user/training/ ## 查看上传的文件的权限\nFound 2 items\ndrwx------   - training supergroup          0 2018-01-11 08:00 /user/training/.Trash\n-rw-r--r--   3 training supergroup        251 2018-01-11 13:51 /user/training/hosts\n[training@elephant ~]$ hdfs dfs -getfacl /user/training/hosts  ## 使用命令查询文件的ACL \n# file: /user/training/hosts\n# owner: training\n# group: supergroup\nuser::rw-\ngroup::r--\nother::r--\n\n[training@elephant ~]$ hdfs dfs -getfacl /user/training/  ## 查看文件夹的ACL\n# file: /user/training\n# owner: training\n# group: supergroup\nuser::rwx\ngroup::r-x\nother::r-x\n\n[training@elephant ~]$\n```\n## Benchmark the cluster (I/O, CPU, network)\n","slug":"CCAH-131-Test","published":1,"updated":"2018-09-14T01:19:51.101Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzf000ainam3gdhmgr9","content":"<h1 id=\"Test\"><a href=\"#Test\" class=\"headerlink\" title=\"Test\"></a>Test</h1><p>Benchmark the cluster operational metrics, test system configuration for operation and efficiency</p>\n<h2 id=\"Execute-file-system-commands-via-HTTPFS\"><a href=\"#Execute-file-system-commands-via-HTTPFS\" class=\"headerlink\" title=\"Execute file system commands via HTTPFS\"></a>Execute file system commands via HTTPFS</h2><p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_ig_httpfs_server_curl.html#topic_25_8\" target=\"_blank\" rel=\"noopener\">Using the HttpFS Server with curl</a></p>\n<p>例如查看 用户training的Home目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ curl &quot;http://elephant:14000/webhdfs/v1?op=gethomedirectory&amp;user.name=training&quot;</span><br><span class=\"line\">&#123;&quot;Path&quot;:&quot;\\/user\\/training&quot;&#125;</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<a id=\"more\"></a> \n<p><a href=\"https://archive.cloudera.com/cdh5/cdh/5/hadoop/hadoop-project-dist/hadoop-hdfs/WebHDFS.html\" target=\"_blank\" rel=\"noopener\">WebHDFS REST API</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -i -X PUT &quot;http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=CREATE</span><br><span class=\"line\">                    [&amp;overwrite=&lt;true|false&gt;][&amp;blocksize=&lt;LONG&gt;][&amp;replication=&lt;SHORT&gt;]</span><br><span class=\"line\">                    [&amp;permission=&lt;OCTAL&gt;][&amp;buffersize=&lt;INT&gt;]&quot;</span><br></pre></td></tr></table></figure>\n<p>查询一个目录 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ curl -i  &quot;http://elephant:14000/webhdfs/v1/user?op=LISTSTATUS&amp;user.name=training&quot;</span><br><span class=\"line\">HTTP/1.1 200 OK</span><br><span class=\"line\">Server: Apache-Coyote/1.1</span><br><span class=\"line\">Set-Cookie: hadoop.auth=&quot;u=training&amp;p=training&amp;t=simple-dt&amp;e=1515685732115&amp;s=9cv4wBc8rJe83+6WUg7B5xaeafE=&quot;; Path=/; HttpOnly</span><br><span class=\"line\">Content-Type: application/json</span><br><span class=\"line\">Transfer-Encoding: chunked</span><br><span class=\"line\">Date: Thu, 11 Jan 2018 05:48:52 GMT</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&quot;FileStatuses&quot;:&#123;&quot;FileStatus&quot;:[&#123;&quot;pathSuffix&quot;:&quot;history&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;mapred&quot;,&quot;group&quot;:&quot;hadoop&quot;,&quot;permission&quot;:&quot;777&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515572964146,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;hive&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;hive&quot;,&quot;group&quot;:&quot;hive&quot;,&quot;permission&quot;:&quot;1775&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515575604845,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;hue&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;hue&quot;,&quot;group&quot;:&quot;hue&quot;,&quot;permission&quot;:&quot;775&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515575637482,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;impala&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;impala&quot;,&quot;group&quot;:&quot;impala&quot;,&quot;permission&quot;:&quot;775&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515575938788,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;oozie&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;oozie&quot;,&quot;group&quot;:&quot;oozie&quot;,&quot;permission&quot;:&quot;775&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515645000550,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;spark&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;spark&quot;,&quot;group&quot;:&quot;spark&quot;,&quot;permission&quot;:&quot;751&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515573417601,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;training&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;training&quot;,&quot;group&quot;:&quot;supergroup&quot;,&quot;permission&quot;:&quot;755&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515586402387,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;]&#125;&#125;</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<h2 id=\"Efficiently-copy-data-within-a-cluster-between-clusters\"><a href=\"#Efficiently-copy-data-within-a-cluster-between-clusters\" class=\"headerlink\" title=\"Efficiently copy data within a cluster/between clusters\"></a>Efficiently copy data within a cluster/between clusters</h2><p><strong>迁移hdfs数据至新集群</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop distcp -update hdfs://hostname:port/path hdfs://ip:8020/path</span><br></pre></td></tr></table></figure>\n<p><strong>迁移Hive数据仓库</strong></p>\n<p>可以每次迁移一个表或者一个database。<br>注意： 表的内容小文件不应过多，否则会非常慢。<br>源集群metastore数据备份导出(mysql导出)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysqldump -u root -p’密码’--skip-lock-tables -h xxx.xxx.xxx.xxx hive &gt; mysql_hive.sql</span><br></pre></td></tr></table></figure>\n<p>新的集群导入metastore数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql -u root -proot --default-character-set=utf8 hvie &lt; mysql_hive.sql</span><br></pre></td></tr></table></figure>\n<p>升级hive内容库(如果hive版本需要升级操作，同版本不需要操作)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql -uroot -proot risk -hxxx.xxx.xxx.xxx &lt; mysqlupgrade-0.13.0-to-0.14.0.mysql.sql</span><br><span class=\"line\">mysql -uroot -proot risk -hxxx.xxx.xxx.xxx &lt; mysqlupgrade-0.14.0-to-1.1.0.mysql.sql</span><br></pre></td></tr></table></figure>\n<p>版本要依据版本序列升序升级,不可跨越版本，如当前是hive0.12打算升级到0.14，需要先升级到0.13再升级到0.14</p>\n<p>修改 metastore 内容库的集群信息</p>\n<p>因为夸集群，hdfs访问的名字可能变化了，所以需要修改下hive库中的表DBS和SDS内容，除非你的集群名字或者HA的名字跟之前的一致这个就不用修改了<br>登录mysql数据库，查看：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&gt; use hive;</span><br><span class=\"line\">mysql&gt; select * from DBS;</span><br><span class=\"line\">mysql&gt; select * from SDS;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Create-restore-a-snapshot-of-an-HDFS-directory\"><a href=\"#Create-restore-a-snapshot-of-an-HDFS-directory\" class=\"headerlink\" title=\"Create/restore a snapshot of an HDFS directory\"></a>Create/restore a snapshot of an HDFS directory</h2><p>创建一个测试文件夹和文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ hdfs dfs -mkdir /user/training/snapshottest</span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -put /etc/hosts /user/training/snapshottest/</span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -ls /user/training/snapshottest/</span><br><span class=\"line\">Found 1 items</span><br><span class=\"line\">-rw-r--r--   3 training supergroup        251 2018-01-11 14:03 /user/training/snapshottest/hosts</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<p>HOME -&gt; Cluster -&gt; HDFS -&gt; File Browser -&gt; /user/training/snapshottest/ 点击 Enable Snapshot</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot1.png\" alt=\"\"></p>\n<p>启用</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot3.png\" alt=\"\"></p>\n<p>Take Sanpshot 并命名 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot4.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot5.png\" alt=\"\"></p>\n<p>在 Snapshots 下可以看到已有的Sanpshots</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot6.png\" alt=\"\"></p>\n<p>或者到Hadoop 50070上查看</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot7.png\" alt=\"\"></p>\n<p>删除测试文件 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ hdfs dfs -rm /user/training/snapshottest/hosts</span><br><span class=\"line\">18/01/11 14:09:37 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://mycluster/user/training/snapshottest/hosts&apos; to trash at: hdfs://mycluster/user/training/.Trash/Current/user/training/snapshottest/hosts</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<p>从snapshot恢复</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot8.png\" alt=\"\"></p>\n<p>选择使用得分snapshot，并恢复</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot10.png\" alt=\"\"></p>\n<p>查看文件是否已经恢复</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ hdfs dfs -ls /user/training/snapshottest/</span><br><span class=\"line\">Found 1 items</span><br><span class=\"line\">-rw-r--r--   3 hdfs supergroup        251 2018-01-11 14:12 /user/training/snapshottest/hosts</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<h2 id=\"Get-set-ACLs-for-a-file-or-directory-structure\"><a href=\"#Get-set-ACLs-for-a-file-or-directory-structure\" class=\"headerlink\" title=\"Get/set ACLs for a file or directory structure\"></a>Get/set ACLs for a file or directory structure</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ hdfs dfs -ls /user/training/ ## 查看文件夹内内容</span><br><span class=\"line\">Found 1 items</span><br><span class=\"line\">drwx------   - training supergroup          0 2018-01-11 08:00 /user/training/.Trash</span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -put /etc/hosts /user/training/  ## 上传一个文件</span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -ls /user/training/ ## 查看上传的文件的权限</span><br><span class=\"line\">Found 2 items</span><br><span class=\"line\">drwx------   - training supergroup          0 2018-01-11 08:00 /user/training/.Trash</span><br><span class=\"line\">-rw-r--r--   3 training supergroup        251 2018-01-11 13:51 /user/training/hosts</span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -getfacl /user/training/hosts  ## 使用命令查询文件的ACL </span><br><span class=\"line\"># file: /user/training/hosts</span><br><span class=\"line\"># owner: training</span><br><span class=\"line\"># group: supergroup</span><br><span class=\"line\">user::rw-</span><br><span class=\"line\">group::r--</span><br><span class=\"line\">other::r--</span><br><span class=\"line\"></span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -getfacl /user/training/  ## 查看文件夹的ACL</span><br><span class=\"line\"># file: /user/training</span><br><span class=\"line\"># owner: training</span><br><span class=\"line\"># group: supergroup</span><br><span class=\"line\">user::rwx</span><br><span class=\"line\">group::r-x</span><br><span class=\"line\">other::r-x</span><br><span class=\"line\"></span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<h2 id=\"Benchmark-the-cluster-I-O-CPU-network\"><a href=\"#Benchmark-the-cluster-I-O-CPU-network\" class=\"headerlink\" title=\"Benchmark the cluster (I/O, CPU, network)\"></a>Benchmark the cluster (I/O, CPU, network)</h2>","site":{"data":{}},"excerpt":"<h1 id=\"Test\"><a href=\"#Test\" class=\"headerlink\" title=\"Test\"></a>Test</h1><p>Benchmark the cluster operational metrics, test system configuration for operation and efficiency</p>\n<h2 id=\"Execute-file-system-commands-via-HTTPFS\"><a href=\"#Execute-file-system-commands-via-HTTPFS\" class=\"headerlink\" title=\"Execute file system commands via HTTPFS\"></a>Execute file system commands via HTTPFS</h2><p><a href=\"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_ig_httpfs_server_curl.html#topic_25_8\" target=\"_blank\" rel=\"noopener\">Using the HttpFS Server with curl</a></p>\n<p>例如查看 用户training的Home目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ curl &quot;http://elephant:14000/webhdfs/v1?op=gethomedirectory&amp;user.name=training&quot;</span><br><span class=\"line\">&#123;&quot;Path&quot;:&quot;\\/user\\/training&quot;&#125;</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>","more":"<p><a href=\"https://archive.cloudera.com/cdh5/cdh/5/hadoop/hadoop-project-dist/hadoop-hdfs/WebHDFS.html\" target=\"_blank\" rel=\"noopener\">WebHDFS REST API</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -i -X PUT &quot;http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=CREATE</span><br><span class=\"line\">                    [&amp;overwrite=&lt;true|false&gt;][&amp;blocksize=&lt;LONG&gt;][&amp;replication=&lt;SHORT&gt;]</span><br><span class=\"line\">                    [&amp;permission=&lt;OCTAL&gt;][&amp;buffersize=&lt;INT&gt;]&quot;</span><br></pre></td></tr></table></figure>\n<p>查询一个目录 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ curl -i  &quot;http://elephant:14000/webhdfs/v1/user?op=LISTSTATUS&amp;user.name=training&quot;</span><br><span class=\"line\">HTTP/1.1 200 OK</span><br><span class=\"line\">Server: Apache-Coyote/1.1</span><br><span class=\"line\">Set-Cookie: hadoop.auth=&quot;u=training&amp;p=training&amp;t=simple-dt&amp;e=1515685732115&amp;s=9cv4wBc8rJe83+6WUg7B5xaeafE=&quot;; Path=/; HttpOnly</span><br><span class=\"line\">Content-Type: application/json</span><br><span class=\"line\">Transfer-Encoding: chunked</span><br><span class=\"line\">Date: Thu, 11 Jan 2018 05:48:52 GMT</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&quot;FileStatuses&quot;:&#123;&quot;FileStatus&quot;:[&#123;&quot;pathSuffix&quot;:&quot;history&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;mapred&quot;,&quot;group&quot;:&quot;hadoop&quot;,&quot;permission&quot;:&quot;777&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515572964146,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;hive&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;hive&quot;,&quot;group&quot;:&quot;hive&quot;,&quot;permission&quot;:&quot;1775&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515575604845,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;hue&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;hue&quot;,&quot;group&quot;:&quot;hue&quot;,&quot;permission&quot;:&quot;775&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515575637482,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;impala&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;impala&quot;,&quot;group&quot;:&quot;impala&quot;,&quot;permission&quot;:&quot;775&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515575938788,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;oozie&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;oozie&quot;,&quot;group&quot;:&quot;oozie&quot;,&quot;permission&quot;:&quot;775&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515645000550,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;spark&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;spark&quot;,&quot;group&quot;:&quot;spark&quot;,&quot;permission&quot;:&quot;751&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515573417601,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;training&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;training&quot;,&quot;group&quot;:&quot;supergroup&quot;,&quot;permission&quot;:&quot;755&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515586402387,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;]&#125;&#125;</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<h2 id=\"Efficiently-copy-data-within-a-cluster-between-clusters\"><a href=\"#Efficiently-copy-data-within-a-cluster-between-clusters\" class=\"headerlink\" title=\"Efficiently copy data within a cluster/between clusters\"></a>Efficiently copy data within a cluster/between clusters</h2><p><strong>迁移hdfs数据至新集群</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hadoop distcp -update hdfs://hostname:port/path hdfs://ip:8020/path</span><br></pre></td></tr></table></figure>\n<p><strong>迁移Hive数据仓库</strong></p>\n<p>可以每次迁移一个表或者一个database。<br>注意： 表的内容小文件不应过多，否则会非常慢。<br>源集群metastore数据备份导出(mysql导出)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysqldump -u root -p’密码’--skip-lock-tables -h xxx.xxx.xxx.xxx hive &gt; mysql_hive.sql</span><br></pre></td></tr></table></figure>\n<p>新的集群导入metastore数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql -u root -proot --default-character-set=utf8 hvie &lt; mysql_hive.sql</span><br></pre></td></tr></table></figure>\n<p>升级hive内容库(如果hive版本需要升级操作，同版本不需要操作)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql -uroot -proot risk -hxxx.xxx.xxx.xxx &lt; mysqlupgrade-0.13.0-to-0.14.0.mysql.sql</span><br><span class=\"line\">mysql -uroot -proot risk -hxxx.xxx.xxx.xxx &lt; mysqlupgrade-0.14.0-to-1.1.0.mysql.sql</span><br></pre></td></tr></table></figure>\n<p>版本要依据版本序列升序升级,不可跨越版本，如当前是hive0.12打算升级到0.14，需要先升级到0.13再升级到0.14</p>\n<p>修改 metastore 内容库的集群信息</p>\n<p>因为夸集群，hdfs访问的名字可能变化了，所以需要修改下hive库中的表DBS和SDS内容，除非你的集群名字或者HA的名字跟之前的一致这个就不用修改了<br>登录mysql数据库，查看：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&gt; use hive;</span><br><span class=\"line\">mysql&gt; select * from DBS;</span><br><span class=\"line\">mysql&gt; select * from SDS;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Create-restore-a-snapshot-of-an-HDFS-directory\"><a href=\"#Create-restore-a-snapshot-of-an-HDFS-directory\" class=\"headerlink\" title=\"Create/restore a snapshot of an HDFS directory\"></a>Create/restore a snapshot of an HDFS directory</h2><p>创建一个测试文件夹和文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ hdfs dfs -mkdir /user/training/snapshottest</span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -put /etc/hosts /user/training/snapshottest/</span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -ls /user/training/snapshottest/</span><br><span class=\"line\">Found 1 items</span><br><span class=\"line\">-rw-r--r--   3 training supergroup        251 2018-01-11 14:03 /user/training/snapshottest/hosts</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<p>HOME -&gt; Cluster -&gt; HDFS -&gt; File Browser -&gt; /user/training/snapshottest/ 点击 Enable Snapshot</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot1.png\" alt=\"\"></p>\n<p>启用</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot3.png\" alt=\"\"></p>\n<p>Take Sanpshot 并命名 </p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot4.png\" alt=\"\"></p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot5.png\" alt=\"\"></p>\n<p>在 Snapshots 下可以看到已有的Sanpshots</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot6.png\" alt=\"\"></p>\n<p>或者到Hadoop 50070上查看</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot7.png\" alt=\"\"></p>\n<p>删除测试文件 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ hdfs dfs -rm /user/training/snapshottest/hosts</span><br><span class=\"line\">18/01/11 14:09:37 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://mycluster/user/training/snapshottest/hosts&apos; to trash at: hdfs://mycluster/user/training/.Trash/Current/user/training/snapshottest/hosts</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<p>从snapshot恢复</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot8.png\" alt=\"\"></p>\n<p>选择使用得分snapshot，并恢复</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Snapshot/Snapshot10.png\" alt=\"\"></p>\n<p>查看文件是否已经恢复</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ hdfs dfs -ls /user/training/snapshottest/</span><br><span class=\"line\">Found 1 items</span><br><span class=\"line\">-rw-r--r--   3 hdfs supergroup        251 2018-01-11 14:12 /user/training/snapshottest/hosts</span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<h2 id=\"Get-set-ACLs-for-a-file-or-directory-structure\"><a href=\"#Get-set-ACLs-for-a-file-or-directory-structure\" class=\"headerlink\" title=\"Get/set ACLs for a file or directory structure\"></a>Get/set ACLs for a file or directory structure</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[training@elephant ~]$ hdfs dfs -ls /user/training/ ## 查看文件夹内内容</span><br><span class=\"line\">Found 1 items</span><br><span class=\"line\">drwx------   - training supergroup          0 2018-01-11 08:00 /user/training/.Trash</span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -put /etc/hosts /user/training/  ## 上传一个文件</span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -ls /user/training/ ## 查看上传的文件的权限</span><br><span class=\"line\">Found 2 items</span><br><span class=\"line\">drwx------   - training supergroup          0 2018-01-11 08:00 /user/training/.Trash</span><br><span class=\"line\">-rw-r--r--   3 training supergroup        251 2018-01-11 13:51 /user/training/hosts</span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -getfacl /user/training/hosts  ## 使用命令查询文件的ACL </span><br><span class=\"line\"># file: /user/training/hosts</span><br><span class=\"line\"># owner: training</span><br><span class=\"line\"># group: supergroup</span><br><span class=\"line\">user::rw-</span><br><span class=\"line\">group::r--</span><br><span class=\"line\">other::r--</span><br><span class=\"line\"></span><br><span class=\"line\">[training@elephant ~]$ hdfs dfs -getfacl /user/training/  ## 查看文件夹的ACL</span><br><span class=\"line\"># file: /user/training</span><br><span class=\"line\"># owner: training</span><br><span class=\"line\"># group: supergroup</span><br><span class=\"line\">user::rwx</span><br><span class=\"line\">group::r-x</span><br><span class=\"line\">other::r-x</span><br><span class=\"line\"></span><br><span class=\"line\">[training@elephant ~]$</span><br></pre></td></tr></table></figure>\n<h2 id=\"Benchmark-the-cluster-I-O-CPU-network\"><a href=\"#Benchmark-the-cluster-I-O-CPU-network\" class=\"headerlink\" title=\"Benchmark the cluster (I/O, CPU, network)\"></a>Benchmark the cluster (I/O, CPU, network)</h2>"},{"title":"Cloudera Manager API的简单实用","date":"2018-10-23T10:08:57.000Z","_content":"\n参考： https://www.cloudera.com/documentation/enterprise/5-15-x/topics/cm_intro_api.html\n## Alter 告警\n\nAlter是从event中获取的。\nevent的元数据\n\n|property|  type|   描述|\n|:-----|:------|:-------|\n|id|    id (string)|    事件的唯一ID.\n|content    |content (string)   |事件内容描述\n|timeOccurred   |timeOccurred (dateTime)    |事件发生事件\n|timeReceived   |timeReceived (dateTime)    |Cloudera Manager 获取事件的时间. 事件并不是顺时到达的. \n|category   |category (apiEventCategory)    |事件的分类 -- 健康的事件，审计事件或者操作时间等 UNKNOWN：未知分类；HEALTH_EVENT：健康事件；LOG_EVENT：日志事件；AUDIT_EVENT：审计事件；ACTIVITY_EVENT：活动事件；HBASE：HBase事件；SYSTEM：系统事件；\n|severity   |severity (apiEventSeverity)    |事件的严重性 UNKNOWN：未知；INFORMATIONAL：状态修改；IMPORTANT：需要注意的事件；CRITICAL：严重，需要立即解决\n|alert  |alert (boolean)    |事件是否需要晋级\n|attributes |array of attributes/attributes (apiEventAttribute) |属性列表\n\n<!-- more -->\n\n查看所有事件\nhttp://< *cloudera manager host* >:7180/api/v19/events\n查看所有警告事件 \nhttp://< *cloudera manager host* >:7180/api/v19/events/?query=alert==true\n查看严重的警告事件\nhttp://< *cloudera manager host* >:7180/api/v19/events/?query=alert==true;severity==CRITICAL\n查看一个时间区间范围内的严重警告事件\nhttp://< *cloudera manager host* >:7180/api/v19/events/?query=alert==true;severity==critical;timeReceived=ge=2018-10-16T00:00;timeReceived=lt=2018-10-18T00:10\n查看指定eventid的事件内容\nhttp://< *cloudera manager host* >:7180/api/v19/events/272baa75-0fe9-4fd9-ab27-f4e333fd524\n\n## 监控报表\n[tsquery Language](https://www.cloudera.com/documentation/enterprise/latest/topics/cm_dg_tsquery.html)\n\nhttp://< *cloudera manager host* >:7180/api/v19/timeseries/?query=select%20*%20where%20roleType=DATANODE\n\n![Alt text](/img/1540285620938.png)\n\n\nhttp://< *cloudera manager host* >:7180/api/v19/timeseries/?query=select%20cpu_user_rate%20where%20roleType=DATANODE\n![Alt text](/img/1540285662271.png)\n\n\nhttp://< *cloudera manager host* >:7180/api/v19/timeseries/?query=select%20await_time,%20await_read_time,%20await_write_time,%20250%20where%20category=disk\n![Alt text](/img/1540285688689.png)\n\n##  如何获取Cloudera Manager已有报表的数据\n1. 打开对应报表所在的页面\n![Alt text](/img/1540285723630.png)\n2. 点击在报表的右上角上的工具选项\n![Alt text](/img/1540286887142.png)\n3. 选择弹出框的Open in Chart Builder\n![Alt text](/img/1540286907954.png)\n4. 复制查询语句\n![Alt text](/img/1540286924463.png)\n5. 在浏览器或其他restful api工具上执行查询。\nhttp://< *cloudera manager host* >:7180/api/v19/timeseries/?query=select%20cpu_percent_across_hosts%20where%20category%20=%20CLUSTER\n\n![Alt text](/img/1540287176183.png)\n\n\n\n\n","source":"_posts/Cloudera-Manager-API.md","raw":"---\ntitle: Cloudera Manager API的简单实用\ndate: 2018-10-23 18:08:57\ntags:\n- CM\n---\n\n参考： https://www.cloudera.com/documentation/enterprise/5-15-x/topics/cm_intro_api.html\n## Alter 告警\n\nAlter是从event中获取的。\nevent的元数据\n\n|property|  type|   描述|\n|:-----|:------|:-------|\n|id|    id (string)|    事件的唯一ID.\n|content    |content (string)   |事件内容描述\n|timeOccurred   |timeOccurred (dateTime)    |事件发生事件\n|timeReceived   |timeReceived (dateTime)    |Cloudera Manager 获取事件的时间. 事件并不是顺时到达的. \n|category   |category (apiEventCategory)    |事件的分类 -- 健康的事件，审计事件或者操作时间等 UNKNOWN：未知分类；HEALTH_EVENT：健康事件；LOG_EVENT：日志事件；AUDIT_EVENT：审计事件；ACTIVITY_EVENT：活动事件；HBASE：HBase事件；SYSTEM：系统事件；\n|severity   |severity (apiEventSeverity)    |事件的严重性 UNKNOWN：未知；INFORMATIONAL：状态修改；IMPORTANT：需要注意的事件；CRITICAL：严重，需要立即解决\n|alert  |alert (boolean)    |事件是否需要晋级\n|attributes |array of attributes/attributes (apiEventAttribute) |属性列表\n\n<!-- more -->\n\n查看所有事件\nhttp://< *cloudera manager host* >:7180/api/v19/events\n查看所有警告事件 \nhttp://< *cloudera manager host* >:7180/api/v19/events/?query=alert==true\n查看严重的警告事件\nhttp://< *cloudera manager host* >:7180/api/v19/events/?query=alert==true;severity==CRITICAL\n查看一个时间区间范围内的严重警告事件\nhttp://< *cloudera manager host* >:7180/api/v19/events/?query=alert==true;severity==critical;timeReceived=ge=2018-10-16T00:00;timeReceived=lt=2018-10-18T00:10\n查看指定eventid的事件内容\nhttp://< *cloudera manager host* >:7180/api/v19/events/272baa75-0fe9-4fd9-ab27-f4e333fd524\n\n## 监控报表\n[tsquery Language](https://www.cloudera.com/documentation/enterprise/latest/topics/cm_dg_tsquery.html)\n\nhttp://< *cloudera manager host* >:7180/api/v19/timeseries/?query=select%20*%20where%20roleType=DATANODE\n\n![Alt text](/img/1540285620938.png)\n\n\nhttp://< *cloudera manager host* >:7180/api/v19/timeseries/?query=select%20cpu_user_rate%20where%20roleType=DATANODE\n![Alt text](/img/1540285662271.png)\n\n\nhttp://< *cloudera manager host* >:7180/api/v19/timeseries/?query=select%20await_time,%20await_read_time,%20await_write_time,%20250%20where%20category=disk\n![Alt text](/img/1540285688689.png)\n\n##  如何获取Cloudera Manager已有报表的数据\n1. 打开对应报表所在的页面\n![Alt text](/img/1540285723630.png)\n2. 点击在报表的右上角上的工具选项\n![Alt text](/img/1540286887142.png)\n3. 选择弹出框的Open in Chart Builder\n![Alt text](/img/1540286907954.png)\n4. 复制查询语句\n![Alt text](/img/1540286924463.png)\n5. 在浏览器或其他restful api工具上执行查询。\nhttp://< *cloudera manager host* >:7180/api/v19/timeseries/?query=select%20cpu_percent_across_hosts%20where%20category%20=%20CLUSTER\n\n![Alt text](/img/1540287176183.png)\n\n\n\n\n","slug":"Cloudera-Manager-API","published":1,"updated":"2018-10-23T10:14:03.464Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzh000cinamzqsntyfc","content":"<p>参考： <a href=\"https://www.cloudera.com/documentation/enterprise/5-15-x/topics/cm_intro_api.html\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/documentation/enterprise/5-15-x/topics/cm_intro_api.html</a></p>\n<h2 id=\"Alter-告警\"><a href=\"#Alter-告警\" class=\"headerlink\" title=\"Alter 告警\"></a>Alter 告警</h2><p>Alter是从event中获取的。<br>event的元数据</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">property</th>\n<th style=\"text-align:left\">type</th>\n<th style=\"text-align:left\">描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">id</td>\n<td style=\"text-align:left\">id (string)</td>\n<td style=\"text-align:left\">事件的唯一ID.</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">content</td>\n<td style=\"text-align:left\">content (string)</td>\n<td style=\"text-align:left\">事件内容描述</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">timeOccurred</td>\n<td style=\"text-align:left\">timeOccurred (dateTime)</td>\n<td style=\"text-align:left\">事件发生事件</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">timeReceived</td>\n<td style=\"text-align:left\">timeReceived (dateTime)</td>\n<td style=\"text-align:left\">Cloudera Manager 获取事件的时间. 事件并不是顺时到达的. </td>\n</tr>\n<tr>\n<td style=\"text-align:left\">category</td>\n<td style=\"text-align:left\">category (apiEventCategory)</td>\n<td style=\"text-align:left\">事件的分类 – 健康的事件，审计事件或者操作时间等 UNKNOWN：未知分类；HEALTH_EVENT：健康事件；LOG_EVENT：日志事件；AUDIT_EVENT：审计事件；ACTIVITY_EVENT：活动事件；HBASE：HBase事件；SYSTEM：系统事件；</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">severity</td>\n<td style=\"text-align:left\">severity (apiEventSeverity)</td>\n<td style=\"text-align:left\">事件的严重性 UNKNOWN：未知；INFORMATIONAL：状态修改；IMPORTANT：需要注意的事件；CRITICAL：严重，需要立即解决</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">alert</td>\n<td style=\"text-align:left\">alert (boolean)</td>\n<td style=\"text-align:left\">事件是否需要晋级</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">attributes</td>\n<td style=\"text-align:left\">array of attributes/attributes (apiEventAttribute)</td>\n<td style=\"text-align:left\">属性列表</td>\n</tr>\n</tbody>\n</table>\n<a id=\"more\"></a>\n<p>查看所有事件<br>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/events<br>查看所有警告事件<br>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/events/?query=alert==true<br>查看严重的警告事件<br>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/events/?query=alert==true;severity==CRITICAL<br>查看一个时间区间范围内的严重警告事件<br>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/events/?query=alert==true;severity==critical;timeReceived=ge=2018-10-16T00:00;timeReceived=lt=2018-10-18T00:10<br>查看指定eventid的事件内容<br>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/events/272baa75-0fe9-4fd9-ab27-f4e333fd524</p>\n<h2 id=\"监控报表\"><a href=\"#监控报表\" class=\"headerlink\" title=\"监控报表\"></a>监控报表</h2><p><a href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/cm_dg_tsquery.html\" target=\"_blank\" rel=\"noopener\">tsquery Language</a></p>\n<p>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/timeseries/?query=select%20*%20where%20roleType=DATANODE</p>\n<p><img src=\"/img/1540285620938.png\" alt=\"Alt text\"></p>\n<p>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/timeseries/?query=select%20cpu_user_rate%20where%20roleType=DATANODE<br><img src=\"/img/1540285662271.png\" alt=\"Alt text\"></p>\n<p>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/timeseries/?query=select%20await_time,%20await_read_time,%20await_write_time,%20250%20where%20category=disk<br><img src=\"/img/1540285688689.png\" alt=\"Alt text\"></p>\n<h2 id=\"如何获取Cloudera-Manager已有报表的数据\"><a href=\"#如何获取Cloudera-Manager已有报表的数据\" class=\"headerlink\" title=\"如何获取Cloudera Manager已有报表的数据\"></a>如何获取Cloudera Manager已有报表的数据</h2><ol>\n<li>打开对应报表所在的页面<br><img src=\"/img/1540285723630.png\" alt=\"Alt text\"></li>\n<li>点击在报表的右上角上的工具选项<br><img src=\"/img/1540286887142.png\" alt=\"Alt text\"></li>\n<li>选择弹出框的Open in Chart Builder<br><img src=\"/img/1540286907954.png\" alt=\"Alt text\"></li>\n<li>复制查询语句<br><img src=\"/img/1540286924463.png\" alt=\"Alt text\"></li>\n<li>在浏览器或其他restful api工具上执行查询。<br>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/timeseries/?query=select%20cpu_percent_across_hosts%20where%20category%20=%20CLUSTER</li>\n</ol>\n<p><img src=\"/img/1540287176183.png\" alt=\"Alt text\"></p>\n","site":{"data":{}},"excerpt":"<p>参考： <a href=\"https://www.cloudera.com/documentation/enterprise/5-15-x/topics/cm_intro_api.html\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/documentation/enterprise/5-15-x/topics/cm_intro_api.html</a></p>\n<h2 id=\"Alter-告警\"><a href=\"#Alter-告警\" class=\"headerlink\" title=\"Alter 告警\"></a>Alter 告警</h2><p>Alter是从event中获取的。<br>event的元数据</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">property</th>\n<th style=\"text-align:left\">type</th>\n<th style=\"text-align:left\">描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">id</td>\n<td style=\"text-align:left\">id (string)</td>\n<td style=\"text-align:left\">事件的唯一ID.</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">content</td>\n<td style=\"text-align:left\">content (string)</td>\n<td style=\"text-align:left\">事件内容描述</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">timeOccurred</td>\n<td style=\"text-align:left\">timeOccurred (dateTime)</td>\n<td style=\"text-align:left\">事件发生事件</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">timeReceived</td>\n<td style=\"text-align:left\">timeReceived (dateTime)</td>\n<td style=\"text-align:left\">Cloudera Manager 获取事件的时间. 事件并不是顺时到达的. </td>\n</tr>\n<tr>\n<td style=\"text-align:left\">category</td>\n<td style=\"text-align:left\">category (apiEventCategory)</td>\n<td style=\"text-align:left\">事件的分类 – 健康的事件，审计事件或者操作时间等 UNKNOWN：未知分类；HEALTH_EVENT：健康事件；LOG_EVENT：日志事件；AUDIT_EVENT：审计事件；ACTIVITY_EVENT：活动事件；HBASE：HBase事件；SYSTEM：系统事件；</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">severity</td>\n<td style=\"text-align:left\">severity (apiEventSeverity)</td>\n<td style=\"text-align:left\">事件的严重性 UNKNOWN：未知；INFORMATIONAL：状态修改；IMPORTANT：需要注意的事件；CRITICAL：严重，需要立即解决</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">alert</td>\n<td style=\"text-align:left\">alert (boolean)</td>\n<td style=\"text-align:left\">事件是否需要晋级</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">attributes</td>\n<td style=\"text-align:left\">array of attributes/attributes (apiEventAttribute)</td>\n<td style=\"text-align:left\">属性列表</td>\n</tr>\n</tbody>\n</table>","more":"<p>查看所有事件<br>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/events<br>查看所有警告事件<br>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/events/?query=alert==true<br>查看严重的警告事件<br>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/events/?query=alert==true;severity==CRITICAL<br>查看一个时间区间范围内的严重警告事件<br>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/events/?query=alert==true;severity==critical;timeReceived=ge=2018-10-16T00:00;timeReceived=lt=2018-10-18T00:10<br>查看指定eventid的事件内容<br>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/events/272baa75-0fe9-4fd9-ab27-f4e333fd524</p>\n<h2 id=\"监控报表\"><a href=\"#监控报表\" class=\"headerlink\" title=\"监控报表\"></a>监控报表</h2><p><a href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/cm_dg_tsquery.html\" target=\"_blank\" rel=\"noopener\">tsquery Language</a></p>\n<p>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/timeseries/?query=select%20*%20where%20roleType=DATANODE</p>\n<p><img src=\"/img/1540285620938.png\" alt=\"Alt text\"></p>\n<p>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/timeseries/?query=select%20cpu_user_rate%20where%20roleType=DATANODE<br><img src=\"/img/1540285662271.png\" alt=\"Alt text\"></p>\n<p>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/timeseries/?query=select%20await_time,%20await_read_time,%20await_write_time,%20250%20where%20category=disk<br><img src=\"/img/1540285688689.png\" alt=\"Alt text\"></p>\n<h2 id=\"如何获取Cloudera-Manager已有报表的数据\"><a href=\"#如何获取Cloudera-Manager已有报表的数据\" class=\"headerlink\" title=\"如何获取Cloudera Manager已有报表的数据\"></a>如何获取Cloudera Manager已有报表的数据</h2><ol>\n<li>打开对应报表所在的页面<br><img src=\"/img/1540285723630.png\" alt=\"Alt text\"></li>\n<li>点击在报表的右上角上的工具选项<br><img src=\"/img/1540286887142.png\" alt=\"Alt text\"></li>\n<li>选择弹出框的Open in Chart Builder<br><img src=\"/img/1540286907954.png\" alt=\"Alt text\"></li>\n<li>复制查询语句<br><img src=\"/img/1540286924463.png\" alt=\"Alt text\"></li>\n<li>在浏览器或其他restful api工具上执行查询。<br>http://&lt; <em>cloudera manager host</em> &gt;:7180/api/v19/timeseries/?query=select%20cpu_percent_across_hosts%20where%20category%20=%20CLUSTER</li>\n</ol>\n<p><img src=\"/img/1540287176183.png\" alt=\"Alt text\"></p>"},{"title":"CCAH-131 Troubleshoot","date":"2018-05-13T00:37:26.000Z","_content":"# Troubleshoot\n\nDemonstrate ability to find the root cause of a problem, optimize inefficient execution, and resolve resource contention scenarios\n\n##  Resolve errors/warnings in Cloudera Manager\n##  Resolve performance problems/errors in cluster operation\n##  Determine reason for application failure\n##  Configure the Fair Scheduler to resolve application delays\n","source":"_posts/CCAH-131-Troubleshoot.md","raw":"---\ntitle: CCAH-131 Troubleshoot\ndate: 2018-05-13 08:37:26\ntags: CCAH-131\n---\n# Troubleshoot\n\nDemonstrate ability to find the root cause of a problem, optimize inefficient execution, and resolve resource contention scenarios\n\n##  Resolve errors/warnings in Cloudera Manager\n##  Resolve performance problems/errors in cluster operation\n##  Determine reason for application failure\n##  Configure the Fair Scheduler to resolve application delays\n","slug":"CCAH-131-Troubleshoot","published":1,"updated":"2018-09-14T01:19:59.870Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzj000finam4uxdzjyg","content":"<h1 id=\"Troubleshoot\"><a href=\"#Troubleshoot\" class=\"headerlink\" title=\"Troubleshoot\"></a>Troubleshoot</h1><p>Demonstrate ability to find the root cause of a problem, optimize inefficient execution, and resolve resource contention scenarios</p>\n<h2 id=\"Resolve-errors-warnings-in-Cloudera-Manager\"><a href=\"#Resolve-errors-warnings-in-Cloudera-Manager\" class=\"headerlink\" title=\"Resolve errors/warnings in Cloudera Manager\"></a>Resolve errors/warnings in Cloudera Manager</h2><h2 id=\"Resolve-performance-problems-errors-in-cluster-operation\"><a href=\"#Resolve-performance-problems-errors-in-cluster-operation\" class=\"headerlink\" title=\"Resolve performance problems/errors in cluster operation\"></a>Resolve performance problems/errors in cluster operation</h2><h2 id=\"Determine-reason-for-application-failure\"><a href=\"#Determine-reason-for-application-failure\" class=\"headerlink\" title=\"Determine reason for application failure\"></a>Determine reason for application failure</h2><h2 id=\"Configure-the-Fair-Scheduler-to-resolve-application-delays\"><a href=\"#Configure-the-Fair-Scheduler-to-resolve-application-delays\" class=\"headerlink\" title=\"Configure the Fair Scheduler to resolve application delays\"></a>Configure the Fair Scheduler to resolve application delays</h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Troubleshoot\"><a href=\"#Troubleshoot\" class=\"headerlink\" title=\"Troubleshoot\"></a>Troubleshoot</h1><p>Demonstrate ability to find the root cause of a problem, optimize inefficient execution, and resolve resource contention scenarios</p>\n<h2 id=\"Resolve-errors-warnings-in-Cloudera-Manager\"><a href=\"#Resolve-errors-warnings-in-Cloudera-Manager\" class=\"headerlink\" title=\"Resolve errors/warnings in Cloudera Manager\"></a>Resolve errors/warnings in Cloudera Manager</h2><h2 id=\"Resolve-performance-problems-errors-in-cluster-operation\"><a href=\"#Resolve-performance-problems-errors-in-cluster-operation\" class=\"headerlink\" title=\"Resolve performance problems/errors in cluster operation\"></a>Resolve performance problems/errors in cluster operation</h2><h2 id=\"Determine-reason-for-application-failure\"><a href=\"#Determine-reason-for-application-failure\" class=\"headerlink\" title=\"Determine reason for application failure\"></a>Determine reason for application failure</h2><h2 id=\"Configure-the-Fair-Scheduler-to-resolve-application-delays\"><a href=\"#Configure-the-Fair-Scheduler-to-resolve-application-delays\" class=\"headerlink\" title=\"Configure the Fair Scheduler to resolve application delays\"></a>Configure the Fair Scheduler to resolve application delays</h2>"},{"title":"CDH-安装准备","date":"2018-09-21T09:15:19.000Z","_content":"## 操作系统镜像文件\n### CentOS7/RedHat7\nhttps://wiki.centos.org/Download\n### CenOS6/RedHat6\nhttps://wiki.centos.org/Download\n## Cloudera Manager 安装文件\n### CentOS6\n**Cloudera Manager5**\nhttp://archive.cloudera.com/cm5/redhat/6/x86_64/cm/\n**Cloudera Manager6**\nhttps://archive.cloudera.com/cm6/6.0.0/redhat6/yum/\n### CentOS7\n\n<!-- more -->\n**Cloudera Manager5**\nhttp://archive.cloudera.com/cm5/redhat/7/x86_64/cm/\n**Cloudera Manager6**\nhttps://archive.cloudera.com/cm6/redhat/7/x86_64/cm/\n## CDH 安装文件\n### CentOS6\n**CDH5**\nhttp://archive.cloudera.com/cdh5/parcels/\n**CDH6**\nhttps://archive.cloudera.com/cdh6/6.0.0/parcels/\n### CentOS7\n**CDH5**\nhttp://archive.cloudera.com/cdh5/parcels/\n**CDH6**\nhttps://archive.cloudera.com/cdh6/6.0.0/parcels/\n\n## Kakfa\n### CentOS6\n\nhttp://archive.cloudera.com/kafka/parcels/\n\n### CentOS7\nhttp://archive.cloudera.com/kafka/parcels/\n\n## Spark 2\n### CentOS6\nhttps://archive.cloudera.com/spark2/parcels/\n### CentOS7\nhttps://archive.cloudera.com/spark2/parcels/\n\n## Oracle JDK\n**JDK1.7**\nhttp://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html\n**JDK1.8**\nhttp://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html\n## MySQL/MariaDB\n### MySQL\nhttps://dev.mysql.com/downloads/mysql/\n### MariaDB\nhttps://downloads.mariadb.org\n\n## MySQL JDBC\nhttps://dev.mysql.com/downloads/connector/j/\n\n## Hive ODBC JDBC jar\n### JDBC\nhttps://www.cloudera.com/downloads/connectors/hive/jdbc/2-6-1.html\n### ODBC\nhttps://www.cloudera.com/downloads/connectors/hive/odbc/2-5-25.html\n\n## Impala ODBC JDBC jar\n### JDBC\nhttps://www.cloudera.com/downloads/connectors/impala/jdbc/2-6-3.html\n### ODBC\nhttps://www.cloudera.com/downloads/connectors/impala/odbc/2-5-42.html\n## 报表软件\n润乾、帆软、Tableau\n","source":"_posts/CDH-安装准备.md","raw":"---\ntitle: CDH-安装准备\ndate: 2018-09-21 17:15:19\ntags:\n- CDH安装\n---\n## 操作系统镜像文件\n### CentOS7/RedHat7\nhttps://wiki.centos.org/Download\n### CenOS6/RedHat6\nhttps://wiki.centos.org/Download\n## Cloudera Manager 安装文件\n### CentOS6\n**Cloudera Manager5**\nhttp://archive.cloudera.com/cm5/redhat/6/x86_64/cm/\n**Cloudera Manager6**\nhttps://archive.cloudera.com/cm6/6.0.0/redhat6/yum/\n### CentOS7\n\n<!-- more -->\n**Cloudera Manager5**\nhttp://archive.cloudera.com/cm5/redhat/7/x86_64/cm/\n**Cloudera Manager6**\nhttps://archive.cloudera.com/cm6/redhat/7/x86_64/cm/\n## CDH 安装文件\n### CentOS6\n**CDH5**\nhttp://archive.cloudera.com/cdh5/parcels/\n**CDH6**\nhttps://archive.cloudera.com/cdh6/6.0.0/parcels/\n### CentOS7\n**CDH5**\nhttp://archive.cloudera.com/cdh5/parcels/\n**CDH6**\nhttps://archive.cloudera.com/cdh6/6.0.0/parcels/\n\n## Kakfa\n### CentOS6\n\nhttp://archive.cloudera.com/kafka/parcels/\n\n### CentOS7\nhttp://archive.cloudera.com/kafka/parcels/\n\n## Spark 2\n### CentOS6\nhttps://archive.cloudera.com/spark2/parcels/\n### CentOS7\nhttps://archive.cloudera.com/spark2/parcels/\n\n## Oracle JDK\n**JDK1.7**\nhttp://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html\n**JDK1.8**\nhttp://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html\n## MySQL/MariaDB\n### MySQL\nhttps://dev.mysql.com/downloads/mysql/\n### MariaDB\nhttps://downloads.mariadb.org\n\n## MySQL JDBC\nhttps://dev.mysql.com/downloads/connector/j/\n\n## Hive ODBC JDBC jar\n### JDBC\nhttps://www.cloudera.com/downloads/connectors/hive/jdbc/2-6-1.html\n### ODBC\nhttps://www.cloudera.com/downloads/connectors/hive/odbc/2-5-25.html\n\n## Impala ODBC JDBC jar\n### JDBC\nhttps://www.cloudera.com/downloads/connectors/impala/jdbc/2-6-3.html\n### ODBC\nhttps://www.cloudera.com/downloads/connectors/impala/odbc/2-5-42.html\n## 报表软件\n润乾、帆软、Tableau\n","slug":"CDH-安装准备","published":1,"updated":"2018-09-21T09:17:29.212Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzk000hinam9z2k62r4","content":"<h2 id=\"操作系统镜像文件\"><a href=\"#操作系统镜像文件\" class=\"headerlink\" title=\"操作系统镜像文件\"></a>操作系统镜像文件</h2><h3 id=\"CentOS7-RedHat7\"><a href=\"#CentOS7-RedHat7\" class=\"headerlink\" title=\"CentOS7/RedHat7\"></a>CentOS7/RedHat7</h3><p><a href=\"https://wiki.centos.org/Download\" target=\"_blank\" rel=\"noopener\">https://wiki.centos.org/Download</a></p>\n<h3 id=\"CenOS6-RedHat6\"><a href=\"#CenOS6-RedHat6\" class=\"headerlink\" title=\"CenOS6/RedHat6\"></a>CenOS6/RedHat6</h3><p><a href=\"https://wiki.centos.org/Download\" target=\"_blank\" rel=\"noopener\">https://wiki.centos.org/Download</a></p>\n<h2 id=\"Cloudera-Manager-安装文件\"><a href=\"#Cloudera-Manager-安装文件\" class=\"headerlink\" title=\"Cloudera Manager 安装文件\"></a>Cloudera Manager 安装文件</h2><h3 id=\"CentOS6\"><a href=\"#CentOS6\" class=\"headerlink\" title=\"CentOS6\"></a>CentOS6</h3><p><strong>Cloudera Manager5</strong><br><a href=\"http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/\" target=\"_blank\" rel=\"noopener\">http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/</a><br><strong>Cloudera Manager6</strong><br><a href=\"https://archive.cloudera.com/cm6/6.0.0/redhat6/yum/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cm6/6.0.0/redhat6/yum/</a></p>\n<h3 id=\"CentOS7\"><a href=\"#CentOS7\" class=\"headerlink\" title=\"CentOS7\"></a>CentOS7</h3><a id=\"more\"></a>\n<p><strong>Cloudera Manager5</strong><br><a href=\"http://archive.cloudera.com/cm5/redhat/7/x86_64/cm/\" target=\"_blank\" rel=\"noopener\">http://archive.cloudera.com/cm5/redhat/7/x86_64/cm/</a><br><strong>Cloudera Manager6</strong><br><a href=\"https://archive.cloudera.com/cm6/redhat/7/x86_64/cm/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cm6/redhat/7/x86_64/cm/</a></p>\n<h2 id=\"CDH-安装文件\"><a href=\"#CDH-安装文件\" class=\"headerlink\" title=\"CDH 安装文件\"></a>CDH 安装文件</h2><h3 id=\"CentOS6-1\"><a href=\"#CentOS6-1\" class=\"headerlink\" title=\"CentOS6\"></a>CentOS6</h3><p><strong>CDH5</strong><br><a href=\"http://archive.cloudera.com/cdh5/parcels/\" target=\"_blank\" rel=\"noopener\">http://archive.cloudera.com/cdh5/parcels/</a><br><strong>CDH6</strong><br><a href=\"https://archive.cloudera.com/cdh6/6.0.0/parcels/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cdh6/6.0.0/parcels/</a></p>\n<h3 id=\"CentOS7-1\"><a href=\"#CentOS7-1\" class=\"headerlink\" title=\"CentOS7\"></a>CentOS7</h3><p><strong>CDH5</strong><br><a href=\"http://archive.cloudera.com/cdh5/parcels/\" target=\"_blank\" rel=\"noopener\">http://archive.cloudera.com/cdh5/parcels/</a><br><strong>CDH6</strong><br><a href=\"https://archive.cloudera.com/cdh6/6.0.0/parcels/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cdh6/6.0.0/parcels/</a></p>\n<h2 id=\"Kakfa\"><a href=\"#Kakfa\" class=\"headerlink\" title=\"Kakfa\"></a>Kakfa</h2><h3 id=\"CentOS6-2\"><a href=\"#CentOS6-2\" class=\"headerlink\" title=\"CentOS6\"></a>CentOS6</h3><p><a href=\"http://archive.cloudera.com/kafka/parcels/\" target=\"_blank\" rel=\"noopener\">http://archive.cloudera.com/kafka/parcels/</a></p>\n<h3 id=\"CentOS7-2\"><a href=\"#CentOS7-2\" class=\"headerlink\" title=\"CentOS7\"></a>CentOS7</h3><p><a href=\"http://archive.cloudera.com/kafka/parcels/\" target=\"_blank\" rel=\"noopener\">http://archive.cloudera.com/kafka/parcels/</a></p>\n<h2 id=\"Spark-2\"><a href=\"#Spark-2\" class=\"headerlink\" title=\"Spark 2\"></a>Spark 2</h2><h3 id=\"CentOS6-3\"><a href=\"#CentOS6-3\" class=\"headerlink\" title=\"CentOS6\"></a>CentOS6</h3><p><a href=\"https://archive.cloudera.com/spark2/parcels/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/spark2/parcels/</a></p>\n<h3 id=\"CentOS7-3\"><a href=\"#CentOS7-3\" class=\"headerlink\" title=\"CentOS7\"></a>CentOS7</h3><p><a href=\"https://archive.cloudera.com/spark2/parcels/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/spark2/parcels/</a></p>\n<h2 id=\"Oracle-JDK\"><a href=\"#Oracle-JDK\" class=\"headerlink\" title=\"Oracle JDK\"></a>Oracle JDK</h2><p><strong>JDK1.7</strong><br><a href=\"http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html\" target=\"_blank\" rel=\"noopener\">http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html</a><br><strong>JDK1.8</strong><br><a href=\"http://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html\" target=\"_blank\" rel=\"noopener\">http://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html</a></p>\n<h2 id=\"MySQL-MariaDB\"><a href=\"#MySQL-MariaDB\" class=\"headerlink\" title=\"MySQL/MariaDB\"></a>MySQL/MariaDB</h2><h3 id=\"MySQL\"><a href=\"#MySQL\" class=\"headerlink\" title=\"MySQL\"></a>MySQL</h3><p><a href=\"https://dev.mysql.com/downloads/mysql/\" target=\"_blank\" rel=\"noopener\">https://dev.mysql.com/downloads/mysql/</a></p>\n<h3 id=\"MariaDB\"><a href=\"#MariaDB\" class=\"headerlink\" title=\"MariaDB\"></a>MariaDB</h3><p><a href=\"https://downloads.mariadb.org\" target=\"_blank\" rel=\"noopener\">https://downloads.mariadb.org</a></p>\n<h2 id=\"MySQL-JDBC\"><a href=\"#MySQL-JDBC\" class=\"headerlink\" title=\"MySQL JDBC\"></a>MySQL JDBC</h2><p><a href=\"https://dev.mysql.com/downloads/connector/j/\" target=\"_blank\" rel=\"noopener\">https://dev.mysql.com/downloads/connector/j/</a></p>\n<h2 id=\"Hive-ODBC-JDBC-jar\"><a href=\"#Hive-ODBC-JDBC-jar\" class=\"headerlink\" title=\"Hive ODBC JDBC jar\"></a>Hive ODBC JDBC jar</h2><h3 id=\"JDBC\"><a href=\"#JDBC\" class=\"headerlink\" title=\"JDBC\"></a>JDBC</h3><p><a href=\"https://www.cloudera.com/downloads/connectors/hive/jdbc/2-6-1.html\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/downloads/connectors/hive/jdbc/2-6-1.html</a></p>\n<h3 id=\"ODBC\"><a href=\"#ODBC\" class=\"headerlink\" title=\"ODBC\"></a>ODBC</h3><p><a href=\"https://www.cloudera.com/downloads/connectors/hive/odbc/2-5-25.html\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/downloads/connectors/hive/odbc/2-5-25.html</a></p>\n<h2 id=\"Impala-ODBC-JDBC-jar\"><a href=\"#Impala-ODBC-JDBC-jar\" class=\"headerlink\" title=\"Impala ODBC JDBC jar\"></a>Impala ODBC JDBC jar</h2><h3 id=\"JDBC-1\"><a href=\"#JDBC-1\" class=\"headerlink\" title=\"JDBC\"></a>JDBC</h3><p><a href=\"https://www.cloudera.com/downloads/connectors/impala/jdbc/2-6-3.html\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/downloads/connectors/impala/jdbc/2-6-3.html</a></p>\n<h3 id=\"ODBC-1\"><a href=\"#ODBC-1\" class=\"headerlink\" title=\"ODBC\"></a>ODBC</h3><p><a href=\"https://www.cloudera.com/downloads/connectors/impala/odbc/2-5-42.html\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/downloads/connectors/impala/odbc/2-5-42.html</a></p>\n<h2 id=\"报表软件\"><a href=\"#报表软件\" class=\"headerlink\" title=\"报表软件\"></a>报表软件</h2><p>润乾、帆软、Tableau</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"操作系统镜像文件\"><a href=\"#操作系统镜像文件\" class=\"headerlink\" title=\"操作系统镜像文件\"></a>操作系统镜像文件</h2><h3 id=\"CentOS7-RedHat7\"><a href=\"#CentOS7-RedHat7\" class=\"headerlink\" title=\"CentOS7/RedHat7\"></a>CentOS7/RedHat7</h3><p><a href=\"https://wiki.centos.org/Download\" target=\"_blank\" rel=\"noopener\">https://wiki.centos.org/Download</a></p>\n<h3 id=\"CenOS6-RedHat6\"><a href=\"#CenOS6-RedHat6\" class=\"headerlink\" title=\"CenOS6/RedHat6\"></a>CenOS6/RedHat6</h3><p><a href=\"https://wiki.centos.org/Download\" target=\"_blank\" rel=\"noopener\">https://wiki.centos.org/Download</a></p>\n<h2 id=\"Cloudera-Manager-安装文件\"><a href=\"#Cloudera-Manager-安装文件\" class=\"headerlink\" title=\"Cloudera Manager 安装文件\"></a>Cloudera Manager 安装文件</h2><h3 id=\"CentOS6\"><a href=\"#CentOS6\" class=\"headerlink\" title=\"CentOS6\"></a>CentOS6</h3><p><strong>Cloudera Manager5</strong><br><a href=\"http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/\" target=\"_blank\" rel=\"noopener\">http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/</a><br><strong>Cloudera Manager6</strong><br><a href=\"https://archive.cloudera.com/cm6/6.0.0/redhat6/yum/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cm6/6.0.0/redhat6/yum/</a></p>\n<h3 id=\"CentOS7\"><a href=\"#CentOS7\" class=\"headerlink\" title=\"CentOS7\"></a>CentOS7</h3>","more":"<p><strong>Cloudera Manager5</strong><br><a href=\"http://archive.cloudera.com/cm5/redhat/7/x86_64/cm/\" target=\"_blank\" rel=\"noopener\">http://archive.cloudera.com/cm5/redhat/7/x86_64/cm/</a><br><strong>Cloudera Manager6</strong><br><a href=\"https://archive.cloudera.com/cm6/redhat/7/x86_64/cm/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cm6/redhat/7/x86_64/cm/</a></p>\n<h2 id=\"CDH-安装文件\"><a href=\"#CDH-安装文件\" class=\"headerlink\" title=\"CDH 安装文件\"></a>CDH 安装文件</h2><h3 id=\"CentOS6-1\"><a href=\"#CentOS6-1\" class=\"headerlink\" title=\"CentOS6\"></a>CentOS6</h3><p><strong>CDH5</strong><br><a href=\"http://archive.cloudera.com/cdh5/parcels/\" target=\"_blank\" rel=\"noopener\">http://archive.cloudera.com/cdh5/parcels/</a><br><strong>CDH6</strong><br><a href=\"https://archive.cloudera.com/cdh6/6.0.0/parcels/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cdh6/6.0.0/parcels/</a></p>\n<h3 id=\"CentOS7-1\"><a href=\"#CentOS7-1\" class=\"headerlink\" title=\"CentOS7\"></a>CentOS7</h3><p><strong>CDH5</strong><br><a href=\"http://archive.cloudera.com/cdh5/parcels/\" target=\"_blank\" rel=\"noopener\">http://archive.cloudera.com/cdh5/parcels/</a><br><strong>CDH6</strong><br><a href=\"https://archive.cloudera.com/cdh6/6.0.0/parcels/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cdh6/6.0.0/parcels/</a></p>\n<h2 id=\"Kakfa\"><a href=\"#Kakfa\" class=\"headerlink\" title=\"Kakfa\"></a>Kakfa</h2><h3 id=\"CentOS6-2\"><a href=\"#CentOS6-2\" class=\"headerlink\" title=\"CentOS6\"></a>CentOS6</h3><p><a href=\"http://archive.cloudera.com/kafka/parcels/\" target=\"_blank\" rel=\"noopener\">http://archive.cloudera.com/kafka/parcels/</a></p>\n<h3 id=\"CentOS7-2\"><a href=\"#CentOS7-2\" class=\"headerlink\" title=\"CentOS7\"></a>CentOS7</h3><p><a href=\"http://archive.cloudera.com/kafka/parcels/\" target=\"_blank\" rel=\"noopener\">http://archive.cloudera.com/kafka/parcels/</a></p>\n<h2 id=\"Spark-2\"><a href=\"#Spark-2\" class=\"headerlink\" title=\"Spark 2\"></a>Spark 2</h2><h3 id=\"CentOS6-3\"><a href=\"#CentOS6-3\" class=\"headerlink\" title=\"CentOS6\"></a>CentOS6</h3><p><a href=\"https://archive.cloudera.com/spark2/parcels/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/spark2/parcels/</a></p>\n<h3 id=\"CentOS7-3\"><a href=\"#CentOS7-3\" class=\"headerlink\" title=\"CentOS7\"></a>CentOS7</h3><p><a href=\"https://archive.cloudera.com/spark2/parcels/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/spark2/parcels/</a></p>\n<h2 id=\"Oracle-JDK\"><a href=\"#Oracle-JDK\" class=\"headerlink\" title=\"Oracle JDK\"></a>Oracle JDK</h2><p><strong>JDK1.7</strong><br><a href=\"http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html\" target=\"_blank\" rel=\"noopener\">http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html</a><br><strong>JDK1.8</strong><br><a href=\"http://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html\" target=\"_blank\" rel=\"noopener\">http://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html</a></p>\n<h2 id=\"MySQL-MariaDB\"><a href=\"#MySQL-MariaDB\" class=\"headerlink\" title=\"MySQL/MariaDB\"></a>MySQL/MariaDB</h2><h3 id=\"MySQL\"><a href=\"#MySQL\" class=\"headerlink\" title=\"MySQL\"></a>MySQL</h3><p><a href=\"https://dev.mysql.com/downloads/mysql/\" target=\"_blank\" rel=\"noopener\">https://dev.mysql.com/downloads/mysql/</a></p>\n<h3 id=\"MariaDB\"><a href=\"#MariaDB\" class=\"headerlink\" title=\"MariaDB\"></a>MariaDB</h3><p><a href=\"https://downloads.mariadb.org\" target=\"_blank\" rel=\"noopener\">https://downloads.mariadb.org</a></p>\n<h2 id=\"MySQL-JDBC\"><a href=\"#MySQL-JDBC\" class=\"headerlink\" title=\"MySQL JDBC\"></a>MySQL JDBC</h2><p><a href=\"https://dev.mysql.com/downloads/connector/j/\" target=\"_blank\" rel=\"noopener\">https://dev.mysql.com/downloads/connector/j/</a></p>\n<h2 id=\"Hive-ODBC-JDBC-jar\"><a href=\"#Hive-ODBC-JDBC-jar\" class=\"headerlink\" title=\"Hive ODBC JDBC jar\"></a>Hive ODBC JDBC jar</h2><h3 id=\"JDBC\"><a href=\"#JDBC\" class=\"headerlink\" title=\"JDBC\"></a>JDBC</h3><p><a href=\"https://www.cloudera.com/downloads/connectors/hive/jdbc/2-6-1.html\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/downloads/connectors/hive/jdbc/2-6-1.html</a></p>\n<h3 id=\"ODBC\"><a href=\"#ODBC\" class=\"headerlink\" title=\"ODBC\"></a>ODBC</h3><p><a href=\"https://www.cloudera.com/downloads/connectors/hive/odbc/2-5-25.html\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/downloads/connectors/hive/odbc/2-5-25.html</a></p>\n<h2 id=\"Impala-ODBC-JDBC-jar\"><a href=\"#Impala-ODBC-JDBC-jar\" class=\"headerlink\" title=\"Impala ODBC JDBC jar\"></a>Impala ODBC JDBC jar</h2><h3 id=\"JDBC-1\"><a href=\"#JDBC-1\" class=\"headerlink\" title=\"JDBC\"></a>JDBC</h3><p><a href=\"https://www.cloudera.com/downloads/connectors/impala/jdbc/2-6-3.html\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/downloads/connectors/impala/jdbc/2-6-3.html</a></p>\n<h3 id=\"ODBC-1\"><a href=\"#ODBC-1\" class=\"headerlink\" title=\"ODBC\"></a>ODBC</h3><p><a href=\"https://www.cloudera.com/downloads/connectors/impala/odbc/2-5-42.html\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/downloads/connectors/impala/odbc/2-5-42.html</a></p>\n<h2 id=\"报表软件\"><a href=\"#报表软件\" class=\"headerlink\" title=\"报表软件\"></a>报表软件</h2><p>润乾、帆软、Tableau</p>"},{"title":"CDH 安装环境需求","date":"2018-05-13T02:10:26.000Z","_content":"# CDH 安装环境需求\n\nhttps://www.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#concept_mh3_sht_kbb\n## 操作系统\n**Red Hat/CentOS** 7.3, 7.2, 7.1,6.9, 6.8, 6.7, 6.5\n\n**SUSE** 12 SP1, 11 SP4, 11 SP3, 11 SP2\n\n**Ubuntu** 14.04,12.04\n## 文件系统\n挂载时不带atime，可以加速读取\n\n<!-- more -->\n```\n/dev/sdb1 /data1 ext4 defaults,noatime 0\nmount -o remount /data1\n```\next3,ext4，XFS\n\n## 机器\n内存： > 4G \n\nIPv6：不支持，而且必须关闭 \n\n端口： 7180，8088,8888,19888(根据实际需求增加）\n## 硬盘空间\n /var > 20G \n\n /usr > 500M\n\n /opt > 10G \n## yum源\n挂载硬盘镜像并配置本地yum源\n## Others\n\n所有的数据库使用UTF-8编码。\n\n**MySQL** 5.5,5.6,5.7\n\n**MariaDB** 5.5, 10.0\n\n**PostgreSQL** 8.1,8.3,8.4,9.1,9.2,9.3,9.4\n\n**Oracle** 11g R2,12c R1\n\n**JDK** 1.7u80,1.7u75,1.7u67,1.7u55,1.8u121,1.8u111，1.8u102，1.8u91，1.8u74，1.8u31 （如果需要安装Spark2.x 则需要JDK 1.8）\n\n**浏览器** Chrome，Firefox，Internet Explorer，Safari \n\n## 注意项\n* Cloudera Manager对应的CDH版本，el6对应于el6的版本。\n\tCDH的大版本不能比CM的大版本大。\n* 所有的CDH节点必须允许统一版本的操作系统。\n* Kudu 只可以允许在ext4 和 XFS的文件系统上。\n* 必须修改/etc/sysconfig/network里面的host名和/etc/hosts一致**\n* /etc/hosts 里不允许有大写的域名\n\n","source":"_posts/CDH-POC-Env-requirement.md","raw":"---\ntitle: CDH 安装环境需求\ndate: 2018-05-13 10:10:26\ntags: CDH\n---\n# CDH 安装环境需求\n\nhttps://www.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#concept_mh3_sht_kbb\n## 操作系统\n**Red Hat/CentOS** 7.3, 7.2, 7.1,6.9, 6.8, 6.7, 6.5\n\n**SUSE** 12 SP1, 11 SP4, 11 SP3, 11 SP2\n\n**Ubuntu** 14.04,12.04\n## 文件系统\n挂载时不带atime，可以加速读取\n\n<!-- more -->\n```\n/dev/sdb1 /data1 ext4 defaults,noatime 0\nmount -o remount /data1\n```\next3,ext4，XFS\n\n## 机器\n内存： > 4G \n\nIPv6：不支持，而且必须关闭 \n\n端口： 7180，8088,8888,19888(根据实际需求增加）\n## 硬盘空间\n /var > 20G \n\n /usr > 500M\n\n /opt > 10G \n## yum源\n挂载硬盘镜像并配置本地yum源\n## Others\n\n所有的数据库使用UTF-8编码。\n\n**MySQL** 5.5,5.6,5.7\n\n**MariaDB** 5.5, 10.0\n\n**PostgreSQL** 8.1,8.3,8.4,9.1,9.2,9.3,9.4\n\n**Oracle** 11g R2,12c R1\n\n**JDK** 1.7u80,1.7u75,1.7u67,1.7u55,1.8u121,1.8u111，1.8u102，1.8u91，1.8u74，1.8u31 （如果需要安装Spark2.x 则需要JDK 1.8）\n\n**浏览器** Chrome，Firefox，Internet Explorer，Safari \n\n## 注意项\n* Cloudera Manager对应的CDH版本，el6对应于el6的版本。\n\tCDH的大版本不能比CM的大版本大。\n* 所有的CDH节点必须允许统一版本的操作系统。\n* Kudu 只可以允许在ext4 和 XFS的文件系统上。\n* 必须修改/etc/sysconfig/network里面的host名和/etc/hosts一致**\n* /etc/hosts 里不允许有大写的域名\n\n","slug":"CDH-POC-Env-requirement","published":1,"updated":"2018-09-14T01:28:51.794Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzl000kinam3my8s3cm","content":"<h1 id=\"CDH-安装环境需求\"><a href=\"#CDH-安装环境需求\" class=\"headerlink\" title=\"CDH 安装环境需求\"></a>CDH 安装环境需求</h1><p><a href=\"https://www.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#concept_mh3_sht_kbb\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#concept_mh3_sht_kbb</a></p>\n<h2 id=\"操作系统\"><a href=\"#操作系统\" class=\"headerlink\" title=\"操作系统\"></a>操作系统</h2><p><strong>Red Hat/CentOS</strong> 7.3, 7.2, 7.1,6.9, 6.8, 6.7, 6.5</p>\n<p><strong>SUSE</strong> 12 SP1, 11 SP4, 11 SP3, 11 SP2</p>\n<p><strong>Ubuntu</strong> 14.04,12.04</p>\n<h2 id=\"文件系统\"><a href=\"#文件系统\" class=\"headerlink\" title=\"文件系统\"></a>文件系统</h2><p>挂载时不带atime，可以加速读取</p>\n<a id=\"more\"></a>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/dev/sdb1 /data1 ext4 defaults,noatime 0</span><br><span class=\"line\">mount -o remount /data1</span><br></pre></td></tr></table></figure>\n<p>ext3,ext4，XFS</p>\n<h2 id=\"机器\"><a href=\"#机器\" class=\"headerlink\" title=\"机器\"></a>机器</h2><p>内存： &gt; 4G </p>\n<p>IPv6：不支持，而且必须关闭 </p>\n<p>端口： 7180，8088,8888,19888(根据实际需求增加）</p>\n<h2 id=\"硬盘空间\"><a href=\"#硬盘空间\" class=\"headerlink\" title=\"硬盘空间\"></a>硬盘空间</h2><p> /var &gt; 20G </p>\n<p> /usr &gt; 500M</p>\n<p> /opt &gt; 10G </p>\n<h2 id=\"yum源\"><a href=\"#yum源\" class=\"headerlink\" title=\"yum源\"></a>yum源</h2><p>挂载硬盘镜像并配置本地yum源</p>\n<h2 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h2><p>所有的数据库使用UTF-8编码。</p>\n<p><strong>MySQL</strong> 5.5,5.6,5.7</p>\n<p><strong>MariaDB</strong> 5.5, 10.0</p>\n<p><strong>PostgreSQL</strong> 8.1,8.3,8.4,9.1,9.2,9.3,9.4</p>\n<p><strong>Oracle</strong> 11g R2,12c R1</p>\n<p><strong>JDK</strong> 1.7u80,1.7u75,1.7u67,1.7u55,1.8u121,1.8u111，1.8u102，1.8u91，1.8u74，1.8u31 （如果需要安装Spark2.x 则需要JDK 1.8）</p>\n<p><strong>浏览器</strong> Chrome，Firefox，Internet Explorer，Safari </p>\n<h2 id=\"注意项\"><a href=\"#注意项\" class=\"headerlink\" title=\"注意项\"></a>注意项</h2><ul>\n<li>Cloudera Manager对应的CDH版本，el6对应于el6的版本。<br>  CDH的大版本不能比CM的大版本大。</li>\n<li>所有的CDH节点必须允许统一版本的操作系统。</li>\n<li>Kudu 只可以允许在ext4 和 XFS的文件系统上。</li>\n<li>必须修改/etc/sysconfig/network里面的host名和/etc/hosts一致**</li>\n<li>/etc/hosts 里不允许有大写的域名</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"CDH-安装环境需求\"><a href=\"#CDH-安装环境需求\" class=\"headerlink\" title=\"CDH 安装环境需求\"></a>CDH 安装环境需求</h1><p><a href=\"https://www.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#concept_mh3_sht_kbb\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#concept_mh3_sht_kbb</a></p>\n<h2 id=\"操作系统\"><a href=\"#操作系统\" class=\"headerlink\" title=\"操作系统\"></a>操作系统</h2><p><strong>Red Hat/CentOS</strong> 7.3, 7.2, 7.1,6.9, 6.8, 6.7, 6.5</p>\n<p><strong>SUSE</strong> 12 SP1, 11 SP4, 11 SP3, 11 SP2</p>\n<p><strong>Ubuntu</strong> 14.04,12.04</p>\n<h2 id=\"文件系统\"><a href=\"#文件系统\" class=\"headerlink\" title=\"文件系统\"></a>文件系统</h2><p>挂载时不带atime，可以加速读取</p>","more":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/dev/sdb1 /data1 ext4 defaults,noatime 0</span><br><span class=\"line\">mount -o remount /data1</span><br></pre></td></tr></table></figure>\n<p>ext3,ext4，XFS</p>\n<h2 id=\"机器\"><a href=\"#机器\" class=\"headerlink\" title=\"机器\"></a>机器</h2><p>内存： &gt; 4G </p>\n<p>IPv6：不支持，而且必须关闭 </p>\n<p>端口： 7180，8088,8888,19888(根据实际需求增加）</p>\n<h2 id=\"硬盘空间\"><a href=\"#硬盘空间\" class=\"headerlink\" title=\"硬盘空间\"></a>硬盘空间</h2><p> /var &gt; 20G </p>\n<p> /usr &gt; 500M</p>\n<p> /opt &gt; 10G </p>\n<h2 id=\"yum源\"><a href=\"#yum源\" class=\"headerlink\" title=\"yum源\"></a>yum源</h2><p>挂载硬盘镜像并配置本地yum源</p>\n<h2 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h2><p>所有的数据库使用UTF-8编码。</p>\n<p><strong>MySQL</strong> 5.5,5.6,5.7</p>\n<p><strong>MariaDB</strong> 5.5, 10.0</p>\n<p><strong>PostgreSQL</strong> 8.1,8.3,8.4,9.1,9.2,9.3,9.4</p>\n<p><strong>Oracle</strong> 11g R2,12c R1</p>\n<p><strong>JDK</strong> 1.7u80,1.7u75,1.7u67,1.7u55,1.8u121,1.8u111，1.8u102，1.8u91，1.8u74，1.8u31 （如果需要安装Spark2.x 则需要JDK 1.8）</p>\n<p><strong>浏览器</strong> Chrome，Firefox，Internet Explorer，Safari </p>\n<h2 id=\"注意项\"><a href=\"#注意项\" class=\"headerlink\" title=\"注意项\"></a>注意项</h2><ul>\n<li>Cloudera Manager对应的CDH版本，el6对应于el6的版本。<br>  CDH的大版本不能比CM的大版本大。</li>\n<li>所有的CDH节点必须允许统一版本的操作系统。</li>\n<li>Kudu 只可以允许在ext4 和 XFS的文件系统上。</li>\n<li>必须修改/etc/sysconfig/network里面的host名和/etc/hosts一致**</li>\n<li>/etc/hosts 里不允许有大写的域名</li>\n</ul>"},{"title":"Configuration DbVisualize to Connect Hive with Kerberos On CDH","date":"2018-05-13T03:20:26.000Z","_content":"# Configuration DbVisualize to Connect Hive with Kerberos On CDH\n\n## What you need\n1. DbVisualize  Download From *[Here](http://www.dbvis.com/download/)*\n2. Kerbers Windows Tool. Download From *[Here(64)](http://web.mit.edu/kerberos/dist/kfw/4.0/kfw-4.0.1amd64.msi)*  *[Here(32)](http://web.mit.edu/kerberos/dist/kfw/4.0/kfw-4.0.1amd64.msi)*\n3. Jre Download From *[Here](http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html)*\n4. krb5.conf / krb5.ini *This should copy from your kerberos server.*\n5. jaas.conf\n6. Cloudera Hive JDBC Driver Download from *[Here](https://www.cloudera.com/downloads/connectors/hive/jdbc/2-5-4.html)*\n7. Make sure your laptop have the **UTP** access from port 88 to the kdc server.\n\n<!-- more -->\n\n## Install *dbvis_windows-x64_10_0_jre.exe* \n\n## Install *kfw-4.0.1-amd64.msi* \n\n## Config jre\ncp local_policy.jar US_export_policy.jar from jre folder to %DbViisualize_HOME%\\jre\\lib\\security\n\n![](https://i.imgur.com/b7xoKOb.png)\n\n## Configuration Envrionment Parameters\n\nKRB5_CONFIG=C:\\Windows\\krb5.ini\nKRB5CCNAME=C:\\Users\\Username\\krb5cache\n![](https://i.imgur.com/vbaCQCS.png)\n![](https://i.imgur.com/qMqzOhE.png)\n\n## Kinit\nUse Administrator run cmd\n\n    cd C:\\\"Program Files\"\\MIT\\Kerberos\\bin\n    kinit -kt hive.keytab -c *cachefilename* hive/h2_server.com@REALM\n    klist -c *cachefilename*\n\n cachefilename is KRB5CCNAME\n\n## jass.conf \n\nHiveClient {\n  com.sun.security.auth.module.Krb5LoginModule required\n  doNotPrompt=true\n  useTicketCache=true\n  principal=\"hive/example@HTSEC.COM\"\n  useKeyTab=true\n  keyTab=\"C:/Users/Administrator/hive.keytab\"\n  client=true;\n};\n\n\n## Start DbVisualize\nUse cmd to start DbVisualize \n\n![](https://i.imgur.com/fGFxAZv.png)\n\nset jvm on DbVisualize\n![](https://i.imgur.com/lljTOzx.png)\n\n\t-Dsun.security.jgss.debug=true\n\t-Dsun.security.krb5.debug=true\n\t-Djava.security.krb5.realm=REALM.COM\n\t-Djava.security.krb5.kdc=example.com\n\t-Djava.security.auth.login.config=C:\\Users\\Users\\jaas.conf\n\t-Djava.security.krb5.conf=C:\\Windows\\krb5.ini\n\nrestart DbVisualize\n\n## Load Driver\n![](https://i.imgur.com/nvHuG8q.png)\n![](https://i.imgur.com/Mfb3ZlN.png)\n![](https://i.imgur.com/aI4q02J.png)\n\n## Connect Hive2\n\n    jdbc:hive2://example.com:10000/default;AuthMech=1;KrbRealm=REALM.COM;KrbHostFQDN=h2_server.com;KrbServiceName=hive;\n","source":"_posts/Configuration-DbVisualize-Connect-Hive.md","raw":"---\ntitle: Configuration DbVisualize to Connect Hive with Kerberos On CDH\ndate: 2018-05-13 11:20:26\ntags: \n  - CDH\n  - Hive\n  - Kerberos\n---\n# Configuration DbVisualize to Connect Hive with Kerberos On CDH\n\n## What you need\n1. DbVisualize  Download From *[Here](http://www.dbvis.com/download/)*\n2. Kerbers Windows Tool. Download From *[Here(64)](http://web.mit.edu/kerberos/dist/kfw/4.0/kfw-4.0.1amd64.msi)*  *[Here(32)](http://web.mit.edu/kerberos/dist/kfw/4.0/kfw-4.0.1amd64.msi)*\n3. Jre Download From *[Here](http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html)*\n4. krb5.conf / krb5.ini *This should copy from your kerberos server.*\n5. jaas.conf\n6. Cloudera Hive JDBC Driver Download from *[Here](https://www.cloudera.com/downloads/connectors/hive/jdbc/2-5-4.html)*\n7. Make sure your laptop have the **UTP** access from port 88 to the kdc server.\n\n<!-- more -->\n\n## Install *dbvis_windows-x64_10_0_jre.exe* \n\n## Install *kfw-4.0.1-amd64.msi* \n\n## Config jre\ncp local_policy.jar US_export_policy.jar from jre folder to %DbViisualize_HOME%\\jre\\lib\\security\n\n![](https://i.imgur.com/b7xoKOb.png)\n\n## Configuration Envrionment Parameters\n\nKRB5_CONFIG=C:\\Windows\\krb5.ini\nKRB5CCNAME=C:\\Users\\Username\\krb5cache\n![](https://i.imgur.com/vbaCQCS.png)\n![](https://i.imgur.com/qMqzOhE.png)\n\n## Kinit\nUse Administrator run cmd\n\n    cd C:\\\"Program Files\"\\MIT\\Kerberos\\bin\n    kinit -kt hive.keytab -c *cachefilename* hive/h2_server.com@REALM\n    klist -c *cachefilename*\n\n cachefilename is KRB5CCNAME\n\n## jass.conf \n\nHiveClient {\n  com.sun.security.auth.module.Krb5LoginModule required\n  doNotPrompt=true\n  useTicketCache=true\n  principal=\"hive/example@HTSEC.COM\"\n  useKeyTab=true\n  keyTab=\"C:/Users/Administrator/hive.keytab\"\n  client=true;\n};\n\n\n## Start DbVisualize\nUse cmd to start DbVisualize \n\n![](https://i.imgur.com/fGFxAZv.png)\n\nset jvm on DbVisualize\n![](https://i.imgur.com/lljTOzx.png)\n\n\t-Dsun.security.jgss.debug=true\n\t-Dsun.security.krb5.debug=true\n\t-Djava.security.krb5.realm=REALM.COM\n\t-Djava.security.krb5.kdc=example.com\n\t-Djava.security.auth.login.config=C:\\Users\\Users\\jaas.conf\n\t-Djava.security.krb5.conf=C:\\Windows\\krb5.ini\n\nrestart DbVisualize\n\n## Load Driver\n![](https://i.imgur.com/nvHuG8q.png)\n![](https://i.imgur.com/Mfb3ZlN.png)\n![](https://i.imgur.com/aI4q02J.png)\n\n## Connect Hive2\n\n    jdbc:hive2://example.com:10000/default;AuthMech=1;KrbRealm=REALM.COM;KrbHostFQDN=h2_server.com;KrbServiceName=hive;\n","slug":"Configuration-DbVisualize-Connect-Hive","published":1,"updated":"2018-09-14T01:26:31.073Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzm000minamicibc82a","content":"<h1 id=\"Configuration-DbVisualize-to-Connect-Hive-with-Kerberos-On-CDH\"><a href=\"#Configuration-DbVisualize-to-Connect-Hive-with-Kerberos-On-CDH\" class=\"headerlink\" title=\"Configuration DbVisualize to Connect Hive with Kerberos On CDH\"></a>Configuration DbVisualize to Connect Hive with Kerberos On CDH</h1><h2 id=\"What-you-need\"><a href=\"#What-you-need\" class=\"headerlink\" title=\"What you need\"></a>What you need</h2><ol>\n<li>DbVisualize  Download From <em><a href=\"http://www.dbvis.com/download/\" target=\"_blank\" rel=\"noopener\">Here</a></em></li>\n<li>Kerbers Windows Tool. Download From <em><a href=\"http://web.mit.edu/kerberos/dist/kfw/4.0/kfw-4.0.1amd64.msi\" target=\"_blank\" rel=\"noopener\">Here(64)</a></em>  <em><a href=\"http://web.mit.edu/kerberos/dist/kfw/4.0/kfw-4.0.1amd64.msi\" target=\"_blank\" rel=\"noopener\">Here(32)</a></em></li>\n<li>Jre Download From <em><a href=\"http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html\" target=\"_blank\" rel=\"noopener\">Here</a></em></li>\n<li>krb5.conf / krb5.ini <em>This should copy from your kerberos server.</em></li>\n<li>jaas.conf</li>\n<li>Cloudera Hive JDBC Driver Download from <em><a href=\"https://www.cloudera.com/downloads/connectors/hive/jdbc/2-5-4.html\" target=\"_blank\" rel=\"noopener\">Here</a></em></li>\n<li>Make sure your laptop have the <strong>UTP</strong> access from port 88 to the kdc server.</li>\n</ol>\n<a id=\"more\"></a>\n<h2 id=\"Install-dbvis-windows-x64-10-0-jre-exe\"><a href=\"#Install-dbvis-windows-x64-10-0-jre-exe\" class=\"headerlink\" title=\"Install dbvis_windows-x64_10_0_jre.exe\"></a>Install <em>dbvis_windows-x64_10_0_jre.exe</em></h2><h2 id=\"Install-kfw-4-0-1-amd64-msi\"><a href=\"#Install-kfw-4-0-1-amd64-msi\" class=\"headerlink\" title=\"Install kfw-4.0.1-amd64.msi\"></a>Install <em>kfw-4.0.1-amd64.msi</em></h2><h2 id=\"Config-jre\"><a href=\"#Config-jre\" class=\"headerlink\" title=\"Config jre\"></a>Config jre</h2><p>cp local_policy.jar US_export_policy.jar from jre folder to %DbViisualize_HOME%\\jre\\lib\\security</p>\n<p><img src=\"https://i.imgur.com/b7xoKOb.png\" alt=\"\"></p>\n<h2 id=\"Configuration-Envrionment-Parameters\"><a href=\"#Configuration-Envrionment-Parameters\" class=\"headerlink\" title=\"Configuration Envrionment Parameters\"></a>Configuration Envrionment Parameters</h2><p>KRB5_CONFIG=C:\\Windows\\krb5.ini<br>KRB5CCNAME=C:\\Users\\Username\\krb5cache<br><img src=\"https://i.imgur.com/vbaCQCS.png\" alt=\"\"><br><img src=\"https://i.imgur.com/qMqzOhE.png\" alt=\"\"></p>\n<h2 id=\"Kinit\"><a href=\"#Kinit\" class=\"headerlink\" title=\"Kinit\"></a>Kinit</h2><p>Use Administrator run cmd</p>\n<pre><code>cd C:\\&quot;Program Files&quot;\\MIT\\Kerberos\\bin\nkinit -kt hive.keytab -c *cachefilename* hive/h2_server.com@REALM\nklist -c *cachefilename*\n</code></pre><p> cachefilename is KRB5CCNAME</p>\n<h2 id=\"jass-conf\"><a href=\"#jass-conf\" class=\"headerlink\" title=\"jass.conf\"></a>jass.conf</h2><p>HiveClient {<br>  com.sun.security.auth.module.Krb5LoginModule required<br>  doNotPrompt=true<br>  useTicketCache=true<br>  principal=”hive/example@HTSEC.COM”<br>  useKeyTab=true<br>  keyTab=”C:/Users/Administrator/hive.keytab”<br>  client=true;<br>};</p>\n<h2 id=\"Start-DbVisualize\"><a href=\"#Start-DbVisualize\" class=\"headerlink\" title=\"Start DbVisualize\"></a>Start DbVisualize</h2><p>Use cmd to start DbVisualize </p>\n<p><img src=\"https://i.imgur.com/fGFxAZv.png\" alt=\"\"></p>\n<p>set jvm on DbVisualize<br><img src=\"https://i.imgur.com/lljTOzx.png\" alt=\"\"></p>\n<pre><code>-Dsun.security.jgss.debug=true\n-Dsun.security.krb5.debug=true\n-Djava.security.krb5.realm=REALM.COM\n-Djava.security.krb5.kdc=example.com\n-Djava.security.auth.login.config=C:\\Users\\Users\\jaas.conf\n-Djava.security.krb5.conf=C:\\Windows\\krb5.ini\n</code></pre><p>restart DbVisualize</p>\n<h2 id=\"Load-Driver\"><a href=\"#Load-Driver\" class=\"headerlink\" title=\"Load Driver\"></a>Load Driver</h2><p><img src=\"https://i.imgur.com/nvHuG8q.png\" alt=\"\"><br><img src=\"https://i.imgur.com/Mfb3ZlN.png\" alt=\"\"><br><img src=\"https://i.imgur.com/aI4q02J.png\" alt=\"\"></p>\n<h2 id=\"Connect-Hive2\"><a href=\"#Connect-Hive2\" class=\"headerlink\" title=\"Connect Hive2\"></a>Connect Hive2</h2><pre><code>jdbc:hive2://example.com:10000/default;AuthMech=1;KrbRealm=REALM.COM;KrbHostFQDN=h2_server.com;KrbServiceName=hive;\n</code></pre>","site":{"data":{}},"excerpt":"<h1 id=\"Configuration-DbVisualize-to-Connect-Hive-with-Kerberos-On-CDH\"><a href=\"#Configuration-DbVisualize-to-Connect-Hive-with-Kerberos-On-CDH\" class=\"headerlink\" title=\"Configuration DbVisualize to Connect Hive with Kerberos On CDH\"></a>Configuration DbVisualize to Connect Hive with Kerberos On CDH</h1><h2 id=\"What-you-need\"><a href=\"#What-you-need\" class=\"headerlink\" title=\"What you need\"></a>What you need</h2><ol>\n<li>DbVisualize  Download From <em><a href=\"http://www.dbvis.com/download/\" target=\"_blank\" rel=\"noopener\">Here</a></em></li>\n<li>Kerbers Windows Tool. Download From <em><a href=\"http://web.mit.edu/kerberos/dist/kfw/4.0/kfw-4.0.1amd64.msi\" target=\"_blank\" rel=\"noopener\">Here(64)</a></em>  <em><a href=\"http://web.mit.edu/kerberos/dist/kfw/4.0/kfw-4.0.1amd64.msi\" target=\"_blank\" rel=\"noopener\">Here(32)</a></em></li>\n<li>Jre Download From <em><a href=\"http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html\" target=\"_blank\" rel=\"noopener\">Here</a></em></li>\n<li>krb5.conf / krb5.ini <em>This should copy from your kerberos server.</em></li>\n<li>jaas.conf</li>\n<li>Cloudera Hive JDBC Driver Download from <em><a href=\"https://www.cloudera.com/downloads/connectors/hive/jdbc/2-5-4.html\" target=\"_blank\" rel=\"noopener\">Here</a></em></li>\n<li>Make sure your laptop have the <strong>UTP</strong> access from port 88 to the kdc server.</li>\n</ol>","more":"<h2 id=\"Install-dbvis-windows-x64-10-0-jre-exe\"><a href=\"#Install-dbvis-windows-x64-10-0-jre-exe\" class=\"headerlink\" title=\"Install dbvis_windows-x64_10_0_jre.exe\"></a>Install <em>dbvis_windows-x64_10_0_jre.exe</em></h2><h2 id=\"Install-kfw-4-0-1-amd64-msi\"><a href=\"#Install-kfw-4-0-1-amd64-msi\" class=\"headerlink\" title=\"Install kfw-4.0.1-amd64.msi\"></a>Install <em>kfw-4.0.1-amd64.msi</em></h2><h2 id=\"Config-jre\"><a href=\"#Config-jre\" class=\"headerlink\" title=\"Config jre\"></a>Config jre</h2><p>cp local_policy.jar US_export_policy.jar from jre folder to %DbViisualize_HOME%\\jre\\lib\\security</p>\n<p><img src=\"https://i.imgur.com/b7xoKOb.png\" alt=\"\"></p>\n<h2 id=\"Configuration-Envrionment-Parameters\"><a href=\"#Configuration-Envrionment-Parameters\" class=\"headerlink\" title=\"Configuration Envrionment Parameters\"></a>Configuration Envrionment Parameters</h2><p>KRB5_CONFIG=C:\\Windows\\krb5.ini<br>KRB5CCNAME=C:\\Users\\Username\\krb5cache<br><img src=\"https://i.imgur.com/vbaCQCS.png\" alt=\"\"><br><img src=\"https://i.imgur.com/qMqzOhE.png\" alt=\"\"></p>\n<h2 id=\"Kinit\"><a href=\"#Kinit\" class=\"headerlink\" title=\"Kinit\"></a>Kinit</h2><p>Use Administrator run cmd</p>\n<pre><code>cd C:\\&quot;Program Files&quot;\\MIT\\Kerberos\\bin\nkinit -kt hive.keytab -c *cachefilename* hive/h2_server.com@REALM\nklist -c *cachefilename*\n</code></pre><p> cachefilename is KRB5CCNAME</p>\n<h2 id=\"jass-conf\"><a href=\"#jass-conf\" class=\"headerlink\" title=\"jass.conf\"></a>jass.conf</h2><p>HiveClient {<br>  com.sun.security.auth.module.Krb5LoginModule required<br>  doNotPrompt=true<br>  useTicketCache=true<br>  principal=”hive/example@HTSEC.COM”<br>  useKeyTab=true<br>  keyTab=”C:/Users/Administrator/hive.keytab”<br>  client=true;<br>};</p>\n<h2 id=\"Start-DbVisualize\"><a href=\"#Start-DbVisualize\" class=\"headerlink\" title=\"Start DbVisualize\"></a>Start DbVisualize</h2><p>Use cmd to start DbVisualize </p>\n<p><img src=\"https://i.imgur.com/fGFxAZv.png\" alt=\"\"></p>\n<p>set jvm on DbVisualize<br><img src=\"https://i.imgur.com/lljTOzx.png\" alt=\"\"></p>\n<pre><code>-Dsun.security.jgss.debug=true\n-Dsun.security.krb5.debug=true\n-Djava.security.krb5.realm=REALM.COM\n-Djava.security.krb5.kdc=example.com\n-Djava.security.auth.login.config=C:\\Users\\Users\\jaas.conf\n-Djava.security.krb5.conf=C:\\Windows\\krb5.ini\n</code></pre><p>restart DbVisualize</p>\n<h2 id=\"Load-Driver\"><a href=\"#Load-Driver\" class=\"headerlink\" title=\"Load Driver\"></a>Load Driver</h2><p><img src=\"https://i.imgur.com/nvHuG8q.png\" alt=\"\"><br><img src=\"https://i.imgur.com/Mfb3ZlN.png\" alt=\"\"><br><img src=\"https://i.imgur.com/aI4q02J.png\" alt=\"\"></p>\n<h2 id=\"Connect-Hive2\"><a href=\"#Connect-Hive2\" class=\"headerlink\" title=\"Connect Hive2\"></a>Connect Hive2</h2><pre><code>jdbc:hive2://example.com:10000/default;AuthMech=1;KrbRealm=REALM.COM;KrbHostFQDN=h2_server.com;KrbServiceName=hive;\n</code></pre>"},{"title":"CDH 用户管理","date":"2018-05-13T02:15:26.000Z","_content":"# CDH User Managerment\n\n## Hive\n\n**创建一个普通用户 hive_test**\n\n- 在OS上创建这个普通用户 hive_test\n\n```\t\nnodes=`cat /etc/hosts  | grep -v ^# | grep htsec | awk '{print $2}'`\nfor target in $nodes; do\nssh root@$target <<comd\nuseradd hive_test\nusermod -a -G hive hive_test\ncomd\ndone\n```\n<!-- more -->\n- 在hdfs上创建用户文件夹\n  如果集群启用了Kerberos，那么需要首先kinit，否则不需要   \n\t\n```\nkinit -kt hdfs.keytab hdfs/nn1.example.com\nhdfs dfs -mkdir -p /user/hive_test\nhdfs dfs -chown -R dev_user:hive_test /user/hive_test\n```\n\t\n**创建role**\n\n只有admin用户可以创建和删除roles；hive、hue、impala都是默认的admin用户。\n\n```\nkinit -kt hive.keytab hive/nn1.example.com \nbeeline -u \"jdbc:hive2://nn1.example.com:10000/default;principal=hive/nn1.example.com@EXAMPLE.COM\"\ncreate role hive_test;  //创建用户 \n```\n\n**删除role**\n\n\tdrop role hive_test；\n\t\n**grant role**\n\n\tgrant role hive_test to group hive_test;  ## 将用户分到一个组，在Sentry里用户必须在组里才能使用。\n**revoke role**\n\n\trevoke role hive_test from group hive_test；\n\n- 在hive上grant权限\n\n**grant权限**\n\n```\nGRANT    \n    <PRIVILEGE> [, <PRIVILEGE> ]    \n    ON <OBJECT> <object_name>    \n    TO ROLE <roleName> [,ROLE <roleName>]\n\ne.g. grant all on server server1 to role hive_test;\n```\n\n- hive的权限分类\n\n* **ALL**： 所有权限\n* **ALTER**：允许修改元数据（modify metadata data of  object）---表信息数据\n* **UPDATE**： 允许修改物理数据（modify physical data of  object）---实际数据\n* **CREATE**： 允许进行Create操作\n* **DROP**： 允许进行DROP操作\n* **INDEX**： 允许建索引（目前还没有实现）\n* **LOCK**： 当出现并发的使用允许用户进行LOCK和UNLOCK操作\n* **SELECT**：允许用户进行SELECT操作\n* **SHOW_DATABASE**： 允许用户查看可用的数据库\n\n- 查询权限\n\n```\nshow roles;\nshow grant role hive_test;\n```\n**revoke 权限**\n\n\tREVOKE    \n    <PRIVILEGE> [, <PRIVILEGE> ]    \n    ON <OBJECT> <object_name>    \n    FROM ROLE <roleName> [,ROLE <roleName>]\n\te.g. REVOKE SELECT(column_name) ON TABLE table_name FROM ROLE \n\t     REVOKE SELECT(column_name) ON TABLE table_name FROM ROLE role_name; // 将列的查询权限赋值给用户\n\t     REVOKE ROLE role_name from GROUP group_name[,GROUP group_name] // 将用户移除一个组\n\n\n**grant URIs**\n\t\n\tGRANT ALL ON URI 'hdfs://namenode:XXX/path/to/table'\n\tCREATE EXTERNAL TABLE foo LOCATION 'namenode:XXX/path/to/table'\n\n\n**SET ROLE Statement**\n\n*SET ROLE NONE*\n   使所有的role变成非活动状态。\n\n*SET ROLE ALL*\n   使ROLE拥有所有的权限。\n\n*SET ROLE <role name>*\n   激活一个role，并将分配其的权限激活。\n\n\n## Impala\nimpala 的权限和hive一致\n\n## Hbase\n- 在OS上创建用户 hive_test （同Hive)\n\t需要添加用户到组hbase\n\tusermod -a -G base hive_test\n- 在hdfs上创建用户文件夹 （同Hive)\n- 在Hbase上grant权限\n\t如果集群启用了Kerberos，那么需要首先kinit，否则不需要 \n```\nkinit -kt hbase.keytab hbase/nn1.example.com\ngrant 'hive_test' ,'RWXCA','@hbase'\n```\n- Hbase权限分类\n  HBase提供的五个权限标识符：RWXCA,分别对应着READ('R'), WRITE('W'), EXEC('X'), CREATE('C'), ADMIN('A')\n   HBase提供的五个权限标识符：RWXCA,分别对应着READ('R'), WRITE('W'), EXEC('X'), CREATE('C'), ADMIN('A')\nHBase提供的安全管控级别包括：\nSuperuser：拥有所有权限的超级管理员用户。通过hbase.superuser参数配置\nGlobal：全局权限可以作用在集群所有的表上。\nNamespace ：命名空间级。\nTable：表级。\nColumnFamily：列簇级权限。\nCell：单元级。\n\t\n将namespace权限符给用户\n    \n    grant 'user', 'RWXCA', '@namespace'\n将表的权限赋给用户\n\n    grant 'user', 'RWXCA', 'tablename'\n将表的一个字段的权限赋给用户\n    \n    grant 'user', 'RWXCA', 'TABLE', 'CF', 'CQ'\nhttp://hbase.apache.org/book.html\n\n- 查看用户权限\n```\n    user_permission '@namespace'\n```\n","source":"_posts/CDH-User-Managerment.md","raw":"---\ntitle: CDH 用户管理\ndate: 2018-05-13 10:15:26\ntags: \n  - CDH\n  - User\n---\n# CDH User Managerment\n\n## Hive\n\n**创建一个普通用户 hive_test**\n\n- 在OS上创建这个普通用户 hive_test\n\n```\t\nnodes=`cat /etc/hosts  | grep -v ^# | grep htsec | awk '{print $2}'`\nfor target in $nodes; do\nssh root@$target <<comd\nuseradd hive_test\nusermod -a -G hive hive_test\ncomd\ndone\n```\n<!-- more -->\n- 在hdfs上创建用户文件夹\n  如果集群启用了Kerberos，那么需要首先kinit，否则不需要   \n\t\n```\nkinit -kt hdfs.keytab hdfs/nn1.example.com\nhdfs dfs -mkdir -p /user/hive_test\nhdfs dfs -chown -R dev_user:hive_test /user/hive_test\n```\n\t\n**创建role**\n\n只有admin用户可以创建和删除roles；hive、hue、impala都是默认的admin用户。\n\n```\nkinit -kt hive.keytab hive/nn1.example.com \nbeeline -u \"jdbc:hive2://nn1.example.com:10000/default;principal=hive/nn1.example.com@EXAMPLE.COM\"\ncreate role hive_test;  //创建用户 \n```\n\n**删除role**\n\n\tdrop role hive_test；\n\t\n**grant role**\n\n\tgrant role hive_test to group hive_test;  ## 将用户分到一个组，在Sentry里用户必须在组里才能使用。\n**revoke role**\n\n\trevoke role hive_test from group hive_test；\n\n- 在hive上grant权限\n\n**grant权限**\n\n```\nGRANT    \n    <PRIVILEGE> [, <PRIVILEGE> ]    \n    ON <OBJECT> <object_name>    \n    TO ROLE <roleName> [,ROLE <roleName>]\n\ne.g. grant all on server server1 to role hive_test;\n```\n\n- hive的权限分类\n\n* **ALL**： 所有权限\n* **ALTER**：允许修改元数据（modify metadata data of  object）---表信息数据\n* **UPDATE**： 允许修改物理数据（modify physical data of  object）---实际数据\n* **CREATE**： 允许进行Create操作\n* **DROP**： 允许进行DROP操作\n* **INDEX**： 允许建索引（目前还没有实现）\n* **LOCK**： 当出现并发的使用允许用户进行LOCK和UNLOCK操作\n* **SELECT**：允许用户进行SELECT操作\n* **SHOW_DATABASE**： 允许用户查看可用的数据库\n\n- 查询权限\n\n```\nshow roles;\nshow grant role hive_test;\n```\n**revoke 权限**\n\n\tREVOKE    \n    <PRIVILEGE> [, <PRIVILEGE> ]    \n    ON <OBJECT> <object_name>    \n    FROM ROLE <roleName> [,ROLE <roleName>]\n\te.g. REVOKE SELECT(column_name) ON TABLE table_name FROM ROLE \n\t     REVOKE SELECT(column_name) ON TABLE table_name FROM ROLE role_name; // 将列的查询权限赋值给用户\n\t     REVOKE ROLE role_name from GROUP group_name[,GROUP group_name] // 将用户移除一个组\n\n\n**grant URIs**\n\t\n\tGRANT ALL ON URI 'hdfs://namenode:XXX/path/to/table'\n\tCREATE EXTERNAL TABLE foo LOCATION 'namenode:XXX/path/to/table'\n\n\n**SET ROLE Statement**\n\n*SET ROLE NONE*\n   使所有的role变成非活动状态。\n\n*SET ROLE ALL*\n   使ROLE拥有所有的权限。\n\n*SET ROLE <role name>*\n   激活一个role，并将分配其的权限激活。\n\n\n## Impala\nimpala 的权限和hive一致\n\n## Hbase\n- 在OS上创建用户 hive_test （同Hive)\n\t需要添加用户到组hbase\n\tusermod -a -G base hive_test\n- 在hdfs上创建用户文件夹 （同Hive)\n- 在Hbase上grant权限\n\t如果集群启用了Kerberos，那么需要首先kinit，否则不需要 \n```\nkinit -kt hbase.keytab hbase/nn1.example.com\ngrant 'hive_test' ,'RWXCA','@hbase'\n```\n- Hbase权限分类\n  HBase提供的五个权限标识符：RWXCA,分别对应着READ('R'), WRITE('W'), EXEC('X'), CREATE('C'), ADMIN('A')\n   HBase提供的五个权限标识符：RWXCA,分别对应着READ('R'), WRITE('W'), EXEC('X'), CREATE('C'), ADMIN('A')\nHBase提供的安全管控级别包括：\nSuperuser：拥有所有权限的超级管理员用户。通过hbase.superuser参数配置\nGlobal：全局权限可以作用在集群所有的表上。\nNamespace ：命名空间级。\nTable：表级。\nColumnFamily：列簇级权限。\nCell：单元级。\n\t\n将namespace权限符给用户\n    \n    grant 'user', 'RWXCA', '@namespace'\n将表的权限赋给用户\n\n    grant 'user', 'RWXCA', 'tablename'\n将表的一个字段的权限赋给用户\n    \n    grant 'user', 'RWXCA', 'TABLE', 'CF', 'CQ'\nhttp://hbase.apache.org/book.html\n\n- 查看用户权限\n```\n    user_permission '@namespace'\n```\n","slug":"CDH-User-Managerment","published":1,"updated":"2018-09-14T01:26:50.494Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzn000pinamp4u04s7f","content":"<h1 id=\"CDH-User-Managerment\"><a href=\"#CDH-User-Managerment\" class=\"headerlink\" title=\"CDH User Managerment\"></a>CDH User Managerment</h1><h2 id=\"Hive\"><a href=\"#Hive\" class=\"headerlink\" title=\"Hive\"></a>Hive</h2><p><strong>创建一个普通用户 hive_test</strong></p>\n<ul>\n<li>在OS上创建这个普通用户 hive_test</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nodes=`cat /etc/hosts  | grep -v ^# | grep htsec | awk &apos;&#123;print $2&#125;&apos;`</span><br><span class=\"line\">for target in $nodes; do</span><br><span class=\"line\">ssh root@$target &lt;&lt;comd</span><br><span class=\"line\">useradd hive_test</span><br><span class=\"line\">usermod -a -G hive hive_test</span><br><span class=\"line\">comd</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<a id=\"more\"></a>\n<ul>\n<li>在hdfs上创建用户文件夹<br>如果集群启用了Kerberos，那么需要首先kinit，否则不需要   </li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kinit -kt hdfs.keytab hdfs/nn1.example.com</span><br><span class=\"line\">hdfs dfs -mkdir -p /user/hive_test</span><br><span class=\"line\">hdfs dfs -chown -R dev_user:hive_test /user/hive_test</span><br></pre></td></tr></table></figure>\n<p><strong>创建role</strong></p>\n<p>只有admin用户可以创建和删除roles；hive、hue、impala都是默认的admin用户。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kinit -kt hive.keytab hive/nn1.example.com </span><br><span class=\"line\">beeline -u &quot;jdbc:hive2://nn1.example.com:10000/default;principal=hive/nn1.example.com@EXAMPLE.COM&quot;</span><br><span class=\"line\">create role hive_test;  //创建用户</span><br></pre></td></tr></table></figure>\n<p><strong>删除role</strong></p>\n<pre><code>drop role hive_test；\n</code></pre><p><strong>grant role</strong></p>\n<pre><code>grant role hive_test to group hive_test;  ## 将用户分到一个组，在Sentry里用户必须在组里才能使用。\n</code></pre><p><strong>revoke role</strong></p>\n<pre><code>revoke role hive_test from group hive_test；\n</code></pre><ul>\n<li>在hive上grant权限</li>\n</ul>\n<p><strong>grant权限</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GRANT    </span><br><span class=\"line\">    &lt;PRIVILEGE&gt; [, &lt;PRIVILEGE&gt; ]    </span><br><span class=\"line\">    ON &lt;OBJECT&gt; &lt;object_name&gt;    </span><br><span class=\"line\">    TO ROLE &lt;roleName&gt; [,ROLE &lt;roleName&gt;]</span><br><span class=\"line\"></span><br><span class=\"line\">e.g. grant all on server server1 to role hive_test;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>hive的权限分类</li>\n</ul>\n<ul>\n<li><strong>ALL</strong>： 所有权限</li>\n<li><strong>ALTER</strong>：允许修改元数据（modify metadata data of  object）—表信息数据</li>\n<li><strong>UPDATE</strong>： 允许修改物理数据（modify physical data of  object）—实际数据</li>\n<li><strong>CREATE</strong>： 允许进行Create操作</li>\n<li><strong>DROP</strong>： 允许进行DROP操作</li>\n<li><strong>INDEX</strong>： 允许建索引（目前还没有实现）</li>\n<li><strong>LOCK</strong>： 当出现并发的使用允许用户进行LOCK和UNLOCK操作</li>\n<li><strong>SELECT</strong>：允许用户进行SELECT操作</li>\n<li><strong>SHOW_DATABASE</strong>： 允许用户查看可用的数据库</li>\n</ul>\n<ul>\n<li>查询权限</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show roles;</span><br><span class=\"line\">show grant role hive_test;</span><br></pre></td></tr></table></figure>\n<p><strong>revoke 权限</strong></p>\n<pre><code>REVOKE    \n&lt;PRIVILEGE&gt; [, &lt;PRIVILEGE&gt; ]    \nON &lt;OBJECT&gt; &lt;object_name&gt;    \nFROM ROLE &lt;roleName&gt; [,ROLE &lt;roleName&gt;]\ne.g. REVOKE SELECT(column_name) ON TABLE table_name FROM ROLE \n     REVOKE SELECT(column_name) ON TABLE table_name FROM ROLE role_name; // 将列的查询权限赋值给用户\n     REVOKE ROLE role_name from GROUP group_name[,GROUP group_name] // 将用户移除一个组\n</code></pre><p><strong>grant URIs</strong></p>\n<pre><code>GRANT ALL ON URI &apos;hdfs://namenode:XXX/path/to/table&apos;\nCREATE EXTERNAL TABLE foo LOCATION &apos;namenode:XXX/path/to/table&apos;\n</code></pre><p><strong>SET ROLE Statement</strong></p>\n<p><em>SET ROLE NONE</em><br>   使所有的role变成非活动状态。</p>\n<p><em>SET ROLE ALL</em><br>   使ROLE拥有所有的权限。</p>\n<p><em>SET ROLE <role name=\"\"></role></em><br>   激活一个role，并将分配其的权限激活。</p>\n<h2 id=\"Impala\"><a href=\"#Impala\" class=\"headerlink\" title=\"Impala\"></a>Impala</h2><p>impala 的权限和hive一致</p>\n<h2 id=\"Hbase\"><a href=\"#Hbase\" class=\"headerlink\" title=\"Hbase\"></a>Hbase</h2><ul>\n<li>在OS上创建用户 hive_test （同Hive)<br>  需要添加用户到组hbase<br>  usermod -a -G base hive_test</li>\n<li>在hdfs上创建用户文件夹 （同Hive)</li>\n<li><p>在Hbase上grant权限<br>  如果集群启用了Kerberos，那么需要首先kinit，否则不需要 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kinit -kt hbase.keytab hbase/nn1.example.com</span><br><span class=\"line\">grant &apos;hive_test&apos; ,&apos;RWXCA&apos;,&apos;@hbase&apos;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Hbase权限分类<br>HBase提供的五个权限标识符：RWXCA,分别对应着READ(‘R’), WRITE(‘W’), EXEC(‘X’), CREATE(‘C’), ADMIN(‘A’)<br> HBase提供的五个权限标识符：RWXCA,分别对应着READ(‘R’), WRITE(‘W’), EXEC(‘X’), CREATE(‘C’), ADMIN(‘A’)<br>HBase提供的安全管控级别包括：<br>Superuser：拥有所有权限的超级管理员用户。通过hbase.superuser参数配置<br>Global：全局权限可以作用在集群所有的表上。<br>Namespace ：命名空间级。<br>Table：表级。<br>ColumnFamily：列簇级权限。<br>Cell：单元级。</p>\n</li>\n</ul>\n<p>将namespace权限符给用户</p>\n<pre><code>grant &apos;user&apos;, &apos;RWXCA&apos;, &apos;@namespace&apos;\n</code></pre><p>将表的权限赋给用户</p>\n<pre><code>grant &apos;user&apos;, &apos;RWXCA&apos;, &apos;tablename&apos;\n</code></pre><p>将表的一个字段的权限赋给用户</p>\n<pre><code>grant &apos;user&apos;, &apos;RWXCA&apos;, &apos;TABLE&apos;, &apos;CF&apos;, &apos;CQ&apos;\n</code></pre><p><a href=\"http://hbase.apache.org/book.html\" target=\"_blank\" rel=\"noopener\">http://hbase.apache.org/book.html</a></p>\n<ul>\n<li>查看用户权限<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">user_permission &apos;@namespace&apos;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"CDH-User-Managerment\"><a href=\"#CDH-User-Managerment\" class=\"headerlink\" title=\"CDH User Managerment\"></a>CDH User Managerment</h1><h2 id=\"Hive\"><a href=\"#Hive\" class=\"headerlink\" title=\"Hive\"></a>Hive</h2><p><strong>创建一个普通用户 hive_test</strong></p>\n<ul>\n<li>在OS上创建这个普通用户 hive_test</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nodes=`cat /etc/hosts  | grep -v ^# | grep htsec | awk &apos;&#123;print $2&#125;&apos;`</span><br><span class=\"line\">for target in $nodes; do</span><br><span class=\"line\">ssh root@$target &lt;&lt;comd</span><br><span class=\"line\">useradd hive_test</span><br><span class=\"line\">usermod -a -G hive hive_test</span><br><span class=\"line\">comd</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>","more":"<ul>\n<li>在hdfs上创建用户文件夹<br>如果集群启用了Kerberos，那么需要首先kinit，否则不需要   </li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kinit -kt hdfs.keytab hdfs/nn1.example.com</span><br><span class=\"line\">hdfs dfs -mkdir -p /user/hive_test</span><br><span class=\"line\">hdfs dfs -chown -R dev_user:hive_test /user/hive_test</span><br></pre></td></tr></table></figure>\n<p><strong>创建role</strong></p>\n<p>只有admin用户可以创建和删除roles；hive、hue、impala都是默认的admin用户。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kinit -kt hive.keytab hive/nn1.example.com </span><br><span class=\"line\">beeline -u &quot;jdbc:hive2://nn1.example.com:10000/default;principal=hive/nn1.example.com@EXAMPLE.COM&quot;</span><br><span class=\"line\">create role hive_test;  //创建用户</span><br></pre></td></tr></table></figure>\n<p><strong>删除role</strong></p>\n<pre><code>drop role hive_test；\n</code></pre><p><strong>grant role</strong></p>\n<pre><code>grant role hive_test to group hive_test;  ## 将用户分到一个组，在Sentry里用户必须在组里才能使用。\n</code></pre><p><strong>revoke role</strong></p>\n<pre><code>revoke role hive_test from group hive_test；\n</code></pre><ul>\n<li>在hive上grant权限</li>\n</ul>\n<p><strong>grant权限</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GRANT    </span><br><span class=\"line\">    &lt;PRIVILEGE&gt; [, &lt;PRIVILEGE&gt; ]    </span><br><span class=\"line\">    ON &lt;OBJECT&gt; &lt;object_name&gt;    </span><br><span class=\"line\">    TO ROLE &lt;roleName&gt; [,ROLE &lt;roleName&gt;]</span><br><span class=\"line\"></span><br><span class=\"line\">e.g. grant all on server server1 to role hive_test;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>hive的权限分类</li>\n</ul>\n<ul>\n<li><strong>ALL</strong>： 所有权限</li>\n<li><strong>ALTER</strong>：允许修改元数据（modify metadata data of  object）—表信息数据</li>\n<li><strong>UPDATE</strong>： 允许修改物理数据（modify physical data of  object）—实际数据</li>\n<li><strong>CREATE</strong>： 允许进行Create操作</li>\n<li><strong>DROP</strong>： 允许进行DROP操作</li>\n<li><strong>INDEX</strong>： 允许建索引（目前还没有实现）</li>\n<li><strong>LOCK</strong>： 当出现并发的使用允许用户进行LOCK和UNLOCK操作</li>\n<li><strong>SELECT</strong>：允许用户进行SELECT操作</li>\n<li><strong>SHOW_DATABASE</strong>： 允许用户查看可用的数据库</li>\n</ul>\n<ul>\n<li>查询权限</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show roles;</span><br><span class=\"line\">show grant role hive_test;</span><br></pre></td></tr></table></figure>\n<p><strong>revoke 权限</strong></p>\n<pre><code>REVOKE    \n&lt;PRIVILEGE&gt; [, &lt;PRIVILEGE&gt; ]    \nON &lt;OBJECT&gt; &lt;object_name&gt;    \nFROM ROLE &lt;roleName&gt; [,ROLE &lt;roleName&gt;]\ne.g. REVOKE SELECT(column_name) ON TABLE table_name FROM ROLE \n     REVOKE SELECT(column_name) ON TABLE table_name FROM ROLE role_name; // 将列的查询权限赋值给用户\n     REVOKE ROLE role_name from GROUP group_name[,GROUP group_name] // 将用户移除一个组\n</code></pre><p><strong>grant URIs</strong></p>\n<pre><code>GRANT ALL ON URI &apos;hdfs://namenode:XXX/path/to/table&apos;\nCREATE EXTERNAL TABLE foo LOCATION &apos;namenode:XXX/path/to/table&apos;\n</code></pre><p><strong>SET ROLE Statement</strong></p>\n<p><em>SET ROLE NONE</em><br>   使所有的role变成非活动状态。</p>\n<p><em>SET ROLE ALL</em><br>   使ROLE拥有所有的权限。</p>\n<p><em>SET ROLE <role name=\"\"></role></em><br>   激活一个role，并将分配其的权限激活。</p>\n<h2 id=\"Impala\"><a href=\"#Impala\" class=\"headerlink\" title=\"Impala\"></a>Impala</h2><p>impala 的权限和hive一致</p>\n<h2 id=\"Hbase\"><a href=\"#Hbase\" class=\"headerlink\" title=\"Hbase\"></a>Hbase</h2><ul>\n<li>在OS上创建用户 hive_test （同Hive)<br>  需要添加用户到组hbase<br>  usermod -a -G base hive_test</li>\n<li>在hdfs上创建用户文件夹 （同Hive)</li>\n<li><p>在Hbase上grant权限<br>  如果集群启用了Kerberos，那么需要首先kinit，否则不需要 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kinit -kt hbase.keytab hbase/nn1.example.com</span><br><span class=\"line\">grant &apos;hive_test&apos; ,&apos;RWXCA&apos;,&apos;@hbase&apos;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Hbase权限分类<br>HBase提供的五个权限标识符：RWXCA,分别对应着READ(‘R’), WRITE(‘W’), EXEC(‘X’), CREATE(‘C’), ADMIN(‘A’)<br> HBase提供的五个权限标识符：RWXCA,分别对应着READ(‘R’), WRITE(‘W’), EXEC(‘X’), CREATE(‘C’), ADMIN(‘A’)<br>HBase提供的安全管控级别包括：<br>Superuser：拥有所有权限的超级管理员用户。通过hbase.superuser参数配置<br>Global：全局权限可以作用在集群所有的表上。<br>Namespace ：命名空间级。<br>Table：表级。<br>ColumnFamily：列簇级权限。<br>Cell：单元级。</p>\n</li>\n</ul>\n<p>将namespace权限符给用户</p>\n<pre><code>grant &apos;user&apos;, &apos;RWXCA&apos;, &apos;@namespace&apos;\n</code></pre><p>将表的权限赋给用户</p>\n<pre><code>grant &apos;user&apos;, &apos;RWXCA&apos;, &apos;tablename&apos;\n</code></pre><p>将表的一个字段的权限赋给用户</p>\n<pre><code>grant &apos;user&apos;, &apos;RWXCA&apos;, &apos;TABLE&apos;, &apos;CF&apos;, &apos;CQ&apos;\n</code></pre><p><a href=\"http://hbase.apache.org/book.html\" target=\"_blank\" rel=\"noopener\">http://hbase.apache.org/book.html</a></p>\n<ul>\n<li>查看用户权限<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">user_permission &apos;@namespace&apos;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>"},{"title":"Cloudera Navigator","date":"2018-05-13T03:56:26.000Z","_content":"# Cloudera Navigator\n\n\n## Analytics Audit\n可以查看最活跃的用户\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Audit.png)\n\n<!-- more --> \n## Analytics Dashboard\n可以查看一些活动的报表结果，如最近一段时间内每天或者每周创建了多少表，每天有多少查询等等。虽然页面上没有但是应该能通过添加配置出每天数据的增量。\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Dashboard.png)\n\n## Analytics Dashboard2\n可以看到一些单个文件比较大的文件列表\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Dashboard2.png)\n\n## Analytics Data Exploprer\n趋势详情，可以点击跳转到Search页面，查看更多的详细信息。\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Data%20Explorer.png)\n\n## Analytics HDFS\n对HDFS的存储信息和操作信息进行分组分用户的统计信息。\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20HDFS.png)\n\n## AuditEvent\n审计日志，包含用户、IP、操作类型、操作的数据为准、目标服务器等信息。\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/AuditEvent.png)\n\n## AuditEvent Filter\n添加过滤条件的审计日志\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/AuditEvent-Filter.png)\n\n\n## Lineage\n血缘关系：可以查看一段SQL里面的血缘关系。\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Lineage.png)\n\n## Search\n使用的是Solr，可以查询被添加了标签的数据，也可以自己添加标签。\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Search.png)\n\n## Policies\n规则或者叫策略\n\n* 可以给HDFS内的实体自定义标签\n* 定时对文件进行操作、移动、删除等。\n* 管路元数据\n* 对特定的实体执行命令等。\n* 发送JMS消息\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/%20Policies.png)\n","source":"_posts/Cloudera-Navigator.md","raw":"---\ntitle: Cloudera Navigator\ndate: 2018-05-13 11:56:26\ntags: \n  - Navigator\n  - CDH\n---\n# Cloudera Navigator\n\n\n## Analytics Audit\n可以查看最活跃的用户\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Audit.png)\n\n<!-- more --> \n## Analytics Dashboard\n可以查看一些活动的报表结果，如最近一段时间内每天或者每周创建了多少表，每天有多少查询等等。虽然页面上没有但是应该能通过添加配置出每天数据的增量。\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Dashboard.png)\n\n## Analytics Dashboard2\n可以看到一些单个文件比较大的文件列表\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Dashboard2.png)\n\n## Analytics Data Exploprer\n趋势详情，可以点击跳转到Search页面，查看更多的详细信息。\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Data%20Explorer.png)\n\n## Analytics HDFS\n对HDFS的存储信息和操作信息进行分组分用户的统计信息。\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20HDFS.png)\n\n## AuditEvent\n审计日志，包含用户、IP、操作类型、操作的数据为准、目标服务器等信息。\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/AuditEvent.png)\n\n## AuditEvent Filter\n添加过滤条件的审计日志\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/AuditEvent-Filter.png)\n\n\n## Lineage\n血缘关系：可以查看一段SQL里面的血缘关系。\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Lineage.png)\n\n## Search\n使用的是Solr，可以查询被添加了标签的数据，也可以自己添加标签。\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Search.png)\n\n## Policies\n规则或者叫策略\n\n* 可以给HDFS内的实体自定义标签\n* 定时对文件进行操作、移动、删除等。\n* 管路元数据\n* 对特定的实体执行命令等。\n* 发送JMS消息\n![](https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/%20Policies.png)\n","slug":"Cloudera-Navigator","published":1,"updated":"2018-09-14T01:23:20.090Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzo000rinamaucitr73","content":"<h1 id=\"Cloudera-Navigator\"><a href=\"#Cloudera-Navigator\" class=\"headerlink\" title=\"Cloudera Navigator\"></a>Cloudera Navigator</h1><h2 id=\"Analytics-Audit\"><a href=\"#Analytics-Audit\" class=\"headerlink\" title=\"Analytics Audit\"></a>Analytics Audit</h2><p>可以查看最活跃的用户<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Audit.png\" alt=\"\"></p>\n<a id=\"more\"></a> \n<h2 id=\"Analytics-Dashboard\"><a href=\"#Analytics-Dashboard\" class=\"headerlink\" title=\"Analytics Dashboard\"></a>Analytics Dashboard</h2><p>可以查看一些活动的报表结果，如最近一段时间内每天或者每周创建了多少表，每天有多少查询等等。虽然页面上没有但是应该能通过添加配置出每天数据的增量。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Dashboard.png\" alt=\"\"></p>\n<h2 id=\"Analytics-Dashboard2\"><a href=\"#Analytics-Dashboard2\" class=\"headerlink\" title=\"Analytics Dashboard2\"></a>Analytics Dashboard2</h2><p>可以看到一些单个文件比较大的文件列表<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Dashboard2.png\" alt=\"\"></p>\n<h2 id=\"Analytics-Data-Exploprer\"><a href=\"#Analytics-Data-Exploprer\" class=\"headerlink\" title=\"Analytics Data Exploprer\"></a>Analytics Data Exploprer</h2><p>趋势详情，可以点击跳转到Search页面，查看更多的详细信息。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Data%20Explorer.png\" alt=\"\"></p>\n<h2 id=\"Analytics-HDFS\"><a href=\"#Analytics-HDFS\" class=\"headerlink\" title=\"Analytics HDFS\"></a>Analytics HDFS</h2><p>对HDFS的存储信息和操作信息进行分组分用户的统计信息。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20HDFS.png\" alt=\"\"></p>\n<h2 id=\"AuditEvent\"><a href=\"#AuditEvent\" class=\"headerlink\" title=\"AuditEvent\"></a>AuditEvent</h2><p>审计日志，包含用户、IP、操作类型、操作的数据为准、目标服务器等信息。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/AuditEvent.png\" alt=\"\"></p>\n<h2 id=\"AuditEvent-Filter\"><a href=\"#AuditEvent-Filter\" class=\"headerlink\" title=\"AuditEvent Filter\"></a>AuditEvent Filter</h2><p>添加过滤条件的审计日志<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/AuditEvent-Filter.png\" alt=\"\"></p>\n<h2 id=\"Lineage\"><a href=\"#Lineage\" class=\"headerlink\" title=\"Lineage\"></a>Lineage</h2><p>血缘关系：可以查看一段SQL里面的血缘关系。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Lineage.png\" alt=\"\"></p>\n<h2 id=\"Search\"><a href=\"#Search\" class=\"headerlink\" title=\"Search\"></a>Search</h2><p>使用的是Solr，可以查询被添加了标签的数据，也可以自己添加标签。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Search.png\" alt=\"\"></p>\n<h2 id=\"Policies\"><a href=\"#Policies\" class=\"headerlink\" title=\"Policies\"></a>Policies</h2><p>规则或者叫策略</p>\n<ul>\n<li>可以给HDFS内的实体自定义标签</li>\n<li>定时对文件进行操作、移动、删除等。</li>\n<li>管路元数据</li>\n<li>对特定的实体执行命令等。</li>\n<li>发送JMS消息<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/%20Policies.png\" alt=\"\"></li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"Cloudera-Navigator\"><a href=\"#Cloudera-Navigator\" class=\"headerlink\" title=\"Cloudera Navigator\"></a>Cloudera Navigator</h1><h2 id=\"Analytics-Audit\"><a href=\"#Analytics-Audit\" class=\"headerlink\" title=\"Analytics Audit\"></a>Analytics Audit</h2><p>可以查看最活跃的用户<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Audit.png\" alt=\"\"></p>","more":"<h2 id=\"Analytics-Dashboard\"><a href=\"#Analytics-Dashboard\" class=\"headerlink\" title=\"Analytics Dashboard\"></a>Analytics Dashboard</h2><p>可以查看一些活动的报表结果，如最近一段时间内每天或者每周创建了多少表，每天有多少查询等等。虽然页面上没有但是应该能通过添加配置出每天数据的增量。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Dashboard.png\" alt=\"\"></p>\n<h2 id=\"Analytics-Dashboard2\"><a href=\"#Analytics-Dashboard2\" class=\"headerlink\" title=\"Analytics Dashboard2\"></a>Analytics Dashboard2</h2><p>可以看到一些单个文件比较大的文件列表<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Dashboard2.png\" alt=\"\"></p>\n<h2 id=\"Analytics-Data-Exploprer\"><a href=\"#Analytics-Data-Exploprer\" class=\"headerlink\" title=\"Analytics Data Exploprer\"></a>Analytics Data Exploprer</h2><p>趋势详情，可以点击跳转到Search页面，查看更多的详细信息。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20Data%20Explorer.png\" alt=\"\"></p>\n<h2 id=\"Analytics-HDFS\"><a href=\"#Analytics-HDFS\" class=\"headerlink\" title=\"Analytics HDFS\"></a>Analytics HDFS</h2><p>对HDFS的存储信息和操作信息进行分组分用户的统计信息。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Analytics%20HDFS.png\" alt=\"\"></p>\n<h2 id=\"AuditEvent\"><a href=\"#AuditEvent\" class=\"headerlink\" title=\"AuditEvent\"></a>AuditEvent</h2><p>审计日志，包含用户、IP、操作类型、操作的数据为准、目标服务器等信息。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/AuditEvent.png\" alt=\"\"></p>\n<h2 id=\"AuditEvent-Filter\"><a href=\"#AuditEvent-Filter\" class=\"headerlink\" title=\"AuditEvent Filter\"></a>AuditEvent Filter</h2><p>添加过滤条件的审计日志<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/AuditEvent-Filter.png\" alt=\"\"></p>\n<h2 id=\"Lineage\"><a href=\"#Lineage\" class=\"headerlink\" title=\"Lineage\"></a>Lineage</h2><p>血缘关系：可以查看一段SQL里面的血缘关系。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Lineage.png\" alt=\"\"></p>\n<h2 id=\"Search\"><a href=\"#Search\" class=\"headerlink\" title=\"Search\"></a>Search</h2><p>使用的是Solr，可以查询被添加了标签的数据，也可以自己添加标签。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/Search.png\" alt=\"\"></p>\n<h2 id=\"Policies\"><a href=\"#Policies\" class=\"headerlink\" title=\"Policies\"></a>Policies</h2><p>规则或者叫策略</p>\n<ul>\n<li>可以给HDFS内的实体自定义标签</li>\n<li>定时对文件进行操作、移动、删除等。</li>\n<li>管路元数据</li>\n<li>对特定的实体执行命令等。</li>\n<li>发送JMS消息<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/raw/master/screenshot/Cloudera%20Navigator/%20Policies.png\" alt=\"\"></li>\n</ul>"},{"title":"Cloudera Manager High Availability","date":"2018-10-22T03:28:39.000Z","_content":"\n## 1. 环境准备\n### 1.1 环境\n操作系统：CentOS Linux release 7.3.1611 (Core)\nJDK：jdk1.8.0_111\n服务器：53-58 60-61 共8台\n\n### 1.2 软件下载\nHAProxy: http://www.haproxy.org/download/1.8/src/haproxy-1.8.13.tar.gz\n\n<!-- more --> \n\n### 1.3 架构\n![enter image description here](https://www.cloudera.com/documentation/enterprise/5-4-x/images/cm_ha_lb_setup.png)\n\n### 1.4 修改主机名称\n```\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n172.17.1.53 cms1.test.com cms1\n172.17.1.54 cms2.test.com cms2\n172.17.1.55 mgmt1.test.com mgmt1\n172.17.1.56 mgmt2.test.com mgmt2\n172.17.1.57 nfs.test.com nfs\n172.17.1.58 cms.test.com cms\n172.17.1.60 mgmt.test.com mgmt\n172.17.1.61 dn1.test.com dn1\n```\n\n\n## 2 安装前环境准备\n### 2.1 创建主和副主机\nCloudera Manager Server and Cloudera Management Service Primary host cms1.test.com\nCloudera Manager Server and Cloudera Management Service Secondary host cms2.test.com\n\n此外，Cloudera建议：\n    不要在安装CDH的节点安装Cloudera Manager或者Cloudera Management Service，因为这样会使failover的配置变的复杂。并且覆盖失败的域名可能会造成容错和错误检查的问题。\n    对主副主机都使用相同的主机配置。用来保证故障转移后性能不会降低。\n    对主副主机使用分开的主机和网络组件。\n\n主机分配\n\n| __IP__ | __DomainName__ | __功能__ | __角色__|\n|-----------|-----------|-----------|-----------|\n|172.17.1.53|cms1.test.com| cloudera manager 主节点|cms,cma|\n|172.17.1.54|cms2.test.com| cloudera manager 备份节点|cms,cma|\n|172.17.1.55|mgmt1.test.com|cloudera management service 主节点|cmmg,cma|\n|172.17.1.56|mgmt2.test.com|cloudera managerment service 备份节点|cmmg,cma|\n|172.17.1.57|nfs.test.com|挂载存储服务器|nfs|\n|172.17.1.58|cms.test.com|cm代理服务器|haproxy|\n|172.17.1.60|mgmt.test.com|mgmt代理服务器|haproxy|\n|172.17.1.61|dn1.test.com|数据节点|cma,数据库|\n\n\n### 2.2 安装配置Load Balancer\n在`cms`节点上安装HAProxy\n```\nyum install -y haproxy\n```\n配置haproxy开机启动\n```\nsystemctl enable haproxy\n```\n配置HAProxy\n在`cms`上，编辑*/etc/haproxy/haproxy.cfg*文件，添加需要代理的端口\n\n```\n#---------------------------------------------------------------------\n# Example configuration for a possible web application.  See the\n# full configuration options online.\n#\n#   http://haproxy.1wt.eu/download/1.4/doc/configuration.txt\n#\n#---------------------------------------------------------------------\n\n#---------------------------------------------------------------------\n# Global settings\n#---------------------------------------------------------------------\nglobal\n    # to have these messages end up in /var/log/haproxy.log you will\n    # need to:\n    #\n    # 1) configure syslog to accept network log events.  This is done\n    #    by adding the '-r' option to the SYSLOGD_OPTIONS in\n    #    /etc/sysconfig/syslog\n    #\n    # 2) configure local2 events to go to the /var/log/haproxy.log\n    #   file. A line like the following can be added to\n    #   /etc/sysconfig/syslog\n    #\n    #    local2.*                       /var/log/haproxy.log\n    #\n    log         127.0.0.1 local2\n\n    chroot      /var/lib/haproxy\n    pidfile     /var/run/haproxy.pid\n    maxconn     4000\n    user        haproxy\n    group       haproxy\n    daemon\n\n    # turn on stats unix socket\n    stats socket /var/lib/haproxy/stats\n\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\n    mode                    http\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n    #option forwardfor       except 127.0.0.0/8\n    option                  redispatch\n    retries                 3\n    timeout http-request    10s\n    timeout queue           1m\n    timeout connect         10s\n    timeout client          1m\n    timeout server          1m\n    timeout http-keep-alive 10s\n    timeout check           10s\n    maxconn                 3000\n\n#---------------------------------------------------------------------\n# main frontend which proxys to the backends\n#---------------------------------------------------------------------\n#frontend  main *:5000\n#    acl url_static       path_beg       -i /static /images /javascript /stylesheets\n#    acl url_static       path_end       -i .jpg .gif .png .css .js\n#\n#    use_backend static          if url_static\n#    default_backend             app\n\n#---------------------------------------------------------------------\n# static backend for serving up images, stylesheets and such\n#---------------------------------------------------------------------\n#backend static\n#    balance     roundrobin\n#    server      static 127.0.0.1:4331 check\n\n#---------------------------------------------------------------------\n# round robin balancing between the various backends\n#---------------------------------------------------------------------\n#backend app\n#    balance     roundrobin\n#    server  app1 127.0.0.1:5001 check\n#    server  app2 127.0.0.1:5002 check\n#    server  app3 127.0.0.1:5003 check\n#    server  app4 127.0.0.1:5004 check\n\n#---------------------------------------------------------------------\n# stats port\n#---------------------------------------------------------------------\nlisten stats\n    bind 0.0.0.0:1080\n    mode http\n    option httplog\n    maxconn 5000\n    stats refresh 30s\n    stats  uri /stats\n\n#---------------------------------------------------------------------\n# Cloudera Manager\n#---------------------------------------------------------------------\n\nlisten cmf\n    mode tcp\n    option tcplog\n    bind 0.0.0.0:7180\n    server cmfhttp1 cms1.test.com:7180 check\n    server cmfhttp2 cms2.test.com:7180 check\n\nlisten cmfavro :7182\n    mode tcp\n    option tcplog\n    server cmfavro1 cms1.test.com:7182 check\n    server cmfavro2 cms2.test.com:7182 check\n\n#ssl pass-through, without termination\nlisten cmfhttps :7183\n    mode tcp\n    option tcplog\n    server cmfhttps1 cms1.test.com:7183 check\n    server cmfhttps2 cms2.test.com:7183 check\n\nlisten mgmt1 :5678\n    mode tcp\n    option tcplog\n    server mgmt1a cms1.test.com check\n    server mgmt1b cms2.test.com check\n\nlisten mgmt2 :7184\n    mode tcp\n    option tcplog\n    server mgmt2a cms1.test.com check\n    server mgmt2b cms2.test.com check\n\nlisten mgmt3 :7185\n    mode tcp\n    option tcplog\n    server mgmt3a cms1.test.com check\n    server mgmt3b cms2.test.com check\nlisten mgmt4 :7186\n    mode tcp\n    option tcplog\n    server mgmt4a cms1.test.com check\n    server mgmt4b cms2.test.com check\nlisten mgmt5 :7187\n    mode tcp\n    option tcplog\n    server mgmt5a cms1.test.com check\n    server mgmt5b cms2.test.com check\n\nlisten mgmt6 :8083\n    mode tcp\n    option tcplog\n    server mgmt6a cms1.test.com check\n    server mgmt6b cms2.test.com check\nlisten mgmt7 :8084\n    mode tcp\n    option tcplog\n    server mgmt7a cms1.test.com check\n    server mgmt7b cms2.test.com check\nlisten mgmt8 :8086\n    mode tcp\n    option tcplog\n    server mgmt8a cms1.test.com check\n    server mgmt8b cms2.test.com check\nlisten mgmt9 :8087\n    mode tcp\n    option tcplog\n    server mgmt9a cms1.test.com check\n    server mgmt9b cms2.test.com check\nlisten mgmt10 :8091\n    mode tcp\n    option tcplog\n    server mgmt10a cms1.test.com check\n    server mgmt10b cms2.test.com check\nlisten mgmt-agent :9000\n    mode tcp\n    option tcplog\n    server mgmt-agenta cms1.test.com check\n    server mgmt-agentb cms2.test.com check\nlisten mgmt11 :9994\n    mode tcp\n    option tcplog\n    server mgmt11a cms1.test.com check\n    server mgmt11b cms2.test.com check\nlisten mgmt12 :9995\n    mode tcp\n    option tcplog\n    server mgmt12a cms1.test.com check\n    server mgmt12b cms2.test.com check\nlisten mgmt13 :9996\n    mode tcp\n    option tcplog\n    server mgmt13a cms1.test.com check\n    server mgmt13b cms2.test.com check\nlisten mgmt14 :9997\n    mode tcp\n    option tcplog\n    server mgmt14a cms1.test.com check\n    server mgmt14b cms2.test.com check\nlisten mgmt15 :9998\n    mode tcp\n    option tcplog\n    server mgmt15a cms1.test.com check\n    server mgmt15b cms2.test.com check\nlisten mgmt16 :9999\n    mode tcp\n    option tcplog\n    server mgmt16a cms1.test.com check\n    server mgmt16b cms2.test.com check\nlisten mgmt17 :10101\n    mode tcp\n    option tcplog\n    server mgmt17a cms1.test.com check\n    server mgmt17b cms2.test.com check\n```\n重启haproxy\n\n```\nsystemctl restart  haproxy\n```\n### 2.3 安装配置数据库\n在dn1节点上安装数据库\n```\nyum install -y mariadb mariadb-server\n```\n启动数据库\n```\nsystemctl restart mariadb\n```\n\n初始化\n```\n/usr/bin/mysql_secure_installation\n```\n\n配置mariadb的主从 [参考](https://mariadb.com/kb/en/library/setting-up-replication/)\n在主节点上的/etc/my.cnf添加如下配置并重启\n```\n[mariadb]\nlog-bin\nserver_id=1\nlog-basename=master1\n```\n创建replication用户\n```\nCREATE USER 'replication_user'@'%' IDENTIFIED BY 'bigs3cret';\nGRANT REPLICATION SLAVE ON *.* TO 'replication_user'@'%';\n```\n在从节点上修改/etc/my.cnf文件,并重启\n```\n[mariadb]\nlog-bin\nserver_id=2\nlog-basename=slave1\n```\n获取主节点的Binary Log\n在主节点上执行命令锁住所有的表\n```\nFLUSH TABLES WITH READ LOCK\n```\n执行命令获取主节点的状态，并记住Position的值\n```\nMariaDB [(none)]> SHOW MASTER STATUS;\n+------------------+----------+--------------+------------------+\n| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |\n+------------------+----------+--------------+------------------+\n| mysql-bin.000004 |      495 |              |                  |\n+------------------+----------+--------------+------------------+\n1 row in set (0.00 sec)\n\nMariaDB [(none)]>\n```\n\n在从节点执行如下命令：\n```\nCHANGE MASTER TO\n  MASTER_HOST='cms1.test.com',\n  MASTER_USER='replication_user',\n  MASTER_PASSWORD='bigs3cret',\n  MASTER_PORT=3306,\n  MASTER_LOG_FILE='mysql-bin.000004',\n  MASTER_LOG_POS=495,\n  MASTER_CONNECT_RETRY=10;\n```\n \n 在主节点执行命令\n```\nUNLOCK TABLES;\n```\n\n在从节点执行命令启动slave\n```\nSTART SLAVE;\n```\n使用命令查询是否配置争取\n```\nMariaDB [(none)]> show slave status \\G\n*************************** 1. row ***************************\n               Slave_IO_State: Waiting for master to send event\n                  Master_Host: cms1.test.com\n                  Master_User: replication_user\n                  Master_Port: 3306\n                Connect_Retry: 10\n              Master_Log_File: mysql-bin.000004\n          Read_Master_Log_Pos: 495\n               Relay_Log_File: slave1-relay-bin.000005\n                Relay_Log_Pos: 529\n        Relay_Master_Log_File: mysql-bin.000004\n             Slave_IO_Running: Yes\n            Slave_SQL_Running: Yes\n              Replicate_Do_DB:\n          Replicate_Ignore_DB:\n           Replicate_Do_Table:\n       Replicate_Ignore_Table:\n      Replicate_Wild_Do_Table:\n  Replicate_Wild_Ignore_Table:\n                   Last_Errno: 0\n                   Last_Error:\n                 Skip_Counter: 0\n          Exec_Master_Log_Pos: 495\n              Relay_Log_Space: 824\n              Until_Condition: None\n               Until_Log_File:\n                Until_Log_Pos: 0\n           Master_SSL_Allowed: No\n           Master_SSL_CA_File:\n           Master_SSL_CA_Path:\n              Master_SSL_Cert:\n            Master_SSL_Cipher:\n               Master_SSL_Key:\n        Seconds_Behind_Master: 0\nMaster_SSL_Verify_Server_Cert: No\n                Last_IO_Errno: 0\n                Last_IO_Error:\n               Last_SQL_Errno: 0\n               Last_SQL_Error:\n  Replicate_Ignore_Server_Ids:\n             Master_Server_Id: 1\n1 row in set (0.00 sec)\n```\nSlave_IO_Running 和 Slave_SQL_Running应该是yes\n\n在主节点上执行如下命令来创建数据库和用户\n```\nmysql -u root --password='123456' -e 'create database metastore default character set utf8;'\nmysql -u root --password='123456' -e \"CREATE USER 'hive'@'%' IDENTIFIED BY '123456';\"\nmysql -u root --password='123456' -e \"GRANT ALL PRIVILEGES ON metastore. * TO 'hive'@'%';\"\nmysql -u root --password='123456' -e \"create user 'amon'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database amon default character set utf8' \nmysql -u root --password='123456' -e \"grant all privileges on amon.* to 'amon'@'%'\"\nmysql -u root --password='123456' -e \"create user 'rman'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database rman default character set utf8' \nmysql -u root --password='123456' -e \"grant all privileges on rman.* to 'rman'@'%'\"\nmysql -u root --password='123456' -e \"create user 'sentry'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database sentry default character set utf8' \nmysql -u root --password='123456' -e \"grant all privileges on sentry.* to 'sentry'@'%'\"\nmysql -u root --password='123456' -e \"create user 'nav'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database nav default character set utf8' \nmysql -u root --password='123456' -e \"grant all privileges on nav.* to 'nav'@'%'\"\nmysql -u root --password='123456' -e \"create user 'navms'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database navms default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on navms.* to 'navms'@'%'\"\nmysql -u root --password='123456' -e \"create user 'cm'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database cm default character set utf8' \nmysql -u root --password='123456' -e \"grant all privileges on cm.* to 'cm'@'%'\"\nmysql -u root --password='123456' -e \"create user 'oozie'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database oozie default character set utf8' \nmysql -u root --password='123456' -e \"grant all privileges on oozie.* to 'oozie'@'%'\"\nmysql -u root --password='123456' -e \"create user 'hue'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database hue default character set utf8' \nmysql -u root --password='123456' -e \"grant all privileges on hue.* to 'hue'@'%'\"\nmysql -u root --password='123456' -e \"FLUSH PRIVILEGES;\"\n```\n\n\n### 2.4 安装配置NFS Server\n\n在nfs.test.com节点上安装NFS\n```\nyum install nfs-utils nfs-utils-lib\n```\n启动nfs和rpcbind\n```\nsystemctl enable nfs\nsystemctl start rpcbind\nsystemctl start nfs\n```\n\n创建nfs文件目录\n```\nmkdir -p /media/cloudera-scm-server\n```\n在文件/etc/exports中添加如下的内容\n```\n/media/cloudera-scm-server cms1(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-server cms2(rw,sync,no_root_squash,no_subtree_check)\n```\n执行mounts\n```\nexportfs -a\n```\n在cms1和cms2上创建挂载点\n```\nrm -rf /var/lib/cloudera-scm-server\nmkdir -p /var/lib/cloudera-scm-server\n```\n安装nfs工具并执行挂载命令\n其中nfs-utils是在centos7上有效\n```\nyum install nfs-utils-lib \nyum install nfs-utils\nmount -t nfs nfs.test.com:/media/cloudera-scm-server /var/lib/cloudera-scm-server\n```\n重启rpcbind\n```\nsystemctl restart  rpcbind\n```\n修改/etc/fstab文件，使用其永久有效\n```\nnfs.test.com:/media/cloudera-scm-server /var/lib/cloudera-scm-server nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\n```\n\n查看\n```\ncms1:~>df -h\n文件系统                                 容量  已用  可用 已用% 挂载点\n/dev/vda2                                 90G  6.2G   84G    7% /\ndevtmpfs                                 7.5G     0  7.5G    0% /dev\ntmpfs                                    7.5G     0  7.5G    0% /dev/shm\ntmpfs                                    7.5G  8.4M  7.5G    1% /run\ntmpfs                                    7.5G     0  7.5G    0% /sys/fs/cgroup\ntmpfs                                    1.5G     0  1.5G    0% /run/user/0\n/dev/loop0                               4.1G  4.1G     0  100% /mnt\nnfs.test.com:/media/cloudera-scm-server   90G  2.1G   88G    3% /var/lib/cloudera-scm-server\n```\n\n```\ncms2:~>df -h\n文件系统                                 容量  已用  可用 已用% 挂载点\n/dev/vda2                                 90G  2.1G   88G    3% /\ndevtmpfs                                 7.5G     0  7.5G    0% /dev\ntmpfs                                    7.5G     0  7.5G    0% /dev/shm\ntmpfs                                    7.5G  8.4M  7.5G    1% /run\ntmpfs                                    7.5G     0  7.5G    0% /sys/fs/cgroup\ntmpfs                                    1.5G     0  1.5G    0% /run/user/0\nnfs.test.com:/media/cloudera-scm-server   90G  2.1G   88G    3% /var/lib/cloudera-scm-server\n```\n\n## 3. Cloudera Manager高可用的安装和配置\n\n### 在主节点上安装（cms1）\n安装cloudera manager server 服务\n```\nyum install -y cloudera-manager-server\n```\n配置cloudera manager server所使用的数据库\n```\n/usr/share/cmf/schema/scm_prepare_database.sh mysql -h dn1.test.com cm cm 123456\n```\n启动cloudera manager server\n```\n service cloudera-scm-server start\n```\n验证：\nhttp://CMS1:7180\n\n\n在代理服务器上检查代理的状态\nhttp://cms.test.com:1080/stats\n并查看http://cms.test.com:7180/cmf/login 是否可用（通过代理的ip访问）\n\nHTTP Referer 配置\nCloudera推荐禁用HTTP Referer检查，因为它可能会造成一些代理或者load balancer出错。通过如下步骤手动禁用。\n![Alt text](/img/1539920764009.png)\n\n### 在副节点上安装\n\n\n```\nyum install -y cloudera-manager-server\n```\n复制数据库配置文件\n```\nscp root@cms1:/etc/cloudera-scm-server/db.properties  /etc/cloudera-scm-server/\n```\n关闭开机启动\n```\nsystemctl disable cloudera-scm-server\n```\n如果配置自动故障转移，需要在主节点也禁用自动开机启动\n\n测试故障转移\n停止cms1.test.com上的cloudera-scm-server\n```\nsystemctl stop cloudera-scm-server\n```\n\n**等待1分钟左右**在cms2.test.com上启动cloudera-scm-server\n```\nsystemctl start cloudera-scm-server\n```\n访问http://cm.test.com:1080/stats\n和http://cm.test.com:7180/cmf/login \n\n更新cloudera manager agens使其使用load balancer (***除了cms1，cms2，mgmt1，mgmt2这几个节点***)\n更新配置文件/etc/cloudera-scm-agent/config.ini，修改server_host\n```\nserver_host = cms.test.com\n```\n重启agent\n```\nservice cloudera-scm-agent restart\n```\n\n## 4. Cloudera Management Service的高可用安装和配置\n\n### 4.1 为Cloudera Management Service 设置NFS挂载点\n在NFS服务器 dn3.test.com上执行命令\n```\nmkdir -p /media/cloudera-host-monitor\nmkdir -p /media/cloudera-scm-agent\nmkdir -p /media/cloudera-scm-eventserver\nmkdir -p /media/cloudera-scm-headlamp\nmkdir -p /media/cloudera-service-monitor\nmkdir -p /media/cloudera-scm-navigator\nmkdir -p /media/etc-cloudera-scm-agent\n```\n在NFS服务器的/etc/exports 文件中添加如下内容\n```\n/media/cloudera-host-monitor mgmt1(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-agent mgmt1(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-eventserver mgmt1(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-headlamp mgmt1(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-service-monitor mgmt1(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-navigator mgmt1(rw,sync,no_root_squash,no_subtree_check)\n/media/etc-cloudera-scm-agent mgmt1(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-host-monitor mgmt2(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-agent mgmt2(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-eventserver mgmt2(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-headlamp mgmt2(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-service-monitor mgmt2(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-navigator mgmt2(rw,sync,no_root_squash,no_subtree_check)\n/media/etc-cloudera-scm-agent mgmt2(rw,sync,no_root_squash,no_subtree_check)\n```\n\n在NFS上执行如下命令导出挂载点\n```\nexportfs -a\n```\n在MGMT1和MGMT2节点上配置，此处仍旧使用cms1.test.com 和cms2.test.com\n如果是新的节点，则需要安装nfs-utils\n```\nyum install nfs-utils\n```\n### 4.2 在`MGMT1`和`MGMT2`上创建挂载点\n```\nmkdir -p /var/lib/cloudera-host-monitor\nmkdir -p /var/lib/cloudera-scm-agent\nmkdir -p /var/lib/cloudera-scm-eventserver\nmkdir -p /var/lib/cloudera-scm-headlamp\nmkdir -p /var/lib/cloudera-service-monitor\nmkdir -p /var/lib/cloudera-scm-navigator\nmkdir -p /etc/cloudera-scm-agent\n```\n挂载\n```\nmount -t nfs nfs.test.com:/media/cloudera-host-monitor /var/lib/cloudera-host-monitor\nmount -t nfs nfs.test.com:/media/cloudera-scm-agent /var/lib/cloudera-scm-agent\nmount -t nfs nfs.test.com:/media/cloudera-scm-eventserver /var/lib/cloudera-scm-eventserver\nmount -t nfs nfs.test.com:/media/cloudera-scm-headlamp /var/lib/cloudera-scm-headlamp\nmount -t nfs nfs.test.com:/media/cloudera-service-monitor /var/lib/cloudera-service-monitor\nmount -t nfs nfs.test.com:/media/cloudera-scm-navigator /var/lib/cloudera-scm-navigator\nmount -t nfs nfs.test.com:/media/etc-cloudera-scm-agent /etc/cloudera-scm-agent\n```\n设置fstab，添加如下内容在/etc/fstab文件中\n```\nnfs.test.com:/media/cloudera-host-monitor /var/lib/cloudera-host-monitor nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\nnfs.test.com:/media/cloudera-scm-agent /var/lib/cloudera-scm-agent nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\nnfs.test.com:/media/cloudera-scm-eventserver /var/lib/cloudera-scm-eventserver nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\nnfs.test.com:/media/cloudera-scm-headlamp /var/lib/cloudera-scm-headlamp nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\nnfs.test.com:/media/cloudera-service-monitor /var/lib/cloudera-service-monitor nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\nnfs.test.com:/media/cloudera-scm-navigator /var/lib/cloudera-scm-navigator nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\nnfs.test.com:/media/etc-cloudera-scm-agent /etc/cloudera-scm-agent nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\n```\n\n### 4.3 在主节点上安装 agent\n登陆MGMT1，安装cloudera-manager-daemons 和cloudera-manager-agent\n```\nyum install -y cloudera-manager-daemons cloudera-manager-agent\n```\n配置agent的/etc/cloudera-scm-agent/config.ini\n```\nserver_host = cms.test.com\nlistening_hostname = mgmt.test.com\n```\n编辑/etc/hosts 文件，添加如下内容\n```\n172.17.1.60 dn1.test.com\n```\n使用ping检查\n```\nmgmt1:~>ping dn1.test.com\nPING dn1.test.com (172.17.1.60) 56(84) bytes of data.\n64 bytes from mgmt1.test.com (172.17.1.60): icmp_seq=1 ttl=64 time=0.092 ms\n64 bytes from mgmt1.test.com (172.17.1.60): icmp_seq=2 ttl=64 time=0.059 ms\n```\n\n修改文件夹的权限\n```\nchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-eventserver\nchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-navigator\nchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-service-monitor\nchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-host-monitor\nchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-agent\nchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-headlamp\n```\n\n重启 agent\n```\nservice cloudera-scm-agent restart\n```\n\n![Alt text](/img/1539939416172.png)\n![Alt text](/img/1539939421998.png)\n\n\n\n### 4.4 在副节点上安装agent\n在cm上停掉所有的Cloudera Management Service服务\n\n停掉主节点的`cloudera-scm-agent` 服务\n```\nmgmt1:~>systemctl stop cloudera-scm-agent\nmgmt1:~>\n```\n在副节点上安装`cloudera-manager-agent`\n```\nyum install -y cloudera-manager-agent\n```\n\n在cm上启动所有的`cloudera management service`\n\nNote: *确保cloudera-scm用户UID和GID在Cloudera Management Service的主副节点上是一致的。*\n![Alt text](/img/1539939380054.png)\n启动Cloudera Management Service\n![Alt text](/img/1539939371522.png)\n\n\n## 5. 问题：\n\n![Alt text](/img/1539939461605.png)\n节点的SELINUX没有设施为disabled\n\n\n----------\n\n","source":"_posts/Cloudera-Manager-High-Availability.md","raw":"---\ntitle: Cloudera Manager High Availability\ndate: 2018-10-22 11:28:39\ntags: \n- CDH安装\n---\n\n## 1. 环境准备\n### 1.1 环境\n操作系统：CentOS Linux release 7.3.1611 (Core)\nJDK：jdk1.8.0_111\n服务器：53-58 60-61 共8台\n\n### 1.2 软件下载\nHAProxy: http://www.haproxy.org/download/1.8/src/haproxy-1.8.13.tar.gz\n\n<!-- more --> \n\n### 1.3 架构\n![enter image description here](https://www.cloudera.com/documentation/enterprise/5-4-x/images/cm_ha_lb_setup.png)\n\n### 1.4 修改主机名称\n```\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n172.17.1.53 cms1.test.com cms1\n172.17.1.54 cms2.test.com cms2\n172.17.1.55 mgmt1.test.com mgmt1\n172.17.1.56 mgmt2.test.com mgmt2\n172.17.1.57 nfs.test.com nfs\n172.17.1.58 cms.test.com cms\n172.17.1.60 mgmt.test.com mgmt\n172.17.1.61 dn1.test.com dn1\n```\n\n\n## 2 安装前环境准备\n### 2.1 创建主和副主机\nCloudera Manager Server and Cloudera Management Service Primary host cms1.test.com\nCloudera Manager Server and Cloudera Management Service Secondary host cms2.test.com\n\n此外，Cloudera建议：\n    不要在安装CDH的节点安装Cloudera Manager或者Cloudera Management Service，因为这样会使failover的配置变的复杂。并且覆盖失败的域名可能会造成容错和错误检查的问题。\n    对主副主机都使用相同的主机配置。用来保证故障转移后性能不会降低。\n    对主副主机使用分开的主机和网络组件。\n\n主机分配\n\n| __IP__ | __DomainName__ | __功能__ | __角色__|\n|-----------|-----------|-----------|-----------|\n|172.17.1.53|cms1.test.com| cloudera manager 主节点|cms,cma|\n|172.17.1.54|cms2.test.com| cloudera manager 备份节点|cms,cma|\n|172.17.1.55|mgmt1.test.com|cloudera management service 主节点|cmmg,cma|\n|172.17.1.56|mgmt2.test.com|cloudera managerment service 备份节点|cmmg,cma|\n|172.17.1.57|nfs.test.com|挂载存储服务器|nfs|\n|172.17.1.58|cms.test.com|cm代理服务器|haproxy|\n|172.17.1.60|mgmt.test.com|mgmt代理服务器|haproxy|\n|172.17.1.61|dn1.test.com|数据节点|cma,数据库|\n\n\n### 2.2 安装配置Load Balancer\n在`cms`节点上安装HAProxy\n```\nyum install -y haproxy\n```\n配置haproxy开机启动\n```\nsystemctl enable haproxy\n```\n配置HAProxy\n在`cms`上，编辑*/etc/haproxy/haproxy.cfg*文件，添加需要代理的端口\n\n```\n#---------------------------------------------------------------------\n# Example configuration for a possible web application.  See the\n# full configuration options online.\n#\n#   http://haproxy.1wt.eu/download/1.4/doc/configuration.txt\n#\n#---------------------------------------------------------------------\n\n#---------------------------------------------------------------------\n# Global settings\n#---------------------------------------------------------------------\nglobal\n    # to have these messages end up in /var/log/haproxy.log you will\n    # need to:\n    #\n    # 1) configure syslog to accept network log events.  This is done\n    #    by adding the '-r' option to the SYSLOGD_OPTIONS in\n    #    /etc/sysconfig/syslog\n    #\n    # 2) configure local2 events to go to the /var/log/haproxy.log\n    #   file. A line like the following can be added to\n    #   /etc/sysconfig/syslog\n    #\n    #    local2.*                       /var/log/haproxy.log\n    #\n    log         127.0.0.1 local2\n\n    chroot      /var/lib/haproxy\n    pidfile     /var/run/haproxy.pid\n    maxconn     4000\n    user        haproxy\n    group       haproxy\n    daemon\n\n    # turn on stats unix socket\n    stats socket /var/lib/haproxy/stats\n\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\n    mode                    http\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n    #option forwardfor       except 127.0.0.0/8\n    option                  redispatch\n    retries                 3\n    timeout http-request    10s\n    timeout queue           1m\n    timeout connect         10s\n    timeout client          1m\n    timeout server          1m\n    timeout http-keep-alive 10s\n    timeout check           10s\n    maxconn                 3000\n\n#---------------------------------------------------------------------\n# main frontend which proxys to the backends\n#---------------------------------------------------------------------\n#frontend  main *:5000\n#    acl url_static       path_beg       -i /static /images /javascript /stylesheets\n#    acl url_static       path_end       -i .jpg .gif .png .css .js\n#\n#    use_backend static          if url_static\n#    default_backend             app\n\n#---------------------------------------------------------------------\n# static backend for serving up images, stylesheets and such\n#---------------------------------------------------------------------\n#backend static\n#    balance     roundrobin\n#    server      static 127.0.0.1:4331 check\n\n#---------------------------------------------------------------------\n# round robin balancing between the various backends\n#---------------------------------------------------------------------\n#backend app\n#    balance     roundrobin\n#    server  app1 127.0.0.1:5001 check\n#    server  app2 127.0.0.1:5002 check\n#    server  app3 127.0.0.1:5003 check\n#    server  app4 127.0.0.1:5004 check\n\n#---------------------------------------------------------------------\n# stats port\n#---------------------------------------------------------------------\nlisten stats\n    bind 0.0.0.0:1080\n    mode http\n    option httplog\n    maxconn 5000\n    stats refresh 30s\n    stats  uri /stats\n\n#---------------------------------------------------------------------\n# Cloudera Manager\n#---------------------------------------------------------------------\n\nlisten cmf\n    mode tcp\n    option tcplog\n    bind 0.0.0.0:7180\n    server cmfhttp1 cms1.test.com:7180 check\n    server cmfhttp2 cms2.test.com:7180 check\n\nlisten cmfavro :7182\n    mode tcp\n    option tcplog\n    server cmfavro1 cms1.test.com:7182 check\n    server cmfavro2 cms2.test.com:7182 check\n\n#ssl pass-through, without termination\nlisten cmfhttps :7183\n    mode tcp\n    option tcplog\n    server cmfhttps1 cms1.test.com:7183 check\n    server cmfhttps2 cms2.test.com:7183 check\n\nlisten mgmt1 :5678\n    mode tcp\n    option tcplog\n    server mgmt1a cms1.test.com check\n    server mgmt1b cms2.test.com check\n\nlisten mgmt2 :7184\n    mode tcp\n    option tcplog\n    server mgmt2a cms1.test.com check\n    server mgmt2b cms2.test.com check\n\nlisten mgmt3 :7185\n    mode tcp\n    option tcplog\n    server mgmt3a cms1.test.com check\n    server mgmt3b cms2.test.com check\nlisten mgmt4 :7186\n    mode tcp\n    option tcplog\n    server mgmt4a cms1.test.com check\n    server mgmt4b cms2.test.com check\nlisten mgmt5 :7187\n    mode tcp\n    option tcplog\n    server mgmt5a cms1.test.com check\n    server mgmt5b cms2.test.com check\n\nlisten mgmt6 :8083\n    mode tcp\n    option tcplog\n    server mgmt6a cms1.test.com check\n    server mgmt6b cms2.test.com check\nlisten mgmt7 :8084\n    mode tcp\n    option tcplog\n    server mgmt7a cms1.test.com check\n    server mgmt7b cms2.test.com check\nlisten mgmt8 :8086\n    mode tcp\n    option tcplog\n    server mgmt8a cms1.test.com check\n    server mgmt8b cms2.test.com check\nlisten mgmt9 :8087\n    mode tcp\n    option tcplog\n    server mgmt9a cms1.test.com check\n    server mgmt9b cms2.test.com check\nlisten mgmt10 :8091\n    mode tcp\n    option tcplog\n    server mgmt10a cms1.test.com check\n    server mgmt10b cms2.test.com check\nlisten mgmt-agent :9000\n    mode tcp\n    option tcplog\n    server mgmt-agenta cms1.test.com check\n    server mgmt-agentb cms2.test.com check\nlisten mgmt11 :9994\n    mode tcp\n    option tcplog\n    server mgmt11a cms1.test.com check\n    server mgmt11b cms2.test.com check\nlisten mgmt12 :9995\n    mode tcp\n    option tcplog\n    server mgmt12a cms1.test.com check\n    server mgmt12b cms2.test.com check\nlisten mgmt13 :9996\n    mode tcp\n    option tcplog\n    server mgmt13a cms1.test.com check\n    server mgmt13b cms2.test.com check\nlisten mgmt14 :9997\n    mode tcp\n    option tcplog\n    server mgmt14a cms1.test.com check\n    server mgmt14b cms2.test.com check\nlisten mgmt15 :9998\n    mode tcp\n    option tcplog\n    server mgmt15a cms1.test.com check\n    server mgmt15b cms2.test.com check\nlisten mgmt16 :9999\n    mode tcp\n    option tcplog\n    server mgmt16a cms1.test.com check\n    server mgmt16b cms2.test.com check\nlisten mgmt17 :10101\n    mode tcp\n    option tcplog\n    server mgmt17a cms1.test.com check\n    server mgmt17b cms2.test.com check\n```\n重启haproxy\n\n```\nsystemctl restart  haproxy\n```\n### 2.3 安装配置数据库\n在dn1节点上安装数据库\n```\nyum install -y mariadb mariadb-server\n```\n启动数据库\n```\nsystemctl restart mariadb\n```\n\n初始化\n```\n/usr/bin/mysql_secure_installation\n```\n\n配置mariadb的主从 [参考](https://mariadb.com/kb/en/library/setting-up-replication/)\n在主节点上的/etc/my.cnf添加如下配置并重启\n```\n[mariadb]\nlog-bin\nserver_id=1\nlog-basename=master1\n```\n创建replication用户\n```\nCREATE USER 'replication_user'@'%' IDENTIFIED BY 'bigs3cret';\nGRANT REPLICATION SLAVE ON *.* TO 'replication_user'@'%';\n```\n在从节点上修改/etc/my.cnf文件,并重启\n```\n[mariadb]\nlog-bin\nserver_id=2\nlog-basename=slave1\n```\n获取主节点的Binary Log\n在主节点上执行命令锁住所有的表\n```\nFLUSH TABLES WITH READ LOCK\n```\n执行命令获取主节点的状态，并记住Position的值\n```\nMariaDB [(none)]> SHOW MASTER STATUS;\n+------------------+----------+--------------+------------------+\n| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |\n+------------------+----------+--------------+------------------+\n| mysql-bin.000004 |      495 |              |                  |\n+------------------+----------+--------------+------------------+\n1 row in set (0.00 sec)\n\nMariaDB [(none)]>\n```\n\n在从节点执行如下命令：\n```\nCHANGE MASTER TO\n  MASTER_HOST='cms1.test.com',\n  MASTER_USER='replication_user',\n  MASTER_PASSWORD='bigs3cret',\n  MASTER_PORT=3306,\n  MASTER_LOG_FILE='mysql-bin.000004',\n  MASTER_LOG_POS=495,\n  MASTER_CONNECT_RETRY=10;\n```\n \n 在主节点执行命令\n```\nUNLOCK TABLES;\n```\n\n在从节点执行命令启动slave\n```\nSTART SLAVE;\n```\n使用命令查询是否配置争取\n```\nMariaDB [(none)]> show slave status \\G\n*************************** 1. row ***************************\n               Slave_IO_State: Waiting for master to send event\n                  Master_Host: cms1.test.com\n                  Master_User: replication_user\n                  Master_Port: 3306\n                Connect_Retry: 10\n              Master_Log_File: mysql-bin.000004\n          Read_Master_Log_Pos: 495\n               Relay_Log_File: slave1-relay-bin.000005\n                Relay_Log_Pos: 529\n        Relay_Master_Log_File: mysql-bin.000004\n             Slave_IO_Running: Yes\n            Slave_SQL_Running: Yes\n              Replicate_Do_DB:\n          Replicate_Ignore_DB:\n           Replicate_Do_Table:\n       Replicate_Ignore_Table:\n      Replicate_Wild_Do_Table:\n  Replicate_Wild_Ignore_Table:\n                   Last_Errno: 0\n                   Last_Error:\n                 Skip_Counter: 0\n          Exec_Master_Log_Pos: 495\n              Relay_Log_Space: 824\n              Until_Condition: None\n               Until_Log_File:\n                Until_Log_Pos: 0\n           Master_SSL_Allowed: No\n           Master_SSL_CA_File:\n           Master_SSL_CA_Path:\n              Master_SSL_Cert:\n            Master_SSL_Cipher:\n               Master_SSL_Key:\n        Seconds_Behind_Master: 0\nMaster_SSL_Verify_Server_Cert: No\n                Last_IO_Errno: 0\n                Last_IO_Error:\n               Last_SQL_Errno: 0\n               Last_SQL_Error:\n  Replicate_Ignore_Server_Ids:\n             Master_Server_Id: 1\n1 row in set (0.00 sec)\n```\nSlave_IO_Running 和 Slave_SQL_Running应该是yes\n\n在主节点上执行如下命令来创建数据库和用户\n```\nmysql -u root --password='123456' -e 'create database metastore default character set utf8;'\nmysql -u root --password='123456' -e \"CREATE USER 'hive'@'%' IDENTIFIED BY '123456';\"\nmysql -u root --password='123456' -e \"GRANT ALL PRIVILEGES ON metastore. * TO 'hive'@'%';\"\nmysql -u root --password='123456' -e \"create user 'amon'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database amon default character set utf8' \nmysql -u root --password='123456' -e \"grant all privileges on amon.* to 'amon'@'%'\"\nmysql -u root --password='123456' -e \"create user 'rman'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database rman default character set utf8' \nmysql -u root --password='123456' -e \"grant all privileges on rman.* to 'rman'@'%'\"\nmysql -u root --password='123456' -e \"create user 'sentry'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database sentry default character set utf8' \nmysql -u root --password='123456' -e \"grant all privileges on sentry.* to 'sentry'@'%'\"\nmysql -u root --password='123456' -e \"create user 'nav'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database nav default character set utf8' \nmysql -u root --password='123456' -e \"grant all privileges on nav.* to 'nav'@'%'\"\nmysql -u root --password='123456' -e \"create user 'navms'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database navms default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on navms.* to 'navms'@'%'\"\nmysql -u root --password='123456' -e \"create user 'cm'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database cm default character set utf8' \nmysql -u root --password='123456' -e \"grant all privileges on cm.* to 'cm'@'%'\"\nmysql -u root --password='123456' -e \"create user 'oozie'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database oozie default character set utf8' \nmysql -u root --password='123456' -e \"grant all privileges on oozie.* to 'oozie'@'%'\"\nmysql -u root --password='123456' -e \"create user 'hue'@'%' identified by '123456'\" \nmysql -u root --password='123456' -e 'create database hue default character set utf8' \nmysql -u root --password='123456' -e \"grant all privileges on hue.* to 'hue'@'%'\"\nmysql -u root --password='123456' -e \"FLUSH PRIVILEGES;\"\n```\n\n\n### 2.4 安装配置NFS Server\n\n在nfs.test.com节点上安装NFS\n```\nyum install nfs-utils nfs-utils-lib\n```\n启动nfs和rpcbind\n```\nsystemctl enable nfs\nsystemctl start rpcbind\nsystemctl start nfs\n```\n\n创建nfs文件目录\n```\nmkdir -p /media/cloudera-scm-server\n```\n在文件/etc/exports中添加如下的内容\n```\n/media/cloudera-scm-server cms1(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-server cms2(rw,sync,no_root_squash,no_subtree_check)\n```\n执行mounts\n```\nexportfs -a\n```\n在cms1和cms2上创建挂载点\n```\nrm -rf /var/lib/cloudera-scm-server\nmkdir -p /var/lib/cloudera-scm-server\n```\n安装nfs工具并执行挂载命令\n其中nfs-utils是在centos7上有效\n```\nyum install nfs-utils-lib \nyum install nfs-utils\nmount -t nfs nfs.test.com:/media/cloudera-scm-server /var/lib/cloudera-scm-server\n```\n重启rpcbind\n```\nsystemctl restart  rpcbind\n```\n修改/etc/fstab文件，使用其永久有效\n```\nnfs.test.com:/media/cloudera-scm-server /var/lib/cloudera-scm-server nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\n```\n\n查看\n```\ncms1:~>df -h\n文件系统                                 容量  已用  可用 已用% 挂载点\n/dev/vda2                                 90G  6.2G   84G    7% /\ndevtmpfs                                 7.5G     0  7.5G    0% /dev\ntmpfs                                    7.5G     0  7.5G    0% /dev/shm\ntmpfs                                    7.5G  8.4M  7.5G    1% /run\ntmpfs                                    7.5G     0  7.5G    0% /sys/fs/cgroup\ntmpfs                                    1.5G     0  1.5G    0% /run/user/0\n/dev/loop0                               4.1G  4.1G     0  100% /mnt\nnfs.test.com:/media/cloudera-scm-server   90G  2.1G   88G    3% /var/lib/cloudera-scm-server\n```\n\n```\ncms2:~>df -h\n文件系统                                 容量  已用  可用 已用% 挂载点\n/dev/vda2                                 90G  2.1G   88G    3% /\ndevtmpfs                                 7.5G     0  7.5G    0% /dev\ntmpfs                                    7.5G     0  7.5G    0% /dev/shm\ntmpfs                                    7.5G  8.4M  7.5G    1% /run\ntmpfs                                    7.5G     0  7.5G    0% /sys/fs/cgroup\ntmpfs                                    1.5G     0  1.5G    0% /run/user/0\nnfs.test.com:/media/cloudera-scm-server   90G  2.1G   88G    3% /var/lib/cloudera-scm-server\n```\n\n## 3. Cloudera Manager高可用的安装和配置\n\n### 在主节点上安装（cms1）\n安装cloudera manager server 服务\n```\nyum install -y cloudera-manager-server\n```\n配置cloudera manager server所使用的数据库\n```\n/usr/share/cmf/schema/scm_prepare_database.sh mysql -h dn1.test.com cm cm 123456\n```\n启动cloudera manager server\n```\n service cloudera-scm-server start\n```\n验证：\nhttp://CMS1:7180\n\n\n在代理服务器上检查代理的状态\nhttp://cms.test.com:1080/stats\n并查看http://cms.test.com:7180/cmf/login 是否可用（通过代理的ip访问）\n\nHTTP Referer 配置\nCloudera推荐禁用HTTP Referer检查，因为它可能会造成一些代理或者load balancer出错。通过如下步骤手动禁用。\n![Alt text](/img/1539920764009.png)\n\n### 在副节点上安装\n\n\n```\nyum install -y cloudera-manager-server\n```\n复制数据库配置文件\n```\nscp root@cms1:/etc/cloudera-scm-server/db.properties  /etc/cloudera-scm-server/\n```\n关闭开机启动\n```\nsystemctl disable cloudera-scm-server\n```\n如果配置自动故障转移，需要在主节点也禁用自动开机启动\n\n测试故障转移\n停止cms1.test.com上的cloudera-scm-server\n```\nsystemctl stop cloudera-scm-server\n```\n\n**等待1分钟左右**在cms2.test.com上启动cloudera-scm-server\n```\nsystemctl start cloudera-scm-server\n```\n访问http://cm.test.com:1080/stats\n和http://cm.test.com:7180/cmf/login \n\n更新cloudera manager agens使其使用load balancer (***除了cms1，cms2，mgmt1，mgmt2这几个节点***)\n更新配置文件/etc/cloudera-scm-agent/config.ini，修改server_host\n```\nserver_host = cms.test.com\n```\n重启agent\n```\nservice cloudera-scm-agent restart\n```\n\n## 4. Cloudera Management Service的高可用安装和配置\n\n### 4.1 为Cloudera Management Service 设置NFS挂载点\n在NFS服务器 dn3.test.com上执行命令\n```\nmkdir -p /media/cloudera-host-monitor\nmkdir -p /media/cloudera-scm-agent\nmkdir -p /media/cloudera-scm-eventserver\nmkdir -p /media/cloudera-scm-headlamp\nmkdir -p /media/cloudera-service-monitor\nmkdir -p /media/cloudera-scm-navigator\nmkdir -p /media/etc-cloudera-scm-agent\n```\n在NFS服务器的/etc/exports 文件中添加如下内容\n```\n/media/cloudera-host-monitor mgmt1(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-agent mgmt1(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-eventserver mgmt1(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-headlamp mgmt1(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-service-monitor mgmt1(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-navigator mgmt1(rw,sync,no_root_squash,no_subtree_check)\n/media/etc-cloudera-scm-agent mgmt1(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-host-monitor mgmt2(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-agent mgmt2(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-eventserver mgmt2(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-headlamp mgmt2(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-service-monitor mgmt2(rw,sync,no_root_squash,no_subtree_check)\n/media/cloudera-scm-navigator mgmt2(rw,sync,no_root_squash,no_subtree_check)\n/media/etc-cloudera-scm-agent mgmt2(rw,sync,no_root_squash,no_subtree_check)\n```\n\n在NFS上执行如下命令导出挂载点\n```\nexportfs -a\n```\n在MGMT1和MGMT2节点上配置，此处仍旧使用cms1.test.com 和cms2.test.com\n如果是新的节点，则需要安装nfs-utils\n```\nyum install nfs-utils\n```\n### 4.2 在`MGMT1`和`MGMT2`上创建挂载点\n```\nmkdir -p /var/lib/cloudera-host-monitor\nmkdir -p /var/lib/cloudera-scm-agent\nmkdir -p /var/lib/cloudera-scm-eventserver\nmkdir -p /var/lib/cloudera-scm-headlamp\nmkdir -p /var/lib/cloudera-service-monitor\nmkdir -p /var/lib/cloudera-scm-navigator\nmkdir -p /etc/cloudera-scm-agent\n```\n挂载\n```\nmount -t nfs nfs.test.com:/media/cloudera-host-monitor /var/lib/cloudera-host-monitor\nmount -t nfs nfs.test.com:/media/cloudera-scm-agent /var/lib/cloudera-scm-agent\nmount -t nfs nfs.test.com:/media/cloudera-scm-eventserver /var/lib/cloudera-scm-eventserver\nmount -t nfs nfs.test.com:/media/cloudera-scm-headlamp /var/lib/cloudera-scm-headlamp\nmount -t nfs nfs.test.com:/media/cloudera-service-monitor /var/lib/cloudera-service-monitor\nmount -t nfs nfs.test.com:/media/cloudera-scm-navigator /var/lib/cloudera-scm-navigator\nmount -t nfs nfs.test.com:/media/etc-cloudera-scm-agent /etc/cloudera-scm-agent\n```\n设置fstab，添加如下内容在/etc/fstab文件中\n```\nnfs.test.com:/media/cloudera-host-monitor /var/lib/cloudera-host-monitor nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\nnfs.test.com:/media/cloudera-scm-agent /var/lib/cloudera-scm-agent nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\nnfs.test.com:/media/cloudera-scm-eventserver /var/lib/cloudera-scm-eventserver nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\nnfs.test.com:/media/cloudera-scm-headlamp /var/lib/cloudera-scm-headlamp nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\nnfs.test.com:/media/cloudera-service-monitor /var/lib/cloudera-service-monitor nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\nnfs.test.com:/media/cloudera-scm-navigator /var/lib/cloudera-scm-navigator nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\nnfs.test.com:/media/etc-cloudera-scm-agent /etc/cloudera-scm-agent nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0\n```\n\n### 4.3 在主节点上安装 agent\n登陆MGMT1，安装cloudera-manager-daemons 和cloudera-manager-agent\n```\nyum install -y cloudera-manager-daemons cloudera-manager-agent\n```\n配置agent的/etc/cloudera-scm-agent/config.ini\n```\nserver_host = cms.test.com\nlistening_hostname = mgmt.test.com\n```\n编辑/etc/hosts 文件，添加如下内容\n```\n172.17.1.60 dn1.test.com\n```\n使用ping检查\n```\nmgmt1:~>ping dn1.test.com\nPING dn1.test.com (172.17.1.60) 56(84) bytes of data.\n64 bytes from mgmt1.test.com (172.17.1.60): icmp_seq=1 ttl=64 time=0.092 ms\n64 bytes from mgmt1.test.com (172.17.1.60): icmp_seq=2 ttl=64 time=0.059 ms\n```\n\n修改文件夹的权限\n```\nchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-eventserver\nchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-navigator\nchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-service-monitor\nchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-host-monitor\nchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-agent\nchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-headlamp\n```\n\n重启 agent\n```\nservice cloudera-scm-agent restart\n```\n\n![Alt text](/img/1539939416172.png)\n![Alt text](/img/1539939421998.png)\n\n\n\n### 4.4 在副节点上安装agent\n在cm上停掉所有的Cloudera Management Service服务\n\n停掉主节点的`cloudera-scm-agent` 服务\n```\nmgmt1:~>systemctl stop cloudera-scm-agent\nmgmt1:~>\n```\n在副节点上安装`cloudera-manager-agent`\n```\nyum install -y cloudera-manager-agent\n```\n\n在cm上启动所有的`cloudera management service`\n\nNote: *确保cloudera-scm用户UID和GID在Cloudera Management Service的主副节点上是一致的。*\n![Alt text](/img/1539939380054.png)\n启动Cloudera Management Service\n![Alt text](/img/1539939371522.png)\n\n\n## 5. 问题：\n\n![Alt text](/img/1539939461605.png)\n节点的SELINUX没有设施为disabled\n\n\n----------\n\n","slug":"Cloudera-Manager-High-Availability","published":1,"updated":"2018-10-22T05:20:56.025Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzp000tinammpigqg8r","content":"<h2 id=\"1-环境准备\"><a href=\"#1-环境准备\" class=\"headerlink\" title=\"1. 环境准备\"></a>1. 环境准备</h2><h3 id=\"1-1-环境\"><a href=\"#1-1-环境\" class=\"headerlink\" title=\"1.1 环境\"></a>1.1 环境</h3><p>操作系统：CentOS Linux release 7.3.1611 (Core)<br>JDK：jdk1.8.0_111<br>服务器：53-58 60-61 共8台</p>\n<h3 id=\"1-2-软件下载\"><a href=\"#1-2-软件下载\" class=\"headerlink\" title=\"1.2 软件下载\"></a>1.2 软件下载</h3><p>HAProxy: <a href=\"http://www.haproxy.org/download/1.8/src/haproxy-1.8.13.tar.gz\" target=\"_blank\" rel=\"noopener\">http://www.haproxy.org/download/1.8/src/haproxy-1.8.13.tar.gz</a></p>\n<a id=\"more\"></a> \n<h3 id=\"1-3-架构\"><a href=\"#1-3-架构\" class=\"headerlink\" title=\"1.3 架构\"></a>1.3 架构</h3><p><img src=\"https://www.cloudera.com/documentation/enterprise/5-4-x/images/cm_ha_lb_setup.png\" alt=\"enter image description here\"></p>\n<h3 id=\"1-4-修改主机名称\"><a href=\"#1-4-修改主机名称\" class=\"headerlink\" title=\"1.4 修改主机名称\"></a>1.4 修改主机名称</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class=\"line\">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class=\"line\">172.17.1.53 cms1.test.com cms1</span><br><span class=\"line\">172.17.1.54 cms2.test.com cms2</span><br><span class=\"line\">172.17.1.55 mgmt1.test.com mgmt1</span><br><span class=\"line\">172.17.1.56 mgmt2.test.com mgmt2</span><br><span class=\"line\">172.17.1.57 nfs.test.com nfs</span><br><span class=\"line\">172.17.1.58 cms.test.com cms</span><br><span class=\"line\">172.17.1.60 mgmt.test.com mgmt</span><br><span class=\"line\">172.17.1.61 dn1.test.com dn1</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-安装前环境准备\"><a href=\"#2-安装前环境准备\" class=\"headerlink\" title=\"2 安装前环境准备\"></a>2 安装前环境准备</h2><h3 id=\"2-1-创建主和副主机\"><a href=\"#2-1-创建主和副主机\" class=\"headerlink\" title=\"2.1 创建主和副主机\"></a>2.1 创建主和副主机</h3><p>Cloudera Manager Server and Cloudera Management Service Primary host cms1.test.com<br>Cloudera Manager Server and Cloudera Management Service Secondary host cms2.test.com</p>\n<p>此外，Cloudera建议：<br>    不要在安装CDH的节点安装Cloudera Manager或者Cloudera Management Service，因为这样会使failover的配置变的复杂。并且覆盖失败的域名可能会造成容错和错误检查的问题。<br>    对主副主机都使用相同的主机配置。用来保证故障转移后性能不会降低。<br>    对主副主机使用分开的主机和网络组件。</p>\n<p>主机分配</p>\n<table>\n<thead>\n<tr>\n<th><strong>IP</strong></th>\n<th><strong>DomainName</strong></th>\n<th><strong>功能</strong></th>\n<th><strong>角色</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>172.17.1.53</td>\n<td>cms1.test.com</td>\n<td>cloudera manager 主节点</td>\n<td>cms,cma</td>\n</tr>\n<tr>\n<td>172.17.1.54</td>\n<td>cms2.test.com</td>\n<td>cloudera manager 备份节点</td>\n<td>cms,cma</td>\n</tr>\n<tr>\n<td>172.17.1.55</td>\n<td>mgmt1.test.com</td>\n<td>cloudera management service 主节点</td>\n<td>cmmg,cma</td>\n</tr>\n<tr>\n<td>172.17.1.56</td>\n<td>mgmt2.test.com</td>\n<td>cloudera managerment service 备份节点</td>\n<td>cmmg,cma</td>\n</tr>\n<tr>\n<td>172.17.1.57</td>\n<td>nfs.test.com</td>\n<td>挂载存储服务器</td>\n<td>nfs</td>\n</tr>\n<tr>\n<td>172.17.1.58</td>\n<td>cms.test.com</td>\n<td>cm代理服务器</td>\n<td>haproxy</td>\n</tr>\n<tr>\n<td>172.17.1.60</td>\n<td>mgmt.test.com</td>\n<td>mgmt代理服务器</td>\n<td>haproxy</td>\n</tr>\n<tr>\n<td>172.17.1.61</td>\n<td>dn1.test.com</td>\n<td>数据节点</td>\n<td>cma,数据库</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"2-2-安装配置Load-Balancer\"><a href=\"#2-2-安装配置Load-Balancer\" class=\"headerlink\" title=\"2.2 安装配置Load Balancer\"></a>2.2 安装配置Load Balancer</h3><p>在<code>cms</code>节点上安装HAProxy<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y haproxy</span><br></pre></td></tr></table></figure></p>\n<p>配置haproxy开机启动<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl enable haproxy</span><br></pre></td></tr></table></figure></p>\n<p>配置HAProxy<br>在<code>cms</code>上，编辑<em>/etc/haproxy/haproxy.cfg</em>文件，添加需要代理的端口</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># Example configuration for a possible web application.  See the</span><br><span class=\"line\"># full configuration options online.</span><br><span class=\"line\">#</span><br><span class=\"line\">#   http://haproxy.1wt.eu/download/1.4/doc/configuration.txt</span><br><span class=\"line\">#</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># Global settings</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\">global</span><br><span class=\"line\">    # to have these messages end up in /var/log/haproxy.log you will</span><br><span class=\"line\">    # need to:</span><br><span class=\"line\">    #</span><br><span class=\"line\">    # 1) configure syslog to accept network log events.  This is done</span><br><span class=\"line\">    #    by adding the &apos;-r&apos; option to the SYSLOGD_OPTIONS in</span><br><span class=\"line\">    #    /etc/sysconfig/syslog</span><br><span class=\"line\">    #</span><br><span class=\"line\">    # 2) configure local2 events to go to the /var/log/haproxy.log</span><br><span class=\"line\">    #   file. A line like the following can be added to</span><br><span class=\"line\">    #   /etc/sysconfig/syslog</span><br><span class=\"line\">    #</span><br><span class=\"line\">    #    local2.*                       /var/log/haproxy.log</span><br><span class=\"line\">    #</span><br><span class=\"line\">    log         127.0.0.1 local2</span><br><span class=\"line\"></span><br><span class=\"line\">    chroot      /var/lib/haproxy</span><br><span class=\"line\">    pidfile     /var/run/haproxy.pid</span><br><span class=\"line\">    maxconn     4000</span><br><span class=\"line\">    user        haproxy</span><br><span class=\"line\">    group       haproxy</span><br><span class=\"line\">    daemon</span><br><span class=\"line\"></span><br><span class=\"line\">    # turn on stats unix socket</span><br><span class=\"line\">    stats socket /var/lib/haproxy/stats</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># common defaults that all the &apos;listen&apos; and &apos;backend&apos; sections will</span><br><span class=\"line\"># use if not designated in their block</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\">defaults</span><br><span class=\"line\">    mode                    http</span><br><span class=\"line\">    log                     global</span><br><span class=\"line\">    option                  httplog</span><br><span class=\"line\">    option                  dontlognull</span><br><span class=\"line\">    option http-server-close</span><br><span class=\"line\">    #option forwardfor       except 127.0.0.0/8</span><br><span class=\"line\">    option                  redispatch</span><br><span class=\"line\">    retries                 3</span><br><span class=\"line\">    timeout http-request    10s</span><br><span class=\"line\">    timeout queue           1m</span><br><span class=\"line\">    timeout connect         10s</span><br><span class=\"line\">    timeout client          1m</span><br><span class=\"line\">    timeout server          1m</span><br><span class=\"line\">    timeout http-keep-alive 10s</span><br><span class=\"line\">    timeout check           10s</span><br><span class=\"line\">    maxconn                 3000</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># main frontend which proxys to the backends</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\">#frontend  main *:5000</span><br><span class=\"line\">#    acl url_static       path_beg       -i /static /images /javascript /stylesheets</span><br><span class=\"line\">#    acl url_static       path_end       -i .jpg .gif .png .css .js</span><br><span class=\"line\">#</span><br><span class=\"line\">#    use_backend static          if url_static</span><br><span class=\"line\">#    default_backend             app</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># static backend for serving up images, stylesheets and such</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\">#backend static</span><br><span class=\"line\">#    balance     roundrobin</span><br><span class=\"line\">#    server      static 127.0.0.1:4331 check</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># round robin balancing between the various backends</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\">#backend app</span><br><span class=\"line\">#    balance     roundrobin</span><br><span class=\"line\">#    server  app1 127.0.0.1:5001 check</span><br><span class=\"line\">#    server  app2 127.0.0.1:5002 check</span><br><span class=\"line\">#    server  app3 127.0.0.1:5003 check</span><br><span class=\"line\">#    server  app4 127.0.0.1:5004 check</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># stats port</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\">listen stats</span><br><span class=\"line\">    bind 0.0.0.0:1080</span><br><span class=\"line\">    mode http</span><br><span class=\"line\">    option httplog</span><br><span class=\"line\">    maxconn 5000</span><br><span class=\"line\">    stats refresh 30s</span><br><span class=\"line\">    stats  uri /stats</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># Cloudera Manager</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"></span><br><span class=\"line\">listen cmf</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    bind 0.0.0.0:7180</span><br><span class=\"line\">    server cmfhttp1 cms1.test.com:7180 check</span><br><span class=\"line\">    server cmfhttp2 cms2.test.com:7180 check</span><br><span class=\"line\"></span><br><span class=\"line\">listen cmfavro :7182</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server cmfavro1 cms1.test.com:7182 check</span><br><span class=\"line\">    server cmfavro2 cms2.test.com:7182 check</span><br><span class=\"line\"></span><br><span class=\"line\">#ssl pass-through, without termination</span><br><span class=\"line\">listen cmfhttps :7183</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server cmfhttps1 cms1.test.com:7183 check</span><br><span class=\"line\">    server cmfhttps2 cms2.test.com:7183 check</span><br><span class=\"line\"></span><br><span class=\"line\">listen mgmt1 :5678</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt1a cms1.test.com check</span><br><span class=\"line\">    server mgmt1b cms2.test.com check</span><br><span class=\"line\"></span><br><span class=\"line\">listen mgmt2 :7184</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt2a cms1.test.com check</span><br><span class=\"line\">    server mgmt2b cms2.test.com check</span><br><span class=\"line\"></span><br><span class=\"line\">listen mgmt3 :7185</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt3a cms1.test.com check</span><br><span class=\"line\">    server mgmt3b cms2.test.com check</span><br><span class=\"line\">listen mgmt4 :7186</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt4a cms1.test.com check</span><br><span class=\"line\">    server mgmt4b cms2.test.com check</span><br><span class=\"line\">listen mgmt5 :7187</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt5a cms1.test.com check</span><br><span class=\"line\">    server mgmt5b cms2.test.com check</span><br><span class=\"line\"></span><br><span class=\"line\">listen mgmt6 :8083</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt6a cms1.test.com check</span><br><span class=\"line\">    server mgmt6b cms2.test.com check</span><br><span class=\"line\">listen mgmt7 :8084</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt7a cms1.test.com check</span><br><span class=\"line\">    server mgmt7b cms2.test.com check</span><br><span class=\"line\">listen mgmt8 :8086</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt8a cms1.test.com check</span><br><span class=\"line\">    server mgmt8b cms2.test.com check</span><br><span class=\"line\">listen mgmt9 :8087</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt9a cms1.test.com check</span><br><span class=\"line\">    server mgmt9b cms2.test.com check</span><br><span class=\"line\">listen mgmt10 :8091</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt10a cms1.test.com check</span><br><span class=\"line\">    server mgmt10b cms2.test.com check</span><br><span class=\"line\">listen mgmt-agent :9000</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt-agenta cms1.test.com check</span><br><span class=\"line\">    server mgmt-agentb cms2.test.com check</span><br><span class=\"line\">listen mgmt11 :9994</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt11a cms1.test.com check</span><br><span class=\"line\">    server mgmt11b cms2.test.com check</span><br><span class=\"line\">listen mgmt12 :9995</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt12a cms1.test.com check</span><br><span class=\"line\">    server mgmt12b cms2.test.com check</span><br><span class=\"line\">listen mgmt13 :9996</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt13a cms1.test.com check</span><br><span class=\"line\">    server mgmt13b cms2.test.com check</span><br><span class=\"line\">listen mgmt14 :9997</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt14a cms1.test.com check</span><br><span class=\"line\">    server mgmt14b cms2.test.com check</span><br><span class=\"line\">listen mgmt15 :9998</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt15a cms1.test.com check</span><br><span class=\"line\">    server mgmt15b cms2.test.com check</span><br><span class=\"line\">listen mgmt16 :9999</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt16a cms1.test.com check</span><br><span class=\"line\">    server mgmt16b cms2.test.com check</span><br><span class=\"line\">listen mgmt17 :10101</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt17a cms1.test.com check</span><br><span class=\"line\">    server mgmt17b cms2.test.com check</span><br></pre></td></tr></table></figure>\n<p>重启haproxy</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl restart  haproxy</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-3-安装配置数据库\"><a href=\"#2-3-安装配置数据库\" class=\"headerlink\" title=\"2.3 安装配置数据库\"></a>2.3 安装配置数据库</h3><p>在dn1节点上安装数据库<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y mariadb mariadb-server</span><br></pre></td></tr></table></figure></p>\n<p>启动数据库<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl restart mariadb</span><br></pre></td></tr></table></figure></p>\n<p>初始化<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/usr/bin/mysql_secure_installation</span><br></pre></td></tr></table></figure></p>\n<p>配置mariadb的主从 <a href=\"https://mariadb.com/kb/en/library/setting-up-replication/\" target=\"_blank\" rel=\"noopener\">参考</a><br>在主节点上的/etc/my.cnf添加如下配置并重启<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[mariadb]</span><br><span class=\"line\">log-bin</span><br><span class=\"line\">server_id=1</span><br><span class=\"line\">log-basename=master1</span><br></pre></td></tr></table></figure></p>\n<p>创建replication用户<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE USER &apos;replication_user&apos;@&apos;%&apos; IDENTIFIED BY &apos;bigs3cret&apos;;</span><br><span class=\"line\">GRANT REPLICATION SLAVE ON *.* TO &apos;replication_user&apos;@&apos;%&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>在从节点上修改/etc/my.cnf文件,并重启<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[mariadb]</span><br><span class=\"line\">log-bin</span><br><span class=\"line\">server_id=2</span><br><span class=\"line\">log-basename=slave1</span><br></pre></td></tr></table></figure></p>\n<p>获取主节点的Binary Log<br>在主节点上执行命令锁住所有的表<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FLUSH TABLES WITH READ LOCK</span><br></pre></td></tr></table></figure></p>\n<p>执行命令获取主节点的状态，并记住Position的值<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MariaDB [(none)]&gt; SHOW MASTER STATUS;</span><br><span class=\"line\">+------------------+----------+--------------+------------------+</span><br><span class=\"line\">| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |</span><br><span class=\"line\">+------------------+----------+--------------+------------------+</span><br><span class=\"line\">| mysql-bin.000004 |      495 |              |                  |</span><br><span class=\"line\">+------------------+----------+--------------+------------------+</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">MariaDB [(none)]&gt;</span><br></pre></td></tr></table></figure></p>\n<p>在从节点执行如下命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CHANGE MASTER TO</span><br><span class=\"line\">  MASTER_HOST=&apos;cms1.test.com&apos;,</span><br><span class=\"line\">  MASTER_USER=&apos;replication_user&apos;,</span><br><span class=\"line\">  MASTER_PASSWORD=&apos;bigs3cret&apos;,</span><br><span class=\"line\">  MASTER_PORT=3306,</span><br><span class=\"line\">  MASTER_LOG_FILE=&apos;mysql-bin.000004&apos;,</span><br><span class=\"line\">  MASTER_LOG_POS=495,</span><br><span class=\"line\">  MASTER_CONNECT_RETRY=10;</span><br></pre></td></tr></table></figure></p>\n<p> 在主节点执行命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">UNLOCK TABLES;</span><br></pre></td></tr></table></figure></p>\n<p>在从节点执行命令启动slave<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">START SLAVE;</span><br></pre></td></tr></table></figure></p>\n<p>使用命令查询是否配置争取<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MariaDB [(none)]&gt; show slave status \\G</span><br><span class=\"line\">*************************** 1. row ***************************</span><br><span class=\"line\">               Slave_IO_State: Waiting for master to send event</span><br><span class=\"line\">                  Master_Host: cms1.test.com</span><br><span class=\"line\">                  Master_User: replication_user</span><br><span class=\"line\">                  Master_Port: 3306</span><br><span class=\"line\">                Connect_Retry: 10</span><br><span class=\"line\">              Master_Log_File: mysql-bin.000004</span><br><span class=\"line\">          Read_Master_Log_Pos: 495</span><br><span class=\"line\">               Relay_Log_File: slave1-relay-bin.000005</span><br><span class=\"line\">                Relay_Log_Pos: 529</span><br><span class=\"line\">        Relay_Master_Log_File: mysql-bin.000004</span><br><span class=\"line\">             Slave_IO_Running: Yes</span><br><span class=\"line\">            Slave_SQL_Running: Yes</span><br><span class=\"line\">              Replicate_Do_DB:</span><br><span class=\"line\">          Replicate_Ignore_DB:</span><br><span class=\"line\">           Replicate_Do_Table:</span><br><span class=\"line\">       Replicate_Ignore_Table:</span><br><span class=\"line\">      Replicate_Wild_Do_Table:</span><br><span class=\"line\">  Replicate_Wild_Ignore_Table:</span><br><span class=\"line\">                   Last_Errno: 0</span><br><span class=\"line\">                   Last_Error:</span><br><span class=\"line\">                 Skip_Counter: 0</span><br><span class=\"line\">          Exec_Master_Log_Pos: 495</span><br><span class=\"line\">              Relay_Log_Space: 824</span><br><span class=\"line\">              Until_Condition: None</span><br><span class=\"line\">               Until_Log_File:</span><br><span class=\"line\">                Until_Log_Pos: 0</span><br><span class=\"line\">           Master_SSL_Allowed: No</span><br><span class=\"line\">           Master_SSL_CA_File:</span><br><span class=\"line\">           Master_SSL_CA_Path:</span><br><span class=\"line\">              Master_SSL_Cert:</span><br><span class=\"line\">            Master_SSL_Cipher:</span><br><span class=\"line\">               Master_SSL_Key:</span><br><span class=\"line\">        Seconds_Behind_Master: 0</span><br><span class=\"line\">Master_SSL_Verify_Server_Cert: No</span><br><span class=\"line\">                Last_IO_Errno: 0</span><br><span class=\"line\">                Last_IO_Error:</span><br><span class=\"line\">               Last_SQL_Errno: 0</span><br><span class=\"line\">               Last_SQL_Error:</span><br><span class=\"line\">  Replicate_Ignore_Server_Ids:</span><br><span class=\"line\">             Master_Server_Id: 1</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>Slave_IO_Running 和 Slave_SQL_Running应该是yes</p>\n<p>在主节点上执行如下命令来创建数据库和用户<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database metastore default character set utf8;&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;CREATE USER &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;GRANT ALL PRIVILEGES ON metastore. * TO &apos;hive&apos;@&apos;%&apos;;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;amon&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database amon default character set utf8&apos; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on amon.* to &apos;amon&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;rman&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database rman default character set utf8&apos; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on rman.* to &apos;rman&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;sentry&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database sentry default character set utf8&apos; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on sentry.* to &apos;sentry&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;nav&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database nav default character set utf8&apos; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on nav.* to &apos;nav&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;navms&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database navms default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on navms.* to &apos;navms&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;cm&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database cm default character set utf8&apos; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on cm.* to &apos;cm&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;oozie&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database oozie default character set utf8&apos; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on oozie.* to &apos;oozie&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;hue&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database hue default character set utf8&apos; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on hue.* to &apos;hue&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;FLUSH PRIVILEGES;&quot;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-4-安装配置NFS-Server\"><a href=\"#2-4-安装配置NFS-Server\" class=\"headerlink\" title=\"2.4 安装配置NFS Server\"></a>2.4 安装配置NFS Server</h3><p>在nfs.test.com节点上安装NFS<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install nfs-utils nfs-utils-lib</span><br></pre></td></tr></table></figure></p>\n<p>启动nfs和rpcbind<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl enable nfs</span><br><span class=\"line\">systemctl start rpcbind</span><br><span class=\"line\">systemctl start nfs</span><br></pre></td></tr></table></figure></p>\n<p>创建nfs文件目录<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /media/cloudera-scm-server</span><br></pre></td></tr></table></figure></p>\n<p>在文件/etc/exports中添加如下的内容<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/media/cloudera-scm-server cms1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-server cms2(rw,sync,no_root_squash,no_subtree_check)</span><br></pre></td></tr></table></figure></p>\n<p>执行mounts<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">exportfs -a</span><br></pre></td></tr></table></figure></p>\n<p>在cms1和cms2上创建挂载点<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rm -rf /var/lib/cloudera-scm-server</span><br><span class=\"line\">mkdir -p /var/lib/cloudera-scm-server</span><br></pre></td></tr></table></figure></p>\n<p>安装nfs工具并执行挂载命令<br>其中nfs-utils是在centos7上有效<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install nfs-utils-lib </span><br><span class=\"line\">yum install nfs-utils</span><br><span class=\"line\">mount -t nfs nfs.test.com:/media/cloudera-scm-server /var/lib/cloudera-scm-server</span><br></pre></td></tr></table></figure></p>\n<p>重启rpcbind<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl restart  rpcbind</span><br></pre></td></tr></table></figure></p>\n<p>修改/etc/fstab文件，使用其永久有效<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nfs.test.com:/media/cloudera-scm-server /var/lib/cloudera-scm-server nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br></pre></td></tr></table></figure></p>\n<p>查看<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cms1:~&gt;df -h</span><br><span class=\"line\">文件系统                                 容量  已用  可用 已用% 挂载点</span><br><span class=\"line\">/dev/vda2                                 90G  6.2G   84G    7% /</span><br><span class=\"line\">devtmpfs                                 7.5G     0  7.5G    0% /dev</span><br><span class=\"line\">tmpfs                                    7.5G     0  7.5G    0% /dev/shm</span><br><span class=\"line\">tmpfs                                    7.5G  8.4M  7.5G    1% /run</span><br><span class=\"line\">tmpfs                                    7.5G     0  7.5G    0% /sys/fs/cgroup</span><br><span class=\"line\">tmpfs                                    1.5G     0  1.5G    0% /run/user/0</span><br><span class=\"line\">/dev/loop0                               4.1G  4.1G     0  100% /mnt</span><br><span class=\"line\">nfs.test.com:/media/cloudera-scm-server   90G  2.1G   88G    3% /var/lib/cloudera-scm-server</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cms2:~&gt;df -h</span><br><span class=\"line\">文件系统                                 容量  已用  可用 已用% 挂载点</span><br><span class=\"line\">/dev/vda2                                 90G  2.1G   88G    3% /</span><br><span class=\"line\">devtmpfs                                 7.5G     0  7.5G    0% /dev</span><br><span class=\"line\">tmpfs                                    7.5G     0  7.5G    0% /dev/shm</span><br><span class=\"line\">tmpfs                                    7.5G  8.4M  7.5G    1% /run</span><br><span class=\"line\">tmpfs                                    7.5G     0  7.5G    0% /sys/fs/cgroup</span><br><span class=\"line\">tmpfs                                    1.5G     0  1.5G    0% /run/user/0</span><br><span class=\"line\">nfs.test.com:/media/cloudera-scm-server   90G  2.1G   88G    3% /var/lib/cloudera-scm-server</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-Cloudera-Manager高可用的安装和配置\"><a href=\"#3-Cloudera-Manager高可用的安装和配置\" class=\"headerlink\" title=\"3. Cloudera Manager高可用的安装和配置\"></a>3. Cloudera Manager高可用的安装和配置</h2><h3 id=\"在主节点上安装（cms1）\"><a href=\"#在主节点上安装（cms1）\" class=\"headerlink\" title=\"在主节点上安装（cms1）\"></a>在主节点上安装（cms1）</h3><p>安装cloudera manager server 服务<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y cloudera-manager-server</span><br></pre></td></tr></table></figure></p>\n<p>配置cloudera manager server所使用的数据库<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/usr/share/cmf/schema/scm_prepare_database.sh mysql -h dn1.test.com cm cm 123456</span><br></pre></td></tr></table></figure></p>\n<p>启动cloudera manager server<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">service cloudera-scm-server start</span><br></pre></td></tr></table></figure></p>\n<p>验证：<br><a href=\"http://CMS1:7180\" target=\"_blank\" rel=\"noopener\">http://CMS1:7180</a></p>\n<p>在代理服务器上检查代理的状态<br><a href=\"http://cms.test.com:1080/stats\" target=\"_blank\" rel=\"noopener\">http://cms.test.com:1080/stats</a><br>并查看<a href=\"http://cms.test.com:7180/cmf/login\" target=\"_blank\" rel=\"noopener\">http://cms.test.com:7180/cmf/login</a> 是否可用（通过代理的ip访问）</p>\n<p>HTTP Referer 配置<br>Cloudera推荐禁用HTTP Referer检查，因为它可能会造成一些代理或者load balancer出错。通过如下步骤手动禁用。<br><img src=\"/img/1539920764009.png\" alt=\"Alt text\"></p>\n<h3 id=\"在副节点上安装\"><a href=\"#在副节点上安装\" class=\"headerlink\" title=\"在副节点上安装\"></a>在副节点上安装</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y cloudera-manager-server</span><br></pre></td></tr></table></figure>\n<p>复制数据库配置文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp root@cms1:/etc/cloudera-scm-server/db.properties  /etc/cloudera-scm-server/</span><br></pre></td></tr></table></figure></p>\n<p>关闭开机启动<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl disable cloudera-scm-server</span><br></pre></td></tr></table></figure></p>\n<p>如果配置自动故障转移，需要在主节点也禁用自动开机启动</p>\n<p>测试故障转移<br>停止cms1.test.com上的cloudera-scm-server<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl stop cloudera-scm-server</span><br></pre></td></tr></table></figure></p>\n<p><strong>等待1分钟左右</strong>在cms2.test.com上启动cloudera-scm-server<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl start cloudera-scm-server</span><br></pre></td></tr></table></figure></p>\n<p>访问<a href=\"http://cm.test.com:1080/stats\" target=\"_blank\" rel=\"noopener\">http://cm.test.com:1080/stats</a><br>和<a href=\"http://cm.test.com:7180/cmf/login\" target=\"_blank\" rel=\"noopener\">http://cm.test.com:7180/cmf/login</a> </p>\n<p>更新cloudera manager agens使其使用load balancer (<strong><em>除了cms1，cms2，mgmt1，mgmt2这几个节点</em></strong>)<br>更新配置文件/etc/cloudera-scm-agent/config.ini，修改server_host<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">server_host = cms.test.com</span><br></pre></td></tr></table></figure></p>\n<p>重启agent<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">service cloudera-scm-agent restart</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"4-Cloudera-Management-Service的高可用安装和配置\"><a href=\"#4-Cloudera-Management-Service的高可用安装和配置\" class=\"headerlink\" title=\"4. Cloudera Management Service的高可用安装和配置\"></a>4. Cloudera Management Service的高可用安装和配置</h2><h3 id=\"4-1-为Cloudera-Management-Service-设置NFS挂载点\"><a href=\"#4-1-为Cloudera-Management-Service-设置NFS挂载点\" class=\"headerlink\" title=\"4.1 为Cloudera Management Service 设置NFS挂载点\"></a>4.1 为Cloudera Management Service 设置NFS挂载点</h3><p>在NFS服务器 dn3.test.com上执行命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /media/cloudera-host-monitor</span><br><span class=\"line\">mkdir -p /media/cloudera-scm-agent</span><br><span class=\"line\">mkdir -p /media/cloudera-scm-eventserver</span><br><span class=\"line\">mkdir -p /media/cloudera-scm-headlamp</span><br><span class=\"line\">mkdir -p /media/cloudera-service-monitor</span><br><span class=\"line\">mkdir -p /media/cloudera-scm-navigator</span><br><span class=\"line\">mkdir -p /media/etc-cloudera-scm-agent</span><br></pre></td></tr></table></figure></p>\n<p>在NFS服务器的/etc/exports 文件中添加如下内容<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/media/cloudera-host-monitor mgmt1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-agent mgmt1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-eventserver mgmt1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-headlamp mgmt1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-service-monitor mgmt1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-navigator mgmt1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/etc-cloudera-scm-agent mgmt1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-host-monitor mgmt2(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-agent mgmt2(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-eventserver mgmt2(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-headlamp mgmt2(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-service-monitor mgmt2(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-navigator mgmt2(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/etc-cloudera-scm-agent mgmt2(rw,sync,no_root_squash,no_subtree_check)</span><br></pre></td></tr></table></figure></p>\n<p>在NFS上执行如下命令导出挂载点<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">exportfs -a</span><br></pre></td></tr></table></figure></p>\n<p>在MGMT1和MGMT2节点上配置，此处仍旧使用cms1.test.com 和cms2.test.com<br>如果是新的节点，则需要安装nfs-utils<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install nfs-utils</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-2-在MGMT1和MGMT2上创建挂载点\"><a href=\"#4-2-在MGMT1和MGMT2上创建挂载点\" class=\"headerlink\" title=\"4.2 在MGMT1和MGMT2上创建挂载点\"></a>4.2 在<code>MGMT1</code>和<code>MGMT2</code>上创建挂载点</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /var/lib/cloudera-host-monitor</span><br><span class=\"line\">mkdir -p /var/lib/cloudera-scm-agent</span><br><span class=\"line\">mkdir -p /var/lib/cloudera-scm-eventserver</span><br><span class=\"line\">mkdir -p /var/lib/cloudera-scm-headlamp</span><br><span class=\"line\">mkdir -p /var/lib/cloudera-service-monitor</span><br><span class=\"line\">mkdir -p /var/lib/cloudera-scm-navigator</span><br><span class=\"line\">mkdir -p /etc/cloudera-scm-agent</span><br></pre></td></tr></table></figure>\n<p>挂载<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mount -t nfs nfs.test.com:/media/cloudera-host-monitor /var/lib/cloudera-host-monitor</span><br><span class=\"line\">mount -t nfs nfs.test.com:/media/cloudera-scm-agent /var/lib/cloudera-scm-agent</span><br><span class=\"line\">mount -t nfs nfs.test.com:/media/cloudera-scm-eventserver /var/lib/cloudera-scm-eventserver</span><br><span class=\"line\">mount -t nfs nfs.test.com:/media/cloudera-scm-headlamp /var/lib/cloudera-scm-headlamp</span><br><span class=\"line\">mount -t nfs nfs.test.com:/media/cloudera-service-monitor /var/lib/cloudera-service-monitor</span><br><span class=\"line\">mount -t nfs nfs.test.com:/media/cloudera-scm-navigator /var/lib/cloudera-scm-navigator</span><br><span class=\"line\">mount -t nfs nfs.test.com:/media/etc-cloudera-scm-agent /etc/cloudera-scm-agent</span><br></pre></td></tr></table></figure></p>\n<p>设置fstab，添加如下内容在/etc/fstab文件中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nfs.test.com:/media/cloudera-host-monitor /var/lib/cloudera-host-monitor nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br><span class=\"line\">nfs.test.com:/media/cloudera-scm-agent /var/lib/cloudera-scm-agent nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br><span class=\"line\">nfs.test.com:/media/cloudera-scm-eventserver /var/lib/cloudera-scm-eventserver nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br><span class=\"line\">nfs.test.com:/media/cloudera-scm-headlamp /var/lib/cloudera-scm-headlamp nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br><span class=\"line\">nfs.test.com:/media/cloudera-service-monitor /var/lib/cloudera-service-monitor nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br><span class=\"line\">nfs.test.com:/media/cloudera-scm-navigator /var/lib/cloudera-scm-navigator nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br><span class=\"line\">nfs.test.com:/media/etc-cloudera-scm-agent /etc/cloudera-scm-agent nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-3-在主节点上安装-agent\"><a href=\"#4-3-在主节点上安装-agent\" class=\"headerlink\" title=\"4.3 在主节点上安装 agent\"></a>4.3 在主节点上安装 agent</h3><p>登陆MGMT1，安装cloudera-manager-daemons 和cloudera-manager-agent<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y cloudera-manager-daemons cloudera-manager-agent</span><br></pre></td></tr></table></figure></p>\n<p>配置agent的/etc/cloudera-scm-agent/config.ini<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">server_host = cms.test.com</span><br><span class=\"line\">listening_hostname = mgmt.test.com</span><br></pre></td></tr></table></figure></p>\n<p>编辑/etc/hosts 文件，添加如下内容<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">172.17.1.60 dn1.test.com</span><br></pre></td></tr></table></figure></p>\n<p>使用ping检查<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mgmt1:~&gt;ping dn1.test.com</span><br><span class=\"line\">PING dn1.test.com (172.17.1.60) 56(84) bytes of data.</span><br><span class=\"line\">64 bytes from mgmt1.test.com (172.17.1.60): icmp_seq=1 ttl=64 time=0.092 ms</span><br><span class=\"line\">64 bytes from mgmt1.test.com (172.17.1.60): icmp_seq=2 ttl=64 time=0.059 ms</span><br></pre></td></tr></table></figure></p>\n<p>修改文件夹的权限<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-eventserver</span><br><span class=\"line\">chown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-navigator</span><br><span class=\"line\">chown -R cloudera-scm:cloudera-scm /var/lib/cloudera-service-monitor</span><br><span class=\"line\">chown -R cloudera-scm:cloudera-scm /var/lib/cloudera-host-monitor</span><br><span class=\"line\">chown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-agent</span><br><span class=\"line\">chown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-headlamp</span><br></pre></td></tr></table></figure></p>\n<p>重启 agent<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">service cloudera-scm-agent restart</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/img/1539939416172.png\" alt=\"Alt text\"><br><img src=\"/img/1539939421998.png\" alt=\"Alt text\"></p>\n<h3 id=\"4-4-在副节点上安装agent\"><a href=\"#4-4-在副节点上安装agent\" class=\"headerlink\" title=\"4.4 在副节点上安装agent\"></a>4.4 在副节点上安装agent</h3><p>在cm上停掉所有的Cloudera Management Service服务</p>\n<p>停掉主节点的<code>cloudera-scm-agent</code> 服务<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mgmt1:~&gt;systemctl stop cloudera-scm-agent</span><br><span class=\"line\">mgmt1:~&gt;</span><br></pre></td></tr></table></figure></p>\n<p>在副节点上安装<code>cloudera-manager-agent</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y cloudera-manager-agent</span><br></pre></td></tr></table></figure></p>\n<p>在cm上启动所有的<code>cloudera management service</code></p>\n<p>Note: <em>确保cloudera-scm用户UID和GID在Cloudera Management Service的主副节点上是一致的。</em><br><img src=\"/img/1539939380054.png\" alt=\"Alt text\"><br>启动Cloudera Management Service<br><img src=\"/img/1539939371522.png\" alt=\"Alt text\"></p>\n<h2 id=\"5-问题：\"><a href=\"#5-问题：\" class=\"headerlink\" title=\"5. 问题：\"></a>5. 问题：</h2><p><img src=\"/img/1539939461605.png\" alt=\"Alt text\"><br>节点的SELINUX没有设施为disabled</p>\n<hr>\n","site":{"data":{}},"excerpt":"<h2 id=\"1-环境准备\"><a href=\"#1-环境准备\" class=\"headerlink\" title=\"1. 环境准备\"></a>1. 环境准备</h2><h3 id=\"1-1-环境\"><a href=\"#1-1-环境\" class=\"headerlink\" title=\"1.1 环境\"></a>1.1 环境</h3><p>操作系统：CentOS Linux release 7.3.1611 (Core)<br>JDK：jdk1.8.0_111<br>服务器：53-58 60-61 共8台</p>\n<h3 id=\"1-2-软件下载\"><a href=\"#1-2-软件下载\" class=\"headerlink\" title=\"1.2 软件下载\"></a>1.2 软件下载</h3><p>HAProxy: <a href=\"http://www.haproxy.org/download/1.8/src/haproxy-1.8.13.tar.gz\" target=\"_blank\" rel=\"noopener\">http://www.haproxy.org/download/1.8/src/haproxy-1.8.13.tar.gz</a></p>","more":"<h3 id=\"1-3-架构\"><a href=\"#1-3-架构\" class=\"headerlink\" title=\"1.3 架构\"></a>1.3 架构</h3><p><img src=\"https://www.cloudera.com/documentation/enterprise/5-4-x/images/cm_ha_lb_setup.png\" alt=\"enter image description here\"></p>\n<h3 id=\"1-4-修改主机名称\"><a href=\"#1-4-修改主机名称\" class=\"headerlink\" title=\"1.4 修改主机名称\"></a>1.4 修改主机名称</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class=\"line\">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class=\"line\">172.17.1.53 cms1.test.com cms1</span><br><span class=\"line\">172.17.1.54 cms2.test.com cms2</span><br><span class=\"line\">172.17.1.55 mgmt1.test.com mgmt1</span><br><span class=\"line\">172.17.1.56 mgmt2.test.com mgmt2</span><br><span class=\"line\">172.17.1.57 nfs.test.com nfs</span><br><span class=\"line\">172.17.1.58 cms.test.com cms</span><br><span class=\"line\">172.17.1.60 mgmt.test.com mgmt</span><br><span class=\"line\">172.17.1.61 dn1.test.com dn1</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-安装前环境准备\"><a href=\"#2-安装前环境准备\" class=\"headerlink\" title=\"2 安装前环境准备\"></a>2 安装前环境准备</h2><h3 id=\"2-1-创建主和副主机\"><a href=\"#2-1-创建主和副主机\" class=\"headerlink\" title=\"2.1 创建主和副主机\"></a>2.1 创建主和副主机</h3><p>Cloudera Manager Server and Cloudera Management Service Primary host cms1.test.com<br>Cloudera Manager Server and Cloudera Management Service Secondary host cms2.test.com</p>\n<p>此外，Cloudera建议：<br>    不要在安装CDH的节点安装Cloudera Manager或者Cloudera Management Service，因为这样会使failover的配置变的复杂。并且覆盖失败的域名可能会造成容错和错误检查的问题。<br>    对主副主机都使用相同的主机配置。用来保证故障转移后性能不会降低。<br>    对主副主机使用分开的主机和网络组件。</p>\n<p>主机分配</p>\n<table>\n<thead>\n<tr>\n<th><strong>IP</strong></th>\n<th><strong>DomainName</strong></th>\n<th><strong>功能</strong></th>\n<th><strong>角色</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>172.17.1.53</td>\n<td>cms1.test.com</td>\n<td>cloudera manager 主节点</td>\n<td>cms,cma</td>\n</tr>\n<tr>\n<td>172.17.1.54</td>\n<td>cms2.test.com</td>\n<td>cloudera manager 备份节点</td>\n<td>cms,cma</td>\n</tr>\n<tr>\n<td>172.17.1.55</td>\n<td>mgmt1.test.com</td>\n<td>cloudera management service 主节点</td>\n<td>cmmg,cma</td>\n</tr>\n<tr>\n<td>172.17.1.56</td>\n<td>mgmt2.test.com</td>\n<td>cloudera managerment service 备份节点</td>\n<td>cmmg,cma</td>\n</tr>\n<tr>\n<td>172.17.1.57</td>\n<td>nfs.test.com</td>\n<td>挂载存储服务器</td>\n<td>nfs</td>\n</tr>\n<tr>\n<td>172.17.1.58</td>\n<td>cms.test.com</td>\n<td>cm代理服务器</td>\n<td>haproxy</td>\n</tr>\n<tr>\n<td>172.17.1.60</td>\n<td>mgmt.test.com</td>\n<td>mgmt代理服务器</td>\n<td>haproxy</td>\n</tr>\n<tr>\n<td>172.17.1.61</td>\n<td>dn1.test.com</td>\n<td>数据节点</td>\n<td>cma,数据库</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"2-2-安装配置Load-Balancer\"><a href=\"#2-2-安装配置Load-Balancer\" class=\"headerlink\" title=\"2.2 安装配置Load Balancer\"></a>2.2 安装配置Load Balancer</h3><p>在<code>cms</code>节点上安装HAProxy<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y haproxy</span><br></pre></td></tr></table></figure></p>\n<p>配置haproxy开机启动<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl enable haproxy</span><br></pre></td></tr></table></figure></p>\n<p>配置HAProxy<br>在<code>cms</code>上，编辑<em>/etc/haproxy/haproxy.cfg</em>文件，添加需要代理的端口</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># Example configuration for a possible web application.  See the</span><br><span class=\"line\"># full configuration options online.</span><br><span class=\"line\">#</span><br><span class=\"line\">#   http://haproxy.1wt.eu/download/1.4/doc/configuration.txt</span><br><span class=\"line\">#</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># Global settings</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\">global</span><br><span class=\"line\">    # to have these messages end up in /var/log/haproxy.log you will</span><br><span class=\"line\">    # need to:</span><br><span class=\"line\">    #</span><br><span class=\"line\">    # 1) configure syslog to accept network log events.  This is done</span><br><span class=\"line\">    #    by adding the &apos;-r&apos; option to the SYSLOGD_OPTIONS in</span><br><span class=\"line\">    #    /etc/sysconfig/syslog</span><br><span class=\"line\">    #</span><br><span class=\"line\">    # 2) configure local2 events to go to the /var/log/haproxy.log</span><br><span class=\"line\">    #   file. A line like the following can be added to</span><br><span class=\"line\">    #   /etc/sysconfig/syslog</span><br><span class=\"line\">    #</span><br><span class=\"line\">    #    local2.*                       /var/log/haproxy.log</span><br><span class=\"line\">    #</span><br><span class=\"line\">    log         127.0.0.1 local2</span><br><span class=\"line\"></span><br><span class=\"line\">    chroot      /var/lib/haproxy</span><br><span class=\"line\">    pidfile     /var/run/haproxy.pid</span><br><span class=\"line\">    maxconn     4000</span><br><span class=\"line\">    user        haproxy</span><br><span class=\"line\">    group       haproxy</span><br><span class=\"line\">    daemon</span><br><span class=\"line\"></span><br><span class=\"line\">    # turn on stats unix socket</span><br><span class=\"line\">    stats socket /var/lib/haproxy/stats</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># common defaults that all the &apos;listen&apos; and &apos;backend&apos; sections will</span><br><span class=\"line\"># use if not designated in their block</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\">defaults</span><br><span class=\"line\">    mode                    http</span><br><span class=\"line\">    log                     global</span><br><span class=\"line\">    option                  httplog</span><br><span class=\"line\">    option                  dontlognull</span><br><span class=\"line\">    option http-server-close</span><br><span class=\"line\">    #option forwardfor       except 127.0.0.0/8</span><br><span class=\"line\">    option                  redispatch</span><br><span class=\"line\">    retries                 3</span><br><span class=\"line\">    timeout http-request    10s</span><br><span class=\"line\">    timeout queue           1m</span><br><span class=\"line\">    timeout connect         10s</span><br><span class=\"line\">    timeout client          1m</span><br><span class=\"line\">    timeout server          1m</span><br><span class=\"line\">    timeout http-keep-alive 10s</span><br><span class=\"line\">    timeout check           10s</span><br><span class=\"line\">    maxconn                 3000</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># main frontend which proxys to the backends</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\">#frontend  main *:5000</span><br><span class=\"line\">#    acl url_static       path_beg       -i /static /images /javascript /stylesheets</span><br><span class=\"line\">#    acl url_static       path_end       -i .jpg .gif .png .css .js</span><br><span class=\"line\">#</span><br><span class=\"line\">#    use_backend static          if url_static</span><br><span class=\"line\">#    default_backend             app</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># static backend for serving up images, stylesheets and such</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\">#backend static</span><br><span class=\"line\">#    balance     roundrobin</span><br><span class=\"line\">#    server      static 127.0.0.1:4331 check</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># round robin balancing between the various backends</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\">#backend app</span><br><span class=\"line\">#    balance     roundrobin</span><br><span class=\"line\">#    server  app1 127.0.0.1:5001 check</span><br><span class=\"line\">#    server  app2 127.0.0.1:5002 check</span><br><span class=\"line\">#    server  app3 127.0.0.1:5003 check</span><br><span class=\"line\">#    server  app4 127.0.0.1:5004 check</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># stats port</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\">listen stats</span><br><span class=\"line\">    bind 0.0.0.0:1080</span><br><span class=\"line\">    mode http</span><br><span class=\"line\">    option httplog</span><br><span class=\"line\">    maxconn 5000</span><br><span class=\"line\">    stats refresh 30s</span><br><span class=\"line\">    stats  uri /stats</span><br><span class=\"line\"></span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"># Cloudera Manager</span><br><span class=\"line\">#---------------------------------------------------------------------</span><br><span class=\"line\"></span><br><span class=\"line\">listen cmf</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    bind 0.0.0.0:7180</span><br><span class=\"line\">    server cmfhttp1 cms1.test.com:7180 check</span><br><span class=\"line\">    server cmfhttp2 cms2.test.com:7180 check</span><br><span class=\"line\"></span><br><span class=\"line\">listen cmfavro :7182</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server cmfavro1 cms1.test.com:7182 check</span><br><span class=\"line\">    server cmfavro2 cms2.test.com:7182 check</span><br><span class=\"line\"></span><br><span class=\"line\">#ssl pass-through, without termination</span><br><span class=\"line\">listen cmfhttps :7183</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server cmfhttps1 cms1.test.com:7183 check</span><br><span class=\"line\">    server cmfhttps2 cms2.test.com:7183 check</span><br><span class=\"line\"></span><br><span class=\"line\">listen mgmt1 :5678</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt1a cms1.test.com check</span><br><span class=\"line\">    server mgmt1b cms2.test.com check</span><br><span class=\"line\"></span><br><span class=\"line\">listen mgmt2 :7184</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt2a cms1.test.com check</span><br><span class=\"line\">    server mgmt2b cms2.test.com check</span><br><span class=\"line\"></span><br><span class=\"line\">listen mgmt3 :7185</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt3a cms1.test.com check</span><br><span class=\"line\">    server mgmt3b cms2.test.com check</span><br><span class=\"line\">listen mgmt4 :7186</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt4a cms1.test.com check</span><br><span class=\"line\">    server mgmt4b cms2.test.com check</span><br><span class=\"line\">listen mgmt5 :7187</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt5a cms1.test.com check</span><br><span class=\"line\">    server mgmt5b cms2.test.com check</span><br><span class=\"line\"></span><br><span class=\"line\">listen mgmt6 :8083</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt6a cms1.test.com check</span><br><span class=\"line\">    server mgmt6b cms2.test.com check</span><br><span class=\"line\">listen mgmt7 :8084</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt7a cms1.test.com check</span><br><span class=\"line\">    server mgmt7b cms2.test.com check</span><br><span class=\"line\">listen mgmt8 :8086</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt8a cms1.test.com check</span><br><span class=\"line\">    server mgmt8b cms2.test.com check</span><br><span class=\"line\">listen mgmt9 :8087</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt9a cms1.test.com check</span><br><span class=\"line\">    server mgmt9b cms2.test.com check</span><br><span class=\"line\">listen mgmt10 :8091</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt10a cms1.test.com check</span><br><span class=\"line\">    server mgmt10b cms2.test.com check</span><br><span class=\"line\">listen mgmt-agent :9000</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt-agenta cms1.test.com check</span><br><span class=\"line\">    server mgmt-agentb cms2.test.com check</span><br><span class=\"line\">listen mgmt11 :9994</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt11a cms1.test.com check</span><br><span class=\"line\">    server mgmt11b cms2.test.com check</span><br><span class=\"line\">listen mgmt12 :9995</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt12a cms1.test.com check</span><br><span class=\"line\">    server mgmt12b cms2.test.com check</span><br><span class=\"line\">listen mgmt13 :9996</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt13a cms1.test.com check</span><br><span class=\"line\">    server mgmt13b cms2.test.com check</span><br><span class=\"line\">listen mgmt14 :9997</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt14a cms1.test.com check</span><br><span class=\"line\">    server mgmt14b cms2.test.com check</span><br><span class=\"line\">listen mgmt15 :9998</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt15a cms1.test.com check</span><br><span class=\"line\">    server mgmt15b cms2.test.com check</span><br><span class=\"line\">listen mgmt16 :9999</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt16a cms1.test.com check</span><br><span class=\"line\">    server mgmt16b cms2.test.com check</span><br><span class=\"line\">listen mgmt17 :10101</span><br><span class=\"line\">    mode tcp</span><br><span class=\"line\">    option tcplog</span><br><span class=\"line\">    server mgmt17a cms1.test.com check</span><br><span class=\"line\">    server mgmt17b cms2.test.com check</span><br></pre></td></tr></table></figure>\n<p>重启haproxy</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl restart  haproxy</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-3-安装配置数据库\"><a href=\"#2-3-安装配置数据库\" class=\"headerlink\" title=\"2.3 安装配置数据库\"></a>2.3 安装配置数据库</h3><p>在dn1节点上安装数据库<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y mariadb mariadb-server</span><br></pre></td></tr></table></figure></p>\n<p>启动数据库<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl restart mariadb</span><br></pre></td></tr></table></figure></p>\n<p>初始化<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/usr/bin/mysql_secure_installation</span><br></pre></td></tr></table></figure></p>\n<p>配置mariadb的主从 <a href=\"https://mariadb.com/kb/en/library/setting-up-replication/\" target=\"_blank\" rel=\"noopener\">参考</a><br>在主节点上的/etc/my.cnf添加如下配置并重启<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[mariadb]</span><br><span class=\"line\">log-bin</span><br><span class=\"line\">server_id=1</span><br><span class=\"line\">log-basename=master1</span><br></pre></td></tr></table></figure></p>\n<p>创建replication用户<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE USER &apos;replication_user&apos;@&apos;%&apos; IDENTIFIED BY &apos;bigs3cret&apos;;</span><br><span class=\"line\">GRANT REPLICATION SLAVE ON *.* TO &apos;replication_user&apos;@&apos;%&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>在从节点上修改/etc/my.cnf文件,并重启<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[mariadb]</span><br><span class=\"line\">log-bin</span><br><span class=\"line\">server_id=2</span><br><span class=\"line\">log-basename=slave1</span><br></pre></td></tr></table></figure></p>\n<p>获取主节点的Binary Log<br>在主节点上执行命令锁住所有的表<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FLUSH TABLES WITH READ LOCK</span><br></pre></td></tr></table></figure></p>\n<p>执行命令获取主节点的状态，并记住Position的值<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MariaDB [(none)]&gt; SHOW MASTER STATUS;</span><br><span class=\"line\">+------------------+----------+--------------+------------------+</span><br><span class=\"line\">| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |</span><br><span class=\"line\">+------------------+----------+--------------+------------------+</span><br><span class=\"line\">| mysql-bin.000004 |      495 |              |                  |</span><br><span class=\"line\">+------------------+----------+--------------+------------------+</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">MariaDB [(none)]&gt;</span><br></pre></td></tr></table></figure></p>\n<p>在从节点执行如下命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CHANGE MASTER TO</span><br><span class=\"line\">  MASTER_HOST=&apos;cms1.test.com&apos;,</span><br><span class=\"line\">  MASTER_USER=&apos;replication_user&apos;,</span><br><span class=\"line\">  MASTER_PASSWORD=&apos;bigs3cret&apos;,</span><br><span class=\"line\">  MASTER_PORT=3306,</span><br><span class=\"line\">  MASTER_LOG_FILE=&apos;mysql-bin.000004&apos;,</span><br><span class=\"line\">  MASTER_LOG_POS=495,</span><br><span class=\"line\">  MASTER_CONNECT_RETRY=10;</span><br></pre></td></tr></table></figure></p>\n<p> 在主节点执行命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">UNLOCK TABLES;</span><br></pre></td></tr></table></figure></p>\n<p>在从节点执行命令启动slave<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">START SLAVE;</span><br></pre></td></tr></table></figure></p>\n<p>使用命令查询是否配置争取<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MariaDB [(none)]&gt; show slave status \\G</span><br><span class=\"line\">*************************** 1. row ***************************</span><br><span class=\"line\">               Slave_IO_State: Waiting for master to send event</span><br><span class=\"line\">                  Master_Host: cms1.test.com</span><br><span class=\"line\">                  Master_User: replication_user</span><br><span class=\"line\">                  Master_Port: 3306</span><br><span class=\"line\">                Connect_Retry: 10</span><br><span class=\"line\">              Master_Log_File: mysql-bin.000004</span><br><span class=\"line\">          Read_Master_Log_Pos: 495</span><br><span class=\"line\">               Relay_Log_File: slave1-relay-bin.000005</span><br><span class=\"line\">                Relay_Log_Pos: 529</span><br><span class=\"line\">        Relay_Master_Log_File: mysql-bin.000004</span><br><span class=\"line\">             Slave_IO_Running: Yes</span><br><span class=\"line\">            Slave_SQL_Running: Yes</span><br><span class=\"line\">              Replicate_Do_DB:</span><br><span class=\"line\">          Replicate_Ignore_DB:</span><br><span class=\"line\">           Replicate_Do_Table:</span><br><span class=\"line\">       Replicate_Ignore_Table:</span><br><span class=\"line\">      Replicate_Wild_Do_Table:</span><br><span class=\"line\">  Replicate_Wild_Ignore_Table:</span><br><span class=\"line\">                   Last_Errno: 0</span><br><span class=\"line\">                   Last_Error:</span><br><span class=\"line\">                 Skip_Counter: 0</span><br><span class=\"line\">          Exec_Master_Log_Pos: 495</span><br><span class=\"line\">              Relay_Log_Space: 824</span><br><span class=\"line\">              Until_Condition: None</span><br><span class=\"line\">               Until_Log_File:</span><br><span class=\"line\">                Until_Log_Pos: 0</span><br><span class=\"line\">           Master_SSL_Allowed: No</span><br><span class=\"line\">           Master_SSL_CA_File:</span><br><span class=\"line\">           Master_SSL_CA_Path:</span><br><span class=\"line\">              Master_SSL_Cert:</span><br><span class=\"line\">            Master_SSL_Cipher:</span><br><span class=\"line\">               Master_SSL_Key:</span><br><span class=\"line\">        Seconds_Behind_Master: 0</span><br><span class=\"line\">Master_SSL_Verify_Server_Cert: No</span><br><span class=\"line\">                Last_IO_Errno: 0</span><br><span class=\"line\">                Last_IO_Error:</span><br><span class=\"line\">               Last_SQL_Errno: 0</span><br><span class=\"line\">               Last_SQL_Error:</span><br><span class=\"line\">  Replicate_Ignore_Server_Ids:</span><br><span class=\"line\">             Master_Server_Id: 1</span><br><span class=\"line\">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure></p>\n<p>Slave_IO_Running 和 Slave_SQL_Running应该是yes</p>\n<p>在主节点上执行如下命令来创建数据库和用户<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database metastore default character set utf8;&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;CREATE USER &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;GRANT ALL PRIVILEGES ON metastore. * TO &apos;hive&apos;@&apos;%&apos;;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;amon&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database amon default character set utf8&apos; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on amon.* to &apos;amon&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;rman&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database rman default character set utf8&apos; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on rman.* to &apos;rman&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;sentry&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database sentry default character set utf8&apos; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on sentry.* to &apos;sentry&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;nav&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database nav default character set utf8&apos; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on nav.* to &apos;nav&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;navms&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database navms default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on navms.* to &apos;navms&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;cm&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database cm default character set utf8&apos; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on cm.* to &apos;cm&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;oozie&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database oozie default character set utf8&apos; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on oozie.* to &apos;oozie&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;hue&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database hue default character set utf8&apos; </span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on hue.* to &apos;hue&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;FLUSH PRIVILEGES;&quot;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-4-安装配置NFS-Server\"><a href=\"#2-4-安装配置NFS-Server\" class=\"headerlink\" title=\"2.4 安装配置NFS Server\"></a>2.4 安装配置NFS Server</h3><p>在nfs.test.com节点上安装NFS<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install nfs-utils nfs-utils-lib</span><br></pre></td></tr></table></figure></p>\n<p>启动nfs和rpcbind<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl enable nfs</span><br><span class=\"line\">systemctl start rpcbind</span><br><span class=\"line\">systemctl start nfs</span><br></pre></td></tr></table></figure></p>\n<p>创建nfs文件目录<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /media/cloudera-scm-server</span><br></pre></td></tr></table></figure></p>\n<p>在文件/etc/exports中添加如下的内容<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/media/cloudera-scm-server cms1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-server cms2(rw,sync,no_root_squash,no_subtree_check)</span><br></pre></td></tr></table></figure></p>\n<p>执行mounts<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">exportfs -a</span><br></pre></td></tr></table></figure></p>\n<p>在cms1和cms2上创建挂载点<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rm -rf /var/lib/cloudera-scm-server</span><br><span class=\"line\">mkdir -p /var/lib/cloudera-scm-server</span><br></pre></td></tr></table></figure></p>\n<p>安装nfs工具并执行挂载命令<br>其中nfs-utils是在centos7上有效<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install nfs-utils-lib </span><br><span class=\"line\">yum install nfs-utils</span><br><span class=\"line\">mount -t nfs nfs.test.com:/media/cloudera-scm-server /var/lib/cloudera-scm-server</span><br></pre></td></tr></table></figure></p>\n<p>重启rpcbind<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl restart  rpcbind</span><br></pre></td></tr></table></figure></p>\n<p>修改/etc/fstab文件，使用其永久有效<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nfs.test.com:/media/cloudera-scm-server /var/lib/cloudera-scm-server nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br></pre></td></tr></table></figure></p>\n<p>查看<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cms1:~&gt;df -h</span><br><span class=\"line\">文件系统                                 容量  已用  可用 已用% 挂载点</span><br><span class=\"line\">/dev/vda2                                 90G  6.2G   84G    7% /</span><br><span class=\"line\">devtmpfs                                 7.5G     0  7.5G    0% /dev</span><br><span class=\"line\">tmpfs                                    7.5G     0  7.5G    0% /dev/shm</span><br><span class=\"line\">tmpfs                                    7.5G  8.4M  7.5G    1% /run</span><br><span class=\"line\">tmpfs                                    7.5G     0  7.5G    0% /sys/fs/cgroup</span><br><span class=\"line\">tmpfs                                    1.5G     0  1.5G    0% /run/user/0</span><br><span class=\"line\">/dev/loop0                               4.1G  4.1G     0  100% /mnt</span><br><span class=\"line\">nfs.test.com:/media/cloudera-scm-server   90G  2.1G   88G    3% /var/lib/cloudera-scm-server</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cms2:~&gt;df -h</span><br><span class=\"line\">文件系统                                 容量  已用  可用 已用% 挂载点</span><br><span class=\"line\">/dev/vda2                                 90G  2.1G   88G    3% /</span><br><span class=\"line\">devtmpfs                                 7.5G     0  7.5G    0% /dev</span><br><span class=\"line\">tmpfs                                    7.5G     0  7.5G    0% /dev/shm</span><br><span class=\"line\">tmpfs                                    7.5G  8.4M  7.5G    1% /run</span><br><span class=\"line\">tmpfs                                    7.5G     0  7.5G    0% /sys/fs/cgroup</span><br><span class=\"line\">tmpfs                                    1.5G     0  1.5G    0% /run/user/0</span><br><span class=\"line\">nfs.test.com:/media/cloudera-scm-server   90G  2.1G   88G    3% /var/lib/cloudera-scm-server</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-Cloudera-Manager高可用的安装和配置\"><a href=\"#3-Cloudera-Manager高可用的安装和配置\" class=\"headerlink\" title=\"3. Cloudera Manager高可用的安装和配置\"></a>3. Cloudera Manager高可用的安装和配置</h2><h3 id=\"在主节点上安装（cms1）\"><a href=\"#在主节点上安装（cms1）\" class=\"headerlink\" title=\"在主节点上安装（cms1）\"></a>在主节点上安装（cms1）</h3><p>安装cloudera manager server 服务<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y cloudera-manager-server</span><br></pre></td></tr></table></figure></p>\n<p>配置cloudera manager server所使用的数据库<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/usr/share/cmf/schema/scm_prepare_database.sh mysql -h dn1.test.com cm cm 123456</span><br></pre></td></tr></table></figure></p>\n<p>启动cloudera manager server<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">service cloudera-scm-server start</span><br></pre></td></tr></table></figure></p>\n<p>验证：<br><a href=\"http://CMS1:7180\" target=\"_blank\" rel=\"noopener\">http://CMS1:7180</a></p>\n<p>在代理服务器上检查代理的状态<br><a href=\"http://cms.test.com:1080/stats\" target=\"_blank\" rel=\"noopener\">http://cms.test.com:1080/stats</a><br>并查看<a href=\"http://cms.test.com:7180/cmf/login\" target=\"_blank\" rel=\"noopener\">http://cms.test.com:7180/cmf/login</a> 是否可用（通过代理的ip访问）</p>\n<p>HTTP Referer 配置<br>Cloudera推荐禁用HTTP Referer检查，因为它可能会造成一些代理或者load balancer出错。通过如下步骤手动禁用。<br><img src=\"/img/1539920764009.png\" alt=\"Alt text\"></p>\n<h3 id=\"在副节点上安装\"><a href=\"#在副节点上安装\" class=\"headerlink\" title=\"在副节点上安装\"></a>在副节点上安装</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y cloudera-manager-server</span><br></pre></td></tr></table></figure>\n<p>复制数据库配置文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp root@cms1:/etc/cloudera-scm-server/db.properties  /etc/cloudera-scm-server/</span><br></pre></td></tr></table></figure></p>\n<p>关闭开机启动<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl disable cloudera-scm-server</span><br></pre></td></tr></table></figure></p>\n<p>如果配置自动故障转移，需要在主节点也禁用自动开机启动</p>\n<p>测试故障转移<br>停止cms1.test.com上的cloudera-scm-server<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl stop cloudera-scm-server</span><br></pre></td></tr></table></figure></p>\n<p><strong>等待1分钟左右</strong>在cms2.test.com上启动cloudera-scm-server<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl start cloudera-scm-server</span><br></pre></td></tr></table></figure></p>\n<p>访问<a href=\"http://cm.test.com:1080/stats\" target=\"_blank\" rel=\"noopener\">http://cm.test.com:1080/stats</a><br>和<a href=\"http://cm.test.com:7180/cmf/login\" target=\"_blank\" rel=\"noopener\">http://cm.test.com:7180/cmf/login</a> </p>\n<p>更新cloudera manager agens使其使用load balancer (<strong><em>除了cms1，cms2，mgmt1，mgmt2这几个节点</em></strong>)<br>更新配置文件/etc/cloudera-scm-agent/config.ini，修改server_host<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">server_host = cms.test.com</span><br></pre></td></tr></table></figure></p>\n<p>重启agent<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">service cloudera-scm-agent restart</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"4-Cloudera-Management-Service的高可用安装和配置\"><a href=\"#4-Cloudera-Management-Service的高可用安装和配置\" class=\"headerlink\" title=\"4. Cloudera Management Service的高可用安装和配置\"></a>4. Cloudera Management Service的高可用安装和配置</h2><h3 id=\"4-1-为Cloudera-Management-Service-设置NFS挂载点\"><a href=\"#4-1-为Cloudera-Management-Service-设置NFS挂载点\" class=\"headerlink\" title=\"4.1 为Cloudera Management Service 设置NFS挂载点\"></a>4.1 为Cloudera Management Service 设置NFS挂载点</h3><p>在NFS服务器 dn3.test.com上执行命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /media/cloudera-host-monitor</span><br><span class=\"line\">mkdir -p /media/cloudera-scm-agent</span><br><span class=\"line\">mkdir -p /media/cloudera-scm-eventserver</span><br><span class=\"line\">mkdir -p /media/cloudera-scm-headlamp</span><br><span class=\"line\">mkdir -p /media/cloudera-service-monitor</span><br><span class=\"line\">mkdir -p /media/cloudera-scm-navigator</span><br><span class=\"line\">mkdir -p /media/etc-cloudera-scm-agent</span><br></pre></td></tr></table></figure></p>\n<p>在NFS服务器的/etc/exports 文件中添加如下内容<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/media/cloudera-host-monitor mgmt1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-agent mgmt1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-eventserver mgmt1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-headlamp mgmt1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-service-monitor mgmt1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-navigator mgmt1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/etc-cloudera-scm-agent mgmt1(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-host-monitor mgmt2(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-agent mgmt2(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-eventserver mgmt2(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-headlamp mgmt2(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-service-monitor mgmt2(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/cloudera-scm-navigator mgmt2(rw,sync,no_root_squash,no_subtree_check)</span><br><span class=\"line\">/media/etc-cloudera-scm-agent mgmt2(rw,sync,no_root_squash,no_subtree_check)</span><br></pre></td></tr></table></figure></p>\n<p>在NFS上执行如下命令导出挂载点<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">exportfs -a</span><br></pre></td></tr></table></figure></p>\n<p>在MGMT1和MGMT2节点上配置，此处仍旧使用cms1.test.com 和cms2.test.com<br>如果是新的节点，则需要安装nfs-utils<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install nfs-utils</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-2-在MGMT1和MGMT2上创建挂载点\"><a href=\"#4-2-在MGMT1和MGMT2上创建挂载点\" class=\"headerlink\" title=\"4.2 在MGMT1和MGMT2上创建挂载点\"></a>4.2 在<code>MGMT1</code>和<code>MGMT2</code>上创建挂载点</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /var/lib/cloudera-host-monitor</span><br><span class=\"line\">mkdir -p /var/lib/cloudera-scm-agent</span><br><span class=\"line\">mkdir -p /var/lib/cloudera-scm-eventserver</span><br><span class=\"line\">mkdir -p /var/lib/cloudera-scm-headlamp</span><br><span class=\"line\">mkdir -p /var/lib/cloudera-service-monitor</span><br><span class=\"line\">mkdir -p /var/lib/cloudera-scm-navigator</span><br><span class=\"line\">mkdir -p /etc/cloudera-scm-agent</span><br></pre></td></tr></table></figure>\n<p>挂载<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mount -t nfs nfs.test.com:/media/cloudera-host-monitor /var/lib/cloudera-host-monitor</span><br><span class=\"line\">mount -t nfs nfs.test.com:/media/cloudera-scm-agent /var/lib/cloudera-scm-agent</span><br><span class=\"line\">mount -t nfs nfs.test.com:/media/cloudera-scm-eventserver /var/lib/cloudera-scm-eventserver</span><br><span class=\"line\">mount -t nfs nfs.test.com:/media/cloudera-scm-headlamp /var/lib/cloudera-scm-headlamp</span><br><span class=\"line\">mount -t nfs nfs.test.com:/media/cloudera-service-monitor /var/lib/cloudera-service-monitor</span><br><span class=\"line\">mount -t nfs nfs.test.com:/media/cloudera-scm-navigator /var/lib/cloudera-scm-navigator</span><br><span class=\"line\">mount -t nfs nfs.test.com:/media/etc-cloudera-scm-agent /etc/cloudera-scm-agent</span><br></pre></td></tr></table></figure></p>\n<p>设置fstab，添加如下内容在/etc/fstab文件中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nfs.test.com:/media/cloudera-host-monitor /var/lib/cloudera-host-monitor nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br><span class=\"line\">nfs.test.com:/media/cloudera-scm-agent /var/lib/cloudera-scm-agent nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br><span class=\"line\">nfs.test.com:/media/cloudera-scm-eventserver /var/lib/cloudera-scm-eventserver nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br><span class=\"line\">nfs.test.com:/media/cloudera-scm-headlamp /var/lib/cloudera-scm-headlamp nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br><span class=\"line\">nfs.test.com:/media/cloudera-service-monitor /var/lib/cloudera-service-monitor nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br><span class=\"line\">nfs.test.com:/media/cloudera-scm-navigator /var/lib/cloudera-scm-navigator nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br><span class=\"line\">nfs.test.com:/media/etc-cloudera-scm-agent /etc/cloudera-scm-agent nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-3-在主节点上安装-agent\"><a href=\"#4-3-在主节点上安装-agent\" class=\"headerlink\" title=\"4.3 在主节点上安装 agent\"></a>4.3 在主节点上安装 agent</h3><p>登陆MGMT1，安装cloudera-manager-daemons 和cloudera-manager-agent<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y cloudera-manager-daemons cloudera-manager-agent</span><br></pre></td></tr></table></figure></p>\n<p>配置agent的/etc/cloudera-scm-agent/config.ini<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">server_host = cms.test.com</span><br><span class=\"line\">listening_hostname = mgmt.test.com</span><br></pre></td></tr></table></figure></p>\n<p>编辑/etc/hosts 文件，添加如下内容<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">172.17.1.60 dn1.test.com</span><br></pre></td></tr></table></figure></p>\n<p>使用ping检查<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mgmt1:~&gt;ping dn1.test.com</span><br><span class=\"line\">PING dn1.test.com (172.17.1.60) 56(84) bytes of data.</span><br><span class=\"line\">64 bytes from mgmt1.test.com (172.17.1.60): icmp_seq=1 ttl=64 time=0.092 ms</span><br><span class=\"line\">64 bytes from mgmt1.test.com (172.17.1.60): icmp_seq=2 ttl=64 time=0.059 ms</span><br></pre></td></tr></table></figure></p>\n<p>修改文件夹的权限<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-eventserver</span><br><span class=\"line\">chown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-navigator</span><br><span class=\"line\">chown -R cloudera-scm:cloudera-scm /var/lib/cloudera-service-monitor</span><br><span class=\"line\">chown -R cloudera-scm:cloudera-scm /var/lib/cloudera-host-monitor</span><br><span class=\"line\">chown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-agent</span><br><span class=\"line\">chown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-headlamp</span><br></pre></td></tr></table></figure></p>\n<p>重启 agent<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">service cloudera-scm-agent restart</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"/img/1539939416172.png\" alt=\"Alt text\"><br><img src=\"/img/1539939421998.png\" alt=\"Alt text\"></p>\n<h3 id=\"4-4-在副节点上安装agent\"><a href=\"#4-4-在副节点上安装agent\" class=\"headerlink\" title=\"4.4 在副节点上安装agent\"></a>4.4 在副节点上安装agent</h3><p>在cm上停掉所有的Cloudera Management Service服务</p>\n<p>停掉主节点的<code>cloudera-scm-agent</code> 服务<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mgmt1:~&gt;systemctl stop cloudera-scm-agent</span><br><span class=\"line\">mgmt1:~&gt;</span><br></pre></td></tr></table></figure></p>\n<p>在副节点上安装<code>cloudera-manager-agent</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y cloudera-manager-agent</span><br></pre></td></tr></table></figure></p>\n<p>在cm上启动所有的<code>cloudera management service</code></p>\n<p>Note: <em>确保cloudera-scm用户UID和GID在Cloudera Management Service的主副节点上是一致的。</em><br><img src=\"/img/1539939380054.png\" alt=\"Alt text\"><br>启动Cloudera Management Service<br><img src=\"/img/1539939371522.png\" alt=\"Alt text\"></p>\n<h2 id=\"5-问题：\"><a href=\"#5-问题：\" class=\"headerlink\" title=\"5. 问题：\"></a>5. 问题：</h2><p><img src=\"/img/1539939461605.png\" alt=\"Alt text\"><br>节点的SELINUX没有设施为disabled</p>\n<hr>"},{"title":"Hive UDFs","date":"2018-05-13T03:50:26.000Z","_content":"# Hive UDFs\n\n## 创建自定义UDF\n\n首先需要一个类继承hive.udf类。\n\n```\npackage com.example.hive.udf;\n \nimport org.apache.hadoop.hive.ql.exec.UDF;\nimport org.apache.hadoop.io.Text;\n \npublic final class Lower extends UDF {\n  public Text evaluate(final Text s) {\n    if (s == null) { return null; }\n    return new Text(s.toString().toLowerCase());\n  }\n}\n```\n\n编译之后，将jar包放到hive的classpath路径下。\n\n```\n## 默认读取当前路径，hiveserver2上的路径。\nhive> add jar my_jar.jar;\nAdded my_jar.jar to class path\n\n## 也可以指定绝对路径\nhive> add jar /tmp/my_jar.jar;\nAdded /tmp/my_jar.jar to class path\n\n## 查看所有的jars\nhive> list jars;\nmy_jar.jar\n```\n创建临时函数并使用\n\n```\n## 创建函数\ncreate temporary function my_lower as 'com.example.hive.udf.Lower';\n\n## 使用 \n select my_lower(title), sum(freq) from titles group by my_lower(title);\n \n## 或者使用如下方法创建。\nCREATE FUNCTION myfunc AS 'myclass' USING JAR 'hdfs:///path/to/jar';\n```\n\n## Hive Auxiliary JARs Directory\n在CDH上可以将开发的jar放到 *HIVE\\_AUX\\_JARS_PATH* 配置的路径下（这个是本地路径，并非hdfs路径),然后重启，这样所有的用户都能够读取并使用这些jar包。\n\n\n## UDF 开发\n\n\n\n","source":"_posts/Hive UDFs.md","raw":"---\ntitle: Hive UDFs\ndate: 2018-05-13 11:50:26\ntags: \n  - Hive\n  - Udf\n---\n# Hive UDFs\n\n## 创建自定义UDF\n\n首先需要一个类继承hive.udf类。\n\n```\npackage com.example.hive.udf;\n \nimport org.apache.hadoop.hive.ql.exec.UDF;\nimport org.apache.hadoop.io.Text;\n \npublic final class Lower extends UDF {\n  public Text evaluate(final Text s) {\n    if (s == null) { return null; }\n    return new Text(s.toString().toLowerCase());\n  }\n}\n```\n\n编译之后，将jar包放到hive的classpath路径下。\n\n```\n## 默认读取当前路径，hiveserver2上的路径。\nhive> add jar my_jar.jar;\nAdded my_jar.jar to class path\n\n## 也可以指定绝对路径\nhive> add jar /tmp/my_jar.jar;\nAdded /tmp/my_jar.jar to class path\n\n## 查看所有的jars\nhive> list jars;\nmy_jar.jar\n```\n创建临时函数并使用\n\n```\n## 创建函数\ncreate temporary function my_lower as 'com.example.hive.udf.Lower';\n\n## 使用 \n select my_lower(title), sum(freq) from titles group by my_lower(title);\n \n## 或者使用如下方法创建。\nCREATE FUNCTION myfunc AS 'myclass' USING JAR 'hdfs:///path/to/jar';\n```\n\n## Hive Auxiliary JARs Directory\n在CDH上可以将开发的jar放到 *HIVE\\_AUX\\_JARS_PATH* 配置的路径下（这个是本地路径，并非hdfs路径),然后重启，这样所有的用户都能够读取并使用这些jar包。\n\n\n## UDF 开发\n\n\n\n","slug":"Hive UDFs","published":1,"updated":"2018-05-13T03:51:32.716Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzq000vinamnsnvs99z","content":"<h1 id=\"Hive-UDFs\"><a href=\"#Hive-UDFs\" class=\"headerlink\" title=\"Hive UDFs\"></a>Hive UDFs</h1><h2 id=\"创建自定义UDF\"><a href=\"#创建自定义UDF\" class=\"headerlink\" title=\"创建自定义UDF\"></a>创建自定义UDF</h2><p>首先需要一个类继承hive.udf类。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package com.example.hive.udf;</span><br><span class=\"line\"> </span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class=\"line\">import org.apache.hadoop.io.Text;</span><br><span class=\"line\"> </span><br><span class=\"line\">public final class Lower extends UDF &#123;</span><br><span class=\"line\">  public Text evaluate(final Text s) &#123;</span><br><span class=\"line\">    if (s == null) &#123; return null; &#125;</span><br><span class=\"line\">    return new Text(s.toString().toLowerCase());</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>编译之后，将jar包放到hive的classpath路径下。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">## 默认读取当前路径，hiveserver2上的路径。</span><br><span class=\"line\">hive&gt; add jar my_jar.jar;</span><br><span class=\"line\">Added my_jar.jar to class path</span><br><span class=\"line\"></span><br><span class=\"line\">## 也可以指定绝对路径</span><br><span class=\"line\">hive&gt; add jar /tmp/my_jar.jar;</span><br><span class=\"line\">Added /tmp/my_jar.jar to class path</span><br><span class=\"line\"></span><br><span class=\"line\">## 查看所有的jars</span><br><span class=\"line\">hive&gt; list jars;</span><br><span class=\"line\">my_jar.jar</span><br></pre></td></tr></table></figure>\n<p>创建临时函数并使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">## 创建函数</span><br><span class=\"line\">create temporary function my_lower as &apos;com.example.hive.udf.Lower&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">## 使用 </span><br><span class=\"line\"> select my_lower(title), sum(freq) from titles group by my_lower(title);</span><br><span class=\"line\"> </span><br><span class=\"line\">## 或者使用如下方法创建。</span><br><span class=\"line\">CREATE FUNCTION myfunc AS &apos;myclass&apos; USING JAR &apos;hdfs:///path/to/jar&apos;;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Hive-Auxiliary-JARs-Directory\"><a href=\"#Hive-Auxiliary-JARs-Directory\" class=\"headerlink\" title=\"Hive Auxiliary JARs Directory\"></a>Hive Auxiliary JARs Directory</h2><p>在CDH上可以将开发的jar放到 <em>HIVE_AUX_JARS_PATH</em> 配置的路径下（这个是本地路径，并非hdfs路径),然后重启，这样所有的用户都能够读取并使用这些jar包。</p>\n<h2 id=\"UDF-开发\"><a href=\"#UDF-开发\" class=\"headerlink\" title=\"UDF 开发\"></a>UDF 开发</h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Hive-UDFs\"><a href=\"#Hive-UDFs\" class=\"headerlink\" title=\"Hive UDFs\"></a>Hive UDFs</h1><h2 id=\"创建自定义UDF\"><a href=\"#创建自定义UDF\" class=\"headerlink\" title=\"创建自定义UDF\"></a>创建自定义UDF</h2><p>首先需要一个类继承hive.udf类。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package com.example.hive.udf;</span><br><span class=\"line\"> </span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class=\"line\">import org.apache.hadoop.io.Text;</span><br><span class=\"line\"> </span><br><span class=\"line\">public final class Lower extends UDF &#123;</span><br><span class=\"line\">  public Text evaluate(final Text s) &#123;</span><br><span class=\"line\">    if (s == null) &#123; return null; &#125;</span><br><span class=\"line\">    return new Text(s.toString().toLowerCase());</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>编译之后，将jar包放到hive的classpath路径下。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">## 默认读取当前路径，hiveserver2上的路径。</span><br><span class=\"line\">hive&gt; add jar my_jar.jar;</span><br><span class=\"line\">Added my_jar.jar to class path</span><br><span class=\"line\"></span><br><span class=\"line\">## 也可以指定绝对路径</span><br><span class=\"line\">hive&gt; add jar /tmp/my_jar.jar;</span><br><span class=\"line\">Added /tmp/my_jar.jar to class path</span><br><span class=\"line\"></span><br><span class=\"line\">## 查看所有的jars</span><br><span class=\"line\">hive&gt; list jars;</span><br><span class=\"line\">my_jar.jar</span><br></pre></td></tr></table></figure>\n<p>创建临时函数并使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">## 创建函数</span><br><span class=\"line\">create temporary function my_lower as &apos;com.example.hive.udf.Lower&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">## 使用 </span><br><span class=\"line\"> select my_lower(title), sum(freq) from titles group by my_lower(title);</span><br><span class=\"line\"> </span><br><span class=\"line\">## 或者使用如下方法创建。</span><br><span class=\"line\">CREATE FUNCTION myfunc AS &apos;myclass&apos; USING JAR &apos;hdfs:///path/to/jar&apos;;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Hive-Auxiliary-JARs-Directory\"><a href=\"#Hive-Auxiliary-JARs-Directory\" class=\"headerlink\" title=\"Hive Auxiliary JARs Directory\"></a>Hive Auxiliary JARs Directory</h2><p>在CDH上可以将开发的jar放到 <em>HIVE_AUX_JARS_PATH</em> 配置的路径下（这个是本地路径，并非hdfs路径),然后重启，这样所有的用户都能够读取并使用这些jar包。</p>\n<h2 id=\"UDF-开发\"><a href=\"#UDF-开发\" class=\"headerlink\" title=\"UDF 开发\"></a>UDF 开发</h2>"},{"title":"Hive架构相关","date":"2018-05-12T06:34:44.000Z","_content":"#  Hive架构相关\n\nhttps://cwiki.apache.org/confluence/display/Hive/Design\n\n\n![Alt text](https://cwiki.apache.org/confluence/download/attachments/27362072/system_architecture.png?version=1&modificationDate=1414560669000&api=v2)\n\n## Hive Architecture（架构）\n\n<!-- more -->\n\n-  UI – The user interface for users to submit queries and other operations to the system. As of 2011 the system had a command line interface and a web based GUI was being developed.\n-  UI – 用户向系统提交查询和其他操作的接口。2011年开始有了命令行接口和一个基于网页的图形化界面接口。\n- Driver – The component which receives the queries. This component implements the notion of session handles and provides execute and fetch APIs modeled on JDBC/ODBC interfaces.\n- Driver – 用来接收查询的组件。这个组件包含对session的控制并提供通过JDBC/ODBC的接口来执行和获取的API模型。\n- Compiler – The component that parses the query, does semantic analysis on the different query blocks and query expressions and eventually generates an execution plan with the help of the table and partition metadata looked up from the metastore.\n- Compiler – 对查询语句进行语法分析的组件，对不同的查询块和表达式进行语义上的分析，并根据从元数据存储的表和分区的信息生成最终生成执行计划。\n- Metastore – The component that stores all the structure information of the various tables and partitions in the warehouse including column and column type information, the serializers and deserializers necessary to read and write data and the corresponding HDFS files where the data is stored.\n- Metastore – 存储所有在仓库里的表和分区的结构化信息的组件，包括列和列的信息，序列化和反序列化的方式对数据进行读写，HDFS文件存储位置的映射关系。\n- Execution Engine – The component which executes the execution plan created by the compiler. The plan is a DAG of stages. The execution engine manages the dependencies between these different stages of the plan and executes these stages on the appropriate system components.\n- Execution Engine – 用来执行compiler生成的执行计划。执行计划是一个有向无环图。执行引擎管理执行计划间不同层级的依赖关系和这些层级对应的系统组件。\n\nFigure 1 also shows how a typical query flows through the system. The UI calls the execute interface to the Driver (step 1 in Figure 1). The Driver creates a session handle for the query and sends the query to the compiler to generate an execution plan (step 2). The compiler gets the necessary metadata from the metastore (steps 3 and 4). This metadata is used to typecheck the expressions in the query tree as well as to prune partitions based on query predicates. The plan generated by the compiler (step 5) is a DAG of stages with each stage being either a map/reduce job, a metadata operation or an operation on HDFS. For map/reduce stages, the plan contains map operator trees (operator trees that are executed on the mappers) and a reduce operator tree (for operations that need reducers). The execution engine submits these stages to appropriate components (steps 6, 6.1, 6.2 and 6.3). In each task (mapper/reducer) the deserializer associated with the table or intermediate outputs is used to read the rows from HDFS files and these are passed through the associated operator tree. Once the output is generated, it is written to a temporary HDFS file though the serializer (this happens in the mapper in case the operation does not need a reduce). The temporary files are used to provide data to subsequent map/reduce stages of the plan. For DML operations the final temporary file is moved to the table's location. This scheme is used to ensure that dirty data is not read (file rename being an atomic operation in HDFS). For queries, the contents of the temporary file are read by the execution engine directly from HDFS as part of the fetch call from the Driver (steps 7, 8 and 9).\n图1展示了一个查询在系统中的流向。UI调用Driver的执行接口（步骤1）。Driver为这个查询创建一个session对象并将查询语句发送给compiler用来生成执行计划（步骤2）。compiler从metastore获取需要的元数据信息（步骤3和4）。这些信息用来检查查询树中的表达式，并根据谓词下推修剪分区。compiler生成的是计划是一个层级的有向无环图，每一个层级都是一个用来操作HDFS或者元数据的map或者reduce任务。对于map/reduce层级，执行计划中包含map操作树（在mappers上执行的操作树）和reduce操作数据（需要reducers的操作）。执行引擎提交这些层级到对应的组件上（步骤6，6.1，6.2和6.3）。在每一个任务中（mapper/reducer）反序列化器将表或者从hdfs文件读取的行的输出和操作树关联起来。一定生成输出，通过序列化器将它写入到HDFS的一个临时文件（因为不需要reduce所以这部分都在mapper里面）。临时文件是为了后面的map/reduce提供数据。对于DML操作最终临时文件会被移动到表所在的位置。这个模式是为了确保垃圾数据不会被读取（在HDFS中会自动将文件重新命名）。对于查询，作为Driver调用的一部分临时文件的内容会被执行引擎直接从HDFS读取到（步骤7，8和9）。\n## Hive Data Model\nData in Hive is organized into:\n- Tables – These are analogous to Tables in Relational Databases. Tables can be filtered, projected, joined and unioned. Additionally all the data of a table is stored in a directory in HDFS. Hive also supports the notion of external tables wherein a table can be created on prexisting files or directories in HDFS by providing the appropriate location to the table creation DDL. The rows in a table are organized into typed columns similar to Relational Databases.\n- 表 – 和关系型数据库中的表类似。表可以被过滤，管理，关联和合并。此外所有的表中的数据都存储在HDFS的文件夹内。Hive支持外部表，表可以通过提供HDFS上已经存在的文件或者文件夹的路径来创建。表中行和列的管理都类似于关系型数据库。\n- Partitions – Each Table can have one or more partition keys which determine how the data is stored, for example a table T with a date partition column ds had files with data for a particular date stored in the <table location>/ds=<date> directory in HDFS. Partitions allow the system to prune data to be inspected based on query predicates, for example a query that is interested in rows from T that satisfy the predicate T.ds = '2008-09-01' would only have to look at files in <table location>/ds=2008-09-01/ directory in HDFS.\n- 分区 – 每个表可以有一个或者多个分区键，分区键用来确定数据的存储，例如，一个表T有一个时间分区列ds并且在对应的日期下都有数据存储在HDFS的<表位置>/ds=<日期>文件夹下。分区允许系统根据查询谓词进行修剪，例如从T表中查询T.ds='2008-09-01'的数据只会查询HDFS中<表位置>/ds=2008-09-01下的文件。\n- Buckets – Data in each partition may in turn be divided into Buckets based on the hash of a column in the table. Each bucket is stored as a file in the partition directory. Bucketing allows the system to efficiently evaluate queries that depend on a sample of data (these are queries that use the SAMPLE clause on the table).\n- 桶 – 在每个分区内的数据可以再根据表中一个列的哈希进行分桶。每个桶都是在分区文件夹中的一个文件。分桶允许系统根据样例数据（使用SAMPLE的查询）对查询进行有效的评估。\n\nApart from primitive column types (integers, floating point numbers, generic strings, dates and booleans), Hive also supports arrays and maps. Additionally, users can compose their own types programmatically from any of the primitives, collections or other user-defined types. The typing system is closely tied to the SerDe (Serailization/Deserialization) and object inspector interfaces. User can create their own types by implementing their own object inspectors, and using these object inspectors they can create their own SerDes to serialize and deserialize their data into HDFS files). These two interfaces provide the necessary hooks to extend the capabilities of Hive when it comes to understanding other data formats and richer types. Builtin object inspectors like ListObjectInspector, StructObjectInspector and MapObjectInspector provide the necessary primitives to compose richer types in an extensible manner. For maps (associative arrays) and arrays useful builtin functions like size and index operators are provided. The dotted notation is used to navigate nested types, for example a.b.c = 1 looks at field c of field b of type a and compares that with 1.\n\n除了最原始的列的数据类型（整形，浮点型，普通的字符串，日期和布尔 ），Hive还支持数组和映射类型。此外用户可以根据原始数据类型、集合或者其他用户自定义类型进行自定义类型。类型和SerDe（序列化和反序列化）和接口检查器紧密相连。用户可以通过继承对象接口来创建他们自己的数据类型，并可以使用这些类型创建他们自己的SerDes去序列化和反序列化他们存储在HDFS文件里的数据。这两个接口提供了在理解其他数据格式和较丰富类型时扩展Hive功能的必要钩子。构建对象检查器，如ListObjectInspector、StructObjectInspector和MapObjectInspector提供必要的原语，以可扩展的方式组成更丰富的类型。对maps（联合数组）和数组也提供内置的长度和指针操作。\n## Metastore\n\n**Motivation**\nThe Metastore provides two important but often overlooked features of a data warehouse: data abstraction and data . Without the data abstractions provided in Hive, a user has to provide information about data formats, extractors and loaders along with the query. In Hive, this information is given during table creation and reused every time the table is referenced. This is very similar to the traditional warehousing systems. The second functionality, data discovery, enables users to discover and explore relevant and specific data in the warehouse. Other tools can be built using this metadata to expose and possibly enhance the information about the data and its availability. Hive accomplishes both of these features by providing a metadata repository that is tightly integrated with the Hive query processing system so that data and metadata are in sync.\n**动机**\nMetastore 为数据仓库提供2个非常重要但是经常被忽略的组件：数据抽象和数据发现。如果没有Hive提供的数据抽象，用户不仅要提供查询语句还要提供数据类型、提取器和导入器等信息。在Hive中，每当表创建和重用的时候这些信息都会被提供使用。这点和传统的数据仓库非常类似。第二个功能是数据发现，使用户可以在仓库中发现和浏览相关和特殊的数据。其他工具可以编译使用这些元数据信息用来处理数据。Hive通过提供和Hive查询处理系统紧密相关的元数据仓库来实现这些功能，这样数据和元数据可以保持同步状态。\n**Metadata Objects**\nDatabase – is a namespace for tables. It can be used as an administrative unit in the future. The database 'default' is used for tables with no user-supplied database name.\nTable – Metadata for a table contains list of columns, owner, storage and SerDe information. It can also contain any user-supplied key and value data. Storage information includes location of the underlying data, file inout and output formats and bucketing information. SerDe metadata includes the implementation class of serializer and deserializer and any supporting information required by the implementation. All of this information can be provided during creation of the table.\nPartition – Each partition can have its own columns and SerDe and storage information. This facilitates schema changes without affecting older partitions.\n**元数据对象**\n数据库 – 表的命名空间。它将来可以被用做一个管理单元。默认的数据库'default'被用来存储没有被提供数据库名的表。\n表 – 表的元数据信息包括列的列表，所有者，存储和SerDe信息。也可以包含用户提供的Key和Value数据。包含数据的底层存储信息，文件输入和输出格式和分桶信息。SerDe元数据包含序列化和反序列化的接口和接口所需要的信息。这些信息在创建表的时候都可以提供。\n**Metastore Architecture**\nMetastore is an object store with a database or file backed store. The database backed store is implemented using an object-relational mapping (ORM) solution called the DataNucleus. The prime motivation for storing this in a relational database is queriability of metadata. Some disadvantages of using a separate data store for metadata instead of using HDFS are synchronization and scalability issues. Additionally there is no clear way to implement an object store on top of HDFS due to lack of random updates to files. This, coupled with the advantages of queriability of a relational store, made our approach a sensible one.\nThe metastore can be configured to be used in a couple of ways: remote and embedded. In remote mode, the metastore is a Thrift service. This mode is useful for non-Java clients. In embedded mode, the Hive client directly connects to an underlying metastore using JDBC. This mode is useful because it avoids another system that needs to be maintained and monitored. Both of these modes can co-exist. (Update: Local metastore is a third possibility. See Hive Metastore Administration for details.)\n**元数据存储架构**\n元数据存储是数据或者文件存储对象。数据库支持存储是用一个叫做DataNucleus的对象关系映射（ORM）作为方案的接口。存储在关系型数据库的首要动机就是使这些元数据可被查询。使用一个单独的存储而不使用HDFS作为存储的原因是数据同步和扩展问题。此外，由于缺少文件的随机更新机制，所以不能基于HDFS做对象存储。因此，结合关系型存储的查询优点使我们作出了明智的选择。\n元数据存储可以使用多种方式进行配置：远程和嵌入的。在远程模式下，元数据存储是一个Thrift服务。这种模式对于非Java客户端非常有用。在嵌入模式下，Hive客户端可以直接使用JDBC连接元数据存储。因为避免了需要管理和监控的其他系统所有它非常有用。所有的模式可以并存。\n**Metastore Interface**\nMetastore provides a Thrift interface to manipulate and query Hive metadata. Thrift provides bindings in many popular languages. Third party tools can use this interface to integrate Hive metadata into other business metadata repositories.\n**元数据存储接口**\n元数据存储提供一个Thrift接口用来操作和查询Hive的元数据。Thrift提供多种语言的封装。第三方的工具可以使用这个接口将Hive的元数据集成到其他业务元数据仓库中。\n\n## Hive Query Language\nHiveQL is an SQL-like query language for Hive. It mostly mimics SQL syntax for creation of tables, loading data into tables and querying the tables. HiveQL also allows users to embed their custom map-reduce scripts. These scripts can be written in any language using a simple row-based streaming interface – read rows from standard input and write out rows to standard output. This flexibility comes at a cost of a performance hit caused by converting rows from and to strings. However, we have seen that users do not mind this given that they can implement their scripts in the language of their choice. Another feature unique to HiveQL is multi-table insert. In this construct, users can perform multiple queries on the same input data using a single HiveQL query. Hive optimizes these queries to share the scan of the input data, thus increasing the throughput of these queries several orders of magnitude. We omit more details due to lack of space. For a more complete description of the HiveQL language see the [language manual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual).\n\nHiveQL是一个类SQL的查询语言。它有类SQL的创建表的语法，导入数据和查询表等。HiveQL也允许用户使用自定义的map-reduce代码。这些代码可以是用任何语言用一个简单的基于行的流接口-从标准输入读取和从标准输出写数据。这个兼容性源于将行和字符串之间做转换的性能花费。然而，用户能使用他们想要使用的语言他们就不在乎这点。一个只在HiveQL中有的特性是多表插入。在这种架构中，用户可以使用一个HiveQL查询同时操作多个查询语句在同一个数据输入上。Hive优化了对输入数据的扫描，因此增加了查询的吞吐。\n## Compiler\n- Parser – Transform a query string to a parse tree representation.\n- Semantic Analyser – Transform the parse tree to an internal query representation, which is still block based and not an operator tree. As part of this step, the column names are verified and expansions like * are performed. Type-checking and any implicit type conversions are also performed at this stage. If the table under consideration is a partitioned table, which is the common scenario, all the expressions for that table are collected so that they can be later used to prune the partitions which are not needed. If the query has specified sampling, that is also collected to be used later on.\n- Logical Plan Generator – Convert the internal query representation to a logical plan, which consists of a tree of operators. Some of the operators are relational algebra operators like 'filter', 'join' etc. But some of the operators are Hive specific and are used later on to convert this plan into a series of map-reduce jobs. One such operator is a reduceSink operator which occurs at the map-reduce boundary. This step also includes the optimizer to transform the plan to improve performance – some of those transformations include: converting a series of joins into a single multi-way join, performing a map-side partial aggregation for a group-by, performing a group-by in 2 stages to avoid the scenario when a single reducer can become a bottleneck in presence of skewed data for the grouping key. Each operator comprises a descriptor which is a serializable object.\n- Query Plan Generator – Convert the logical plan to a series of map-reduce tasks. The operator tree is recursively traversed, to be broken up into a series of map-reduce serializable tasks which can be submitted later on to the map-reduce framework for the Hadoop distributed file system. The reduceSink operator is the map-reduce boundary, whose descriptor contains the reduction keys. The reduction keys in the reduceSink descriptor are used as the reduction keys in the map-reduce boundary. The plan consists of the required samples/partitions if the query specified so. The plan is serialized and written to a file.\n\n- Parser – 语法分析程序将一个查询串转换成一个语法树。\n- Semantic Analyser – 语义分析，将语法树转换成一个基于块的而非树的内部查询。在这个步骤中，会对列名进行验证并将像*这样的操作展开。同时也会做类型检查和隐性的类型转换。如果要处理的是一个分区表，会收集表的所在后面的修剪不使用的分区的时候会用到的信息。\n- Logical Plan Generator – 逻辑计划生成器，将内部查询转换成一个操作树的逻辑计划。有些操作是代数关系操作像'filter'，'join'等。但是有些是Hive特有的操作，并用做将这个计划转成成一系列的map-reduce任务。其中就有在map-reduce边界发生的reduceSink操作。在这步中也包括优化器将计划进行优化的转换-这些转换包括将一系列的join转换成一个多表jion，将分区汇总放在map端，将分区放在2个阶段执行，以避免在一个单一的reducer发生由于数据倾斜造成的性能瓶颈。每个操作符包含一个可序列化对象的描述符。\n- Query Plan Generator – 查询计划生成器 将逻辑计划转换成一系列的map-reduce任务。操作树通过递归遍历将被分解成一系列的用来提交到Hadoop分布式文件系统的mapreduce可序列化的任务。reduceSink操作是map-reduce的边界，它包含Reduce使用到的key。在reduceSink描述符中用的的key也会在map-reduce边界中使用。如果查询指定了sample或者分区，计划只会用他们组成。查询计划会被序列化并写入文件。\n## Optimizer\nMore plan transformations are performed by the optimizer. The optimizer is an evolving component. As of 2011, it was rule-based and performed the following: column pruning and predicate pushdown. However, the infrastructure was in place, and there was work under progress to include other optimizations like map-side join. (Hive 0.11 added several [join optimizations](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization).)\n\nThe optimizer can be enhanced to be cost-based (see Cost-based optimization in Hive and HIVE-5775). The sorted nature of output tables can also be preserved and used later on to generate better plans. The query can be performed on a small sample of data to guess the data distribution, which can be used to generate a better plan.\n\nA [correlation optimizer](https://cwiki.apache.org/confluence/display/Hive/Correlation+Optimizer) was added in Hive 0.12.\n\nThe plan is a generic operator tree, and can be easily manipulated.\n\n优化器会做更多的查询计划转换操作。优化器是一个持续更新的组件。从2011年开始，它的基本规则是：列的修剪和谓词下推。然而，基础设施已经改变，现在正在进行的包括其他优化，如map-side join。\n\n优化器可以加强成为基于花费的。输出表的存储可以被后面生成更好的机会使用。可以通过小的抽样数据猜测数据分布，从而生成更好的计划。\n\n\n## Hive APIs\n[Hive APIs Overview](https://cwiki.apache.org/confluence/display/Hive/Hive+APIs+Overview) describes various public-facing APIs that Hive provides.\n","source":"_posts/Hive架构相关.md","raw":"---\ntitle: Hive架构相关\ndate: 2018-05-12 14:34:44\ntags: \n    - Hive\n    - metastore\n---\n#  Hive架构相关\n\nhttps://cwiki.apache.org/confluence/display/Hive/Design\n\n\n![Alt text](https://cwiki.apache.org/confluence/download/attachments/27362072/system_architecture.png?version=1&modificationDate=1414560669000&api=v2)\n\n## Hive Architecture（架构）\n\n<!-- more -->\n\n-  UI – The user interface for users to submit queries and other operations to the system. As of 2011 the system had a command line interface and a web based GUI was being developed.\n-  UI – 用户向系统提交查询和其他操作的接口。2011年开始有了命令行接口和一个基于网页的图形化界面接口。\n- Driver – The component which receives the queries. This component implements the notion of session handles and provides execute and fetch APIs modeled on JDBC/ODBC interfaces.\n- Driver – 用来接收查询的组件。这个组件包含对session的控制并提供通过JDBC/ODBC的接口来执行和获取的API模型。\n- Compiler – The component that parses the query, does semantic analysis on the different query blocks and query expressions and eventually generates an execution plan with the help of the table and partition metadata looked up from the metastore.\n- Compiler – 对查询语句进行语法分析的组件，对不同的查询块和表达式进行语义上的分析，并根据从元数据存储的表和分区的信息生成最终生成执行计划。\n- Metastore – The component that stores all the structure information of the various tables and partitions in the warehouse including column and column type information, the serializers and deserializers necessary to read and write data and the corresponding HDFS files where the data is stored.\n- Metastore – 存储所有在仓库里的表和分区的结构化信息的组件，包括列和列的信息，序列化和反序列化的方式对数据进行读写，HDFS文件存储位置的映射关系。\n- Execution Engine – The component which executes the execution plan created by the compiler. The plan is a DAG of stages. The execution engine manages the dependencies between these different stages of the plan and executes these stages on the appropriate system components.\n- Execution Engine – 用来执行compiler生成的执行计划。执行计划是一个有向无环图。执行引擎管理执行计划间不同层级的依赖关系和这些层级对应的系统组件。\n\nFigure 1 also shows how a typical query flows through the system. The UI calls the execute interface to the Driver (step 1 in Figure 1). The Driver creates a session handle for the query and sends the query to the compiler to generate an execution plan (step 2). The compiler gets the necessary metadata from the metastore (steps 3 and 4). This metadata is used to typecheck the expressions in the query tree as well as to prune partitions based on query predicates. The plan generated by the compiler (step 5) is a DAG of stages with each stage being either a map/reduce job, a metadata operation or an operation on HDFS. For map/reduce stages, the plan contains map operator trees (operator trees that are executed on the mappers) and a reduce operator tree (for operations that need reducers). The execution engine submits these stages to appropriate components (steps 6, 6.1, 6.2 and 6.3). In each task (mapper/reducer) the deserializer associated with the table or intermediate outputs is used to read the rows from HDFS files and these are passed through the associated operator tree. Once the output is generated, it is written to a temporary HDFS file though the serializer (this happens in the mapper in case the operation does not need a reduce). The temporary files are used to provide data to subsequent map/reduce stages of the plan. For DML operations the final temporary file is moved to the table's location. This scheme is used to ensure that dirty data is not read (file rename being an atomic operation in HDFS). For queries, the contents of the temporary file are read by the execution engine directly from HDFS as part of the fetch call from the Driver (steps 7, 8 and 9).\n图1展示了一个查询在系统中的流向。UI调用Driver的执行接口（步骤1）。Driver为这个查询创建一个session对象并将查询语句发送给compiler用来生成执行计划（步骤2）。compiler从metastore获取需要的元数据信息（步骤3和4）。这些信息用来检查查询树中的表达式，并根据谓词下推修剪分区。compiler生成的是计划是一个层级的有向无环图，每一个层级都是一个用来操作HDFS或者元数据的map或者reduce任务。对于map/reduce层级，执行计划中包含map操作树（在mappers上执行的操作树）和reduce操作数据（需要reducers的操作）。执行引擎提交这些层级到对应的组件上（步骤6，6.1，6.2和6.3）。在每一个任务中（mapper/reducer）反序列化器将表或者从hdfs文件读取的行的输出和操作树关联起来。一定生成输出，通过序列化器将它写入到HDFS的一个临时文件（因为不需要reduce所以这部分都在mapper里面）。临时文件是为了后面的map/reduce提供数据。对于DML操作最终临时文件会被移动到表所在的位置。这个模式是为了确保垃圾数据不会被读取（在HDFS中会自动将文件重新命名）。对于查询，作为Driver调用的一部分临时文件的内容会被执行引擎直接从HDFS读取到（步骤7，8和9）。\n## Hive Data Model\nData in Hive is organized into:\n- Tables – These are analogous to Tables in Relational Databases. Tables can be filtered, projected, joined and unioned. Additionally all the data of a table is stored in a directory in HDFS. Hive also supports the notion of external tables wherein a table can be created on prexisting files or directories in HDFS by providing the appropriate location to the table creation DDL. The rows in a table are organized into typed columns similar to Relational Databases.\n- 表 – 和关系型数据库中的表类似。表可以被过滤，管理，关联和合并。此外所有的表中的数据都存储在HDFS的文件夹内。Hive支持外部表，表可以通过提供HDFS上已经存在的文件或者文件夹的路径来创建。表中行和列的管理都类似于关系型数据库。\n- Partitions – Each Table can have one or more partition keys which determine how the data is stored, for example a table T with a date partition column ds had files with data for a particular date stored in the <table location>/ds=<date> directory in HDFS. Partitions allow the system to prune data to be inspected based on query predicates, for example a query that is interested in rows from T that satisfy the predicate T.ds = '2008-09-01' would only have to look at files in <table location>/ds=2008-09-01/ directory in HDFS.\n- 分区 – 每个表可以有一个或者多个分区键，分区键用来确定数据的存储，例如，一个表T有一个时间分区列ds并且在对应的日期下都有数据存储在HDFS的<表位置>/ds=<日期>文件夹下。分区允许系统根据查询谓词进行修剪，例如从T表中查询T.ds='2008-09-01'的数据只会查询HDFS中<表位置>/ds=2008-09-01下的文件。\n- Buckets – Data in each partition may in turn be divided into Buckets based on the hash of a column in the table. Each bucket is stored as a file in the partition directory. Bucketing allows the system to efficiently evaluate queries that depend on a sample of data (these are queries that use the SAMPLE clause on the table).\n- 桶 – 在每个分区内的数据可以再根据表中一个列的哈希进行分桶。每个桶都是在分区文件夹中的一个文件。分桶允许系统根据样例数据（使用SAMPLE的查询）对查询进行有效的评估。\n\nApart from primitive column types (integers, floating point numbers, generic strings, dates and booleans), Hive also supports arrays and maps. Additionally, users can compose their own types programmatically from any of the primitives, collections or other user-defined types. The typing system is closely tied to the SerDe (Serailization/Deserialization) and object inspector interfaces. User can create their own types by implementing their own object inspectors, and using these object inspectors they can create their own SerDes to serialize and deserialize their data into HDFS files). These two interfaces provide the necessary hooks to extend the capabilities of Hive when it comes to understanding other data formats and richer types. Builtin object inspectors like ListObjectInspector, StructObjectInspector and MapObjectInspector provide the necessary primitives to compose richer types in an extensible manner. For maps (associative arrays) and arrays useful builtin functions like size and index operators are provided. The dotted notation is used to navigate nested types, for example a.b.c = 1 looks at field c of field b of type a and compares that with 1.\n\n除了最原始的列的数据类型（整形，浮点型，普通的字符串，日期和布尔 ），Hive还支持数组和映射类型。此外用户可以根据原始数据类型、集合或者其他用户自定义类型进行自定义类型。类型和SerDe（序列化和反序列化）和接口检查器紧密相连。用户可以通过继承对象接口来创建他们自己的数据类型，并可以使用这些类型创建他们自己的SerDes去序列化和反序列化他们存储在HDFS文件里的数据。这两个接口提供了在理解其他数据格式和较丰富类型时扩展Hive功能的必要钩子。构建对象检查器，如ListObjectInspector、StructObjectInspector和MapObjectInspector提供必要的原语，以可扩展的方式组成更丰富的类型。对maps（联合数组）和数组也提供内置的长度和指针操作。\n## Metastore\n\n**Motivation**\nThe Metastore provides two important but often overlooked features of a data warehouse: data abstraction and data . Without the data abstractions provided in Hive, a user has to provide information about data formats, extractors and loaders along with the query. In Hive, this information is given during table creation and reused every time the table is referenced. This is very similar to the traditional warehousing systems. The second functionality, data discovery, enables users to discover and explore relevant and specific data in the warehouse. Other tools can be built using this metadata to expose and possibly enhance the information about the data and its availability. Hive accomplishes both of these features by providing a metadata repository that is tightly integrated with the Hive query processing system so that data and metadata are in sync.\n**动机**\nMetastore 为数据仓库提供2个非常重要但是经常被忽略的组件：数据抽象和数据发现。如果没有Hive提供的数据抽象，用户不仅要提供查询语句还要提供数据类型、提取器和导入器等信息。在Hive中，每当表创建和重用的时候这些信息都会被提供使用。这点和传统的数据仓库非常类似。第二个功能是数据发现，使用户可以在仓库中发现和浏览相关和特殊的数据。其他工具可以编译使用这些元数据信息用来处理数据。Hive通过提供和Hive查询处理系统紧密相关的元数据仓库来实现这些功能，这样数据和元数据可以保持同步状态。\n**Metadata Objects**\nDatabase – is a namespace for tables. It can be used as an administrative unit in the future. The database 'default' is used for tables with no user-supplied database name.\nTable – Metadata for a table contains list of columns, owner, storage and SerDe information. It can also contain any user-supplied key and value data. Storage information includes location of the underlying data, file inout and output formats and bucketing information. SerDe metadata includes the implementation class of serializer and deserializer and any supporting information required by the implementation. All of this information can be provided during creation of the table.\nPartition – Each partition can have its own columns and SerDe and storage information. This facilitates schema changes without affecting older partitions.\n**元数据对象**\n数据库 – 表的命名空间。它将来可以被用做一个管理单元。默认的数据库'default'被用来存储没有被提供数据库名的表。\n表 – 表的元数据信息包括列的列表，所有者，存储和SerDe信息。也可以包含用户提供的Key和Value数据。包含数据的底层存储信息，文件输入和输出格式和分桶信息。SerDe元数据包含序列化和反序列化的接口和接口所需要的信息。这些信息在创建表的时候都可以提供。\n**Metastore Architecture**\nMetastore is an object store with a database or file backed store. The database backed store is implemented using an object-relational mapping (ORM) solution called the DataNucleus. The prime motivation for storing this in a relational database is queriability of metadata. Some disadvantages of using a separate data store for metadata instead of using HDFS are synchronization and scalability issues. Additionally there is no clear way to implement an object store on top of HDFS due to lack of random updates to files. This, coupled with the advantages of queriability of a relational store, made our approach a sensible one.\nThe metastore can be configured to be used in a couple of ways: remote and embedded. In remote mode, the metastore is a Thrift service. This mode is useful for non-Java clients. In embedded mode, the Hive client directly connects to an underlying metastore using JDBC. This mode is useful because it avoids another system that needs to be maintained and monitored. Both of these modes can co-exist. (Update: Local metastore is a third possibility. See Hive Metastore Administration for details.)\n**元数据存储架构**\n元数据存储是数据或者文件存储对象。数据库支持存储是用一个叫做DataNucleus的对象关系映射（ORM）作为方案的接口。存储在关系型数据库的首要动机就是使这些元数据可被查询。使用一个单独的存储而不使用HDFS作为存储的原因是数据同步和扩展问题。此外，由于缺少文件的随机更新机制，所以不能基于HDFS做对象存储。因此，结合关系型存储的查询优点使我们作出了明智的选择。\n元数据存储可以使用多种方式进行配置：远程和嵌入的。在远程模式下，元数据存储是一个Thrift服务。这种模式对于非Java客户端非常有用。在嵌入模式下，Hive客户端可以直接使用JDBC连接元数据存储。因为避免了需要管理和监控的其他系统所有它非常有用。所有的模式可以并存。\n**Metastore Interface**\nMetastore provides a Thrift interface to manipulate and query Hive metadata. Thrift provides bindings in many popular languages. Third party tools can use this interface to integrate Hive metadata into other business metadata repositories.\n**元数据存储接口**\n元数据存储提供一个Thrift接口用来操作和查询Hive的元数据。Thrift提供多种语言的封装。第三方的工具可以使用这个接口将Hive的元数据集成到其他业务元数据仓库中。\n\n## Hive Query Language\nHiveQL is an SQL-like query language for Hive. It mostly mimics SQL syntax for creation of tables, loading data into tables and querying the tables. HiveQL also allows users to embed their custom map-reduce scripts. These scripts can be written in any language using a simple row-based streaming interface – read rows from standard input and write out rows to standard output. This flexibility comes at a cost of a performance hit caused by converting rows from and to strings. However, we have seen that users do not mind this given that they can implement their scripts in the language of their choice. Another feature unique to HiveQL is multi-table insert. In this construct, users can perform multiple queries on the same input data using a single HiveQL query. Hive optimizes these queries to share the scan of the input data, thus increasing the throughput of these queries several orders of magnitude. We omit more details due to lack of space. For a more complete description of the HiveQL language see the [language manual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual).\n\nHiveQL是一个类SQL的查询语言。它有类SQL的创建表的语法，导入数据和查询表等。HiveQL也允许用户使用自定义的map-reduce代码。这些代码可以是用任何语言用一个简单的基于行的流接口-从标准输入读取和从标准输出写数据。这个兼容性源于将行和字符串之间做转换的性能花费。然而，用户能使用他们想要使用的语言他们就不在乎这点。一个只在HiveQL中有的特性是多表插入。在这种架构中，用户可以使用一个HiveQL查询同时操作多个查询语句在同一个数据输入上。Hive优化了对输入数据的扫描，因此增加了查询的吞吐。\n## Compiler\n- Parser – Transform a query string to a parse tree representation.\n- Semantic Analyser – Transform the parse tree to an internal query representation, which is still block based and not an operator tree. As part of this step, the column names are verified and expansions like * are performed. Type-checking and any implicit type conversions are also performed at this stage. If the table under consideration is a partitioned table, which is the common scenario, all the expressions for that table are collected so that they can be later used to prune the partitions which are not needed. If the query has specified sampling, that is also collected to be used later on.\n- Logical Plan Generator – Convert the internal query representation to a logical plan, which consists of a tree of operators. Some of the operators are relational algebra operators like 'filter', 'join' etc. But some of the operators are Hive specific and are used later on to convert this plan into a series of map-reduce jobs. One such operator is a reduceSink operator which occurs at the map-reduce boundary. This step also includes the optimizer to transform the plan to improve performance – some of those transformations include: converting a series of joins into a single multi-way join, performing a map-side partial aggregation for a group-by, performing a group-by in 2 stages to avoid the scenario when a single reducer can become a bottleneck in presence of skewed data for the grouping key. Each operator comprises a descriptor which is a serializable object.\n- Query Plan Generator – Convert the logical plan to a series of map-reduce tasks. The operator tree is recursively traversed, to be broken up into a series of map-reduce serializable tasks which can be submitted later on to the map-reduce framework for the Hadoop distributed file system. The reduceSink operator is the map-reduce boundary, whose descriptor contains the reduction keys. The reduction keys in the reduceSink descriptor are used as the reduction keys in the map-reduce boundary. The plan consists of the required samples/partitions if the query specified so. The plan is serialized and written to a file.\n\n- Parser – 语法分析程序将一个查询串转换成一个语法树。\n- Semantic Analyser – 语义分析，将语法树转换成一个基于块的而非树的内部查询。在这个步骤中，会对列名进行验证并将像*这样的操作展开。同时也会做类型检查和隐性的类型转换。如果要处理的是一个分区表，会收集表的所在后面的修剪不使用的分区的时候会用到的信息。\n- Logical Plan Generator – 逻辑计划生成器，将内部查询转换成一个操作树的逻辑计划。有些操作是代数关系操作像'filter'，'join'等。但是有些是Hive特有的操作，并用做将这个计划转成成一系列的map-reduce任务。其中就有在map-reduce边界发生的reduceSink操作。在这步中也包括优化器将计划进行优化的转换-这些转换包括将一系列的join转换成一个多表jion，将分区汇总放在map端，将分区放在2个阶段执行，以避免在一个单一的reducer发生由于数据倾斜造成的性能瓶颈。每个操作符包含一个可序列化对象的描述符。\n- Query Plan Generator – 查询计划生成器 将逻辑计划转换成一系列的map-reduce任务。操作树通过递归遍历将被分解成一系列的用来提交到Hadoop分布式文件系统的mapreduce可序列化的任务。reduceSink操作是map-reduce的边界，它包含Reduce使用到的key。在reduceSink描述符中用的的key也会在map-reduce边界中使用。如果查询指定了sample或者分区，计划只会用他们组成。查询计划会被序列化并写入文件。\n## Optimizer\nMore plan transformations are performed by the optimizer. The optimizer is an evolving component. As of 2011, it was rule-based and performed the following: column pruning and predicate pushdown. However, the infrastructure was in place, and there was work under progress to include other optimizations like map-side join. (Hive 0.11 added several [join optimizations](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization).)\n\nThe optimizer can be enhanced to be cost-based (see Cost-based optimization in Hive and HIVE-5775). The sorted nature of output tables can also be preserved and used later on to generate better plans. The query can be performed on a small sample of data to guess the data distribution, which can be used to generate a better plan.\n\nA [correlation optimizer](https://cwiki.apache.org/confluence/display/Hive/Correlation+Optimizer) was added in Hive 0.12.\n\nThe plan is a generic operator tree, and can be easily manipulated.\n\n优化器会做更多的查询计划转换操作。优化器是一个持续更新的组件。从2011年开始，它的基本规则是：列的修剪和谓词下推。然而，基础设施已经改变，现在正在进行的包括其他优化，如map-side join。\n\n优化器可以加强成为基于花费的。输出表的存储可以被后面生成更好的机会使用。可以通过小的抽样数据猜测数据分布，从而生成更好的计划。\n\n\n## Hive APIs\n[Hive APIs Overview](https://cwiki.apache.org/confluence/display/Hive/Hive+APIs+Overview) describes various public-facing APIs that Hive provides.\n","slug":"Hive架构相关","published":1,"updated":"2018-09-14T01:29:31.898Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzr000yinamlifirviw","content":"<h1 id=\"Hive架构相关\"><a href=\"#Hive架构相关\" class=\"headerlink\" title=\"Hive架构相关\"></a>Hive架构相关</h1><p><a href=\"https://cwiki.apache.org/confluence/display/Hive/Design\" target=\"_blank\" rel=\"noopener\">https://cwiki.apache.org/confluence/display/Hive/Design</a></p>\n<p><img src=\"https://cwiki.apache.org/confluence/download/attachments/27362072/system_architecture.png?version=1&amp;modificationDate=1414560669000&amp;api=v2\" alt=\"Alt text\"></p>\n<h2 id=\"Hive-Architecture（架构）\"><a href=\"#Hive-Architecture（架构）\" class=\"headerlink\" title=\"Hive Architecture（架构）\"></a>Hive Architecture（架构）</h2><a id=\"more\"></a>\n<ul>\n<li>UI – The user interface for users to submit queries and other operations to the system. As of 2011 the system had a command line interface and a web based GUI was being developed.</li>\n<li>UI – 用户向系统提交查询和其他操作的接口。2011年开始有了命令行接口和一个基于网页的图形化界面接口。</li>\n<li>Driver – The component which receives the queries. This component implements the notion of session handles and provides execute and fetch APIs modeled on JDBC/ODBC interfaces.</li>\n<li>Driver – 用来接收查询的组件。这个组件包含对session的控制并提供通过JDBC/ODBC的接口来执行和获取的API模型。</li>\n<li>Compiler – The component that parses the query, does semantic analysis on the different query blocks and query expressions and eventually generates an execution plan with the help of the table and partition metadata looked up from the metastore.</li>\n<li>Compiler – 对查询语句进行语法分析的组件，对不同的查询块和表达式进行语义上的分析，并根据从元数据存储的表和分区的信息生成最终生成执行计划。</li>\n<li>Metastore – The component that stores all the structure information of the various tables and partitions in the warehouse including column and column type information, the serializers and deserializers necessary to read and write data and the corresponding HDFS files where the data is stored.</li>\n<li>Metastore – 存储所有在仓库里的表和分区的结构化信息的组件，包括列和列的信息，序列化和反序列化的方式对数据进行读写，HDFS文件存储位置的映射关系。</li>\n<li>Execution Engine – The component which executes the execution plan created by the compiler. The plan is a DAG of stages. The execution engine manages the dependencies between these different stages of the plan and executes these stages on the appropriate system components.</li>\n<li>Execution Engine – 用来执行compiler生成的执行计划。执行计划是一个有向无环图。执行引擎管理执行计划间不同层级的依赖关系和这些层级对应的系统组件。</li>\n</ul>\n<p>Figure 1 also shows how a typical query flows through the system. The UI calls the execute interface to the Driver (step 1 in Figure 1). The Driver creates a session handle for the query and sends the query to the compiler to generate an execution plan (step 2). The compiler gets the necessary metadata from the metastore (steps 3 and 4). This metadata is used to typecheck the expressions in the query tree as well as to prune partitions based on query predicates. The plan generated by the compiler (step 5) is a DAG of stages with each stage being either a map/reduce job, a metadata operation or an operation on HDFS. For map/reduce stages, the plan contains map operator trees (operator trees that are executed on the mappers) and a reduce operator tree (for operations that need reducers). The execution engine submits these stages to appropriate components (steps 6, 6.1, 6.2 and 6.3). In each task (mapper/reducer) the deserializer associated with the table or intermediate outputs is used to read the rows from HDFS files and these are passed through the associated operator tree. Once the output is generated, it is written to a temporary HDFS file though the serializer (this happens in the mapper in case the operation does not need a reduce). The temporary files are used to provide data to subsequent map/reduce stages of the plan. For DML operations the final temporary file is moved to the table’s location. This scheme is used to ensure that dirty data is not read (file rename being an atomic operation in HDFS). For queries, the contents of the temporary file are read by the execution engine directly from HDFS as part of the fetch call from the Driver (steps 7, 8 and 9).<br>图1展示了一个查询在系统中的流向。UI调用Driver的执行接口（步骤1）。Driver为这个查询创建一个session对象并将查询语句发送给compiler用来生成执行计划（步骤2）。compiler从metastore获取需要的元数据信息（步骤3和4）。这些信息用来检查查询树中的表达式，并根据谓词下推修剪分区。compiler生成的是计划是一个层级的有向无环图，每一个层级都是一个用来操作HDFS或者元数据的map或者reduce任务。对于map/reduce层级，执行计划中包含map操作树（在mappers上执行的操作树）和reduce操作数据（需要reducers的操作）。执行引擎提交这些层级到对应的组件上（步骤6，6.1，6.2和6.3）。在每一个任务中（mapper/reducer）反序列化器将表或者从hdfs文件读取的行的输出和操作树关联起来。一定生成输出，通过序列化器将它写入到HDFS的一个临时文件（因为不需要reduce所以这部分都在mapper里面）。临时文件是为了后面的map/reduce提供数据。对于DML操作最终临时文件会被移动到表所在的位置。这个模式是为了确保垃圾数据不会被读取（在HDFS中会自动将文件重新命名）。对于查询，作为Driver调用的一部分临时文件的内容会被执行引擎直接从HDFS读取到（步骤7，8和9）。</p>\n<h2 id=\"Hive-Data-Model\"><a href=\"#Hive-Data-Model\" class=\"headerlink\" title=\"Hive Data Model\"></a>Hive Data Model</h2><p>Data in Hive is organized into:</p>\n<ul>\n<li>Tables – These are analogous to Tables in Relational Databases. Tables can be filtered, projected, joined and unioned. Additionally all the data of a table is stored in a directory in HDFS. Hive also supports the notion of external tables wherein a table can be created on prexisting files or directories in HDFS by providing the appropriate location to the table creation DDL. The rows in a table are organized into typed columns similar to Relational Databases.</li>\n<li>表 – 和关系型数据库中的表类似。表可以被过滤，管理，关联和合并。此外所有的表中的数据都存储在HDFS的文件夹内。Hive支持外部表，表可以通过提供HDFS上已经存在的文件或者文件夹的路径来创建。表中行和列的管理都类似于关系型数据库。</li>\n<li>Partitions – Each Table can have one or more partition keys which determine how the data is stored, for example a table T with a date partition column ds had files with data for a particular date stored in the <table location=\"\">/ds=<date> directory in HDFS. Partitions allow the system to prune data to be inspected based on query predicates, for example a query that is interested in rows from T that satisfy the predicate T.ds = ‘2008-09-01’ would only have to look at files in <table location=\"\">/ds=2008-09-01/ directory in HDFS.</table></date></table></li>\n<li>分区 – 每个表可以有一个或者多个分区键，分区键用来确定数据的存储，例如，一个表T有一个时间分区列ds并且在对应的日期下都有数据存储在HDFS的&lt;表位置&gt;/ds=&lt;日期&gt;文件夹下。分区允许系统根据查询谓词进行修剪，例如从T表中查询T.ds=’2008-09-01’的数据只会查询HDFS中&lt;表位置&gt;/ds=2008-09-01下的文件。</li>\n<li>Buckets – Data in each partition may in turn be divided into Buckets based on the hash of a column in the table. Each bucket is stored as a file in the partition directory. Bucketing allows the system to efficiently evaluate queries that depend on a sample of data (these are queries that use the SAMPLE clause on the table).</li>\n<li>桶 – 在每个分区内的数据可以再根据表中一个列的哈希进行分桶。每个桶都是在分区文件夹中的一个文件。分桶允许系统根据样例数据（使用SAMPLE的查询）对查询进行有效的评估。</li>\n</ul>\n<p>Apart from primitive column types (integers, floating point numbers, generic strings, dates and booleans), Hive also supports arrays and maps. Additionally, users can compose their own types programmatically from any of the primitives, collections or other user-defined types. The typing system is closely tied to the SerDe (Serailization/Deserialization) and object inspector interfaces. User can create their own types by implementing their own object inspectors, and using these object inspectors they can create their own SerDes to serialize and deserialize their data into HDFS files). These two interfaces provide the necessary hooks to extend the capabilities of Hive when it comes to understanding other data formats and richer types. Builtin object inspectors like ListObjectInspector, StructObjectInspector and MapObjectInspector provide the necessary primitives to compose richer types in an extensible manner. For maps (associative arrays) and arrays useful builtin functions like size and index operators are provided. The dotted notation is used to navigate nested types, for example a.b.c = 1 looks at field c of field b of type a and compares that with 1.</p>\n<p>除了最原始的列的数据类型（整形，浮点型，普通的字符串，日期和布尔 ），Hive还支持数组和映射类型。此外用户可以根据原始数据类型、集合或者其他用户自定义类型进行自定义类型。类型和SerDe（序列化和反序列化）和接口检查器紧密相连。用户可以通过继承对象接口来创建他们自己的数据类型，并可以使用这些类型创建他们自己的SerDes去序列化和反序列化他们存储在HDFS文件里的数据。这两个接口提供了在理解其他数据格式和较丰富类型时扩展Hive功能的必要钩子。构建对象检查器，如ListObjectInspector、StructObjectInspector和MapObjectInspector提供必要的原语，以可扩展的方式组成更丰富的类型。对maps（联合数组）和数组也提供内置的长度和指针操作。</p>\n<h2 id=\"Metastore\"><a href=\"#Metastore\" class=\"headerlink\" title=\"Metastore\"></a>Metastore</h2><p><strong>Motivation</strong><br>The Metastore provides two important but often overlooked features of a data warehouse: data abstraction and data . Without the data abstractions provided in Hive, a user has to provide information about data formats, extractors and loaders along with the query. In Hive, this information is given during table creation and reused every time the table is referenced. This is very similar to the traditional warehousing systems. The second functionality, data discovery, enables users to discover and explore relevant and specific data in the warehouse. Other tools can be built using this metadata to expose and possibly enhance the information about the data and its availability. Hive accomplishes both of these features by providing a metadata repository that is tightly integrated with the Hive query processing system so that data and metadata are in sync.<br><strong>动机</strong><br>Metastore 为数据仓库提供2个非常重要但是经常被忽略的组件：数据抽象和数据发现。如果没有Hive提供的数据抽象，用户不仅要提供查询语句还要提供数据类型、提取器和导入器等信息。在Hive中，每当表创建和重用的时候这些信息都会被提供使用。这点和传统的数据仓库非常类似。第二个功能是数据发现，使用户可以在仓库中发现和浏览相关和特殊的数据。其他工具可以编译使用这些元数据信息用来处理数据。Hive通过提供和Hive查询处理系统紧密相关的元数据仓库来实现这些功能，这样数据和元数据可以保持同步状态。<br><strong>Metadata Objects</strong><br>Database – is a namespace for tables. It can be used as an administrative unit in the future. The database ‘default’ is used for tables with no user-supplied database name.<br>Table – Metadata for a table contains list of columns, owner, storage and SerDe information. It can also contain any user-supplied key and value data. Storage information includes location of the underlying data, file inout and output formats and bucketing information. SerDe metadata includes the implementation class of serializer and deserializer and any supporting information required by the implementation. All of this information can be provided during creation of the table.<br>Partition – Each partition can have its own columns and SerDe and storage information. This facilitates schema changes without affecting older partitions.<br><strong>元数据对象</strong><br>数据库 – 表的命名空间。它将来可以被用做一个管理单元。默认的数据库’default’被用来存储没有被提供数据库名的表。<br>表 – 表的元数据信息包括列的列表，所有者，存储和SerDe信息。也可以包含用户提供的Key和Value数据。包含数据的底层存储信息，文件输入和输出格式和分桶信息。SerDe元数据包含序列化和反序列化的接口和接口所需要的信息。这些信息在创建表的时候都可以提供。<br><strong>Metastore Architecture</strong><br>Metastore is an object store with a database or file backed store. The database backed store is implemented using an object-relational mapping (ORM) solution called the DataNucleus. The prime motivation for storing this in a relational database is queriability of metadata. Some disadvantages of using a separate data store for metadata instead of using HDFS are synchronization and scalability issues. Additionally there is no clear way to implement an object store on top of HDFS due to lack of random updates to files. This, coupled with the advantages of queriability of a relational store, made our approach a sensible one.<br>The metastore can be configured to be used in a couple of ways: remote and embedded. In remote mode, the metastore is a Thrift service. This mode is useful for non-Java clients. In embedded mode, the Hive client directly connects to an underlying metastore using JDBC. This mode is useful because it avoids another system that needs to be maintained and monitored. Both of these modes can co-exist. (Update: Local metastore is a third possibility. See Hive Metastore Administration for details.)<br><strong>元数据存储架构</strong><br>元数据存储是数据或者文件存储对象。数据库支持存储是用一个叫做DataNucleus的对象关系映射（ORM）作为方案的接口。存储在关系型数据库的首要动机就是使这些元数据可被查询。使用一个单独的存储而不使用HDFS作为存储的原因是数据同步和扩展问题。此外，由于缺少文件的随机更新机制，所以不能基于HDFS做对象存储。因此，结合关系型存储的查询优点使我们作出了明智的选择。<br>元数据存储可以使用多种方式进行配置：远程和嵌入的。在远程模式下，元数据存储是一个Thrift服务。这种模式对于非Java客户端非常有用。在嵌入模式下，Hive客户端可以直接使用JDBC连接元数据存储。因为避免了需要管理和监控的其他系统所有它非常有用。所有的模式可以并存。<br><strong>Metastore Interface</strong><br>Metastore provides a Thrift interface to manipulate and query Hive metadata. Thrift provides bindings in many popular languages. Third party tools can use this interface to integrate Hive metadata into other business metadata repositories.<br><strong>元数据存储接口</strong><br>元数据存储提供一个Thrift接口用来操作和查询Hive的元数据。Thrift提供多种语言的封装。第三方的工具可以使用这个接口将Hive的元数据集成到其他业务元数据仓库中。</p>\n<h2 id=\"Hive-Query-Language\"><a href=\"#Hive-Query-Language\" class=\"headerlink\" title=\"Hive Query Language\"></a>Hive Query Language</h2><p>HiveQL is an SQL-like query language for Hive. It mostly mimics SQL syntax for creation of tables, loading data into tables and querying the tables. HiveQL also allows users to embed their custom map-reduce scripts. These scripts can be written in any language using a simple row-based streaming interface – read rows from standard input and write out rows to standard output. This flexibility comes at a cost of a performance hit caused by converting rows from and to strings. However, we have seen that users do not mind this given that they can implement their scripts in the language of their choice. Another feature unique to HiveQL is multi-table insert. In this construct, users can perform multiple queries on the same input data using a single HiveQL query. Hive optimizes these queries to share the scan of the input data, thus increasing the throughput of these queries several orders of magnitude. We omit more details due to lack of space. For a more complete description of the HiveQL language see the <a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual\" target=\"_blank\" rel=\"noopener\">language manual</a>.</p>\n<p>HiveQL是一个类SQL的查询语言。它有类SQL的创建表的语法，导入数据和查询表等。HiveQL也允许用户使用自定义的map-reduce代码。这些代码可以是用任何语言用一个简单的基于行的流接口-从标准输入读取和从标准输出写数据。这个兼容性源于将行和字符串之间做转换的性能花费。然而，用户能使用他们想要使用的语言他们就不在乎这点。一个只在HiveQL中有的特性是多表插入。在这种架构中，用户可以使用一个HiveQL查询同时操作多个查询语句在同一个数据输入上。Hive优化了对输入数据的扫描，因此增加了查询的吞吐。</p>\n<h2 id=\"Compiler\"><a href=\"#Compiler\" class=\"headerlink\" title=\"Compiler\"></a>Compiler</h2><ul>\n<li>Parser – Transform a query string to a parse tree representation.</li>\n<li>Semantic Analyser – Transform the parse tree to an internal query representation, which is still block based and not an operator tree. As part of this step, the column names are verified and expansions like * are performed. Type-checking and any implicit type conversions are also performed at this stage. If the table under consideration is a partitioned table, which is the common scenario, all the expressions for that table are collected so that they can be later used to prune the partitions which are not needed. If the query has specified sampling, that is also collected to be used later on.</li>\n<li>Logical Plan Generator – Convert the internal query representation to a logical plan, which consists of a tree of operators. Some of the operators are relational algebra operators like ‘filter’, ‘join’ etc. But some of the operators are Hive specific and are used later on to convert this plan into a series of map-reduce jobs. One such operator is a reduceSink operator which occurs at the map-reduce boundary. This step also includes the optimizer to transform the plan to improve performance – some of those transformations include: converting a series of joins into a single multi-way join, performing a map-side partial aggregation for a group-by, performing a group-by in 2 stages to avoid the scenario when a single reducer can become a bottleneck in presence of skewed data for the grouping key. Each operator comprises a descriptor which is a serializable object.</li>\n<li><p>Query Plan Generator – Convert the logical plan to a series of map-reduce tasks. The operator tree is recursively traversed, to be broken up into a series of map-reduce serializable tasks which can be submitted later on to the map-reduce framework for the Hadoop distributed file system. The reduceSink operator is the map-reduce boundary, whose descriptor contains the reduction keys. The reduction keys in the reduceSink descriptor are used as the reduction keys in the map-reduce boundary. The plan consists of the required samples/partitions if the query specified so. The plan is serialized and written to a file.</p>\n</li>\n<li><p>Parser – 语法分析程序将一个查询串转换成一个语法树。</p>\n</li>\n<li>Semantic Analyser – 语义分析，将语法树转换成一个基于块的而非树的内部查询。在这个步骤中，会对列名进行验证并将像*这样的操作展开。同时也会做类型检查和隐性的类型转换。如果要处理的是一个分区表，会收集表的所在后面的修剪不使用的分区的时候会用到的信息。</li>\n<li>Logical Plan Generator – 逻辑计划生成器，将内部查询转换成一个操作树的逻辑计划。有些操作是代数关系操作像’filter’，’join’等。但是有些是Hive特有的操作，并用做将这个计划转成成一系列的map-reduce任务。其中就有在map-reduce边界发生的reduceSink操作。在这步中也包括优化器将计划进行优化的转换-这些转换包括将一系列的join转换成一个多表jion，将分区汇总放在map端，将分区放在2个阶段执行，以避免在一个单一的reducer发生由于数据倾斜造成的性能瓶颈。每个操作符包含一个可序列化对象的描述符。</li>\n<li>Query Plan Generator – 查询计划生成器 将逻辑计划转换成一系列的map-reduce任务。操作树通过递归遍历将被分解成一系列的用来提交到Hadoop分布式文件系统的mapreduce可序列化的任务。reduceSink操作是map-reduce的边界，它包含Reduce使用到的key。在reduceSink描述符中用的的key也会在map-reduce边界中使用。如果查询指定了sample或者分区，计划只会用他们组成。查询计划会被序列化并写入文件。<h2 id=\"Optimizer\"><a href=\"#Optimizer\" class=\"headerlink\" title=\"Optimizer\"></a>Optimizer</h2>More plan transformations are performed by the optimizer. The optimizer is an evolving component. As of 2011, it was rule-based and performed the following: column pruning and predicate pushdown. However, the infrastructure was in place, and there was work under progress to include other optimizations like map-side join. (Hive 0.11 added several <a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization\" target=\"_blank\" rel=\"noopener\">join optimizations</a>.)</li>\n</ul>\n<p>The optimizer can be enhanced to be cost-based (see Cost-based optimization in Hive and HIVE-5775). The sorted nature of output tables can also be preserved and used later on to generate better plans. The query can be performed on a small sample of data to guess the data distribution, which can be used to generate a better plan.</p>\n<p>A <a href=\"https://cwiki.apache.org/confluence/display/Hive/Correlation+Optimizer\" target=\"_blank\" rel=\"noopener\">correlation optimizer</a> was added in Hive 0.12.</p>\n<p>The plan is a generic operator tree, and can be easily manipulated.</p>\n<p>优化器会做更多的查询计划转换操作。优化器是一个持续更新的组件。从2011年开始，它的基本规则是：列的修剪和谓词下推。然而，基础设施已经改变，现在正在进行的包括其他优化，如map-side join。</p>\n<p>优化器可以加强成为基于花费的。输出表的存储可以被后面生成更好的机会使用。可以通过小的抽样数据猜测数据分布，从而生成更好的计划。</p>\n<h2 id=\"Hive-APIs\"><a href=\"#Hive-APIs\" class=\"headerlink\" title=\"Hive APIs\"></a>Hive APIs</h2><p><a href=\"https://cwiki.apache.org/confluence/display/Hive/Hive+APIs+Overview\" target=\"_blank\" rel=\"noopener\">Hive APIs Overview</a> describes various public-facing APIs that Hive provides.</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"Hive架构相关\"><a href=\"#Hive架构相关\" class=\"headerlink\" title=\"Hive架构相关\"></a>Hive架构相关</h1><p><a href=\"https://cwiki.apache.org/confluence/display/Hive/Design\" target=\"_blank\" rel=\"noopener\">https://cwiki.apache.org/confluence/display/Hive/Design</a></p>\n<p><img src=\"https://cwiki.apache.org/confluence/download/attachments/27362072/system_architecture.png?version=1&amp;modificationDate=1414560669000&amp;api=v2\" alt=\"Alt text\"></p>\n<h2 id=\"Hive-Architecture（架构）\"><a href=\"#Hive-Architecture（架构）\" class=\"headerlink\" title=\"Hive Architecture（架构）\"></a>Hive Architecture（架构）</h2>","more":"<ul>\n<li>UI – The user interface for users to submit queries and other operations to the system. As of 2011 the system had a command line interface and a web based GUI was being developed.</li>\n<li>UI – 用户向系统提交查询和其他操作的接口。2011年开始有了命令行接口和一个基于网页的图形化界面接口。</li>\n<li>Driver – The component which receives the queries. This component implements the notion of session handles and provides execute and fetch APIs modeled on JDBC/ODBC interfaces.</li>\n<li>Driver – 用来接收查询的组件。这个组件包含对session的控制并提供通过JDBC/ODBC的接口来执行和获取的API模型。</li>\n<li>Compiler – The component that parses the query, does semantic analysis on the different query blocks and query expressions and eventually generates an execution plan with the help of the table and partition metadata looked up from the metastore.</li>\n<li>Compiler – 对查询语句进行语法分析的组件，对不同的查询块和表达式进行语义上的分析，并根据从元数据存储的表和分区的信息生成最终生成执行计划。</li>\n<li>Metastore – The component that stores all the structure information of the various tables and partitions in the warehouse including column and column type information, the serializers and deserializers necessary to read and write data and the corresponding HDFS files where the data is stored.</li>\n<li>Metastore – 存储所有在仓库里的表和分区的结构化信息的组件，包括列和列的信息，序列化和反序列化的方式对数据进行读写，HDFS文件存储位置的映射关系。</li>\n<li>Execution Engine – The component which executes the execution plan created by the compiler. The plan is a DAG of stages. The execution engine manages the dependencies between these different stages of the plan and executes these stages on the appropriate system components.</li>\n<li>Execution Engine – 用来执行compiler生成的执行计划。执行计划是一个有向无环图。执行引擎管理执行计划间不同层级的依赖关系和这些层级对应的系统组件。</li>\n</ul>\n<p>Figure 1 also shows how a typical query flows through the system. The UI calls the execute interface to the Driver (step 1 in Figure 1). The Driver creates a session handle for the query and sends the query to the compiler to generate an execution plan (step 2). The compiler gets the necessary metadata from the metastore (steps 3 and 4). This metadata is used to typecheck the expressions in the query tree as well as to prune partitions based on query predicates. The plan generated by the compiler (step 5) is a DAG of stages with each stage being either a map/reduce job, a metadata operation or an operation on HDFS. For map/reduce stages, the plan contains map operator trees (operator trees that are executed on the mappers) and a reduce operator tree (for operations that need reducers). The execution engine submits these stages to appropriate components (steps 6, 6.1, 6.2 and 6.3). In each task (mapper/reducer) the deserializer associated with the table or intermediate outputs is used to read the rows from HDFS files and these are passed through the associated operator tree. Once the output is generated, it is written to a temporary HDFS file though the serializer (this happens in the mapper in case the operation does not need a reduce). The temporary files are used to provide data to subsequent map/reduce stages of the plan. For DML operations the final temporary file is moved to the table’s location. This scheme is used to ensure that dirty data is not read (file rename being an atomic operation in HDFS). For queries, the contents of the temporary file are read by the execution engine directly from HDFS as part of the fetch call from the Driver (steps 7, 8 and 9).<br>图1展示了一个查询在系统中的流向。UI调用Driver的执行接口（步骤1）。Driver为这个查询创建一个session对象并将查询语句发送给compiler用来生成执行计划（步骤2）。compiler从metastore获取需要的元数据信息（步骤3和4）。这些信息用来检查查询树中的表达式，并根据谓词下推修剪分区。compiler生成的是计划是一个层级的有向无环图，每一个层级都是一个用来操作HDFS或者元数据的map或者reduce任务。对于map/reduce层级，执行计划中包含map操作树（在mappers上执行的操作树）和reduce操作数据（需要reducers的操作）。执行引擎提交这些层级到对应的组件上（步骤6，6.1，6.2和6.3）。在每一个任务中（mapper/reducer）反序列化器将表或者从hdfs文件读取的行的输出和操作树关联起来。一定生成输出，通过序列化器将它写入到HDFS的一个临时文件（因为不需要reduce所以这部分都在mapper里面）。临时文件是为了后面的map/reduce提供数据。对于DML操作最终临时文件会被移动到表所在的位置。这个模式是为了确保垃圾数据不会被读取（在HDFS中会自动将文件重新命名）。对于查询，作为Driver调用的一部分临时文件的内容会被执行引擎直接从HDFS读取到（步骤7，8和9）。</p>\n<h2 id=\"Hive-Data-Model\"><a href=\"#Hive-Data-Model\" class=\"headerlink\" title=\"Hive Data Model\"></a>Hive Data Model</h2><p>Data in Hive is organized into:</p>\n<ul>\n<li>Tables – These are analogous to Tables in Relational Databases. Tables can be filtered, projected, joined and unioned. Additionally all the data of a table is stored in a directory in HDFS. Hive also supports the notion of external tables wherein a table can be created on prexisting files or directories in HDFS by providing the appropriate location to the table creation DDL. The rows in a table are organized into typed columns similar to Relational Databases.</li>\n<li>表 – 和关系型数据库中的表类似。表可以被过滤，管理，关联和合并。此外所有的表中的数据都存储在HDFS的文件夹内。Hive支持外部表，表可以通过提供HDFS上已经存在的文件或者文件夹的路径来创建。表中行和列的管理都类似于关系型数据库。</li>\n<li>Partitions – Each Table can have one or more partition keys which determine how the data is stored, for example a table T with a date partition column ds had files with data for a particular date stored in the <table location=\"\">/ds=<date> directory in HDFS. Partitions allow the system to prune data to be inspected based on query predicates, for example a query that is interested in rows from T that satisfy the predicate T.ds = ‘2008-09-01’ would only have to look at files in <table location=\"\">/ds=2008-09-01/ directory in HDFS.</table></date></table></li>\n<li>分区 – 每个表可以有一个或者多个分区键，分区键用来确定数据的存储，例如，一个表T有一个时间分区列ds并且在对应的日期下都有数据存储在HDFS的&lt;表位置&gt;/ds=&lt;日期&gt;文件夹下。分区允许系统根据查询谓词进行修剪，例如从T表中查询T.ds=’2008-09-01’的数据只会查询HDFS中&lt;表位置&gt;/ds=2008-09-01下的文件。</li>\n<li>Buckets – Data in each partition may in turn be divided into Buckets based on the hash of a column in the table. Each bucket is stored as a file in the partition directory. Bucketing allows the system to efficiently evaluate queries that depend on a sample of data (these are queries that use the SAMPLE clause on the table).</li>\n<li>桶 – 在每个分区内的数据可以再根据表中一个列的哈希进行分桶。每个桶都是在分区文件夹中的一个文件。分桶允许系统根据样例数据（使用SAMPLE的查询）对查询进行有效的评估。</li>\n</ul>\n<p>Apart from primitive column types (integers, floating point numbers, generic strings, dates and booleans), Hive also supports arrays and maps. Additionally, users can compose their own types programmatically from any of the primitives, collections or other user-defined types. The typing system is closely tied to the SerDe (Serailization/Deserialization) and object inspector interfaces. User can create their own types by implementing their own object inspectors, and using these object inspectors they can create their own SerDes to serialize and deserialize their data into HDFS files). These two interfaces provide the necessary hooks to extend the capabilities of Hive when it comes to understanding other data formats and richer types. Builtin object inspectors like ListObjectInspector, StructObjectInspector and MapObjectInspector provide the necessary primitives to compose richer types in an extensible manner. For maps (associative arrays) and arrays useful builtin functions like size and index operators are provided. The dotted notation is used to navigate nested types, for example a.b.c = 1 looks at field c of field b of type a and compares that with 1.</p>\n<p>除了最原始的列的数据类型（整形，浮点型，普通的字符串，日期和布尔 ），Hive还支持数组和映射类型。此外用户可以根据原始数据类型、集合或者其他用户自定义类型进行自定义类型。类型和SerDe（序列化和反序列化）和接口检查器紧密相连。用户可以通过继承对象接口来创建他们自己的数据类型，并可以使用这些类型创建他们自己的SerDes去序列化和反序列化他们存储在HDFS文件里的数据。这两个接口提供了在理解其他数据格式和较丰富类型时扩展Hive功能的必要钩子。构建对象检查器，如ListObjectInspector、StructObjectInspector和MapObjectInspector提供必要的原语，以可扩展的方式组成更丰富的类型。对maps（联合数组）和数组也提供内置的长度和指针操作。</p>\n<h2 id=\"Metastore\"><a href=\"#Metastore\" class=\"headerlink\" title=\"Metastore\"></a>Metastore</h2><p><strong>Motivation</strong><br>The Metastore provides two important but often overlooked features of a data warehouse: data abstraction and data . Without the data abstractions provided in Hive, a user has to provide information about data formats, extractors and loaders along with the query. In Hive, this information is given during table creation and reused every time the table is referenced. This is very similar to the traditional warehousing systems. The second functionality, data discovery, enables users to discover and explore relevant and specific data in the warehouse. Other tools can be built using this metadata to expose and possibly enhance the information about the data and its availability. Hive accomplishes both of these features by providing a metadata repository that is tightly integrated with the Hive query processing system so that data and metadata are in sync.<br><strong>动机</strong><br>Metastore 为数据仓库提供2个非常重要但是经常被忽略的组件：数据抽象和数据发现。如果没有Hive提供的数据抽象，用户不仅要提供查询语句还要提供数据类型、提取器和导入器等信息。在Hive中，每当表创建和重用的时候这些信息都会被提供使用。这点和传统的数据仓库非常类似。第二个功能是数据发现，使用户可以在仓库中发现和浏览相关和特殊的数据。其他工具可以编译使用这些元数据信息用来处理数据。Hive通过提供和Hive查询处理系统紧密相关的元数据仓库来实现这些功能，这样数据和元数据可以保持同步状态。<br><strong>Metadata Objects</strong><br>Database – is a namespace for tables. It can be used as an administrative unit in the future. The database ‘default’ is used for tables with no user-supplied database name.<br>Table – Metadata for a table contains list of columns, owner, storage and SerDe information. It can also contain any user-supplied key and value data. Storage information includes location of the underlying data, file inout and output formats and bucketing information. SerDe metadata includes the implementation class of serializer and deserializer and any supporting information required by the implementation. All of this information can be provided during creation of the table.<br>Partition – Each partition can have its own columns and SerDe and storage information. This facilitates schema changes without affecting older partitions.<br><strong>元数据对象</strong><br>数据库 – 表的命名空间。它将来可以被用做一个管理单元。默认的数据库’default’被用来存储没有被提供数据库名的表。<br>表 – 表的元数据信息包括列的列表，所有者，存储和SerDe信息。也可以包含用户提供的Key和Value数据。包含数据的底层存储信息，文件输入和输出格式和分桶信息。SerDe元数据包含序列化和反序列化的接口和接口所需要的信息。这些信息在创建表的时候都可以提供。<br><strong>Metastore Architecture</strong><br>Metastore is an object store with a database or file backed store. The database backed store is implemented using an object-relational mapping (ORM) solution called the DataNucleus. The prime motivation for storing this in a relational database is queriability of metadata. Some disadvantages of using a separate data store for metadata instead of using HDFS are synchronization and scalability issues. Additionally there is no clear way to implement an object store on top of HDFS due to lack of random updates to files. This, coupled with the advantages of queriability of a relational store, made our approach a sensible one.<br>The metastore can be configured to be used in a couple of ways: remote and embedded. In remote mode, the metastore is a Thrift service. This mode is useful for non-Java clients. In embedded mode, the Hive client directly connects to an underlying metastore using JDBC. This mode is useful because it avoids another system that needs to be maintained and monitored. Both of these modes can co-exist. (Update: Local metastore is a third possibility. See Hive Metastore Administration for details.)<br><strong>元数据存储架构</strong><br>元数据存储是数据或者文件存储对象。数据库支持存储是用一个叫做DataNucleus的对象关系映射（ORM）作为方案的接口。存储在关系型数据库的首要动机就是使这些元数据可被查询。使用一个单独的存储而不使用HDFS作为存储的原因是数据同步和扩展问题。此外，由于缺少文件的随机更新机制，所以不能基于HDFS做对象存储。因此，结合关系型存储的查询优点使我们作出了明智的选择。<br>元数据存储可以使用多种方式进行配置：远程和嵌入的。在远程模式下，元数据存储是一个Thrift服务。这种模式对于非Java客户端非常有用。在嵌入模式下，Hive客户端可以直接使用JDBC连接元数据存储。因为避免了需要管理和监控的其他系统所有它非常有用。所有的模式可以并存。<br><strong>Metastore Interface</strong><br>Metastore provides a Thrift interface to manipulate and query Hive metadata. Thrift provides bindings in many popular languages. Third party tools can use this interface to integrate Hive metadata into other business metadata repositories.<br><strong>元数据存储接口</strong><br>元数据存储提供一个Thrift接口用来操作和查询Hive的元数据。Thrift提供多种语言的封装。第三方的工具可以使用这个接口将Hive的元数据集成到其他业务元数据仓库中。</p>\n<h2 id=\"Hive-Query-Language\"><a href=\"#Hive-Query-Language\" class=\"headerlink\" title=\"Hive Query Language\"></a>Hive Query Language</h2><p>HiveQL is an SQL-like query language for Hive. It mostly mimics SQL syntax for creation of tables, loading data into tables and querying the tables. HiveQL also allows users to embed their custom map-reduce scripts. These scripts can be written in any language using a simple row-based streaming interface – read rows from standard input and write out rows to standard output. This flexibility comes at a cost of a performance hit caused by converting rows from and to strings. However, we have seen that users do not mind this given that they can implement their scripts in the language of their choice. Another feature unique to HiveQL is multi-table insert. In this construct, users can perform multiple queries on the same input data using a single HiveQL query. Hive optimizes these queries to share the scan of the input data, thus increasing the throughput of these queries several orders of magnitude. We omit more details due to lack of space. For a more complete description of the HiveQL language see the <a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual\" target=\"_blank\" rel=\"noopener\">language manual</a>.</p>\n<p>HiveQL是一个类SQL的查询语言。它有类SQL的创建表的语法，导入数据和查询表等。HiveQL也允许用户使用自定义的map-reduce代码。这些代码可以是用任何语言用一个简单的基于行的流接口-从标准输入读取和从标准输出写数据。这个兼容性源于将行和字符串之间做转换的性能花费。然而，用户能使用他们想要使用的语言他们就不在乎这点。一个只在HiveQL中有的特性是多表插入。在这种架构中，用户可以使用一个HiveQL查询同时操作多个查询语句在同一个数据输入上。Hive优化了对输入数据的扫描，因此增加了查询的吞吐。</p>\n<h2 id=\"Compiler\"><a href=\"#Compiler\" class=\"headerlink\" title=\"Compiler\"></a>Compiler</h2><ul>\n<li>Parser – Transform a query string to a parse tree representation.</li>\n<li>Semantic Analyser – Transform the parse tree to an internal query representation, which is still block based and not an operator tree. As part of this step, the column names are verified and expansions like * are performed. Type-checking and any implicit type conversions are also performed at this stage. If the table under consideration is a partitioned table, which is the common scenario, all the expressions for that table are collected so that they can be later used to prune the partitions which are not needed. If the query has specified sampling, that is also collected to be used later on.</li>\n<li>Logical Plan Generator – Convert the internal query representation to a logical plan, which consists of a tree of operators. Some of the operators are relational algebra operators like ‘filter’, ‘join’ etc. But some of the operators are Hive specific and are used later on to convert this plan into a series of map-reduce jobs. One such operator is a reduceSink operator which occurs at the map-reduce boundary. This step also includes the optimizer to transform the plan to improve performance – some of those transformations include: converting a series of joins into a single multi-way join, performing a map-side partial aggregation for a group-by, performing a group-by in 2 stages to avoid the scenario when a single reducer can become a bottleneck in presence of skewed data for the grouping key. Each operator comprises a descriptor which is a serializable object.</li>\n<li><p>Query Plan Generator – Convert the logical plan to a series of map-reduce tasks. The operator tree is recursively traversed, to be broken up into a series of map-reduce serializable tasks which can be submitted later on to the map-reduce framework for the Hadoop distributed file system. The reduceSink operator is the map-reduce boundary, whose descriptor contains the reduction keys. The reduction keys in the reduceSink descriptor are used as the reduction keys in the map-reduce boundary. The plan consists of the required samples/partitions if the query specified so. The plan is serialized and written to a file.</p>\n</li>\n<li><p>Parser – 语法分析程序将一个查询串转换成一个语法树。</p>\n</li>\n<li>Semantic Analyser – 语义分析，将语法树转换成一个基于块的而非树的内部查询。在这个步骤中，会对列名进行验证并将像*这样的操作展开。同时也会做类型检查和隐性的类型转换。如果要处理的是一个分区表，会收集表的所在后面的修剪不使用的分区的时候会用到的信息。</li>\n<li>Logical Plan Generator – 逻辑计划生成器，将内部查询转换成一个操作树的逻辑计划。有些操作是代数关系操作像’filter’，’join’等。但是有些是Hive特有的操作，并用做将这个计划转成成一系列的map-reduce任务。其中就有在map-reduce边界发生的reduceSink操作。在这步中也包括优化器将计划进行优化的转换-这些转换包括将一系列的join转换成一个多表jion，将分区汇总放在map端，将分区放在2个阶段执行，以避免在一个单一的reducer发生由于数据倾斜造成的性能瓶颈。每个操作符包含一个可序列化对象的描述符。</li>\n<li>Query Plan Generator – 查询计划生成器 将逻辑计划转换成一系列的map-reduce任务。操作树通过递归遍历将被分解成一系列的用来提交到Hadoop分布式文件系统的mapreduce可序列化的任务。reduceSink操作是map-reduce的边界，它包含Reduce使用到的key。在reduceSink描述符中用的的key也会在map-reduce边界中使用。如果查询指定了sample或者分区，计划只会用他们组成。查询计划会被序列化并写入文件。<h2 id=\"Optimizer\"><a href=\"#Optimizer\" class=\"headerlink\" title=\"Optimizer\"></a>Optimizer</h2>More plan transformations are performed by the optimizer. The optimizer is an evolving component. As of 2011, it was rule-based and performed the following: column pruning and predicate pushdown. However, the infrastructure was in place, and there was work under progress to include other optimizations like map-side join. (Hive 0.11 added several <a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization\" target=\"_blank\" rel=\"noopener\">join optimizations</a>.)</li>\n</ul>\n<p>The optimizer can be enhanced to be cost-based (see Cost-based optimization in Hive and HIVE-5775). The sorted nature of output tables can also be preserved and used later on to generate better plans. The query can be performed on a small sample of data to guess the data distribution, which can be used to generate a better plan.</p>\n<p>A <a href=\"https://cwiki.apache.org/confluence/display/Hive/Correlation+Optimizer\" target=\"_blank\" rel=\"noopener\">correlation optimizer</a> was added in Hive 0.12.</p>\n<p>The plan is a generic operator tree, and can be easily manipulated.</p>\n<p>优化器会做更多的查询计划转换操作。优化器是一个持续更新的组件。从2011年开始，它的基本规则是：列的修剪和谓词下推。然而，基础设施已经改变，现在正在进行的包括其他优化，如map-side join。</p>\n<p>优化器可以加强成为基于花费的。输出表的存储可以被后面生成更好的机会使用。可以通过小的抽样数据猜测数据分布，从而生成更好的计划。</p>\n<h2 id=\"Hive-APIs\"><a href=\"#Hive-APIs\" class=\"headerlink\" title=\"Hive APIs\"></a>Hive APIs</h2><p><a href=\"https://cwiki.apache.org/confluence/display/Hive/Hive+APIs+Overview\" target=\"_blank\" rel=\"noopener\">Hive APIs Overview</a> describes various public-facing APIs that Hive provides.</p>"},{"title":"Hive窗口函数和分析函数","date":"2018-05-13T03:53:26.000Z","_content":"# Hive窗口函数和分析函数\n\n## 创建测试表并插入测试数据\n\n```\ncreate table test_analy_funs (\nstu_id int\n,stu_nm varchar(20)\n,class int\n,course varchar(20)\n,score decimal(6,2));\n\ninsert into table test_analy_funs\nselect 1,'Daniel',201,'maths',85 union all\nselect 2,'Tom',201,'maths',82 union all\nselect 3,'Macal',201,'maths',26 union all\nselect 4,'Lily',201,'maths',99 union all\nselect 5,'Onel',201,'maths',100 union all\nselect 6,'Kebe',201,'maths',67 union all\nselect 7,'Kaka',201,'maths',89 union all\nselect 8,'Piye',201,'maths',93 union all\nselect 9,'Luo',202,'maths',73 union all\nselect 10,'Lufi', 202,'maths',98 union all\nselect 11,'Tony', 202,'maths',100 union all\nselect 12,'Miya', 202,'maths',100 union all\nselect 13,'Joy', 202,'maths',87 union all\nselect 14,'Tong', 202,'maths',79 union all\nselect 15,'Roy', 202,'maths',84 union all\nselect 16,'Erw', 202,'maths',84 union all\nselect 1,'Daniel',201,'english',82 union all\nselect 2,'Tom',201,'english',26 union all\nselect 3,'Macal',201,'english',99 union all\nselect 4,'Lily',201,'english',100 union all\nselect 5,'Onel',201,'english',77 union all\nselect 6,'Kebe',201,'english',89 union all\nselect 7,'Kaka',201,'english',77 union all\nselect 8,'Piye',201,'english',85 union all\nselect 9,'Luo',202,'english',80 union all\nselect 10,'Lufi', 202,'english',100 union all\nselect 11,'Tony', 202,'english',59 union all\nselect 12,'Miya', 202,'english',84 union all\nselect 13,'Joy', 202,'english',82 union all\nselect 14,'Tong', 202,'english',89 union all\nselect 15,'Roy', 202,'english',90 union all\nselect 16,'Erw', 202,'english',95 union all\nselect 1,'Daniel',201,'chinese',28 union all\nselect 2,'Tom',201,'chinese',99 union all\nselect 3,'Macal',201,'chinese',89 union all\nselect 4,'Lily',201,'chinese',100 union all\nselect 5,'Onel',201,'chinese',88 union all\nselect 6,'Kebe',201,'chinese',83 union all\nselect 7,'Kaka',201,'chinese',92 union all\nselect 8,'Piye',201,'chinese',95 union all\nselect 9,'Luo',202,'chinese',82 union all\nselect 10,'Lufi', 202,'chinese',92 union all\nselect 11,'Tony', 202,'chinese',99 union all\nselect 12,'Miya', 202,'chinese',78 union all\nselect 13,'Joy', 202,'chinese',28 union all\nselect 14,'Tong', 202,'chinese',50 union all\nselect 15,'Roy', 202,'chinese',30 union all\nselect 16,'Erw', 202,'chinese',88 \n\n\n\n```\n\n<!-- more -->\n## 排序\n\ndense_rank() OVER([partition_by_clause] order_by_clause)\n返回从1开始的升序序列，根据不同的order by的字段生成不同的值，如果值一样则排序结果也一样。\nReturns an ascending sequence of integers, starting with 1. The output sequence produces duplicate integers for duplicate values of the ORDER BY expressions.\n\nrank() OVER([partition_by_clause] order_by_clause)\n返回从1开始的升序序列，根据不同的order by的字段生成不同的值，如果值一样则排序结果也一样。然后序列值会根据实际的数值增加。\nReturns an ascending sequence of integers, starting with 1. The output sequence produces duplicate integers for duplicate values of the ORDER BY expressions. After generating duplicate output values for the \"tied\" input values, the function increments the sequence by the number of tied values.\n\nrow_number() OVER([partition_by_clause] order_by_clause)\n返回从1开始的升序序列，根据不同的order by的字段生成不同的值，即使值一样，排序结果也不一样。\nReturns an ascending sequence of integers, starting with 1. Starts the sequence over for each group produced by the PARTITIONED BY clause. The output sequence includes different values for duplicate input values. Therefore, the sequence never contains any duplicates or gaps, regardless of duplicate input values.\n\n```\nselect\n   stu_nm, \n   class,\n   course,\n   score,\n   rank() over(partition by class, course order by score desc) rk1,\n   dense_rank() over(partition by class, course order by score desc) rk2,\n   row_number() over(partition by class, course order by score desc) rk3,\n   percent_rank() over(partition by class, course order by score desc)\nfrom test_analy_funs;\n\n+---------+--------+----------+--------+------+------+------+----------------------+--+\n| stu_nm  | class  |  course  | score  | rk1  | rk2  | rk3  |        _wcol3        |\n+---------+--------+----------+--------+------+------+------+----------------------+--+\n| Lily    | 201    | english  | 100    | 1    | 1    | 1    | 0.0                  |\n| Macal   | 201    | english  | 99     | 2    | 2    | 2    | 0.14285714285714285  |\n| Kebe    | 201    | english  | 89     | 3    | 3    | 3    | 0.2857142857142857   |\n| Piye    | 201    | english  | 85     | 4    | 4    | 4    | 0.42857142857142855  |\n| Daniel  | 201    | english  | 82     | 5    | 5    | 5    | 0.5714285714285714   |\n| Onel    | 201    | english  | 77     | 6    | 6    | 6    | 0.7142857142857143   |\n| Kaka    | 201    | english  | 77     | 6    | 6    | 7    | 0.7142857142857143   |\n| Tom     | 201    | english  | 26     | 8    | 7    | 8    | 1.0                  |\n\n| Tony    | 202    | maths    | 100    | 1    | 1    | 1    | 0.0                  |\n| Miya    | 202    | maths    | 100    | 1    | 1    | 2    | 0.0                  |\n| Lufi    | 202    | maths    | 98     | 3    | 2    | 3    | 0.2857142857142857   |\n| Joy     | 202    | maths    | 87     | 4    | 3    | 4    | 0.42857142857142855  |\n| Roy     | 202    | maths    | 84     | 5    | 4    | 5    | 0.5714285714285714   |\n| Erw     | 202    | maths    | 84     | 5    | 4    | 6    | 0.5714285714285714   |\n| Tong    | 202    | maths    | 79     | 7    | 5    | 7    | 0.8571428571428571   |\n| Luo     | 202    | maths    | 73     | 8    | 6    | 8    | 1.0                  |\n+---------+--------+----------+--------+------+------+------+----------------------+--+\n\n```\n## 差值\n\nlag(expr [, offset] [, default]) OVER ([partition_by_clause] order_by_clause)\nThis function returns the value of an expression using column values from a preceding row. You specify an integer offset, which designates a row position some number of rows previous to the current row. Any column references in the expression argument refer to column values from that prior row.\n\nlead(expr [, offset] [, default]) OVER([partition_by_clause] order_by_clause)\nThis function returns the value of an expression using column values from a following row. You specify an integer offset, which designates a row position some number of rows after to the current row. Any column references in the expression argument refer to column values from that later row.\n\n```\nselect\n    class,\n    course,\n    score,\n    rank() over(partition by class, course order by score desc) rk1,\n    LAG(score,2) over(partition by class, course order by score) as lag,\n    lead(score,2) over(partition by class, course order by score) as lead\nfrom test_analy_funs\nwhere course='chinese';\n+--------+----------+--------+------+-------+-------+--+\n| class  |  course  | score  | rk1  |  lag  | lead  |\n+--------+----------+--------+------+-------+-------+--+\n| 201    | chinese  | 28     | 8    | NULL  | 88    |\n| 201    | chinese  | 83     | 7    | NULL  | 89    |\n| 201    | chinese  | 88     | 6    | 28    | 92    |\n| 201    | chinese  | 89     | 5    | 83    | 95    |\n| 201    | chinese  | 92     | 4    | 88    | 99    |\n| 201    | chinese  | 95     | 3    | 89    | 100   |\n| 201    | chinese  | 99     | 2    | 92    | NULL  |\n| 201    | chinese  | 100    | 1    | 95    | NULL  |\n| 202    | chinese  | 28     | 8    | NULL  | 50    |\n| 202    | chinese  | 30     | 7    | NULL  | 78    |\n| 202    | chinese  | 50     | 6    | 28    | 82    |\n| 202    | chinese  | 78     | 5    | 30    | 88    |\n| 202    | chinese  | 82     | 4    | 50    | 92    |\n| 202    | chinese  | 88     | 3    | 78    | 99    |\n| 202    | chinese  | 92     | 2    | 82    | NULL  |\n| 202    | chinese  | 99     | 1    | 88    | NULL  |\n+--------+----------+--------+------+-------+-------+--+\n```\n\n## 第一个或者最后一个值\n\n获取分组集合中的第一个值或者最后一个值，last_value需要加rows between current row and unbounded following\n\nfirst_value(expr) OVER([partition_by_clause] order_by_clause [window_clause])\nReturns the expression value from the first row in the window. The return value is NULL if the input expression is NULL.\n\nlast_value(expr) OVER([partition_by_clause] order_by_clause [window_clause])\nReturns the expression value from the last row in the window. The return value is NULL if the input expression is NULL.\n\n```\nselect\n    stu_id,\n    class,\n    course,\n    score,\n    rank() over(partition by class, course order by score desc) rk1,\n    first_value(score) over(partition by class, course order by score) first,\n    last_value(score) over(partition by class, course order by score) last,\n    last_value(score) over(partition by class, course order by score rows between current row and unbounded following) last\nfrom test_analy_funs\nwhere course='chinese'\norder by class, stu_id;\n\n+---------+--------+----------+--------+------+--------+-------+-------+--+\n| stu_id  | class  |  course  | score  | rk1  | first  | last  | last  |\n+---------+--------+----------+--------+------+--------+-------+-------+--+\n| 1       | 201    | chinese  | 28     | 8    | 28     | 28    | 100   |\n| 2       | 201    | chinese  | 99     | 2    | 28     | 99    | 100   |\n| 3       | 201    | chinese  | 89     | 5    | 28     | 89    | 100   |\n| 4       | 201    | chinese  | 100    | 1    | 28     | 100   | 100   |\n| 5       | 201    | chinese  | 88     | 6    | 28     | 88    | 100   |\n| 6       | 201    | chinese  | 83     | 7    | 28     | 83    | 100   |\n| 7       | 201    | chinese  | 92     | 4    | 28     | 92    | 100   |\n| 8       | 201    | chinese  | 95     | 3    | 28     | 95    | 100   |\n| 9       | 202    | chinese  | 82     | 4    | 28     | 82    | 99    |\n| 10      | 202    | chinese  | 92     | 2    | 28     | 92    | 99    |\n| 11      | 202    | chinese  | 99     | 1    | 28     | 99    | 99    |\n| 12      | 202    | chinese  | 78     | 5    | 28     | 78    | 99    |\n| 13      | 202    | chinese  | 28     | 8    | 28     | 28    | 99    |\n| 14      | 202    | chinese  | 50     | 6    | 28     | 50    | 99    |\n| 15      | 202    | chinese  | 30     | 7    | 28     | 30    | 99    |\n| 16      | 202    | chinese  | 88     | 3    | 28     | 88    | 99    |\n+---------+--------+----------+--------+------+--------+-------+-------+--+\n16 rows selected (25.701 seconds)\n```\n\n\n## 分桶\n\nntile()\n\nNtile 是Hive很强大的一个分析函数。\n可以看成是：它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。\n语法是：\n     ntile (num)  over ([partition_clause]  order_by_clause)  as your_bucket_num\n\n   然后可以根据桶号，选取前或后 n分之几的数据。\n\n```  \nselect \n      stu_id, \n      class ,\n      score,\n      NTILE(5) OVER( partition by class, course order by score desc) AS rn \nfrom test_analy_funs\nwhere course='chinese';\n+---------+--------+--------+-----+--+\n| stu_id  | class  | score  | rn  |\n+---------+--------+--------+-----+--+\n| 4       | 201    | 100    | 1   |\n| 2       | 201    | 99     | 1   |\n| 8       | 201    | 95     | 2   |\n| 7       | 201    | 92     | 2   |\n| 3       | 201    | 89     | 3   |\n| 5       | 201    | 88     | 3   |\n| 6       | 201    | 83     | 4   |\n| 1       | 201    | 28     | 5   |\n| 11      | 202    | 99     | 1   |\n| 10      | 202    | 92     | 1   |\n| 16      | 202    | 88     | 2   |\n| 9       | 202    | 82     | 2   |\n| 12      | 202    | 78     | 3   |\n| 14      | 202    | 50     | 3   |\n| 15      | 202    | 30     | 4   |\n| 13      | 202    | 28     | 5   |\n+---------+--------+--------+-----+--+\n\n```\n\n## 值排序占比\n\ncume_dist()\n\nCUME_DIST 小于等于当前值的行数/分组内总行数\n\n```\nSELECT \nclass,\nstu_id,\nscore,\nCUME_DIST() OVER(partition by class ORDER BY score) AS cm1\nfrom test_analy_funs\nwhere course='chinese';\n\n+--------+---------+--------+--------+--+\n| class  | stu_id  | score  |  cm1   |\n+--------+---------+--------+--------+--+\n| 201    | 1       | 28     | 0.125  |\n| 201    | 6       | 83     | 0.25   |\n| 201    | 5       | 88     | 0.375  |\n| 201    | 3       | 89     | 0.5    |\n| 201    | 7       | 92     | 0.625  |\n| 201    | 8       | 95     | 0.75   |\n| 201    | 2       | 99     | 0.875  |\n| 201    | 4       | 100    | 1.0    |\n| 202    | 13      | 28     | 0.125  |\n| 202    | 15      | 30     | 0.25   |\n| 202    | 14      | 50     | 0.375  |\n| 202    | 12      | 78     | 0.5    |\n| 202    | 9       | 82     | 0.625  |\n| 202    | 16      | 88     | 0.75   |\n| 202    | 10      | 92     | 0.875  |\n| 202    | 11      | 99     | 1.0    |\n+--------+---------+--------+--------+--+\n\n```\n","source":"_posts/Hive窗口函数和分析函数.md","raw":"---\ntitle: Hive窗口函数和分析函数\ndate: 2018-05-13 11:53:26\ntags: \n  - Hive\n  - 函数\n---\n# Hive窗口函数和分析函数\n\n## 创建测试表并插入测试数据\n\n```\ncreate table test_analy_funs (\nstu_id int\n,stu_nm varchar(20)\n,class int\n,course varchar(20)\n,score decimal(6,2));\n\ninsert into table test_analy_funs\nselect 1,'Daniel',201,'maths',85 union all\nselect 2,'Tom',201,'maths',82 union all\nselect 3,'Macal',201,'maths',26 union all\nselect 4,'Lily',201,'maths',99 union all\nselect 5,'Onel',201,'maths',100 union all\nselect 6,'Kebe',201,'maths',67 union all\nselect 7,'Kaka',201,'maths',89 union all\nselect 8,'Piye',201,'maths',93 union all\nselect 9,'Luo',202,'maths',73 union all\nselect 10,'Lufi', 202,'maths',98 union all\nselect 11,'Tony', 202,'maths',100 union all\nselect 12,'Miya', 202,'maths',100 union all\nselect 13,'Joy', 202,'maths',87 union all\nselect 14,'Tong', 202,'maths',79 union all\nselect 15,'Roy', 202,'maths',84 union all\nselect 16,'Erw', 202,'maths',84 union all\nselect 1,'Daniel',201,'english',82 union all\nselect 2,'Tom',201,'english',26 union all\nselect 3,'Macal',201,'english',99 union all\nselect 4,'Lily',201,'english',100 union all\nselect 5,'Onel',201,'english',77 union all\nselect 6,'Kebe',201,'english',89 union all\nselect 7,'Kaka',201,'english',77 union all\nselect 8,'Piye',201,'english',85 union all\nselect 9,'Luo',202,'english',80 union all\nselect 10,'Lufi', 202,'english',100 union all\nselect 11,'Tony', 202,'english',59 union all\nselect 12,'Miya', 202,'english',84 union all\nselect 13,'Joy', 202,'english',82 union all\nselect 14,'Tong', 202,'english',89 union all\nselect 15,'Roy', 202,'english',90 union all\nselect 16,'Erw', 202,'english',95 union all\nselect 1,'Daniel',201,'chinese',28 union all\nselect 2,'Tom',201,'chinese',99 union all\nselect 3,'Macal',201,'chinese',89 union all\nselect 4,'Lily',201,'chinese',100 union all\nselect 5,'Onel',201,'chinese',88 union all\nselect 6,'Kebe',201,'chinese',83 union all\nselect 7,'Kaka',201,'chinese',92 union all\nselect 8,'Piye',201,'chinese',95 union all\nselect 9,'Luo',202,'chinese',82 union all\nselect 10,'Lufi', 202,'chinese',92 union all\nselect 11,'Tony', 202,'chinese',99 union all\nselect 12,'Miya', 202,'chinese',78 union all\nselect 13,'Joy', 202,'chinese',28 union all\nselect 14,'Tong', 202,'chinese',50 union all\nselect 15,'Roy', 202,'chinese',30 union all\nselect 16,'Erw', 202,'chinese',88 \n\n\n\n```\n\n<!-- more -->\n## 排序\n\ndense_rank() OVER([partition_by_clause] order_by_clause)\n返回从1开始的升序序列，根据不同的order by的字段生成不同的值，如果值一样则排序结果也一样。\nReturns an ascending sequence of integers, starting with 1. The output sequence produces duplicate integers for duplicate values of the ORDER BY expressions.\n\nrank() OVER([partition_by_clause] order_by_clause)\n返回从1开始的升序序列，根据不同的order by的字段生成不同的值，如果值一样则排序结果也一样。然后序列值会根据实际的数值增加。\nReturns an ascending sequence of integers, starting with 1. The output sequence produces duplicate integers for duplicate values of the ORDER BY expressions. After generating duplicate output values for the \"tied\" input values, the function increments the sequence by the number of tied values.\n\nrow_number() OVER([partition_by_clause] order_by_clause)\n返回从1开始的升序序列，根据不同的order by的字段生成不同的值，即使值一样，排序结果也不一样。\nReturns an ascending sequence of integers, starting with 1. Starts the sequence over for each group produced by the PARTITIONED BY clause. The output sequence includes different values for duplicate input values. Therefore, the sequence never contains any duplicates or gaps, regardless of duplicate input values.\n\n```\nselect\n   stu_nm, \n   class,\n   course,\n   score,\n   rank() over(partition by class, course order by score desc) rk1,\n   dense_rank() over(partition by class, course order by score desc) rk2,\n   row_number() over(partition by class, course order by score desc) rk3,\n   percent_rank() over(partition by class, course order by score desc)\nfrom test_analy_funs;\n\n+---------+--------+----------+--------+------+------+------+----------------------+--+\n| stu_nm  | class  |  course  | score  | rk1  | rk2  | rk3  |        _wcol3        |\n+---------+--------+----------+--------+------+------+------+----------------------+--+\n| Lily    | 201    | english  | 100    | 1    | 1    | 1    | 0.0                  |\n| Macal   | 201    | english  | 99     | 2    | 2    | 2    | 0.14285714285714285  |\n| Kebe    | 201    | english  | 89     | 3    | 3    | 3    | 0.2857142857142857   |\n| Piye    | 201    | english  | 85     | 4    | 4    | 4    | 0.42857142857142855  |\n| Daniel  | 201    | english  | 82     | 5    | 5    | 5    | 0.5714285714285714   |\n| Onel    | 201    | english  | 77     | 6    | 6    | 6    | 0.7142857142857143   |\n| Kaka    | 201    | english  | 77     | 6    | 6    | 7    | 0.7142857142857143   |\n| Tom     | 201    | english  | 26     | 8    | 7    | 8    | 1.0                  |\n\n| Tony    | 202    | maths    | 100    | 1    | 1    | 1    | 0.0                  |\n| Miya    | 202    | maths    | 100    | 1    | 1    | 2    | 0.0                  |\n| Lufi    | 202    | maths    | 98     | 3    | 2    | 3    | 0.2857142857142857   |\n| Joy     | 202    | maths    | 87     | 4    | 3    | 4    | 0.42857142857142855  |\n| Roy     | 202    | maths    | 84     | 5    | 4    | 5    | 0.5714285714285714   |\n| Erw     | 202    | maths    | 84     | 5    | 4    | 6    | 0.5714285714285714   |\n| Tong    | 202    | maths    | 79     | 7    | 5    | 7    | 0.8571428571428571   |\n| Luo     | 202    | maths    | 73     | 8    | 6    | 8    | 1.0                  |\n+---------+--------+----------+--------+------+------+------+----------------------+--+\n\n```\n## 差值\n\nlag(expr [, offset] [, default]) OVER ([partition_by_clause] order_by_clause)\nThis function returns the value of an expression using column values from a preceding row. You specify an integer offset, which designates a row position some number of rows previous to the current row. Any column references in the expression argument refer to column values from that prior row.\n\nlead(expr [, offset] [, default]) OVER([partition_by_clause] order_by_clause)\nThis function returns the value of an expression using column values from a following row. You specify an integer offset, which designates a row position some number of rows after to the current row. Any column references in the expression argument refer to column values from that later row.\n\n```\nselect\n    class,\n    course,\n    score,\n    rank() over(partition by class, course order by score desc) rk1,\n    LAG(score,2) over(partition by class, course order by score) as lag,\n    lead(score,2) over(partition by class, course order by score) as lead\nfrom test_analy_funs\nwhere course='chinese';\n+--------+----------+--------+------+-------+-------+--+\n| class  |  course  | score  | rk1  |  lag  | lead  |\n+--------+----------+--------+------+-------+-------+--+\n| 201    | chinese  | 28     | 8    | NULL  | 88    |\n| 201    | chinese  | 83     | 7    | NULL  | 89    |\n| 201    | chinese  | 88     | 6    | 28    | 92    |\n| 201    | chinese  | 89     | 5    | 83    | 95    |\n| 201    | chinese  | 92     | 4    | 88    | 99    |\n| 201    | chinese  | 95     | 3    | 89    | 100   |\n| 201    | chinese  | 99     | 2    | 92    | NULL  |\n| 201    | chinese  | 100    | 1    | 95    | NULL  |\n| 202    | chinese  | 28     | 8    | NULL  | 50    |\n| 202    | chinese  | 30     | 7    | NULL  | 78    |\n| 202    | chinese  | 50     | 6    | 28    | 82    |\n| 202    | chinese  | 78     | 5    | 30    | 88    |\n| 202    | chinese  | 82     | 4    | 50    | 92    |\n| 202    | chinese  | 88     | 3    | 78    | 99    |\n| 202    | chinese  | 92     | 2    | 82    | NULL  |\n| 202    | chinese  | 99     | 1    | 88    | NULL  |\n+--------+----------+--------+------+-------+-------+--+\n```\n\n## 第一个或者最后一个值\n\n获取分组集合中的第一个值或者最后一个值，last_value需要加rows between current row and unbounded following\n\nfirst_value(expr) OVER([partition_by_clause] order_by_clause [window_clause])\nReturns the expression value from the first row in the window. The return value is NULL if the input expression is NULL.\n\nlast_value(expr) OVER([partition_by_clause] order_by_clause [window_clause])\nReturns the expression value from the last row in the window. The return value is NULL if the input expression is NULL.\n\n```\nselect\n    stu_id,\n    class,\n    course,\n    score,\n    rank() over(partition by class, course order by score desc) rk1,\n    first_value(score) over(partition by class, course order by score) first,\n    last_value(score) over(partition by class, course order by score) last,\n    last_value(score) over(partition by class, course order by score rows between current row and unbounded following) last\nfrom test_analy_funs\nwhere course='chinese'\norder by class, stu_id;\n\n+---------+--------+----------+--------+------+--------+-------+-------+--+\n| stu_id  | class  |  course  | score  | rk1  | first  | last  | last  |\n+---------+--------+----------+--------+------+--------+-------+-------+--+\n| 1       | 201    | chinese  | 28     | 8    | 28     | 28    | 100   |\n| 2       | 201    | chinese  | 99     | 2    | 28     | 99    | 100   |\n| 3       | 201    | chinese  | 89     | 5    | 28     | 89    | 100   |\n| 4       | 201    | chinese  | 100    | 1    | 28     | 100   | 100   |\n| 5       | 201    | chinese  | 88     | 6    | 28     | 88    | 100   |\n| 6       | 201    | chinese  | 83     | 7    | 28     | 83    | 100   |\n| 7       | 201    | chinese  | 92     | 4    | 28     | 92    | 100   |\n| 8       | 201    | chinese  | 95     | 3    | 28     | 95    | 100   |\n| 9       | 202    | chinese  | 82     | 4    | 28     | 82    | 99    |\n| 10      | 202    | chinese  | 92     | 2    | 28     | 92    | 99    |\n| 11      | 202    | chinese  | 99     | 1    | 28     | 99    | 99    |\n| 12      | 202    | chinese  | 78     | 5    | 28     | 78    | 99    |\n| 13      | 202    | chinese  | 28     | 8    | 28     | 28    | 99    |\n| 14      | 202    | chinese  | 50     | 6    | 28     | 50    | 99    |\n| 15      | 202    | chinese  | 30     | 7    | 28     | 30    | 99    |\n| 16      | 202    | chinese  | 88     | 3    | 28     | 88    | 99    |\n+---------+--------+----------+--------+------+--------+-------+-------+--+\n16 rows selected (25.701 seconds)\n```\n\n\n## 分桶\n\nntile()\n\nNtile 是Hive很强大的一个分析函数。\n可以看成是：它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。\n语法是：\n     ntile (num)  over ([partition_clause]  order_by_clause)  as your_bucket_num\n\n   然后可以根据桶号，选取前或后 n分之几的数据。\n\n```  \nselect \n      stu_id, \n      class ,\n      score,\n      NTILE(5) OVER( partition by class, course order by score desc) AS rn \nfrom test_analy_funs\nwhere course='chinese';\n+---------+--------+--------+-----+--+\n| stu_id  | class  | score  | rn  |\n+---------+--------+--------+-----+--+\n| 4       | 201    | 100    | 1   |\n| 2       | 201    | 99     | 1   |\n| 8       | 201    | 95     | 2   |\n| 7       | 201    | 92     | 2   |\n| 3       | 201    | 89     | 3   |\n| 5       | 201    | 88     | 3   |\n| 6       | 201    | 83     | 4   |\n| 1       | 201    | 28     | 5   |\n| 11      | 202    | 99     | 1   |\n| 10      | 202    | 92     | 1   |\n| 16      | 202    | 88     | 2   |\n| 9       | 202    | 82     | 2   |\n| 12      | 202    | 78     | 3   |\n| 14      | 202    | 50     | 3   |\n| 15      | 202    | 30     | 4   |\n| 13      | 202    | 28     | 5   |\n+---------+--------+--------+-----+--+\n\n```\n\n## 值排序占比\n\ncume_dist()\n\nCUME_DIST 小于等于当前值的行数/分组内总行数\n\n```\nSELECT \nclass,\nstu_id,\nscore,\nCUME_DIST() OVER(partition by class ORDER BY score) AS cm1\nfrom test_analy_funs\nwhere course='chinese';\n\n+--------+---------+--------+--------+--+\n| class  | stu_id  | score  |  cm1   |\n+--------+---------+--------+--------+--+\n| 201    | 1       | 28     | 0.125  |\n| 201    | 6       | 83     | 0.25   |\n| 201    | 5       | 88     | 0.375  |\n| 201    | 3       | 89     | 0.5    |\n| 201    | 7       | 92     | 0.625  |\n| 201    | 8       | 95     | 0.75   |\n| 201    | 2       | 99     | 0.875  |\n| 201    | 4       | 100    | 1.0    |\n| 202    | 13      | 28     | 0.125  |\n| 202    | 15      | 30     | 0.25   |\n| 202    | 14      | 50     | 0.375  |\n| 202    | 12      | 78     | 0.5    |\n| 202    | 9       | 82     | 0.625  |\n| 202    | 16      | 88     | 0.75   |\n| 202    | 10      | 92     | 0.875  |\n| 202    | 11      | 99     | 1.0    |\n+--------+---------+--------+--------+--+\n\n```\n","slug":"Hive窗口函数和分析函数","published":1,"updated":"2018-09-14T01:24:09.678Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzt0010inamm9ywsk94","content":"<h1 id=\"Hive窗口函数和分析函数\"><a href=\"#Hive窗口函数和分析函数\" class=\"headerlink\" title=\"Hive窗口函数和分析函数\"></a>Hive窗口函数和分析函数</h1><h2 id=\"创建测试表并插入测试数据\"><a href=\"#创建测试表并插入测试数据\" class=\"headerlink\" title=\"创建测试表并插入测试数据\"></a>创建测试表并插入测试数据</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create table test_analy_funs (</span><br><span class=\"line\">stu_id int</span><br><span class=\"line\">,stu_nm varchar(20)</span><br><span class=\"line\">,class int</span><br><span class=\"line\">,course varchar(20)</span><br><span class=\"line\">,score decimal(6,2));</span><br><span class=\"line\"></span><br><span class=\"line\">insert into table test_analy_funs</span><br><span class=\"line\">select 1,&apos;Daniel&apos;,201,&apos;maths&apos;,85 union all</span><br><span class=\"line\">select 2,&apos;Tom&apos;,201,&apos;maths&apos;,82 union all</span><br><span class=\"line\">select 3,&apos;Macal&apos;,201,&apos;maths&apos;,26 union all</span><br><span class=\"line\">select 4,&apos;Lily&apos;,201,&apos;maths&apos;,99 union all</span><br><span class=\"line\">select 5,&apos;Onel&apos;,201,&apos;maths&apos;,100 union all</span><br><span class=\"line\">select 6,&apos;Kebe&apos;,201,&apos;maths&apos;,67 union all</span><br><span class=\"line\">select 7,&apos;Kaka&apos;,201,&apos;maths&apos;,89 union all</span><br><span class=\"line\">select 8,&apos;Piye&apos;,201,&apos;maths&apos;,93 union all</span><br><span class=\"line\">select 9,&apos;Luo&apos;,202,&apos;maths&apos;,73 union all</span><br><span class=\"line\">select 10,&apos;Lufi&apos;, 202,&apos;maths&apos;,98 union all</span><br><span class=\"line\">select 11,&apos;Tony&apos;, 202,&apos;maths&apos;,100 union all</span><br><span class=\"line\">select 12,&apos;Miya&apos;, 202,&apos;maths&apos;,100 union all</span><br><span class=\"line\">select 13,&apos;Joy&apos;, 202,&apos;maths&apos;,87 union all</span><br><span class=\"line\">select 14,&apos;Tong&apos;, 202,&apos;maths&apos;,79 union all</span><br><span class=\"line\">select 15,&apos;Roy&apos;, 202,&apos;maths&apos;,84 union all</span><br><span class=\"line\">select 16,&apos;Erw&apos;, 202,&apos;maths&apos;,84 union all</span><br><span class=\"line\">select 1,&apos;Daniel&apos;,201,&apos;english&apos;,82 union all</span><br><span class=\"line\">select 2,&apos;Tom&apos;,201,&apos;english&apos;,26 union all</span><br><span class=\"line\">select 3,&apos;Macal&apos;,201,&apos;english&apos;,99 union all</span><br><span class=\"line\">select 4,&apos;Lily&apos;,201,&apos;english&apos;,100 union all</span><br><span class=\"line\">select 5,&apos;Onel&apos;,201,&apos;english&apos;,77 union all</span><br><span class=\"line\">select 6,&apos;Kebe&apos;,201,&apos;english&apos;,89 union all</span><br><span class=\"line\">select 7,&apos;Kaka&apos;,201,&apos;english&apos;,77 union all</span><br><span class=\"line\">select 8,&apos;Piye&apos;,201,&apos;english&apos;,85 union all</span><br><span class=\"line\">select 9,&apos;Luo&apos;,202,&apos;english&apos;,80 union all</span><br><span class=\"line\">select 10,&apos;Lufi&apos;, 202,&apos;english&apos;,100 union all</span><br><span class=\"line\">select 11,&apos;Tony&apos;, 202,&apos;english&apos;,59 union all</span><br><span class=\"line\">select 12,&apos;Miya&apos;, 202,&apos;english&apos;,84 union all</span><br><span class=\"line\">select 13,&apos;Joy&apos;, 202,&apos;english&apos;,82 union all</span><br><span class=\"line\">select 14,&apos;Tong&apos;, 202,&apos;english&apos;,89 union all</span><br><span class=\"line\">select 15,&apos;Roy&apos;, 202,&apos;english&apos;,90 union all</span><br><span class=\"line\">select 16,&apos;Erw&apos;, 202,&apos;english&apos;,95 union all</span><br><span class=\"line\">select 1,&apos;Daniel&apos;,201,&apos;chinese&apos;,28 union all</span><br><span class=\"line\">select 2,&apos;Tom&apos;,201,&apos;chinese&apos;,99 union all</span><br><span class=\"line\">select 3,&apos;Macal&apos;,201,&apos;chinese&apos;,89 union all</span><br><span class=\"line\">select 4,&apos;Lily&apos;,201,&apos;chinese&apos;,100 union all</span><br><span class=\"line\">select 5,&apos;Onel&apos;,201,&apos;chinese&apos;,88 union all</span><br><span class=\"line\">select 6,&apos;Kebe&apos;,201,&apos;chinese&apos;,83 union all</span><br><span class=\"line\">select 7,&apos;Kaka&apos;,201,&apos;chinese&apos;,92 union all</span><br><span class=\"line\">select 8,&apos;Piye&apos;,201,&apos;chinese&apos;,95 union all</span><br><span class=\"line\">select 9,&apos;Luo&apos;,202,&apos;chinese&apos;,82 union all</span><br><span class=\"line\">select 10,&apos;Lufi&apos;, 202,&apos;chinese&apos;,92 union all</span><br><span class=\"line\">select 11,&apos;Tony&apos;, 202,&apos;chinese&apos;,99 union all</span><br><span class=\"line\">select 12,&apos;Miya&apos;, 202,&apos;chinese&apos;,78 union all</span><br><span class=\"line\">select 13,&apos;Joy&apos;, 202,&apos;chinese&apos;,28 union all</span><br><span class=\"line\">select 14,&apos;Tong&apos;, 202,&apos;chinese&apos;,50 union all</span><br><span class=\"line\">select 15,&apos;Roy&apos;, 202,&apos;chinese&apos;,30 union all</span><br><span class=\"line\">select 16,&apos;Erw&apos;, 202,&apos;chinese&apos;,88</span><br></pre></td></tr></table></figure>\n<a id=\"more\"></a>\n<h2 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h2><p>dense_rank() OVER([partition_by_clause] order_by_clause)<br>返回从1开始的升序序列，根据不同的order by的字段生成不同的值，如果值一样则排序结果也一样。<br>Returns an ascending sequence of integers, starting with 1. The output sequence produces duplicate integers for duplicate values of the ORDER BY expressions.</p>\n<p>rank() OVER([partition_by_clause] order_by_clause)<br>返回从1开始的升序序列，根据不同的order by的字段生成不同的值，如果值一样则排序结果也一样。然后序列值会根据实际的数值增加。<br>Returns an ascending sequence of integers, starting with 1. The output sequence produces duplicate integers for duplicate values of the ORDER BY expressions. After generating duplicate output values for the “tied” input values, the function increments the sequence by the number of tied values.</p>\n<p>row_number() OVER([partition_by_clause] order_by_clause)<br>返回从1开始的升序序列，根据不同的order by的字段生成不同的值，即使值一样，排序结果也不一样。<br>Returns an ascending sequence of integers, starting with 1. Starts the sequence over for each group produced by the PARTITIONED BY clause. The output sequence includes different values for duplicate input values. Therefore, the sequence never contains any duplicates or gaps, regardless of duplicate input values.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select</span><br><span class=\"line\">   stu_nm, </span><br><span class=\"line\">   class,</span><br><span class=\"line\">   course,</span><br><span class=\"line\">   score,</span><br><span class=\"line\">   rank() over(partition by class, course order by score desc) rk1,</span><br><span class=\"line\">   dense_rank() over(partition by class, course order by score desc) rk2,</span><br><span class=\"line\">   row_number() over(partition by class, course order by score desc) rk3,</span><br><span class=\"line\">   percent_rank() over(partition by class, course order by score desc)</span><br><span class=\"line\">from test_analy_funs;</span><br><span class=\"line\"></span><br><span class=\"line\">+---------+--------+----------+--------+------+------+------+----------------------+--+</span><br><span class=\"line\">| stu_nm  | class  |  course  | score  | rk1  | rk2  | rk3  |        _wcol3        |</span><br><span class=\"line\">+---------+--------+----------+--------+------+------+------+----------------------+--+</span><br><span class=\"line\">| Lily    | 201    | english  | 100    | 1    | 1    | 1    | 0.0                  |</span><br><span class=\"line\">| Macal   | 201    | english  | 99     | 2    | 2    | 2    | 0.14285714285714285  |</span><br><span class=\"line\">| Kebe    | 201    | english  | 89     | 3    | 3    | 3    | 0.2857142857142857   |</span><br><span class=\"line\">| Piye    | 201    | english  | 85     | 4    | 4    | 4    | 0.42857142857142855  |</span><br><span class=\"line\">| Daniel  | 201    | english  | 82     | 5    | 5    | 5    | 0.5714285714285714   |</span><br><span class=\"line\">| Onel    | 201    | english  | 77     | 6    | 6    | 6    | 0.7142857142857143   |</span><br><span class=\"line\">| Kaka    | 201    | english  | 77     | 6    | 6    | 7    | 0.7142857142857143   |</span><br><span class=\"line\">| Tom     | 201    | english  | 26     | 8    | 7    | 8    | 1.0                  |</span><br><span class=\"line\"></span><br><span class=\"line\">| Tony    | 202    | maths    | 100    | 1    | 1    | 1    | 0.0                  |</span><br><span class=\"line\">| Miya    | 202    | maths    | 100    | 1    | 1    | 2    | 0.0                  |</span><br><span class=\"line\">| Lufi    | 202    | maths    | 98     | 3    | 2    | 3    | 0.2857142857142857   |</span><br><span class=\"line\">| Joy     | 202    | maths    | 87     | 4    | 3    | 4    | 0.42857142857142855  |</span><br><span class=\"line\">| Roy     | 202    | maths    | 84     | 5    | 4    | 5    | 0.5714285714285714   |</span><br><span class=\"line\">| Erw     | 202    | maths    | 84     | 5    | 4    | 6    | 0.5714285714285714   |</span><br><span class=\"line\">| Tong    | 202    | maths    | 79     | 7    | 5    | 7    | 0.8571428571428571   |</span><br><span class=\"line\">| Luo     | 202    | maths    | 73     | 8    | 6    | 8    | 1.0                  |</span><br><span class=\"line\">+---------+--------+----------+--------+------+------+------+----------------------+--+</span><br></pre></td></tr></table></figure>\n<h2 id=\"差值\"><a href=\"#差值\" class=\"headerlink\" title=\"差值\"></a>差值</h2><p>lag(expr [, offset] [, default]) OVER ([partition_by_clause] order_by_clause)<br>This function returns the value of an expression using column values from a preceding row. You specify an integer offset, which designates a row position some number of rows previous to the current row. Any column references in the expression argument refer to column values from that prior row.</p>\n<p>lead(expr [, offset] [, default]) OVER([partition_by_clause] order_by_clause)<br>This function returns the value of an expression using column values from a following row. You specify an integer offset, which designates a row position some number of rows after to the current row. Any column references in the expression argument refer to column values from that later row.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select</span><br><span class=\"line\">    class,</span><br><span class=\"line\">    course,</span><br><span class=\"line\">    score,</span><br><span class=\"line\">    rank() over(partition by class, course order by score desc) rk1,</span><br><span class=\"line\">    LAG(score,2) over(partition by class, course order by score) as lag,</span><br><span class=\"line\">    lead(score,2) over(partition by class, course order by score) as lead</span><br><span class=\"line\">from test_analy_funs</span><br><span class=\"line\">where course=&apos;chinese&apos;;</span><br><span class=\"line\">+--------+----------+--------+------+-------+-------+--+</span><br><span class=\"line\">| class  |  course  | score  | rk1  |  lag  | lead  |</span><br><span class=\"line\">+--------+----------+--------+------+-------+-------+--+</span><br><span class=\"line\">| 201    | chinese  | 28     | 8    | NULL  | 88    |</span><br><span class=\"line\">| 201    | chinese  | 83     | 7    | NULL  | 89    |</span><br><span class=\"line\">| 201    | chinese  | 88     | 6    | 28    | 92    |</span><br><span class=\"line\">| 201    | chinese  | 89     | 5    | 83    | 95    |</span><br><span class=\"line\">| 201    | chinese  | 92     | 4    | 88    | 99    |</span><br><span class=\"line\">| 201    | chinese  | 95     | 3    | 89    | 100   |</span><br><span class=\"line\">| 201    | chinese  | 99     | 2    | 92    | NULL  |</span><br><span class=\"line\">| 201    | chinese  | 100    | 1    | 95    | NULL  |</span><br><span class=\"line\">| 202    | chinese  | 28     | 8    | NULL  | 50    |</span><br><span class=\"line\">| 202    | chinese  | 30     | 7    | NULL  | 78    |</span><br><span class=\"line\">| 202    | chinese  | 50     | 6    | 28    | 82    |</span><br><span class=\"line\">| 202    | chinese  | 78     | 5    | 30    | 88    |</span><br><span class=\"line\">| 202    | chinese  | 82     | 4    | 50    | 92    |</span><br><span class=\"line\">| 202    | chinese  | 88     | 3    | 78    | 99    |</span><br><span class=\"line\">| 202    | chinese  | 92     | 2    | 82    | NULL  |</span><br><span class=\"line\">| 202    | chinese  | 99     | 1    | 88    | NULL  |</span><br><span class=\"line\">+--------+----------+--------+------+-------+-------+--+</span><br></pre></td></tr></table></figure>\n<h2 id=\"第一个或者最后一个值\"><a href=\"#第一个或者最后一个值\" class=\"headerlink\" title=\"第一个或者最后一个值\"></a>第一个或者最后一个值</h2><p>获取分组集合中的第一个值或者最后一个值，last_value需要加rows between current row and unbounded following</p>\n<p>first_value(expr) OVER([partition_by_clause] order_by_clause [window_clause])<br>Returns the expression value from the first row in the window. The return value is NULL if the input expression is NULL.</p>\n<p>last_value(expr) OVER([partition_by_clause] order_by_clause [window_clause])<br>Returns the expression value from the last row in the window. The return value is NULL if the input expression is NULL.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select</span><br><span class=\"line\">    stu_id,</span><br><span class=\"line\">    class,</span><br><span class=\"line\">    course,</span><br><span class=\"line\">    score,</span><br><span class=\"line\">    rank() over(partition by class, course order by score desc) rk1,</span><br><span class=\"line\">    first_value(score) over(partition by class, course order by score) first,</span><br><span class=\"line\">    last_value(score) over(partition by class, course order by score) last,</span><br><span class=\"line\">    last_value(score) over(partition by class, course order by score rows between current row and unbounded following) last</span><br><span class=\"line\">from test_analy_funs</span><br><span class=\"line\">where course=&apos;chinese&apos;</span><br><span class=\"line\">order by class, stu_id;</span><br><span class=\"line\"></span><br><span class=\"line\">+---------+--------+----------+--------+------+--------+-------+-------+--+</span><br><span class=\"line\">| stu_id  | class  |  course  | score  | rk1  | first  | last  | last  |</span><br><span class=\"line\">+---------+--------+----------+--------+------+--------+-------+-------+--+</span><br><span class=\"line\">| 1       | 201    | chinese  | 28     | 8    | 28     | 28    | 100   |</span><br><span class=\"line\">| 2       | 201    | chinese  | 99     | 2    | 28     | 99    | 100   |</span><br><span class=\"line\">| 3       | 201    | chinese  | 89     | 5    | 28     | 89    | 100   |</span><br><span class=\"line\">| 4       | 201    | chinese  | 100    | 1    | 28     | 100   | 100   |</span><br><span class=\"line\">| 5       | 201    | chinese  | 88     | 6    | 28     | 88    | 100   |</span><br><span class=\"line\">| 6       | 201    | chinese  | 83     | 7    | 28     | 83    | 100   |</span><br><span class=\"line\">| 7       | 201    | chinese  | 92     | 4    | 28     | 92    | 100   |</span><br><span class=\"line\">| 8       | 201    | chinese  | 95     | 3    | 28     | 95    | 100   |</span><br><span class=\"line\">| 9       | 202    | chinese  | 82     | 4    | 28     | 82    | 99    |</span><br><span class=\"line\">| 10      | 202    | chinese  | 92     | 2    | 28     | 92    | 99    |</span><br><span class=\"line\">| 11      | 202    | chinese  | 99     | 1    | 28     | 99    | 99    |</span><br><span class=\"line\">| 12      | 202    | chinese  | 78     | 5    | 28     | 78    | 99    |</span><br><span class=\"line\">| 13      | 202    | chinese  | 28     | 8    | 28     | 28    | 99    |</span><br><span class=\"line\">| 14      | 202    | chinese  | 50     | 6    | 28     | 50    | 99    |</span><br><span class=\"line\">| 15      | 202    | chinese  | 30     | 7    | 28     | 30    | 99    |</span><br><span class=\"line\">| 16      | 202    | chinese  | 88     | 3    | 28     | 88    | 99    |</span><br><span class=\"line\">+---------+--------+----------+--------+------+--------+-------+-------+--+</span><br><span class=\"line\">16 rows selected (25.701 seconds)</span><br></pre></td></tr></table></figure>\n<h2 id=\"分桶\"><a href=\"#分桶\" class=\"headerlink\" title=\"分桶\"></a>分桶</h2><p>ntile()</p>\n<p>Ntile 是Hive很强大的一个分析函数。<br>可以看成是：它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。<br>语法是：<br>     ntile (num)  over ([partition_clause]  order_by_clause)  as your_bucket_num</p>\n<p>   然后可以根据桶号，选取前或后 n分之几的数据。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select </span><br><span class=\"line\">      stu_id, </span><br><span class=\"line\">      class ,</span><br><span class=\"line\">      score,</span><br><span class=\"line\">      NTILE(5) OVER( partition by class, course order by score desc) AS rn </span><br><span class=\"line\">from test_analy_funs</span><br><span class=\"line\">where course=&apos;chinese&apos;;</span><br><span class=\"line\">+---------+--------+--------+-----+--+</span><br><span class=\"line\">| stu_id  | class  | score  | rn  |</span><br><span class=\"line\">+---------+--------+--------+-----+--+</span><br><span class=\"line\">| 4       | 201    | 100    | 1   |</span><br><span class=\"line\">| 2       | 201    | 99     | 1   |</span><br><span class=\"line\">| 8       | 201    | 95     | 2   |</span><br><span class=\"line\">| 7       | 201    | 92     | 2   |</span><br><span class=\"line\">| 3       | 201    | 89     | 3   |</span><br><span class=\"line\">| 5       | 201    | 88     | 3   |</span><br><span class=\"line\">| 6       | 201    | 83     | 4   |</span><br><span class=\"line\">| 1       | 201    | 28     | 5   |</span><br><span class=\"line\">| 11      | 202    | 99     | 1   |</span><br><span class=\"line\">| 10      | 202    | 92     | 1   |</span><br><span class=\"line\">| 16      | 202    | 88     | 2   |</span><br><span class=\"line\">| 9       | 202    | 82     | 2   |</span><br><span class=\"line\">| 12      | 202    | 78     | 3   |</span><br><span class=\"line\">| 14      | 202    | 50     | 3   |</span><br><span class=\"line\">| 15      | 202    | 30     | 4   |</span><br><span class=\"line\">| 13      | 202    | 28     | 5   |</span><br><span class=\"line\">+---------+--------+--------+-----+--+</span><br></pre></td></tr></table></figure>\n<h2 id=\"值排序占比\"><a href=\"#值排序占比\" class=\"headerlink\" title=\"值排序占比\"></a>值排序占比</h2><p>cume_dist()</p>\n<p>CUME_DIST 小于等于当前值的行数/分组内总行数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">class,</span><br><span class=\"line\">stu_id,</span><br><span class=\"line\">score,</span><br><span class=\"line\">CUME_DIST() OVER(partition by class ORDER BY score) AS cm1</span><br><span class=\"line\">from test_analy_funs</span><br><span class=\"line\">where course=&apos;chinese&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">+--------+---------+--------+--------+--+</span><br><span class=\"line\">| class  | stu_id  | score  |  cm1   |</span><br><span class=\"line\">+--------+---------+--------+--------+--+</span><br><span class=\"line\">| 201    | 1       | 28     | 0.125  |</span><br><span class=\"line\">| 201    | 6       | 83     | 0.25   |</span><br><span class=\"line\">| 201    | 5       | 88     | 0.375  |</span><br><span class=\"line\">| 201    | 3       | 89     | 0.5    |</span><br><span class=\"line\">| 201    | 7       | 92     | 0.625  |</span><br><span class=\"line\">| 201    | 8       | 95     | 0.75   |</span><br><span class=\"line\">| 201    | 2       | 99     | 0.875  |</span><br><span class=\"line\">| 201    | 4       | 100    | 1.0    |</span><br><span class=\"line\">| 202    | 13      | 28     | 0.125  |</span><br><span class=\"line\">| 202    | 15      | 30     | 0.25   |</span><br><span class=\"line\">| 202    | 14      | 50     | 0.375  |</span><br><span class=\"line\">| 202    | 12      | 78     | 0.5    |</span><br><span class=\"line\">| 202    | 9       | 82     | 0.625  |</span><br><span class=\"line\">| 202    | 16      | 88     | 0.75   |</span><br><span class=\"line\">| 202    | 10      | 92     | 0.875  |</span><br><span class=\"line\">| 202    | 11      | 99     | 1.0    |</span><br><span class=\"line\">+--------+---------+--------+--------+--+</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<h1 id=\"Hive窗口函数和分析函数\"><a href=\"#Hive窗口函数和分析函数\" class=\"headerlink\" title=\"Hive窗口函数和分析函数\"></a>Hive窗口函数和分析函数</h1><h2 id=\"创建测试表并插入测试数据\"><a href=\"#创建测试表并插入测试数据\" class=\"headerlink\" title=\"创建测试表并插入测试数据\"></a>创建测试表并插入测试数据</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create table test_analy_funs (</span><br><span class=\"line\">stu_id int</span><br><span class=\"line\">,stu_nm varchar(20)</span><br><span class=\"line\">,class int</span><br><span class=\"line\">,course varchar(20)</span><br><span class=\"line\">,score decimal(6,2));</span><br><span class=\"line\"></span><br><span class=\"line\">insert into table test_analy_funs</span><br><span class=\"line\">select 1,&apos;Daniel&apos;,201,&apos;maths&apos;,85 union all</span><br><span class=\"line\">select 2,&apos;Tom&apos;,201,&apos;maths&apos;,82 union all</span><br><span class=\"line\">select 3,&apos;Macal&apos;,201,&apos;maths&apos;,26 union all</span><br><span class=\"line\">select 4,&apos;Lily&apos;,201,&apos;maths&apos;,99 union all</span><br><span class=\"line\">select 5,&apos;Onel&apos;,201,&apos;maths&apos;,100 union all</span><br><span class=\"line\">select 6,&apos;Kebe&apos;,201,&apos;maths&apos;,67 union all</span><br><span class=\"line\">select 7,&apos;Kaka&apos;,201,&apos;maths&apos;,89 union all</span><br><span class=\"line\">select 8,&apos;Piye&apos;,201,&apos;maths&apos;,93 union all</span><br><span class=\"line\">select 9,&apos;Luo&apos;,202,&apos;maths&apos;,73 union all</span><br><span class=\"line\">select 10,&apos;Lufi&apos;, 202,&apos;maths&apos;,98 union all</span><br><span class=\"line\">select 11,&apos;Tony&apos;, 202,&apos;maths&apos;,100 union all</span><br><span class=\"line\">select 12,&apos;Miya&apos;, 202,&apos;maths&apos;,100 union all</span><br><span class=\"line\">select 13,&apos;Joy&apos;, 202,&apos;maths&apos;,87 union all</span><br><span class=\"line\">select 14,&apos;Tong&apos;, 202,&apos;maths&apos;,79 union all</span><br><span class=\"line\">select 15,&apos;Roy&apos;, 202,&apos;maths&apos;,84 union all</span><br><span class=\"line\">select 16,&apos;Erw&apos;, 202,&apos;maths&apos;,84 union all</span><br><span class=\"line\">select 1,&apos;Daniel&apos;,201,&apos;english&apos;,82 union all</span><br><span class=\"line\">select 2,&apos;Tom&apos;,201,&apos;english&apos;,26 union all</span><br><span class=\"line\">select 3,&apos;Macal&apos;,201,&apos;english&apos;,99 union all</span><br><span class=\"line\">select 4,&apos;Lily&apos;,201,&apos;english&apos;,100 union all</span><br><span class=\"line\">select 5,&apos;Onel&apos;,201,&apos;english&apos;,77 union all</span><br><span class=\"line\">select 6,&apos;Kebe&apos;,201,&apos;english&apos;,89 union all</span><br><span class=\"line\">select 7,&apos;Kaka&apos;,201,&apos;english&apos;,77 union all</span><br><span class=\"line\">select 8,&apos;Piye&apos;,201,&apos;english&apos;,85 union all</span><br><span class=\"line\">select 9,&apos;Luo&apos;,202,&apos;english&apos;,80 union all</span><br><span class=\"line\">select 10,&apos;Lufi&apos;, 202,&apos;english&apos;,100 union all</span><br><span class=\"line\">select 11,&apos;Tony&apos;, 202,&apos;english&apos;,59 union all</span><br><span class=\"line\">select 12,&apos;Miya&apos;, 202,&apos;english&apos;,84 union all</span><br><span class=\"line\">select 13,&apos;Joy&apos;, 202,&apos;english&apos;,82 union all</span><br><span class=\"line\">select 14,&apos;Tong&apos;, 202,&apos;english&apos;,89 union all</span><br><span class=\"line\">select 15,&apos;Roy&apos;, 202,&apos;english&apos;,90 union all</span><br><span class=\"line\">select 16,&apos;Erw&apos;, 202,&apos;english&apos;,95 union all</span><br><span class=\"line\">select 1,&apos;Daniel&apos;,201,&apos;chinese&apos;,28 union all</span><br><span class=\"line\">select 2,&apos;Tom&apos;,201,&apos;chinese&apos;,99 union all</span><br><span class=\"line\">select 3,&apos;Macal&apos;,201,&apos;chinese&apos;,89 union all</span><br><span class=\"line\">select 4,&apos;Lily&apos;,201,&apos;chinese&apos;,100 union all</span><br><span class=\"line\">select 5,&apos;Onel&apos;,201,&apos;chinese&apos;,88 union all</span><br><span class=\"line\">select 6,&apos;Kebe&apos;,201,&apos;chinese&apos;,83 union all</span><br><span class=\"line\">select 7,&apos;Kaka&apos;,201,&apos;chinese&apos;,92 union all</span><br><span class=\"line\">select 8,&apos;Piye&apos;,201,&apos;chinese&apos;,95 union all</span><br><span class=\"line\">select 9,&apos;Luo&apos;,202,&apos;chinese&apos;,82 union all</span><br><span class=\"line\">select 10,&apos;Lufi&apos;, 202,&apos;chinese&apos;,92 union all</span><br><span class=\"line\">select 11,&apos;Tony&apos;, 202,&apos;chinese&apos;,99 union all</span><br><span class=\"line\">select 12,&apos;Miya&apos;, 202,&apos;chinese&apos;,78 union all</span><br><span class=\"line\">select 13,&apos;Joy&apos;, 202,&apos;chinese&apos;,28 union all</span><br><span class=\"line\">select 14,&apos;Tong&apos;, 202,&apos;chinese&apos;,50 union all</span><br><span class=\"line\">select 15,&apos;Roy&apos;, 202,&apos;chinese&apos;,30 union all</span><br><span class=\"line\">select 16,&apos;Erw&apos;, 202,&apos;chinese&apos;,88</span><br></pre></td></tr></table></figure>","more":"<h2 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h2><p>dense_rank() OVER([partition_by_clause] order_by_clause)<br>返回从1开始的升序序列，根据不同的order by的字段生成不同的值，如果值一样则排序结果也一样。<br>Returns an ascending sequence of integers, starting with 1. The output sequence produces duplicate integers for duplicate values of the ORDER BY expressions.</p>\n<p>rank() OVER([partition_by_clause] order_by_clause)<br>返回从1开始的升序序列，根据不同的order by的字段生成不同的值，如果值一样则排序结果也一样。然后序列值会根据实际的数值增加。<br>Returns an ascending sequence of integers, starting with 1. The output sequence produces duplicate integers for duplicate values of the ORDER BY expressions. After generating duplicate output values for the “tied” input values, the function increments the sequence by the number of tied values.</p>\n<p>row_number() OVER([partition_by_clause] order_by_clause)<br>返回从1开始的升序序列，根据不同的order by的字段生成不同的值，即使值一样，排序结果也不一样。<br>Returns an ascending sequence of integers, starting with 1. Starts the sequence over for each group produced by the PARTITIONED BY clause. The output sequence includes different values for duplicate input values. Therefore, the sequence never contains any duplicates or gaps, regardless of duplicate input values.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select</span><br><span class=\"line\">   stu_nm, </span><br><span class=\"line\">   class,</span><br><span class=\"line\">   course,</span><br><span class=\"line\">   score,</span><br><span class=\"line\">   rank() over(partition by class, course order by score desc) rk1,</span><br><span class=\"line\">   dense_rank() over(partition by class, course order by score desc) rk2,</span><br><span class=\"line\">   row_number() over(partition by class, course order by score desc) rk3,</span><br><span class=\"line\">   percent_rank() over(partition by class, course order by score desc)</span><br><span class=\"line\">from test_analy_funs;</span><br><span class=\"line\"></span><br><span class=\"line\">+---------+--------+----------+--------+------+------+------+----------------------+--+</span><br><span class=\"line\">| stu_nm  | class  |  course  | score  | rk1  | rk2  | rk3  |        _wcol3        |</span><br><span class=\"line\">+---------+--------+----------+--------+------+------+------+----------------------+--+</span><br><span class=\"line\">| Lily    | 201    | english  | 100    | 1    | 1    | 1    | 0.0                  |</span><br><span class=\"line\">| Macal   | 201    | english  | 99     | 2    | 2    | 2    | 0.14285714285714285  |</span><br><span class=\"line\">| Kebe    | 201    | english  | 89     | 3    | 3    | 3    | 0.2857142857142857   |</span><br><span class=\"line\">| Piye    | 201    | english  | 85     | 4    | 4    | 4    | 0.42857142857142855  |</span><br><span class=\"line\">| Daniel  | 201    | english  | 82     | 5    | 5    | 5    | 0.5714285714285714   |</span><br><span class=\"line\">| Onel    | 201    | english  | 77     | 6    | 6    | 6    | 0.7142857142857143   |</span><br><span class=\"line\">| Kaka    | 201    | english  | 77     | 6    | 6    | 7    | 0.7142857142857143   |</span><br><span class=\"line\">| Tom     | 201    | english  | 26     | 8    | 7    | 8    | 1.0                  |</span><br><span class=\"line\"></span><br><span class=\"line\">| Tony    | 202    | maths    | 100    | 1    | 1    | 1    | 0.0                  |</span><br><span class=\"line\">| Miya    | 202    | maths    | 100    | 1    | 1    | 2    | 0.0                  |</span><br><span class=\"line\">| Lufi    | 202    | maths    | 98     | 3    | 2    | 3    | 0.2857142857142857   |</span><br><span class=\"line\">| Joy     | 202    | maths    | 87     | 4    | 3    | 4    | 0.42857142857142855  |</span><br><span class=\"line\">| Roy     | 202    | maths    | 84     | 5    | 4    | 5    | 0.5714285714285714   |</span><br><span class=\"line\">| Erw     | 202    | maths    | 84     | 5    | 4    | 6    | 0.5714285714285714   |</span><br><span class=\"line\">| Tong    | 202    | maths    | 79     | 7    | 5    | 7    | 0.8571428571428571   |</span><br><span class=\"line\">| Luo     | 202    | maths    | 73     | 8    | 6    | 8    | 1.0                  |</span><br><span class=\"line\">+---------+--------+----------+--------+------+------+------+----------------------+--+</span><br></pre></td></tr></table></figure>\n<h2 id=\"差值\"><a href=\"#差值\" class=\"headerlink\" title=\"差值\"></a>差值</h2><p>lag(expr [, offset] [, default]) OVER ([partition_by_clause] order_by_clause)<br>This function returns the value of an expression using column values from a preceding row. You specify an integer offset, which designates a row position some number of rows previous to the current row. Any column references in the expression argument refer to column values from that prior row.</p>\n<p>lead(expr [, offset] [, default]) OVER([partition_by_clause] order_by_clause)<br>This function returns the value of an expression using column values from a following row. You specify an integer offset, which designates a row position some number of rows after to the current row. Any column references in the expression argument refer to column values from that later row.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select</span><br><span class=\"line\">    class,</span><br><span class=\"line\">    course,</span><br><span class=\"line\">    score,</span><br><span class=\"line\">    rank() over(partition by class, course order by score desc) rk1,</span><br><span class=\"line\">    LAG(score,2) over(partition by class, course order by score) as lag,</span><br><span class=\"line\">    lead(score,2) over(partition by class, course order by score) as lead</span><br><span class=\"line\">from test_analy_funs</span><br><span class=\"line\">where course=&apos;chinese&apos;;</span><br><span class=\"line\">+--------+----------+--------+------+-------+-------+--+</span><br><span class=\"line\">| class  |  course  | score  | rk1  |  lag  | lead  |</span><br><span class=\"line\">+--------+----------+--------+------+-------+-------+--+</span><br><span class=\"line\">| 201    | chinese  | 28     | 8    | NULL  | 88    |</span><br><span class=\"line\">| 201    | chinese  | 83     | 7    | NULL  | 89    |</span><br><span class=\"line\">| 201    | chinese  | 88     | 6    | 28    | 92    |</span><br><span class=\"line\">| 201    | chinese  | 89     | 5    | 83    | 95    |</span><br><span class=\"line\">| 201    | chinese  | 92     | 4    | 88    | 99    |</span><br><span class=\"line\">| 201    | chinese  | 95     | 3    | 89    | 100   |</span><br><span class=\"line\">| 201    | chinese  | 99     | 2    | 92    | NULL  |</span><br><span class=\"line\">| 201    | chinese  | 100    | 1    | 95    | NULL  |</span><br><span class=\"line\">| 202    | chinese  | 28     | 8    | NULL  | 50    |</span><br><span class=\"line\">| 202    | chinese  | 30     | 7    | NULL  | 78    |</span><br><span class=\"line\">| 202    | chinese  | 50     | 6    | 28    | 82    |</span><br><span class=\"line\">| 202    | chinese  | 78     | 5    | 30    | 88    |</span><br><span class=\"line\">| 202    | chinese  | 82     | 4    | 50    | 92    |</span><br><span class=\"line\">| 202    | chinese  | 88     | 3    | 78    | 99    |</span><br><span class=\"line\">| 202    | chinese  | 92     | 2    | 82    | NULL  |</span><br><span class=\"line\">| 202    | chinese  | 99     | 1    | 88    | NULL  |</span><br><span class=\"line\">+--------+----------+--------+------+-------+-------+--+</span><br></pre></td></tr></table></figure>\n<h2 id=\"第一个或者最后一个值\"><a href=\"#第一个或者最后一个值\" class=\"headerlink\" title=\"第一个或者最后一个值\"></a>第一个或者最后一个值</h2><p>获取分组集合中的第一个值或者最后一个值，last_value需要加rows between current row and unbounded following</p>\n<p>first_value(expr) OVER([partition_by_clause] order_by_clause [window_clause])<br>Returns the expression value from the first row in the window. The return value is NULL if the input expression is NULL.</p>\n<p>last_value(expr) OVER([partition_by_clause] order_by_clause [window_clause])<br>Returns the expression value from the last row in the window. The return value is NULL if the input expression is NULL.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select</span><br><span class=\"line\">    stu_id,</span><br><span class=\"line\">    class,</span><br><span class=\"line\">    course,</span><br><span class=\"line\">    score,</span><br><span class=\"line\">    rank() over(partition by class, course order by score desc) rk1,</span><br><span class=\"line\">    first_value(score) over(partition by class, course order by score) first,</span><br><span class=\"line\">    last_value(score) over(partition by class, course order by score) last,</span><br><span class=\"line\">    last_value(score) over(partition by class, course order by score rows between current row and unbounded following) last</span><br><span class=\"line\">from test_analy_funs</span><br><span class=\"line\">where course=&apos;chinese&apos;</span><br><span class=\"line\">order by class, stu_id;</span><br><span class=\"line\"></span><br><span class=\"line\">+---------+--------+----------+--------+------+--------+-------+-------+--+</span><br><span class=\"line\">| stu_id  | class  |  course  | score  | rk1  | first  | last  | last  |</span><br><span class=\"line\">+---------+--------+----------+--------+------+--------+-------+-------+--+</span><br><span class=\"line\">| 1       | 201    | chinese  | 28     | 8    | 28     | 28    | 100   |</span><br><span class=\"line\">| 2       | 201    | chinese  | 99     | 2    | 28     | 99    | 100   |</span><br><span class=\"line\">| 3       | 201    | chinese  | 89     | 5    | 28     | 89    | 100   |</span><br><span class=\"line\">| 4       | 201    | chinese  | 100    | 1    | 28     | 100   | 100   |</span><br><span class=\"line\">| 5       | 201    | chinese  | 88     | 6    | 28     | 88    | 100   |</span><br><span class=\"line\">| 6       | 201    | chinese  | 83     | 7    | 28     | 83    | 100   |</span><br><span class=\"line\">| 7       | 201    | chinese  | 92     | 4    | 28     | 92    | 100   |</span><br><span class=\"line\">| 8       | 201    | chinese  | 95     | 3    | 28     | 95    | 100   |</span><br><span class=\"line\">| 9       | 202    | chinese  | 82     | 4    | 28     | 82    | 99    |</span><br><span class=\"line\">| 10      | 202    | chinese  | 92     | 2    | 28     | 92    | 99    |</span><br><span class=\"line\">| 11      | 202    | chinese  | 99     | 1    | 28     | 99    | 99    |</span><br><span class=\"line\">| 12      | 202    | chinese  | 78     | 5    | 28     | 78    | 99    |</span><br><span class=\"line\">| 13      | 202    | chinese  | 28     | 8    | 28     | 28    | 99    |</span><br><span class=\"line\">| 14      | 202    | chinese  | 50     | 6    | 28     | 50    | 99    |</span><br><span class=\"line\">| 15      | 202    | chinese  | 30     | 7    | 28     | 30    | 99    |</span><br><span class=\"line\">| 16      | 202    | chinese  | 88     | 3    | 28     | 88    | 99    |</span><br><span class=\"line\">+---------+--------+----------+--------+------+--------+-------+-------+--+</span><br><span class=\"line\">16 rows selected (25.701 seconds)</span><br></pre></td></tr></table></figure>\n<h2 id=\"分桶\"><a href=\"#分桶\" class=\"headerlink\" title=\"分桶\"></a>分桶</h2><p>ntile()</p>\n<p>Ntile 是Hive很强大的一个分析函数。<br>可以看成是：它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。<br>语法是：<br>     ntile (num)  over ([partition_clause]  order_by_clause)  as your_bucket_num</p>\n<p>   然后可以根据桶号，选取前或后 n分之几的数据。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select </span><br><span class=\"line\">      stu_id, </span><br><span class=\"line\">      class ,</span><br><span class=\"line\">      score,</span><br><span class=\"line\">      NTILE(5) OVER( partition by class, course order by score desc) AS rn </span><br><span class=\"line\">from test_analy_funs</span><br><span class=\"line\">where course=&apos;chinese&apos;;</span><br><span class=\"line\">+---------+--------+--------+-----+--+</span><br><span class=\"line\">| stu_id  | class  | score  | rn  |</span><br><span class=\"line\">+---------+--------+--------+-----+--+</span><br><span class=\"line\">| 4       | 201    | 100    | 1   |</span><br><span class=\"line\">| 2       | 201    | 99     | 1   |</span><br><span class=\"line\">| 8       | 201    | 95     | 2   |</span><br><span class=\"line\">| 7       | 201    | 92     | 2   |</span><br><span class=\"line\">| 3       | 201    | 89     | 3   |</span><br><span class=\"line\">| 5       | 201    | 88     | 3   |</span><br><span class=\"line\">| 6       | 201    | 83     | 4   |</span><br><span class=\"line\">| 1       | 201    | 28     | 5   |</span><br><span class=\"line\">| 11      | 202    | 99     | 1   |</span><br><span class=\"line\">| 10      | 202    | 92     | 1   |</span><br><span class=\"line\">| 16      | 202    | 88     | 2   |</span><br><span class=\"line\">| 9       | 202    | 82     | 2   |</span><br><span class=\"line\">| 12      | 202    | 78     | 3   |</span><br><span class=\"line\">| 14      | 202    | 50     | 3   |</span><br><span class=\"line\">| 15      | 202    | 30     | 4   |</span><br><span class=\"line\">| 13      | 202    | 28     | 5   |</span><br><span class=\"line\">+---------+--------+--------+-----+--+</span><br></pre></td></tr></table></figure>\n<h2 id=\"值排序占比\"><a href=\"#值排序占比\" class=\"headerlink\" title=\"值排序占比\"></a>值排序占比</h2><p>cume_dist()</p>\n<p>CUME_DIST 小于等于当前值的行数/分组内总行数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT </span><br><span class=\"line\">class,</span><br><span class=\"line\">stu_id,</span><br><span class=\"line\">score,</span><br><span class=\"line\">CUME_DIST() OVER(partition by class ORDER BY score) AS cm1</span><br><span class=\"line\">from test_analy_funs</span><br><span class=\"line\">where course=&apos;chinese&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">+--------+---------+--------+--------+--+</span><br><span class=\"line\">| class  | stu_id  | score  |  cm1   |</span><br><span class=\"line\">+--------+---------+--------+--------+--+</span><br><span class=\"line\">| 201    | 1       | 28     | 0.125  |</span><br><span class=\"line\">| 201    | 6       | 83     | 0.25   |</span><br><span class=\"line\">| 201    | 5       | 88     | 0.375  |</span><br><span class=\"line\">| 201    | 3       | 89     | 0.5    |</span><br><span class=\"line\">| 201    | 7       | 92     | 0.625  |</span><br><span class=\"line\">| 201    | 8       | 95     | 0.75   |</span><br><span class=\"line\">| 201    | 2       | 99     | 0.875  |</span><br><span class=\"line\">| 201    | 4       | 100    | 1.0    |</span><br><span class=\"line\">| 202    | 13      | 28     | 0.125  |</span><br><span class=\"line\">| 202    | 15      | 30     | 0.25   |</span><br><span class=\"line\">| 202    | 14      | 50     | 0.375  |</span><br><span class=\"line\">| 202    | 12      | 78     | 0.5    |</span><br><span class=\"line\">| 202    | 9       | 82     | 0.625  |</span><br><span class=\"line\">| 202    | 16      | 88     | 0.75   |</span><br><span class=\"line\">| 202    | 10      | 92     | 0.875  |</span><br><span class=\"line\">| 202    | 11      | 99     | 1.0    |</span><br><span class=\"line\">+--------+---------+--------+--------+--+</span><br></pre></td></tr></table></figure>"},{"title":"Hive 复杂数据类型","date":"2018-05-13T07:08:12.000Z","_content":"\nhttps://community.hortonworks.com/questions/22262/insert-values-in-array-data-type-hive.html\n\n1 ) \n\t\n\tINSERT INTO table test_array VALUES (1,array('a','b'));\nError: Error while compiling statement: FAILED: SemanticException [Error 10293]: Unable to create temp file for insert values Expression of type TOK_FUNCTION not supported in insert/values (state=42000,code=10293)\n2 ) \n\t\n\tINSERT INTO test_array (col2) VALUES (array('a','b')) from dummy limit 1; \nError: Error while compiling statement: FAILED: ParseException line 1:54 missing EOF at 'from' near ')' (state=42000,code=40000)\n\n<!-- more -->\n\tcreate table  test_array_type(\n\tarr array<string>\n\t);\n\tcreate table dummy(a string); insert into table dummy values ('a');\n\tINSERT INTO table test_array_type SELECT array('a','b') from dummy;\n\n\tSELECT * from test_array_type;\n\t\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest1.png?raw=true)\n\n\tselect  arr[1] from test_array_type;\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest2.png?raw=true)\n\n\n\tselect explode(arr) from test_array_type;\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest3.png?raw=true)\n\n\tselect * from (select explode(arr) from test_array_type) s join (select \"AA\")t on 1 =1 ;\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest4.png?raw=true)\n\n\tcreate table  test_map_type(\n\tmap1 map<int,string>\n\t);\n\tINSERT INTO table test_map_type SELECT map(1,'b',2,'c') from dummy;\n\tselect * from test_map_type;\n\t\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest5.png?raw=true)\n\n\tselect map1,map_keys(map1),map_values(map1),map1[2] from test_map_type;\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest6.png?raw=true)\n\n\n\n\tcreate table  test_struct_type(\n\tmap1 struct<col1:int,col2:string, col3:int>\n\t);\n\n\tINSERT INTO table test_struct_type SELECT struct(1,'b',2) from dummy;\n\tselect * from test_struct_type;\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest7.png?raw=true)\n\n\n\n\tcreate table  test_arr_map_type(\n\tmap1 array<map<int,string>>\n\t);\n\n\tINSERT INTO table test_arr_map_type SELECT array(map(1,'b',2,'c'),map(3,'e')) from dummy;\n\tselect * from test_arr_map_type;\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest8.png?raw=true)\n\n\n\n\tselect map1,map_keys(map1[0]),map_keys(map1[1]),map1[1][3] from test_arr_map_type;\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest9.png?raw=true)\n\n\n\n","source":"_posts/Hive-复杂数据类型.md","raw":"---\ntitle: Hive 复杂数据类型\ndate: 2018-05-13 15:08:12\ntags:\n  - Hive\n  - Data Type\n---\n\nhttps://community.hortonworks.com/questions/22262/insert-values-in-array-data-type-hive.html\n\n1 ) \n\t\n\tINSERT INTO table test_array VALUES (1,array('a','b'));\nError: Error while compiling statement: FAILED: SemanticException [Error 10293]: Unable to create temp file for insert values Expression of type TOK_FUNCTION not supported in insert/values (state=42000,code=10293)\n2 ) \n\t\n\tINSERT INTO test_array (col2) VALUES (array('a','b')) from dummy limit 1; \nError: Error while compiling statement: FAILED: ParseException line 1:54 missing EOF at 'from' near ')' (state=42000,code=40000)\n\n<!-- more -->\n\tcreate table  test_array_type(\n\tarr array<string>\n\t);\n\tcreate table dummy(a string); insert into table dummy values ('a');\n\tINSERT INTO table test_array_type SELECT array('a','b') from dummy;\n\n\tSELECT * from test_array_type;\n\t\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest1.png?raw=true)\n\n\tselect  arr[1] from test_array_type;\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest2.png?raw=true)\n\n\n\tselect explode(arr) from test_array_type;\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest3.png?raw=true)\n\n\tselect * from (select explode(arr) from test_array_type) s join (select \"AA\")t on 1 =1 ;\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest4.png?raw=true)\n\n\tcreate table  test_map_type(\n\tmap1 map<int,string>\n\t);\n\tINSERT INTO table test_map_type SELECT map(1,'b',2,'c') from dummy;\n\tselect * from test_map_type;\n\t\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest5.png?raw=true)\n\n\tselect map1,map_keys(map1),map_values(map1),map1[2] from test_map_type;\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest6.png?raw=true)\n\n\n\n\tcreate table  test_struct_type(\n\tmap1 struct<col1:int,col2:string, col3:int>\n\t);\n\n\tINSERT INTO table test_struct_type SELECT struct(1,'b',2) from dummy;\n\tselect * from test_struct_type;\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest7.png?raw=true)\n\n\n\n\tcreate table  test_arr_map_type(\n\tmap1 array<map<int,string>>\n\t);\n\n\tINSERT INTO table test_arr_map_type SELECT array(map(1,'b',2,'c'),map(3,'e')) from dummy;\n\tselect * from test_arr_map_type;\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest8.png?raw=true)\n\n\n\n\tselect map1,map_keys(map1[0]),map_keys(map1[1]),map1[1][3] from test_arr_map_type;\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest9.png?raw=true)\n\n\n\n","slug":"Hive-复杂数据类型","published":1,"updated":"2018-09-14T01:21:43.961Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzu0012inamd3n0en5m","content":"<p><a href=\"https://community.hortonworks.com/questions/22262/insert-values-in-array-data-type-hive.html\" target=\"_blank\" rel=\"noopener\">https://community.hortonworks.com/questions/22262/insert-values-in-array-data-type-hive.html</a></p>\n<p>1 ) </p>\n<pre><code>INSERT INTO table test_array VALUES (1,array(&apos;a&apos;,&apos;b&apos;));\n</code></pre><p>Error: Error while compiling statement: FAILED: SemanticException [Error 10293]: Unable to create temp file for insert values Expression of type TOK_FUNCTION not supported in insert/values (state=42000,code=10293)<br>2 ) </p>\n<pre><code>INSERT INTO test_array (col2) VALUES (array(&apos;a&apos;,&apos;b&apos;)) from dummy limit 1; \n</code></pre><p>Error: Error while compiling statement: FAILED: ParseException line 1:54 missing EOF at ‘from’ near ‘)’ (state=42000,code=40000)</p>\n<a id=\"more\"></a>\n<pre><code>create table  test_array_type(\narr array&lt;string&gt;\n);\ncreate table dummy(a string); insert into table dummy values (&apos;a&apos;);\nINSERT INTO table test_array_type SELECT array(&apos;a&apos;,&apos;b&apos;) from dummy;\n\nSELECT * from test_array_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest1.png?raw=true\" alt=\"\"></p>\n<pre><code>select  arr[1] from test_array_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest2.png?raw=true\" alt=\"\"></p>\n<pre><code>select explode(arr) from test_array_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest3.png?raw=true\" alt=\"\"></p>\n<pre><code>select * from (select explode(arr) from test_array_type) s join (select &quot;AA&quot;)t on 1 =1 ;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest4.png?raw=true\" alt=\"\"></p>\n<pre><code>create table  test_map_type(\nmap1 map&lt;int,string&gt;\n);\nINSERT INTO table test_map_type SELECT map(1,&apos;b&apos;,2,&apos;c&apos;) from dummy;\nselect * from test_map_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest5.png?raw=true\" alt=\"\"></p>\n<pre><code>select map1,map_keys(map1),map_values(map1),map1[2] from test_map_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest6.png?raw=true\" alt=\"\"></p>\n<pre><code>create table  test_struct_type(\nmap1 struct&lt;col1:int,col2:string, col3:int&gt;\n);\n\nINSERT INTO table test_struct_type SELECT struct(1,&apos;b&apos;,2) from dummy;\nselect * from test_struct_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest7.png?raw=true\" alt=\"\"></p>\n<pre><code>create table  test_arr_map_type(\nmap1 array&lt;map&lt;int,string&gt;&gt;\n);\n\nINSERT INTO table test_arr_map_type SELECT array(map(1,&apos;b&apos;,2,&apos;c&apos;),map(3,&apos;e&apos;)) from dummy;\nselect * from test_arr_map_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest8.png?raw=true\" alt=\"\"></p>\n<pre><code>select map1,map_keys(map1[0]),map_keys(map1[1]),map1[1][3] from test_arr_map_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest9.png?raw=true\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"https://community.hortonworks.com/questions/22262/insert-values-in-array-data-type-hive.html\" target=\"_blank\" rel=\"noopener\">https://community.hortonworks.com/questions/22262/insert-values-in-array-data-type-hive.html</a></p>\n<p>1 ) </p>\n<pre><code>INSERT INTO table test_array VALUES (1,array(&apos;a&apos;,&apos;b&apos;));\n</code></pre><p>Error: Error while compiling statement: FAILED: SemanticException [Error 10293]: Unable to create temp file for insert values Expression of type TOK_FUNCTION not supported in insert/values (state=42000,code=10293)<br>2 ) </p>\n<pre><code>INSERT INTO test_array (col2) VALUES (array(&apos;a&apos;,&apos;b&apos;)) from dummy limit 1; \n</code></pre><p>Error: Error while compiling statement: FAILED: ParseException line 1:54 missing EOF at ‘from’ near ‘)’ (state=42000,code=40000)</p>","more":"<pre><code>create table  test_array_type(\narr array&lt;string&gt;\n);\ncreate table dummy(a string); insert into table dummy values (&apos;a&apos;);\nINSERT INTO table test_array_type SELECT array(&apos;a&apos;,&apos;b&apos;) from dummy;\n\nSELECT * from test_array_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest1.png?raw=true\" alt=\"\"></p>\n<pre><code>select  arr[1] from test_array_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest2.png?raw=true\" alt=\"\"></p>\n<pre><code>select explode(arr) from test_array_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest3.png?raw=true\" alt=\"\"></p>\n<pre><code>select * from (select explode(arr) from test_array_type) s join (select &quot;AA&quot;)t on 1 =1 ;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest4.png?raw=true\" alt=\"\"></p>\n<pre><code>create table  test_map_type(\nmap1 map&lt;int,string&gt;\n);\nINSERT INTO table test_map_type SELECT map(1,&apos;b&apos;,2,&apos;c&apos;) from dummy;\nselect * from test_map_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest5.png?raw=true\" alt=\"\"></p>\n<pre><code>select map1,map_keys(map1),map_values(map1),map1[2] from test_map_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest6.png?raw=true\" alt=\"\"></p>\n<pre><code>create table  test_struct_type(\nmap1 struct&lt;col1:int,col2:string, col3:int&gt;\n);\n\nINSERT INTO table test_struct_type SELECT struct(1,&apos;b&apos;,2) from dummy;\nselect * from test_struct_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest7.png?raw=true\" alt=\"\"></p>\n<pre><code>create table  test_arr_map_type(\nmap1 array&lt;map&lt;int,string&gt;&gt;\n);\n\nINSERT INTO table test_arr_map_type SELECT array(map(1,&apos;b&apos;,2,&apos;c&apos;),map(3,&apos;e&apos;)) from dummy;\nselect * from test_arr_map_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest8.png?raw=true\" alt=\"\"></p>\n<pre><code>select map1,map_keys(map1[0]),map_keys(map1[1]),map1[1][3] from test_arr_map_type;\n</code></pre><p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Hive/DataTypeTest/DateTypeTest9.png?raw=true\" alt=\"\"></p>"},{"title":"HBase Region Splitting and Merging","date":"2018-11-27T22:01:03.000Z","_content":"\n源文：https://hortonworks.com/blog/apache-hbase-region-splitting-and-merging/\n\nFor this post, we take a technical deep-dive into one of the core areas of HBase. Specifically, we will look at how Apache HBase distributes load through regions, and manages region splitting. HBase stores rows of data in tables. Tables are split into chunks of rows called “regions”. Those regions are distributed across the cluster, hosted and made available to client processes by the RegionServer process. A region is a continuous range within the key space, meaning all rows in the table that sort between the region’s start key and end key are stored in the same region. Regions are non-overlapping, i.e. a single row key belongs to exactly one region at any point in time. A region is only served by a single region server at any point in time, which is how HBase guarantees strong consistency within a single row#. Together with the -ROOT- and .META. regions, a table’s regions effectively form a 3 level B-Tree for the purposes of locating a row within a table.\n\n在这篇文章中，我们将深入到HBase的一块核心内容。尤其是我们将要看下Apache Hbase怎么通过regions做负载的，怎么管理region的切分。HBase将数据存储在表中。表被切分为数据块即regions。这些regions被分配在整个集群，可以被客户端程序调用RegionServer来使用。一个region就是一个连续的key空间，意味着在一个表中，所有在region的开始key和结束key之间的数据都被存储在一个region里。Regions是不重叠的，例如，在任何一个时间点上一个row key都只属于一个特定的region，一个region只会存在一个region server上。这样HBase就能保证数据的强一致性。算上-ROOT-和.META这2个region，一个表的region可以有效地通过一个3层B-树获取一行数据在一个表中的位置。\n<!-- more -->\nA Region in turn, consists of many “Stores”, which correspond to column families. A store contains one memstore and zero or more store files. The data for each column family is stored and accessed separately.\n一个Region反过来，由许多列簇的存储组成。一个列簇的存储包含一个内存存储和零到多个存储文件。数据的每一个列簇都被分开存储和使用。\n\nA table typically consists of many regions, which are in turn hosted by many region servers. Thus, regions are the physical mechanism used to distribute the write and query load across region servers. When a table is first created, HBase, by default, will allocate only one region for the table. This means that initially, all requests will go to a single region server, regardless of the number of region servers. This is the primary reason why initial phases of loading data into an empty table cannot utilize the whole capacity of the cluster.\n一个表通常由多个region组成，这些region分布在多个region server上。因此，region被分布式的region server在物理上进行读写和查询等操作。当一个表被初始创建时，HBase默认会为其分配一个region。这就意味着，在开始时所有的请求都会发送到一个单独的region servers上，无论还有多少其他的region server。这就在初始加载数据进入一个空表时，不能利用整个集群的性能的主要原因。\n\n## PRE-SPLITTING\n---\n\nThe reason HBase creates only one region for the table is that it cannot possibly know how to create the split points within the row key space. Making such decisions is based highly on the distribution of the keys in your data. Rather than taking a guess and leaving you to deal with the consequences, HBase does provide you with tools to manage this from the client. With a process called pre-splitting, you can create a table with many regions by supplying the split points at the table creation time. Since pre-splitting will ensure that the initial load is more evenly distributed throughout the cluster, you should always consider using it if you know your key distribution beforehand. However, pre-splitting also has a risk of creating regions, that do not truly distribute the load evenly because of data skew, or in the presence of very hot or large rows. If the initial set of region split points is chosen poorly, you may end up with heterogeneous load distribution, which will in turn limit your clusters performance.\nHBase在初始时只创建一个region的原因是它不知道怎么去创建row key空间的分割点。需要根据数据的分布才能做出这个决定。HBase提供了一个工具帮助你从客户端管理它，而不是随便猜测一个值或者留给你去处理。通过一个叫做pre-splitting的程序，一个可以在创建表的时候通过提供切割点来创建多个region。因为pre-splitting将确保在初始加载数据时会更均匀的分布在整个集群。所以如果你只要你的key的分布情况，就应该尽量的考虑使用它。然而，pre-splitting在创建表时也有一个风险，就是由于数据倾斜或者热冷数据或者某些行的数据非常大使它不能完全的平均分布在集群上。如果初始的region切割点设置的不好，可能会产生各种各样的加载，这可能会影响你集群的性能。\n\nThere is no short answer for the optimal number of regions for a given load, but you can start with a lower multiple of the number of region servers as number of splits, then let automated splitting take care of the rest.\n对于一个load任务使用多少region适合并没有一个很直观的答案，但是你可以在开始时设置较为region server的几倍数，然后随着数据的增加让它自动切分。\n\nOne issue with pre-splitting is calculating the split points for the table. You can use the RegionSplitter utility. RegionSplitter creates the split points, by using a pluggable SplitAlgorithm. HexStringSplit and UniformSplit are two predefined algorithms. The former can be used if the row keys have a prefix for hexadecimal strings (like if you are using hashes as prefixes). The latter divides up the key space evenly assuming they are random byte arrays. You can also implement your custom SplitAlgorithm and use it from the RegionSplitter utility.\n\npre-splitting的一个问题就是计算表的切割点。你可以使用RegionSplitter工具。RegionSplitter使用一个可插拔的算法来创建切割点。HexStringSplit和UniformSplit是2个预定义的算法。当row key有一个16进制字符串的前缀时，可以使用HexStringSplit算法。UniformSplit算法假设key是随机的字节数组，然后去切割key空间。你也可以自己写Split算法并通过RegionSplitter工具来使用它。\n\n```\n$ hbase org.apache.hadoop.hbase.util.RegionSplitter test_table HexStringSplit -c 10 -f f1\n```\n-c 10,指定需要的region是10个，-f 指定列簇, 分隔符是“:”. 这个工具将会创建一个有10个region的名为 “test_table”的表。\n```\n13/01/18 18:49:32 DEBUG hbase.HRegionInfo: Current INFO from scan results = {NAME => 'test_table,,1358563771069.acc1ad1b7962564fc3a43e5907e8db33.', STARTKEY => '', ENDKEY => '19999999', ENCODED => acc1ad1b7962564fc3a43e5907e8db33,}\n13/01/18 18:49:32 DEBUG hbase.HRegionInfo: Current INFO from scan results = {NAME => 'test_table,19999999,1358563771096.37ec12df6bd0078f5573565af415c91b.', STARTKEY => '19999999', ENDKEY => '33333332', ENCODED => 37ec12df6bd0078f5573565af415c91b,}\n...\n```\nIf you have split points at hand, you can also use the HBase shell, to create the table with the desired split points.\n如果你知道切割点，你也可以使用HBase shell在创建表的时候指定切割点。\n```\nhbase(main):015:0> create 'test_table', 'f1', SPLITS=> ['a', 'b', 'c']\n```\nor\n```\n$ echo -e  \"anbnc\" >/tmp/splits\nhbase(main):015:0> create 'test_table', 'f1', SPLITSFILE=>'/tmp/splits'\n```\n\nFor optimum load distribution, you should think about your data model, and key distribution for choosing the correct split algorithm or split points. Regardless of the method you chose to create the table with pre determined number of regions, you can now start loading the data into the table, and see that the load is distributed throughout your cluster. You can let automated splitting take over once data ingest starts, and continuously monitor the total number of regions for the table.\n为了更好的分布加载数据，你应该思考下你的数据模型和key的分布情况，然后选择合适的分割算法或者分割点。无论你选择哪种方法创建你的表，你可以开始导入数据进入你的表，并查看加载是否分布在你的集群上。当你已经开始导入数据了，你也可以让它自动切割，然后监控表的region的个数。\n\n## AUTO SPLITTING\n---\nRegardless of whether pre-splitting is used or not, once a region gets to a certain limit, it is automatically split into two regions. If you are using HBase 0.94 (which comes with HDP-1.2), you can configure when HBase decides to split a region, and how it calculates the split points via the pluggable RegionSplitPolicy API. There are a couple predefined region split policies: ConstantSizeRegionSplitPolicy, IncreasingToUpperBoundRegionSplitPolicy, and KeyPrefixRegionSplitPolicy.\n无论是否使用pre-splitting，当一个region达到一定的限制，它会自动的分割成2个region。如果使用HBase0.94 （HDP-1.2），你可以配置什么时候HBase决定去切割一个region，和它怎么通过可插拔[RegionSplitPolicy API](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.html)来计算切割点。已经有一些预定义的切割策略：[ConstantSizeRegionSplitPolicy](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.html)，[IncreasingToUpperBoundRegionSplitPolicy](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.html)和[KeyPrefixRegionSplitPilicy](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/KeyPrefixRegionSplitPolicy.html)。\n\nThe first one is the default and only split policy for HBase versions before 0.94. It splits the regions when the total data size for one of the stores (corresponding to a column-family) in the region gets bigger than configured “hbase.hregion.max.filesize”, which has a default value of 10GB. This split policy is ideal in cases, where you are have done pre-splitting, and are interested in getting lower number of regions per region server.\nConstantSizeRegionSplitPolicy是默认的切割策略，并且是HBase0.94之前的唯一策略。当一个region的一个store存储的数据大小大于配置的“hbase.hregion.max.filesize”值时，默认10GB，它就会自动的切割分区。这个切割策略是理想的，你已经预分区你的表，并且每个region server不会有太多的region。\n\nThe default split policy for HBase 0.94 and trunk is IncreasingToUpperBoundRegionSplitPolicy, which does more aggressive splitting based on the number of regions hosted in the same region server. The split policy uses the max store file size based on Min (R^2 * “hbase.hregion.memstore.flush.size”, “hbase.hregion.max.filesize”), where R is the number of regions of the same table hosted on the same regionserver. So for example, with the default memstore flush size of 128MB and the default max store size of 10GB, the first region on the region server will be split just after the first flush at 128MB. As number of regions hosted in the region server increases, it will use increasing split sizes: 512MB, 1152MB, 2GB, 3.2GB, 4.6GB, 6.2GB, etc. After reaching 9 regions, the split size will go beyond the configured “hbase.hregion.max.filesize”, at which point, 10GB split size will be used from then on. For both of these algorithms, regardless of when splitting occurs, the split point used is the rowkey that corresponds to the mid point in the “block index” for the largest store file in the largest store.\nHBase0.94默认的切割策略变成了IncreasingToUpperBoundRegionSplitPolicy，它会基于每个region server拥有的region个数对数据进行切割。这个切割策略使用最大store文件基于Min (R^2 * “hbase.hregion.memstore.flush.size”, “hbase.hregion.max.filesize”)，R是在一个region server上一个表拥有的region个数。例如，默认的内存存储是128MB，默认的最大store大小是10GB，只有在第一个flush达到128MB时，第一个region才会被切割。随着在region server上的region个数的增长，它将会增加分割大小：512MB, 1152MB, 2GB, 3.2GB, 4.6GB, 6.2GB,等待。当达到9个region时，分割大小将会超过参数 “hbase.hregion.max.filesize”，此时，10GB的切割大小将会被开始使用。对于所有的这些算法，无论何时发生拆分，所使用的拆分点都是对应于“块索引”中最大存储文件的中点的rowkey。\n\nKeyPrefixRegionSplitPolicy is a curious addition to the HBase arsenal. You can configure the length of the prefix for your row keys for grouping them, and this split policy ensures that the regions are not split in the middle of a group of rows having the same prefix. If you have set prefixes for your keys, then you can use this split policy to ensure that rows having the same rowkey prefix always end up in the same region. This grouping of records is sometimes referred to as “Entity Groups” or “Row Groups”. This is a key feature when considering use of the “local transactions” (alternative link) feature in your application design.\nKeyPrefixRegionSplitPolicy是一个很好的补充。你可以配置你用来分组的row key的前缀的长度，这个分割策略可以确保有相同的前缀的一组数据不会被从中间切割。如果你设置了你的key的前缀，你可以使用这个切割策略来确保你所有有相同前缀的数据不会被分割到不同的region中。这个分组是指实例组或者行组。当在你的应用设计考虑使用本地处理时这是一个关键的功能。\n\nYou can configure the default split policy to be used by setting the configuration “hbase.regionserver.region.split.policy”, or by configuring the table descriptor. For you brave souls, you can also implement your own custom split policy, and plug that in at table creation time, or by modifying an existing table:\n你可以使用配置参数“hbase.regionserver.region.split.policy”来配置你的默认切割策略，或者通过配置表的描述信息。你也可以使用你自己的切割策略，并且在你创建表的时候使用它，或者修改已经存在的表。\n```\nHTableDescriptor tableDesc = new HTableDescriptor(\"example-table\");\ntableDesc.setValue(HTableDescriptor.SPLIT_POLICY, AwesomeSplitPolicy.class.getName());\n//add columns etc\nadmin.createTable(tableDesc);\n```\n\nIf you are doing pre-splitting, and want to manually manage region splits, you can also disable region splits, by setting “hbase.hregion.max.filesize” to a high number and setting the split policy to ConstantSizeRegionSplitPolicy. However, you should use a safeguard value of like 100GB, so that regions does not grow beyond a region server’s capabilities. You can consider disabling automated splitting and rely on the initial set of regions from pre-splitting for example, if you are using uniform hashes for your key prefixes, and you can ensure that the read/write load to each region as well as its size is uniform across the regions in the table.\n如果你正在做pre-splitting，并且向要手动的管理你的region切割，你可以通过设置“hbase.hregion.max.filesize”一个比较大的值和设置切割策略为ConstantSizeRegionSplitPolicy来禁用它。然后，你应该使用一个比较安全的值如100GB，这样region不会增长到超过一个region server的容量。你可以考虑禁用自动切割和通过pre-splitting的初始设置，例如，如果你对你的key的前缀使用统一的hash，你可以确保对每个区域的读/写负载以及表中各个region的大小都是统一的。\n\n## FORCED SPLITS\n---\n\nHBase also enables clients to force split an online table from the client side. For example, the HBase shell can be used to split all regions of the table, or split a region, optionally by supplying a split point.\n```\nhbase(main):024:0> split 'b07d0034cbe72cb040ae9cf66300a10c', 'b'\n0 row(s) in 0.1620 seconds\n```\n\nWith careful monitoring of your HBase load distribution, if you see that some regions are getting uneven loads, you may consider manually splitting those regions to even-out the load and improve throughput. Another reason why you might want to do manual splits is when you see that the initial splits for the region turns out to be suboptimal, and you have disabled automated splits. That might happen for example, if the data distribution changes over time.\nHBase也提供通过客户端去强制分割在线的表。例如，可以使用HBase shell去分割表的所有region或者一个region，也可以通过提供分割点去切割。\n仔细监控你的HBase的加载分布，如果有些region变的不均匀，你需要考虑手分割这些region来提高图吞吐量。为什么你需要手动分割的另外一个原因是当你发现初始的region变的不标准或者你禁用了自分割。例如，你的数据分布随着时间一直在变化。\n\n## HOW REGION SPLITS ARE IMPLEMENTED\n---\n\nAs write requests are handled by the region server, they accumulate in an in-memory storage system called the “memstore”. Once the memstore fills, its content are written to disk as additional store files. This event is called a “memstore flush”. As store files accumulate, the RegionServer will “compact” them into combined, larger files. After each flush or compaction finishes, a region split request is enqueued if the RegionSplitPolicy decides that the region should be split into two. Since all data files in HBase are immutable, when a split happens, the newly created daughter regions will not rewrite all the data into new files. Instead, they will create  small sym-link like files, named Reference files, which point to either top or bottom part of the parent store file according to the split point. The reference file will be used just like a regular data file, but only half of the records. The region can only be split if there are no more references to the immutable data files of the parent region. Those reference files are cleaned gradually by compactions, so that the region will stop referring to its parents files, and can be split further.\n在HBase中，写的需求是由region server来处理的，他们被存储在内存存储系统“memstore”中。当memstore文件满了，它开始将数据写入存储在磁盘的存储文件中。这个过程叫做“memstore flush”。RegionServer将会把这些存储文件合并成大文件。每次flush或者合并完成后，如果RegionSplitPolicy决定这个region应该被分割成2个region，就会触发一个分割的请求。因为在HBase中所有的数据文件都是不可变的，当需要一个分割时，对于一个region的2个子region，他们不是重新将所有的数据写入新的文件，而是会先创建小的类似软连接的叫做引用文件的文件。一个region只有在没有引用指向它的父文件时才可以被分割。这些引用文件会被compaction清除，这样它就不会执行他们的父文件，然后可以被分割。\n\n\nAlthough splitting the region is a local decision made at the RegionServer, the split process itself must coordinate with many actors. The RegionServer notifies the Master before and after the split, updates the .META. table so that clients can discover the new daughter regions, and rearranges the directory structure and data files in HDFS. Split is a multi task process. To enable rollback in case of an error, the RegionServer keeps an in-memory journal about the execution state. The steps taken by the RegionServer to execute the split are illustrated by Figure 1. Each step is labeled with its step number. Actions from RegionServers or Master are shown in red, while actions from the clients are show in green.\n尽管region分割是被在RegionServer本地处理的问题，但是分割过程要考虑很多动作。RegionServer在分割的前后会通知Master去更新.META.表，这样客户端可以发现新的子region和文件夹结构和HDFS的数据文件。分割是有多个任务的程序。为了再错误时能够回滚，RegionServer将执行的状态保存在内存中。RegionServer执行分割的过程如下图。每一步都标注了号码。RegionServer的操作或者Master操作用红色标注，客户端的操作是绿颜色。\n![Alt text](/img/DF2947BB-B48F-4C4E-B27A-C259D57EDA86.jpg)\n\n1. RegionServer decides locally to split the region, and prepares the split. As a first step, it creates a znode in zookeeper under /hbase/region-in-transition/region-name in SPLITTING state.\nRegionServe决定在本地分割region，并且开始准备分割。第一步，它首先在zookeeper的/hbase/region-in-transition/region-name下创建一个SPLITTING状态的znode。\n2. The Master learns about this znode, since it has a watcher for the parent region-in-transition znode.\n因为Master会监控父级znode region-in-transition，所以Master将获取到这个znode。\n3. RegionServer creates a sub-directory named “.splits” under the parent’s region directory in HDFS.\nRegionServer在HDFS上的父region目录下创建一个名为“.splits”的子文件夹\n4. RegionServer closes the parent region, forces a flush of the cache and marks the region as offline in its local data structures. At this point, client requests coming to the parent region will throw NotServingRegionException. The client will retry with some backoff.\nRegionServer关闭父region，强制执行一个缓存的flush并且在它本地的数据结构中标志这个region为offline。此时，客户端发送到父region的请求将会抛出NotServingRegionException，客户端重试一些回退。\n5. RegionServer create the region directories under .splits directory, for daughter regions A and B, and creates necessary data structures. Then it splits the store files, in the sense that it creates two Reference files per store file in the parent region. Those reference files will point to the parent regions files.\nRegionServer在.splits文件夹下为子regionA和B创建region文件夹并创建需要的数据结构。然后开始分割存储文件，此时它会为每一个存储文件创建2个引用文件指向父region。这些引用文件将会指向父region的文件。\n6. RegionServer creates the actual region directory in HDFS, and moves the reference files for each daughter.\nRegionServe在HDFS上创建真正的region文件夹并移动引用文件到每一个子region下。\n7. RegionServer sends a Put request to the .META. table, and sets the parent as offline in the .META. table and adds information about daughter regions. At this point, there won’t be individual entries in .META. for the daughters. Clients will see the parent region is split if they scan .META., but won’t know about the daughters until they appear in .META.. Also, if this Put to .META. succeeds, the parent will be effectively split. If the RegionServer fails before this RPC succeeds, Master and the next region server opening the region will clean dirty state about the region split. After the .META. update, though, the region split will be rolled-forward by Master.\nRegionServer发送一个Put请求到.META.表，并在.META.表中将父region设置为offline，并添加子region的信息。客户端通过扫描.META.会发现父region正在做分割，但是并不知道子region知道在.META.表中有他们的信息。如果这个PUT请求成功了，父region会被分割。如果RegionServer在这个RPC成功之前失败了，Master和下一个访问region的region server将会清除region分割的垃圾状态。在更新.META.表后，Master会继续向前处理region的分割。\n8. RegionServer opens daughters in parallel to accept writes.\nRegionServer打开子region并接受向其中写入数据。\n9. RegionServer adds the daughters A and B to .META. together with information that it hosts the regions. After this point, clients can discover the new regions, and issue requests to the new region. Clients cache the .META. entries locally, but when they make requests to the region server or .META., their caches will be invalidated, and they will learn about the new regions from .META..\nRegionServer添加子region A和B和他们所在节点的信息到.META.。在这个操作完成后，客户端可以发现新的region，并发送请求到新的region。客户端本地缓存.META.，但是当他们发送请求到region server或者 .META.，他们的缓存将会验证，这时他们会更新他们的 .META.。\n10. RegionServer updates znode /hbase/region-in-transition/region-name in zookeeper to state SPLIT, so that the master can learn about it. The balancer can freely re-assign the daughter regions to other region servers if it chooses so.\nRegionServer在zookeeper中更新znode /hbase/region-in-transition/region-name的状态为SPLIT，这样master就获取到这个状态。balancer可以将子region重新分配给其他的region server。\n11. After the split, meta and HDFS will still contain references to the parent region. Those references will be removed when compactions in daughter regions rewrite the data files. Garbage collection tasks in the master periodically checks whether the daughter regions still refer to parents files.  If not, the parent region will be removed.\n在分割后，HDFS仍旧保留这执行父region的引用文件。这些引用在子region重写数据时将会别移除。Master垃圾回收任务也会检查子region是否还有引用文件执行父region，如果没有父region将会被移除。\n\n## REGION MERGES\n---\n\nUnlike region splitting, HBase at this point does not provide usable tools for merging regions. Although there are HMerge, and Merge tools, they are not very suited for general usage. There currently is no support for online tables, and auto-merging functionality. However, with issues like OnlineMerge, Master initiated automatic region merges, ZK-based Read/Write locks for table operations, we are working to stabilize region splits and enable better support for region merges. Stay tuned!\n\nCONCLUSION\n\nAs you can see, under-the-hood HBase does a lot of housekeeping to manage regions splits and do automated sharding through regions. However, HBase also provides the necessary tools around region management, so that you can manage the splitting process. You can also control precisely when and how region splits are happening via a RegionSplitPolicy.\nThe number of regions in a table, and how those regions are split are crucial factors in understanding, and tuning your HBase cluster load. If you can estimate your key distribution, you should create the table with pre-splitting to get the optimum initial load performance. You can start with a lower multiple of number of region servers as a starting point for initial number of regions, and let automated splitting take over. If you cannot correctly estimate the initial split points, it is better to just create the table with one region, and start some initial load with automated splitting, and use IncreasingToUpperBoundRegionSplitPolicy. However, keep in mind that, the total number of regions will stabilize over time, and the current set of region split points will be determined from the data that the table has received so far. You may want to monitor the load distribution across the regions at all times, and if the load distribution changes over time, use manual splitting, or set more aggressive region split sizes. Lastly, you can try out the upcoming online merge feature and contribute your use case.\n\n","source":"_posts/HBase-Region-Splitting-and-Merging.md","raw":"---\ntitle: HBase Region Splitting and Merging\ndate: 2018-11-28 06:01:03\ntags: HBase\n---\n\n源文：https://hortonworks.com/blog/apache-hbase-region-splitting-and-merging/\n\nFor this post, we take a technical deep-dive into one of the core areas of HBase. Specifically, we will look at how Apache HBase distributes load through regions, and manages region splitting. HBase stores rows of data in tables. Tables are split into chunks of rows called “regions”. Those regions are distributed across the cluster, hosted and made available to client processes by the RegionServer process. A region is a continuous range within the key space, meaning all rows in the table that sort between the region’s start key and end key are stored in the same region. Regions are non-overlapping, i.e. a single row key belongs to exactly one region at any point in time. A region is only served by a single region server at any point in time, which is how HBase guarantees strong consistency within a single row#. Together with the -ROOT- and .META. regions, a table’s regions effectively form a 3 level B-Tree for the purposes of locating a row within a table.\n\n在这篇文章中，我们将深入到HBase的一块核心内容。尤其是我们将要看下Apache Hbase怎么通过regions做负载的，怎么管理region的切分。HBase将数据存储在表中。表被切分为数据块即regions。这些regions被分配在整个集群，可以被客户端程序调用RegionServer来使用。一个region就是一个连续的key空间，意味着在一个表中，所有在region的开始key和结束key之间的数据都被存储在一个region里。Regions是不重叠的，例如，在任何一个时间点上一个row key都只属于一个特定的region，一个region只会存在一个region server上。这样HBase就能保证数据的强一致性。算上-ROOT-和.META这2个region，一个表的region可以有效地通过一个3层B-树获取一行数据在一个表中的位置。\n<!-- more -->\nA Region in turn, consists of many “Stores”, which correspond to column families. A store contains one memstore and zero or more store files. The data for each column family is stored and accessed separately.\n一个Region反过来，由许多列簇的存储组成。一个列簇的存储包含一个内存存储和零到多个存储文件。数据的每一个列簇都被分开存储和使用。\n\nA table typically consists of many regions, which are in turn hosted by many region servers. Thus, regions are the physical mechanism used to distribute the write and query load across region servers. When a table is first created, HBase, by default, will allocate only one region for the table. This means that initially, all requests will go to a single region server, regardless of the number of region servers. This is the primary reason why initial phases of loading data into an empty table cannot utilize the whole capacity of the cluster.\n一个表通常由多个region组成，这些region分布在多个region server上。因此，region被分布式的region server在物理上进行读写和查询等操作。当一个表被初始创建时，HBase默认会为其分配一个region。这就意味着，在开始时所有的请求都会发送到一个单独的region servers上，无论还有多少其他的region server。这就在初始加载数据进入一个空表时，不能利用整个集群的性能的主要原因。\n\n## PRE-SPLITTING\n---\n\nThe reason HBase creates only one region for the table is that it cannot possibly know how to create the split points within the row key space. Making such decisions is based highly on the distribution of the keys in your data. Rather than taking a guess and leaving you to deal with the consequences, HBase does provide you with tools to manage this from the client. With a process called pre-splitting, you can create a table with many regions by supplying the split points at the table creation time. Since pre-splitting will ensure that the initial load is more evenly distributed throughout the cluster, you should always consider using it if you know your key distribution beforehand. However, pre-splitting also has a risk of creating regions, that do not truly distribute the load evenly because of data skew, or in the presence of very hot or large rows. If the initial set of region split points is chosen poorly, you may end up with heterogeneous load distribution, which will in turn limit your clusters performance.\nHBase在初始时只创建一个region的原因是它不知道怎么去创建row key空间的分割点。需要根据数据的分布才能做出这个决定。HBase提供了一个工具帮助你从客户端管理它，而不是随便猜测一个值或者留给你去处理。通过一个叫做pre-splitting的程序，一个可以在创建表的时候通过提供切割点来创建多个region。因为pre-splitting将确保在初始加载数据时会更均匀的分布在整个集群。所以如果你只要你的key的分布情况，就应该尽量的考虑使用它。然而，pre-splitting在创建表时也有一个风险，就是由于数据倾斜或者热冷数据或者某些行的数据非常大使它不能完全的平均分布在集群上。如果初始的region切割点设置的不好，可能会产生各种各样的加载，这可能会影响你集群的性能。\n\nThere is no short answer for the optimal number of regions for a given load, but you can start with a lower multiple of the number of region servers as number of splits, then let automated splitting take care of the rest.\n对于一个load任务使用多少region适合并没有一个很直观的答案，但是你可以在开始时设置较为region server的几倍数，然后随着数据的增加让它自动切分。\n\nOne issue with pre-splitting is calculating the split points for the table. You can use the RegionSplitter utility. RegionSplitter creates the split points, by using a pluggable SplitAlgorithm. HexStringSplit and UniformSplit are two predefined algorithms. The former can be used if the row keys have a prefix for hexadecimal strings (like if you are using hashes as prefixes). The latter divides up the key space evenly assuming they are random byte arrays. You can also implement your custom SplitAlgorithm and use it from the RegionSplitter utility.\n\npre-splitting的一个问题就是计算表的切割点。你可以使用RegionSplitter工具。RegionSplitter使用一个可插拔的算法来创建切割点。HexStringSplit和UniformSplit是2个预定义的算法。当row key有一个16进制字符串的前缀时，可以使用HexStringSplit算法。UniformSplit算法假设key是随机的字节数组，然后去切割key空间。你也可以自己写Split算法并通过RegionSplitter工具来使用它。\n\n```\n$ hbase org.apache.hadoop.hbase.util.RegionSplitter test_table HexStringSplit -c 10 -f f1\n```\n-c 10,指定需要的region是10个，-f 指定列簇, 分隔符是“:”. 这个工具将会创建一个有10个region的名为 “test_table”的表。\n```\n13/01/18 18:49:32 DEBUG hbase.HRegionInfo: Current INFO from scan results = {NAME => 'test_table,,1358563771069.acc1ad1b7962564fc3a43e5907e8db33.', STARTKEY => '', ENDKEY => '19999999', ENCODED => acc1ad1b7962564fc3a43e5907e8db33,}\n13/01/18 18:49:32 DEBUG hbase.HRegionInfo: Current INFO from scan results = {NAME => 'test_table,19999999,1358563771096.37ec12df6bd0078f5573565af415c91b.', STARTKEY => '19999999', ENDKEY => '33333332', ENCODED => 37ec12df6bd0078f5573565af415c91b,}\n...\n```\nIf you have split points at hand, you can also use the HBase shell, to create the table with the desired split points.\n如果你知道切割点，你也可以使用HBase shell在创建表的时候指定切割点。\n```\nhbase(main):015:0> create 'test_table', 'f1', SPLITS=> ['a', 'b', 'c']\n```\nor\n```\n$ echo -e  \"anbnc\" >/tmp/splits\nhbase(main):015:0> create 'test_table', 'f1', SPLITSFILE=>'/tmp/splits'\n```\n\nFor optimum load distribution, you should think about your data model, and key distribution for choosing the correct split algorithm or split points. Regardless of the method you chose to create the table with pre determined number of regions, you can now start loading the data into the table, and see that the load is distributed throughout your cluster. You can let automated splitting take over once data ingest starts, and continuously monitor the total number of regions for the table.\n为了更好的分布加载数据，你应该思考下你的数据模型和key的分布情况，然后选择合适的分割算法或者分割点。无论你选择哪种方法创建你的表，你可以开始导入数据进入你的表，并查看加载是否分布在你的集群上。当你已经开始导入数据了，你也可以让它自动切割，然后监控表的region的个数。\n\n## AUTO SPLITTING\n---\nRegardless of whether pre-splitting is used or not, once a region gets to a certain limit, it is automatically split into two regions. If you are using HBase 0.94 (which comes with HDP-1.2), you can configure when HBase decides to split a region, and how it calculates the split points via the pluggable RegionSplitPolicy API. There are a couple predefined region split policies: ConstantSizeRegionSplitPolicy, IncreasingToUpperBoundRegionSplitPolicy, and KeyPrefixRegionSplitPolicy.\n无论是否使用pre-splitting，当一个region达到一定的限制，它会自动的分割成2个region。如果使用HBase0.94 （HDP-1.2），你可以配置什么时候HBase决定去切割一个region，和它怎么通过可插拔[RegionSplitPolicy API](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.html)来计算切割点。已经有一些预定义的切割策略：[ConstantSizeRegionSplitPolicy](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.html)，[IncreasingToUpperBoundRegionSplitPolicy](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.html)和[KeyPrefixRegionSplitPilicy](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/KeyPrefixRegionSplitPolicy.html)。\n\nThe first one is the default and only split policy for HBase versions before 0.94. It splits the regions when the total data size for one of the stores (corresponding to a column-family) in the region gets bigger than configured “hbase.hregion.max.filesize”, which has a default value of 10GB. This split policy is ideal in cases, where you are have done pre-splitting, and are interested in getting lower number of regions per region server.\nConstantSizeRegionSplitPolicy是默认的切割策略，并且是HBase0.94之前的唯一策略。当一个region的一个store存储的数据大小大于配置的“hbase.hregion.max.filesize”值时，默认10GB，它就会自动的切割分区。这个切割策略是理想的，你已经预分区你的表，并且每个region server不会有太多的region。\n\nThe default split policy for HBase 0.94 and trunk is IncreasingToUpperBoundRegionSplitPolicy, which does more aggressive splitting based on the number of regions hosted in the same region server. The split policy uses the max store file size based on Min (R^2 * “hbase.hregion.memstore.flush.size”, “hbase.hregion.max.filesize”), where R is the number of regions of the same table hosted on the same regionserver. So for example, with the default memstore flush size of 128MB and the default max store size of 10GB, the first region on the region server will be split just after the first flush at 128MB. As number of regions hosted in the region server increases, it will use increasing split sizes: 512MB, 1152MB, 2GB, 3.2GB, 4.6GB, 6.2GB, etc. After reaching 9 regions, the split size will go beyond the configured “hbase.hregion.max.filesize”, at which point, 10GB split size will be used from then on. For both of these algorithms, regardless of when splitting occurs, the split point used is the rowkey that corresponds to the mid point in the “block index” for the largest store file in the largest store.\nHBase0.94默认的切割策略变成了IncreasingToUpperBoundRegionSplitPolicy，它会基于每个region server拥有的region个数对数据进行切割。这个切割策略使用最大store文件基于Min (R^2 * “hbase.hregion.memstore.flush.size”, “hbase.hregion.max.filesize”)，R是在一个region server上一个表拥有的region个数。例如，默认的内存存储是128MB，默认的最大store大小是10GB，只有在第一个flush达到128MB时，第一个region才会被切割。随着在region server上的region个数的增长，它将会增加分割大小：512MB, 1152MB, 2GB, 3.2GB, 4.6GB, 6.2GB,等待。当达到9个region时，分割大小将会超过参数 “hbase.hregion.max.filesize”，此时，10GB的切割大小将会被开始使用。对于所有的这些算法，无论何时发生拆分，所使用的拆分点都是对应于“块索引”中最大存储文件的中点的rowkey。\n\nKeyPrefixRegionSplitPolicy is a curious addition to the HBase arsenal. You can configure the length of the prefix for your row keys for grouping them, and this split policy ensures that the regions are not split in the middle of a group of rows having the same prefix. If you have set prefixes for your keys, then you can use this split policy to ensure that rows having the same rowkey prefix always end up in the same region. This grouping of records is sometimes referred to as “Entity Groups” or “Row Groups”. This is a key feature when considering use of the “local transactions” (alternative link) feature in your application design.\nKeyPrefixRegionSplitPolicy是一个很好的补充。你可以配置你用来分组的row key的前缀的长度，这个分割策略可以确保有相同的前缀的一组数据不会被从中间切割。如果你设置了你的key的前缀，你可以使用这个切割策略来确保你所有有相同前缀的数据不会被分割到不同的region中。这个分组是指实例组或者行组。当在你的应用设计考虑使用本地处理时这是一个关键的功能。\n\nYou can configure the default split policy to be used by setting the configuration “hbase.regionserver.region.split.policy”, or by configuring the table descriptor. For you brave souls, you can also implement your own custom split policy, and plug that in at table creation time, or by modifying an existing table:\n你可以使用配置参数“hbase.regionserver.region.split.policy”来配置你的默认切割策略，或者通过配置表的描述信息。你也可以使用你自己的切割策略，并且在你创建表的时候使用它，或者修改已经存在的表。\n```\nHTableDescriptor tableDesc = new HTableDescriptor(\"example-table\");\ntableDesc.setValue(HTableDescriptor.SPLIT_POLICY, AwesomeSplitPolicy.class.getName());\n//add columns etc\nadmin.createTable(tableDesc);\n```\n\nIf you are doing pre-splitting, and want to manually manage region splits, you can also disable region splits, by setting “hbase.hregion.max.filesize” to a high number and setting the split policy to ConstantSizeRegionSplitPolicy. However, you should use a safeguard value of like 100GB, so that regions does not grow beyond a region server’s capabilities. You can consider disabling automated splitting and rely on the initial set of regions from pre-splitting for example, if you are using uniform hashes for your key prefixes, and you can ensure that the read/write load to each region as well as its size is uniform across the regions in the table.\n如果你正在做pre-splitting，并且向要手动的管理你的region切割，你可以通过设置“hbase.hregion.max.filesize”一个比较大的值和设置切割策略为ConstantSizeRegionSplitPolicy来禁用它。然后，你应该使用一个比较安全的值如100GB，这样region不会增长到超过一个region server的容量。你可以考虑禁用自动切割和通过pre-splitting的初始设置，例如，如果你对你的key的前缀使用统一的hash，你可以确保对每个区域的读/写负载以及表中各个region的大小都是统一的。\n\n## FORCED SPLITS\n---\n\nHBase also enables clients to force split an online table from the client side. For example, the HBase shell can be used to split all regions of the table, or split a region, optionally by supplying a split point.\n```\nhbase(main):024:0> split 'b07d0034cbe72cb040ae9cf66300a10c', 'b'\n0 row(s) in 0.1620 seconds\n```\n\nWith careful monitoring of your HBase load distribution, if you see that some regions are getting uneven loads, you may consider manually splitting those regions to even-out the load and improve throughput. Another reason why you might want to do manual splits is when you see that the initial splits for the region turns out to be suboptimal, and you have disabled automated splits. That might happen for example, if the data distribution changes over time.\nHBase也提供通过客户端去强制分割在线的表。例如，可以使用HBase shell去分割表的所有region或者一个region，也可以通过提供分割点去切割。\n仔细监控你的HBase的加载分布，如果有些region变的不均匀，你需要考虑手分割这些region来提高图吞吐量。为什么你需要手动分割的另外一个原因是当你发现初始的region变的不标准或者你禁用了自分割。例如，你的数据分布随着时间一直在变化。\n\n## HOW REGION SPLITS ARE IMPLEMENTED\n---\n\nAs write requests are handled by the region server, they accumulate in an in-memory storage system called the “memstore”. Once the memstore fills, its content are written to disk as additional store files. This event is called a “memstore flush”. As store files accumulate, the RegionServer will “compact” them into combined, larger files. After each flush or compaction finishes, a region split request is enqueued if the RegionSplitPolicy decides that the region should be split into two. Since all data files in HBase are immutable, when a split happens, the newly created daughter regions will not rewrite all the data into new files. Instead, they will create  small sym-link like files, named Reference files, which point to either top or bottom part of the parent store file according to the split point. The reference file will be used just like a regular data file, but only half of the records. The region can only be split if there are no more references to the immutable data files of the parent region. Those reference files are cleaned gradually by compactions, so that the region will stop referring to its parents files, and can be split further.\n在HBase中，写的需求是由region server来处理的，他们被存储在内存存储系统“memstore”中。当memstore文件满了，它开始将数据写入存储在磁盘的存储文件中。这个过程叫做“memstore flush”。RegionServer将会把这些存储文件合并成大文件。每次flush或者合并完成后，如果RegionSplitPolicy决定这个region应该被分割成2个region，就会触发一个分割的请求。因为在HBase中所有的数据文件都是不可变的，当需要一个分割时，对于一个region的2个子region，他们不是重新将所有的数据写入新的文件，而是会先创建小的类似软连接的叫做引用文件的文件。一个region只有在没有引用指向它的父文件时才可以被分割。这些引用文件会被compaction清除，这样它就不会执行他们的父文件，然后可以被分割。\n\n\nAlthough splitting the region is a local decision made at the RegionServer, the split process itself must coordinate with many actors. The RegionServer notifies the Master before and after the split, updates the .META. table so that clients can discover the new daughter regions, and rearranges the directory structure and data files in HDFS. Split is a multi task process. To enable rollback in case of an error, the RegionServer keeps an in-memory journal about the execution state. The steps taken by the RegionServer to execute the split are illustrated by Figure 1. Each step is labeled with its step number. Actions from RegionServers or Master are shown in red, while actions from the clients are show in green.\n尽管region分割是被在RegionServer本地处理的问题，但是分割过程要考虑很多动作。RegionServer在分割的前后会通知Master去更新.META.表，这样客户端可以发现新的子region和文件夹结构和HDFS的数据文件。分割是有多个任务的程序。为了再错误时能够回滚，RegionServer将执行的状态保存在内存中。RegionServer执行分割的过程如下图。每一步都标注了号码。RegionServer的操作或者Master操作用红色标注，客户端的操作是绿颜色。\n![Alt text](/img/DF2947BB-B48F-4C4E-B27A-C259D57EDA86.jpg)\n\n1. RegionServer decides locally to split the region, and prepares the split. As a first step, it creates a znode in zookeeper under /hbase/region-in-transition/region-name in SPLITTING state.\nRegionServe决定在本地分割region，并且开始准备分割。第一步，它首先在zookeeper的/hbase/region-in-transition/region-name下创建一个SPLITTING状态的znode。\n2. The Master learns about this znode, since it has a watcher for the parent region-in-transition znode.\n因为Master会监控父级znode region-in-transition，所以Master将获取到这个znode。\n3. RegionServer creates a sub-directory named “.splits” under the parent’s region directory in HDFS.\nRegionServer在HDFS上的父region目录下创建一个名为“.splits”的子文件夹\n4. RegionServer closes the parent region, forces a flush of the cache and marks the region as offline in its local data structures. At this point, client requests coming to the parent region will throw NotServingRegionException. The client will retry with some backoff.\nRegionServer关闭父region，强制执行一个缓存的flush并且在它本地的数据结构中标志这个region为offline。此时，客户端发送到父region的请求将会抛出NotServingRegionException，客户端重试一些回退。\n5. RegionServer create the region directories under .splits directory, for daughter regions A and B, and creates necessary data structures. Then it splits the store files, in the sense that it creates two Reference files per store file in the parent region. Those reference files will point to the parent regions files.\nRegionServer在.splits文件夹下为子regionA和B创建region文件夹并创建需要的数据结构。然后开始分割存储文件，此时它会为每一个存储文件创建2个引用文件指向父region。这些引用文件将会指向父region的文件。\n6. RegionServer creates the actual region directory in HDFS, and moves the reference files for each daughter.\nRegionServe在HDFS上创建真正的region文件夹并移动引用文件到每一个子region下。\n7. RegionServer sends a Put request to the .META. table, and sets the parent as offline in the .META. table and adds information about daughter regions. At this point, there won’t be individual entries in .META. for the daughters. Clients will see the parent region is split if they scan .META., but won’t know about the daughters until they appear in .META.. Also, if this Put to .META. succeeds, the parent will be effectively split. If the RegionServer fails before this RPC succeeds, Master and the next region server opening the region will clean dirty state about the region split. After the .META. update, though, the region split will be rolled-forward by Master.\nRegionServer发送一个Put请求到.META.表，并在.META.表中将父region设置为offline，并添加子region的信息。客户端通过扫描.META.会发现父region正在做分割，但是并不知道子region知道在.META.表中有他们的信息。如果这个PUT请求成功了，父region会被分割。如果RegionServer在这个RPC成功之前失败了，Master和下一个访问region的region server将会清除region分割的垃圾状态。在更新.META.表后，Master会继续向前处理region的分割。\n8. RegionServer opens daughters in parallel to accept writes.\nRegionServer打开子region并接受向其中写入数据。\n9. RegionServer adds the daughters A and B to .META. together with information that it hosts the regions. After this point, clients can discover the new regions, and issue requests to the new region. Clients cache the .META. entries locally, but when they make requests to the region server or .META., their caches will be invalidated, and they will learn about the new regions from .META..\nRegionServer添加子region A和B和他们所在节点的信息到.META.。在这个操作完成后，客户端可以发现新的region，并发送请求到新的region。客户端本地缓存.META.，但是当他们发送请求到region server或者 .META.，他们的缓存将会验证，这时他们会更新他们的 .META.。\n10. RegionServer updates znode /hbase/region-in-transition/region-name in zookeeper to state SPLIT, so that the master can learn about it. The balancer can freely re-assign the daughter regions to other region servers if it chooses so.\nRegionServer在zookeeper中更新znode /hbase/region-in-transition/region-name的状态为SPLIT，这样master就获取到这个状态。balancer可以将子region重新分配给其他的region server。\n11. After the split, meta and HDFS will still contain references to the parent region. Those references will be removed when compactions in daughter regions rewrite the data files. Garbage collection tasks in the master periodically checks whether the daughter regions still refer to parents files.  If not, the parent region will be removed.\n在分割后，HDFS仍旧保留这执行父region的引用文件。这些引用在子region重写数据时将会别移除。Master垃圾回收任务也会检查子region是否还有引用文件执行父region，如果没有父region将会被移除。\n\n## REGION MERGES\n---\n\nUnlike region splitting, HBase at this point does not provide usable tools for merging regions. Although there are HMerge, and Merge tools, they are not very suited for general usage. There currently is no support for online tables, and auto-merging functionality. However, with issues like OnlineMerge, Master initiated automatic region merges, ZK-based Read/Write locks for table operations, we are working to stabilize region splits and enable better support for region merges. Stay tuned!\n\nCONCLUSION\n\nAs you can see, under-the-hood HBase does a lot of housekeeping to manage regions splits and do automated sharding through regions. However, HBase also provides the necessary tools around region management, so that you can manage the splitting process. You can also control precisely when and how region splits are happening via a RegionSplitPolicy.\nThe number of regions in a table, and how those regions are split are crucial factors in understanding, and tuning your HBase cluster load. If you can estimate your key distribution, you should create the table with pre-splitting to get the optimum initial load performance. You can start with a lower multiple of number of region servers as a starting point for initial number of regions, and let automated splitting take over. If you cannot correctly estimate the initial split points, it is better to just create the table with one region, and start some initial load with automated splitting, and use IncreasingToUpperBoundRegionSplitPolicy. However, keep in mind that, the total number of regions will stabilize over time, and the current set of region split points will be determined from the data that the table has received so far. You may want to monitor the load distribution across the regions at all times, and if the load distribution changes over time, use manual splitting, or set more aggressive region split sizes. Lastly, you can try out the upcoming online merge feature and contribute your use case.\n\n","slug":"HBase-Region-Splitting-and-Merging","published":1,"updated":"2018-11-27T22:12:01.973Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzv0013inamkuwqi8g9","content":"<p>源文：<a href=\"https://hortonworks.com/blog/apache-hbase-region-splitting-and-merging/\" target=\"_blank\" rel=\"noopener\">https://hortonworks.com/blog/apache-hbase-region-splitting-and-merging/</a></p>\n<p>For this post, we take a technical deep-dive into one of the core areas of HBase. Specifically, we will look at how Apache HBase distributes load through regions, and manages region splitting. HBase stores rows of data in tables. Tables are split into chunks of rows called “regions”. Those regions are distributed across the cluster, hosted and made available to client processes by the RegionServer process. A region is a continuous range within the key space, meaning all rows in the table that sort between the region’s start key and end key are stored in the same region. Regions are non-overlapping, i.e. a single row key belongs to exactly one region at any point in time. A region is only served by a single region server at any point in time, which is how HBase guarantees strong consistency within a single row#. Together with the -ROOT- and .META. regions, a table’s regions effectively form a 3 level B-Tree for the purposes of locating a row within a table.</p>\n<p>在这篇文章中，我们将深入到HBase的一块核心内容。尤其是我们将要看下Apache Hbase怎么通过regions做负载的，怎么管理region的切分。HBase将数据存储在表中。表被切分为数据块即regions。这些regions被分配在整个集群，可以被客户端程序调用RegionServer来使用。一个region就是一个连续的key空间，意味着在一个表中，所有在region的开始key和结束key之间的数据都被存储在一个region里。Regions是不重叠的，例如，在任何一个时间点上一个row key都只属于一个特定的region，一个region只会存在一个region server上。这样HBase就能保证数据的强一致性。算上-ROOT-和.META这2个region，一个表的region可以有效地通过一个3层B-树获取一行数据在一个表中的位置。<br><a id=\"more\"></a><br>A Region in turn, consists of many “Stores”, which correspond to column families. A store contains one memstore and zero or more store files. The data for each column family is stored and accessed separately.<br>一个Region反过来，由许多列簇的存储组成。一个列簇的存储包含一个内存存储和零到多个存储文件。数据的每一个列簇都被分开存储和使用。</p>\n<p>A table typically consists of many regions, which are in turn hosted by many region servers. Thus, regions are the physical mechanism used to distribute the write and query load across region servers. When a table is first created, HBase, by default, will allocate only one region for the table. This means that initially, all requests will go to a single region server, regardless of the number of region servers. This is the primary reason why initial phases of loading data into an empty table cannot utilize the whole capacity of the cluster.<br>一个表通常由多个region组成，这些region分布在多个region server上。因此，region被分布式的region server在物理上进行读写和查询等操作。当一个表被初始创建时，HBase默认会为其分配一个region。这就意味着，在开始时所有的请求都会发送到一个单独的region servers上，无论还有多少其他的region server。这就在初始加载数据进入一个空表时，不能利用整个集群的性能的主要原因。</p>\n<h2 id=\"PRE-SPLITTING\"><a href=\"#PRE-SPLITTING\" class=\"headerlink\" title=\"PRE-SPLITTING\"></a>PRE-SPLITTING</h2><hr>\n<p>The reason HBase creates only one region for the table is that it cannot possibly know how to create the split points within the row key space. Making such decisions is based highly on the distribution of the keys in your data. Rather than taking a guess and leaving you to deal with the consequences, HBase does provide you with tools to manage this from the client. With a process called pre-splitting, you can create a table with many regions by supplying the split points at the table creation time. Since pre-splitting will ensure that the initial load is more evenly distributed throughout the cluster, you should always consider using it if you know your key distribution beforehand. However, pre-splitting also has a risk of creating regions, that do not truly distribute the load evenly because of data skew, or in the presence of very hot or large rows. If the initial set of region split points is chosen poorly, you may end up with heterogeneous load distribution, which will in turn limit your clusters performance.<br>HBase在初始时只创建一个region的原因是它不知道怎么去创建row key空间的分割点。需要根据数据的分布才能做出这个决定。HBase提供了一个工具帮助你从客户端管理它，而不是随便猜测一个值或者留给你去处理。通过一个叫做pre-splitting的程序，一个可以在创建表的时候通过提供切割点来创建多个region。因为pre-splitting将确保在初始加载数据时会更均匀的分布在整个集群。所以如果你只要你的key的分布情况，就应该尽量的考虑使用它。然而，pre-splitting在创建表时也有一个风险，就是由于数据倾斜或者热冷数据或者某些行的数据非常大使它不能完全的平均分布在集群上。如果初始的region切割点设置的不好，可能会产生各种各样的加载，这可能会影响你集群的性能。</p>\n<p>There is no short answer for the optimal number of regions for a given load, but you can start with a lower multiple of the number of region servers as number of splits, then let automated splitting take care of the rest.<br>对于一个load任务使用多少region适合并没有一个很直观的答案，但是你可以在开始时设置较为region server的几倍数，然后随着数据的增加让它自动切分。</p>\n<p>One issue with pre-splitting is calculating the split points for the table. You can use the RegionSplitter utility. RegionSplitter creates the split points, by using a pluggable SplitAlgorithm. HexStringSplit and UniformSplit are two predefined algorithms. The former can be used if the row keys have a prefix for hexadecimal strings (like if you are using hashes as prefixes). The latter divides up the key space evenly assuming they are random byte arrays. You can also implement your custom SplitAlgorithm and use it from the RegionSplitter utility.</p>\n<p>pre-splitting的一个问题就是计算表的切割点。你可以使用RegionSplitter工具。RegionSplitter使用一个可插拔的算法来创建切割点。HexStringSplit和UniformSplit是2个预定义的算法。当row key有一个16进制字符串的前缀时，可以使用HexStringSplit算法。UniformSplit算法假设key是随机的字节数组，然后去切割key空间。你也可以自己写Split算法并通过RegionSplitter工具来使用它。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hbase org.apache.hadoop.hbase.util.RegionSplitter test_table HexStringSplit -c 10 -f f1</span><br></pre></td></tr></table></figure>\n<p>-c 10,指定需要的region是10个，-f 指定列簇, 分隔符是“:”. 这个工具将会创建一个有10个region的名为 “test_table”的表。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">13/01/18 18:49:32 DEBUG hbase.HRegionInfo: Current INFO from scan results = &#123;NAME =&gt; &apos;test_table,,1358563771069.acc1ad1b7962564fc3a43e5907e8db33.&apos;, STARTKEY =&gt; &apos;&apos;, ENDKEY =&gt; &apos;19999999&apos;, ENCODED =&gt; acc1ad1b7962564fc3a43e5907e8db33,&#125;</span><br><span class=\"line\">13/01/18 18:49:32 DEBUG hbase.HRegionInfo: Current INFO from scan results = &#123;NAME =&gt; &apos;test_table,19999999,1358563771096.37ec12df6bd0078f5573565af415c91b.&apos;, STARTKEY =&gt; &apos;19999999&apos;, ENDKEY =&gt; &apos;33333332&apos;, ENCODED =&gt; 37ec12df6bd0078f5573565af415c91b,&#125;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<p>If you have split points at hand, you can also use the HBase shell, to create the table with the desired split points.<br>如果你知道切割点，你也可以使用HBase shell在创建表的时候指定切割点。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):015:0&gt; create &apos;test_table&apos;, &apos;f1&apos;, SPLITS=&gt; [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]</span><br></pre></td></tr></table></figure></p>\n<p>or<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo -e  &quot;anbnc&quot; &gt;/tmp/splits</span><br><span class=\"line\">hbase(main):015:0&gt; create &apos;test_table&apos;, &apos;f1&apos;, SPLITSFILE=&gt;&apos;/tmp/splits&apos;</span><br></pre></td></tr></table></figure></p>\n<p>For optimum load distribution, you should think about your data model, and key distribution for choosing the correct split algorithm or split points. Regardless of the method you chose to create the table with pre determined number of regions, you can now start loading the data into the table, and see that the load is distributed throughout your cluster. You can let automated splitting take over once data ingest starts, and continuously monitor the total number of regions for the table.<br>为了更好的分布加载数据，你应该思考下你的数据模型和key的分布情况，然后选择合适的分割算法或者分割点。无论你选择哪种方法创建你的表，你可以开始导入数据进入你的表，并查看加载是否分布在你的集群上。当你已经开始导入数据了，你也可以让它自动切割，然后监控表的region的个数。</p>\n<h2 id=\"AUTO-SPLITTING\"><a href=\"#AUTO-SPLITTING\" class=\"headerlink\" title=\"AUTO SPLITTING\"></a>AUTO SPLITTING</h2><hr>\n<p>Regardless of whether pre-splitting is used or not, once a region gets to a certain limit, it is automatically split into two regions. If you are using HBase 0.94 (which comes with HDP-1.2), you can configure when HBase decides to split a region, and how it calculates the split points via the pluggable RegionSplitPolicy API. There are a couple predefined region split policies: ConstantSizeRegionSplitPolicy, IncreasingToUpperBoundRegionSplitPolicy, and KeyPrefixRegionSplitPolicy.<br>无论是否使用pre-splitting，当一个region达到一定的限制，它会自动的分割成2个region。如果使用HBase0.94 （HDP-1.2），你可以配置什么时候HBase决定去切割一个region，和它怎么通过可插拔<a href=\"http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.html\" target=\"_blank\" rel=\"noopener\">RegionSplitPolicy API</a>来计算切割点。已经有一些预定义的切割策略：<a href=\"http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.html\" target=\"_blank\" rel=\"noopener\">ConstantSizeRegionSplitPolicy</a>，<a href=\"http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.html\" target=\"_blank\" rel=\"noopener\">IncreasingToUpperBoundRegionSplitPolicy</a>和<a href=\"http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/KeyPrefixRegionSplitPolicy.html\" target=\"_blank\" rel=\"noopener\">KeyPrefixRegionSplitPilicy</a>。</p>\n<p>The first one is the default and only split policy for HBase versions before 0.94. It splits the regions when the total data size for one of the stores (corresponding to a column-family) in the region gets bigger than configured “hbase.hregion.max.filesize”, which has a default value of 10GB. This split policy is ideal in cases, where you are have done pre-splitting, and are interested in getting lower number of regions per region server.<br>ConstantSizeRegionSplitPolicy是默认的切割策略，并且是HBase0.94之前的唯一策略。当一个region的一个store存储的数据大小大于配置的“hbase.hregion.max.filesize”值时，默认10GB，它就会自动的切割分区。这个切割策略是理想的，你已经预分区你的表，并且每个region server不会有太多的region。</p>\n<p>The default split policy for HBase 0.94 and trunk is IncreasingToUpperBoundRegionSplitPolicy, which does more aggressive splitting based on the number of regions hosted in the same region server. The split policy uses the max store file size based on Min (R^2 <em> “hbase.hregion.memstore.flush.size”, “hbase.hregion.max.filesize”), where R is the number of regions of the same table hosted on the same regionserver. So for example, with the default memstore flush size of 128MB and the default max store size of 10GB, the first region on the region server will be split just after the first flush at 128MB. As number of regions hosted in the region server increases, it will use increasing split sizes: 512MB, 1152MB, 2GB, 3.2GB, 4.6GB, 6.2GB, etc. After reaching 9 regions, the split size will go beyond the configured “hbase.hregion.max.filesize”, at which point, 10GB split size will be used from then on. For both of these algorithms, regardless of when splitting occurs, the split point used is the rowkey that corresponds to the mid point in the “block index” for the largest store file in the largest store.<br>HBase0.94默认的切割策略变成了IncreasingToUpperBoundRegionSplitPolicy，它会基于每个region server拥有的region个数对数据进行切割。这个切割策略使用最大store文件基于Min (R^2 </em> “hbase.hregion.memstore.flush.size”, “hbase.hregion.max.filesize”)，R是在一个region server上一个表拥有的region个数。例如，默认的内存存储是128MB，默认的最大store大小是10GB，只有在第一个flush达到128MB时，第一个region才会被切割。随着在region server上的region个数的增长，它将会增加分割大小：512MB, 1152MB, 2GB, 3.2GB, 4.6GB, 6.2GB,等待。当达到9个region时，分割大小将会超过参数 “hbase.hregion.max.filesize”，此时，10GB的切割大小将会被开始使用。对于所有的这些算法，无论何时发生拆分，所使用的拆分点都是对应于“块索引”中最大存储文件的中点的rowkey。</p>\n<p>KeyPrefixRegionSplitPolicy is a curious addition to the HBase arsenal. You can configure the length of the prefix for your row keys for grouping them, and this split policy ensures that the regions are not split in the middle of a group of rows having the same prefix. If you have set prefixes for your keys, then you can use this split policy to ensure that rows having the same rowkey prefix always end up in the same region. This grouping of records is sometimes referred to as “Entity Groups” or “Row Groups”. This is a key feature when considering use of the “local transactions” (alternative link) feature in your application design.<br>KeyPrefixRegionSplitPolicy是一个很好的补充。你可以配置你用来分组的row key的前缀的长度，这个分割策略可以确保有相同的前缀的一组数据不会被从中间切割。如果你设置了你的key的前缀，你可以使用这个切割策略来确保你所有有相同前缀的数据不会被分割到不同的region中。这个分组是指实例组或者行组。当在你的应用设计考虑使用本地处理时这是一个关键的功能。</p>\n<p>You can configure the default split policy to be used by setting the configuration “hbase.regionserver.region.split.policy”, or by configuring the table descriptor. For you brave souls, you can also implement your own custom split policy, and plug that in at table creation time, or by modifying an existing table:<br>你可以使用配置参数“hbase.regionserver.region.split.policy”来配置你的默认切割策略，或者通过配置表的描述信息。你也可以使用你自己的切割策略，并且在你创建表的时候使用它，或者修改已经存在的表。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">HTableDescriptor tableDesc = new HTableDescriptor(&quot;example-table&quot;);</span><br><span class=\"line\">tableDesc.setValue(HTableDescriptor.SPLIT_POLICY, AwesomeSplitPolicy.class.getName());</span><br><span class=\"line\">//add columns etc</span><br><span class=\"line\">admin.createTable(tableDesc);</span><br></pre></td></tr></table></figure></p>\n<p>If you are doing pre-splitting, and want to manually manage region splits, you can also disable region splits, by setting “hbase.hregion.max.filesize” to a high number and setting the split policy to ConstantSizeRegionSplitPolicy. However, you should use a safeguard value of like 100GB, so that regions does not grow beyond a region server’s capabilities. You can consider disabling automated splitting and rely on the initial set of regions from pre-splitting for example, if you are using uniform hashes for your key prefixes, and you can ensure that the read/write load to each region as well as its size is uniform across the regions in the table.<br>如果你正在做pre-splitting，并且向要手动的管理你的region切割，你可以通过设置“hbase.hregion.max.filesize”一个比较大的值和设置切割策略为ConstantSizeRegionSplitPolicy来禁用它。然后，你应该使用一个比较安全的值如100GB，这样region不会增长到超过一个region server的容量。你可以考虑禁用自动切割和通过pre-splitting的初始设置，例如，如果你对你的key的前缀使用统一的hash，你可以确保对每个区域的读/写负载以及表中各个region的大小都是统一的。</p>\n<h2 id=\"FORCED-SPLITS\"><a href=\"#FORCED-SPLITS\" class=\"headerlink\" title=\"FORCED SPLITS\"></a>FORCED SPLITS</h2><hr>\n<p>HBase also enables clients to force split an online table from the client side. For example, the HBase shell can be used to split all regions of the table, or split a region, optionally by supplying a split point.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):024:0&gt; split &apos;b07d0034cbe72cb040ae9cf66300a10c&apos;, &apos;b&apos;</span><br><span class=\"line\">0 row(s) in 0.1620 seconds</span><br></pre></td></tr></table></figure></p>\n<p>With careful monitoring of your HBase load distribution, if you see that some regions are getting uneven loads, you may consider manually splitting those regions to even-out the load and improve throughput. Another reason why you might want to do manual splits is when you see that the initial splits for the region turns out to be suboptimal, and you have disabled automated splits. That might happen for example, if the data distribution changes over time.<br>HBase也提供通过客户端去强制分割在线的表。例如，可以使用HBase shell去分割表的所有region或者一个region，也可以通过提供分割点去切割。<br>仔细监控你的HBase的加载分布，如果有些region变的不均匀，你需要考虑手分割这些region来提高图吞吐量。为什么你需要手动分割的另外一个原因是当你发现初始的region变的不标准或者你禁用了自分割。例如，你的数据分布随着时间一直在变化。</p>\n<h2 id=\"HOW-REGION-SPLITS-ARE-IMPLEMENTED\"><a href=\"#HOW-REGION-SPLITS-ARE-IMPLEMENTED\" class=\"headerlink\" title=\"HOW REGION SPLITS ARE IMPLEMENTED\"></a>HOW REGION SPLITS ARE IMPLEMENTED</h2><hr>\n<p>As write requests are handled by the region server, they accumulate in an in-memory storage system called the “memstore”. Once the memstore fills, its content are written to disk as additional store files. This event is called a “memstore flush”. As store files accumulate, the RegionServer will “compact” them into combined, larger files. After each flush or compaction finishes, a region split request is enqueued if the RegionSplitPolicy decides that the region should be split into two. Since all data files in HBase are immutable, when a split happens, the newly created daughter regions will not rewrite all the data into new files. Instead, they will create  small sym-link like files, named Reference files, which point to either top or bottom part of the parent store file according to the split point. The reference file will be used just like a regular data file, but only half of the records. The region can only be split if there are no more references to the immutable data files of the parent region. Those reference files are cleaned gradually by compactions, so that the region will stop referring to its parents files, and can be split further.<br>在HBase中，写的需求是由region server来处理的，他们被存储在内存存储系统“memstore”中。当memstore文件满了，它开始将数据写入存储在磁盘的存储文件中。这个过程叫做“memstore flush”。RegionServer将会把这些存储文件合并成大文件。每次flush或者合并完成后，如果RegionSplitPolicy决定这个region应该被分割成2个region，就会触发一个分割的请求。因为在HBase中所有的数据文件都是不可变的，当需要一个分割时，对于一个region的2个子region，他们不是重新将所有的数据写入新的文件，而是会先创建小的类似软连接的叫做引用文件的文件。一个region只有在没有引用指向它的父文件时才可以被分割。这些引用文件会被compaction清除，这样它就不会执行他们的父文件，然后可以被分割。</p>\n<p>Although splitting the region is a local decision made at the RegionServer, the split process itself must coordinate with many actors. The RegionServer notifies the Master before and after the split, updates the .META. table so that clients can discover the new daughter regions, and rearranges the directory structure and data files in HDFS. Split is a multi task process. To enable rollback in case of an error, the RegionServer keeps an in-memory journal about the execution state. The steps taken by the RegionServer to execute the split are illustrated by Figure 1. Each step is labeled with its step number. Actions from RegionServers or Master are shown in red, while actions from the clients are show in green.<br>尽管region分割是被在RegionServer本地处理的问题，但是分割过程要考虑很多动作。RegionServer在分割的前后会通知Master去更新.META.表，这样客户端可以发现新的子region和文件夹结构和HDFS的数据文件。分割是有多个任务的程序。为了再错误时能够回滚，RegionServer将执行的状态保存在内存中。RegionServer执行分割的过程如下图。每一步都标注了号码。RegionServer的操作或者Master操作用红色标注，客户端的操作是绿颜色。<br><img src=\"/img/DF2947BB-B48F-4C4E-B27A-C259D57EDA86.jpg\" alt=\"Alt text\"></p>\n<ol>\n<li>RegionServer decides locally to split the region, and prepares the split. As a first step, it creates a znode in zookeeper under /hbase/region-in-transition/region-name in SPLITTING state.<br>RegionServe决定在本地分割region，并且开始准备分割。第一步，它首先在zookeeper的/hbase/region-in-transition/region-name下创建一个SPLITTING状态的znode。</li>\n<li>The Master learns about this znode, since it has a watcher for the parent region-in-transition znode.<br>因为Master会监控父级znode region-in-transition，所以Master将获取到这个znode。</li>\n<li>RegionServer creates a sub-directory named “.splits” under the parent’s region directory in HDFS.<br>RegionServer在HDFS上的父region目录下创建一个名为“.splits”的子文件夹</li>\n<li>RegionServer closes the parent region, forces a flush of the cache and marks the region as offline in its local data structures. At this point, client requests coming to the parent region will throw NotServingRegionException. The client will retry with some backoff.<br>RegionServer关闭父region，强制执行一个缓存的flush并且在它本地的数据结构中标志这个region为offline。此时，客户端发送到父region的请求将会抛出NotServingRegionException，客户端重试一些回退。</li>\n<li>RegionServer create the region directories under .splits directory, for daughter regions A and B, and creates necessary data structures. Then it splits the store files, in the sense that it creates two Reference files per store file in the parent region. Those reference files will point to the parent regions files.<br>RegionServer在.splits文件夹下为子regionA和B创建region文件夹并创建需要的数据结构。然后开始分割存储文件，此时它会为每一个存储文件创建2个引用文件指向父region。这些引用文件将会指向父region的文件。</li>\n<li>RegionServer creates the actual region directory in HDFS, and moves the reference files for each daughter.<br>RegionServe在HDFS上创建真正的region文件夹并移动引用文件到每一个子region下。</li>\n<li>RegionServer sends a Put request to the .META. table, and sets the parent as offline in the .META. table and adds information about daughter regions. At this point, there won’t be individual entries in .META. for the daughters. Clients will see the parent region is split if they scan .META., but won’t know about the daughters until they appear in .META.. Also, if this Put to .META. succeeds, the parent will be effectively split. If the RegionServer fails before this RPC succeeds, Master and the next region server opening the region will clean dirty state about the region split. After the .META. update, though, the region split will be rolled-forward by Master.<br>RegionServer发送一个Put请求到.META.表，并在.META.表中将父region设置为offline，并添加子region的信息。客户端通过扫描.META.会发现父region正在做分割，但是并不知道子region知道在.META.表中有他们的信息。如果这个PUT请求成功了，父region会被分割。如果RegionServer在这个RPC成功之前失败了，Master和下一个访问region的region server将会清除region分割的垃圾状态。在更新.META.表后，Master会继续向前处理region的分割。</li>\n<li>RegionServer opens daughters in parallel to accept writes.<br>RegionServer打开子region并接受向其中写入数据。</li>\n<li>RegionServer adds the daughters A and B to .META. together with information that it hosts the regions. After this point, clients can discover the new regions, and issue requests to the new region. Clients cache the .META. entries locally, but when they make requests to the region server or .META., their caches will be invalidated, and they will learn about the new regions from .META..<br>RegionServer添加子region A和B和他们所在节点的信息到.META.。在这个操作完成后，客户端可以发现新的region，并发送请求到新的region。客户端本地缓存.META.，但是当他们发送请求到region server或者 .META.，他们的缓存将会验证，这时他们会更新他们的 .META.。</li>\n<li>RegionServer updates znode /hbase/region-in-transition/region-name in zookeeper to state SPLIT, so that the master can learn about it. The balancer can freely re-assign the daughter regions to other region servers if it chooses so.<br>RegionServer在zookeeper中更新znode /hbase/region-in-transition/region-name的状态为SPLIT，这样master就获取到这个状态。balancer可以将子region重新分配给其他的region server。</li>\n<li>After the split, meta and HDFS will still contain references to the parent region. Those references will be removed when compactions in daughter regions rewrite the data files. Garbage collection tasks in the master periodically checks whether the daughter regions still refer to parents files.  If not, the parent region will be removed.<br>在分割后，HDFS仍旧保留这执行父region的引用文件。这些引用在子region重写数据时将会别移除。Master垃圾回收任务也会检查子region是否还有引用文件执行父region，如果没有父region将会被移除。</li>\n</ol>\n<h2 id=\"REGION-MERGES\"><a href=\"#REGION-MERGES\" class=\"headerlink\" title=\"REGION MERGES\"></a>REGION MERGES</h2><hr>\n<p>Unlike region splitting, HBase at this point does not provide usable tools for merging regions. Although there are HMerge, and Merge tools, they are not very suited for general usage. There currently is no support for online tables, and auto-merging functionality. However, with issues like OnlineMerge, Master initiated automatic region merges, ZK-based Read/Write locks for table operations, we are working to stabilize region splits and enable better support for region merges. Stay tuned!</p>\n<p>CONCLUSION</p>\n<p>As you can see, under-the-hood HBase does a lot of housekeeping to manage regions splits and do automated sharding through regions. However, HBase also provides the necessary tools around region management, so that you can manage the splitting process. You can also control precisely when and how region splits are happening via a RegionSplitPolicy.<br>The number of regions in a table, and how those regions are split are crucial factors in understanding, and tuning your HBase cluster load. If you can estimate your key distribution, you should create the table with pre-splitting to get the optimum initial load performance. You can start with a lower multiple of number of region servers as a starting point for initial number of regions, and let automated splitting take over. If you cannot correctly estimate the initial split points, it is better to just create the table with one region, and start some initial load with automated splitting, and use IncreasingToUpperBoundRegionSplitPolicy. However, keep in mind that, the total number of regions will stabilize over time, and the current set of region split points will be determined from the data that the table has received so far. You may want to monitor the load distribution across the regions at all times, and if the load distribution changes over time, use manual splitting, or set more aggressive region split sizes. Lastly, you can try out the upcoming online merge feature and contribute your use case.</p>\n","site":{"data":{}},"excerpt":"<p>源文：<a href=\"https://hortonworks.com/blog/apache-hbase-region-splitting-and-merging/\" target=\"_blank\" rel=\"noopener\">https://hortonworks.com/blog/apache-hbase-region-splitting-and-merging/</a></p>\n<p>For this post, we take a technical deep-dive into one of the core areas of HBase. Specifically, we will look at how Apache HBase distributes load through regions, and manages region splitting. HBase stores rows of data in tables. Tables are split into chunks of rows called “regions”. Those regions are distributed across the cluster, hosted and made available to client processes by the RegionServer process. A region is a continuous range within the key space, meaning all rows in the table that sort between the region’s start key and end key are stored in the same region. Regions are non-overlapping, i.e. a single row key belongs to exactly one region at any point in time. A region is only served by a single region server at any point in time, which is how HBase guarantees strong consistency within a single row#. Together with the -ROOT- and .META. regions, a table’s regions effectively form a 3 level B-Tree for the purposes of locating a row within a table.</p>\n<p>在这篇文章中，我们将深入到HBase的一块核心内容。尤其是我们将要看下Apache Hbase怎么通过regions做负载的，怎么管理region的切分。HBase将数据存储在表中。表被切分为数据块即regions。这些regions被分配在整个集群，可以被客户端程序调用RegionServer来使用。一个region就是一个连续的key空间，意味着在一个表中，所有在region的开始key和结束key之间的数据都被存储在一个region里。Regions是不重叠的，例如，在任何一个时间点上一个row key都只属于一个特定的region，一个region只会存在一个region server上。这样HBase就能保证数据的强一致性。算上-ROOT-和.META这2个region，一个表的region可以有效地通过一个3层B-树获取一行数据在一个表中的位置。<br>","more":"<br>A Region in turn, consists of many “Stores”, which correspond to column families. A store contains one memstore and zero or more store files. The data for each column family is stored and accessed separately.<br>一个Region反过来，由许多列簇的存储组成。一个列簇的存储包含一个内存存储和零到多个存储文件。数据的每一个列簇都被分开存储和使用。</p>\n<p>A table typically consists of many regions, which are in turn hosted by many region servers. Thus, regions are the physical mechanism used to distribute the write and query load across region servers. When a table is first created, HBase, by default, will allocate only one region for the table. This means that initially, all requests will go to a single region server, regardless of the number of region servers. This is the primary reason why initial phases of loading data into an empty table cannot utilize the whole capacity of the cluster.<br>一个表通常由多个region组成，这些region分布在多个region server上。因此，region被分布式的region server在物理上进行读写和查询等操作。当一个表被初始创建时，HBase默认会为其分配一个region。这就意味着，在开始时所有的请求都会发送到一个单独的region servers上，无论还有多少其他的region server。这就在初始加载数据进入一个空表时，不能利用整个集群的性能的主要原因。</p>\n<h2 id=\"PRE-SPLITTING\"><a href=\"#PRE-SPLITTING\" class=\"headerlink\" title=\"PRE-SPLITTING\"></a>PRE-SPLITTING</h2><hr>\n<p>The reason HBase creates only one region for the table is that it cannot possibly know how to create the split points within the row key space. Making such decisions is based highly on the distribution of the keys in your data. Rather than taking a guess and leaving you to deal with the consequences, HBase does provide you with tools to manage this from the client. With a process called pre-splitting, you can create a table with many regions by supplying the split points at the table creation time. Since pre-splitting will ensure that the initial load is more evenly distributed throughout the cluster, you should always consider using it if you know your key distribution beforehand. However, pre-splitting also has a risk of creating regions, that do not truly distribute the load evenly because of data skew, or in the presence of very hot or large rows. If the initial set of region split points is chosen poorly, you may end up with heterogeneous load distribution, which will in turn limit your clusters performance.<br>HBase在初始时只创建一个region的原因是它不知道怎么去创建row key空间的分割点。需要根据数据的分布才能做出这个决定。HBase提供了一个工具帮助你从客户端管理它，而不是随便猜测一个值或者留给你去处理。通过一个叫做pre-splitting的程序，一个可以在创建表的时候通过提供切割点来创建多个region。因为pre-splitting将确保在初始加载数据时会更均匀的分布在整个集群。所以如果你只要你的key的分布情况，就应该尽量的考虑使用它。然而，pre-splitting在创建表时也有一个风险，就是由于数据倾斜或者热冷数据或者某些行的数据非常大使它不能完全的平均分布在集群上。如果初始的region切割点设置的不好，可能会产生各种各样的加载，这可能会影响你集群的性能。</p>\n<p>There is no short answer for the optimal number of regions for a given load, but you can start with a lower multiple of the number of region servers as number of splits, then let automated splitting take care of the rest.<br>对于一个load任务使用多少region适合并没有一个很直观的答案，但是你可以在开始时设置较为region server的几倍数，然后随着数据的增加让它自动切分。</p>\n<p>One issue with pre-splitting is calculating the split points for the table. You can use the RegionSplitter utility. RegionSplitter creates the split points, by using a pluggable SplitAlgorithm. HexStringSplit and UniformSplit are two predefined algorithms. The former can be used if the row keys have a prefix for hexadecimal strings (like if you are using hashes as prefixes). The latter divides up the key space evenly assuming they are random byte arrays. You can also implement your custom SplitAlgorithm and use it from the RegionSplitter utility.</p>\n<p>pre-splitting的一个问题就是计算表的切割点。你可以使用RegionSplitter工具。RegionSplitter使用一个可插拔的算法来创建切割点。HexStringSplit和UniformSplit是2个预定义的算法。当row key有一个16进制字符串的前缀时，可以使用HexStringSplit算法。UniformSplit算法假设key是随机的字节数组，然后去切割key空间。你也可以自己写Split算法并通过RegionSplitter工具来使用它。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hbase org.apache.hadoop.hbase.util.RegionSplitter test_table HexStringSplit -c 10 -f f1</span><br></pre></td></tr></table></figure>\n<p>-c 10,指定需要的region是10个，-f 指定列簇, 分隔符是“:”. 这个工具将会创建一个有10个region的名为 “test_table”的表。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">13/01/18 18:49:32 DEBUG hbase.HRegionInfo: Current INFO from scan results = &#123;NAME =&gt; &apos;test_table,,1358563771069.acc1ad1b7962564fc3a43e5907e8db33.&apos;, STARTKEY =&gt; &apos;&apos;, ENDKEY =&gt; &apos;19999999&apos;, ENCODED =&gt; acc1ad1b7962564fc3a43e5907e8db33,&#125;</span><br><span class=\"line\">13/01/18 18:49:32 DEBUG hbase.HRegionInfo: Current INFO from scan results = &#123;NAME =&gt; &apos;test_table,19999999,1358563771096.37ec12df6bd0078f5573565af415c91b.&apos;, STARTKEY =&gt; &apos;19999999&apos;, ENDKEY =&gt; &apos;33333332&apos;, ENCODED =&gt; 37ec12df6bd0078f5573565af415c91b,&#125;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure></p>\n<p>If you have split points at hand, you can also use the HBase shell, to create the table with the desired split points.<br>如果你知道切割点，你也可以使用HBase shell在创建表的时候指定切割点。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):015:0&gt; create &apos;test_table&apos;, &apos;f1&apos;, SPLITS=&gt; [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]</span><br></pre></td></tr></table></figure></p>\n<p>or<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo -e  &quot;anbnc&quot; &gt;/tmp/splits</span><br><span class=\"line\">hbase(main):015:0&gt; create &apos;test_table&apos;, &apos;f1&apos;, SPLITSFILE=&gt;&apos;/tmp/splits&apos;</span><br></pre></td></tr></table></figure></p>\n<p>For optimum load distribution, you should think about your data model, and key distribution for choosing the correct split algorithm or split points. Regardless of the method you chose to create the table with pre determined number of regions, you can now start loading the data into the table, and see that the load is distributed throughout your cluster. You can let automated splitting take over once data ingest starts, and continuously monitor the total number of regions for the table.<br>为了更好的分布加载数据，你应该思考下你的数据模型和key的分布情况，然后选择合适的分割算法或者分割点。无论你选择哪种方法创建你的表，你可以开始导入数据进入你的表，并查看加载是否分布在你的集群上。当你已经开始导入数据了，你也可以让它自动切割，然后监控表的region的个数。</p>\n<h2 id=\"AUTO-SPLITTING\"><a href=\"#AUTO-SPLITTING\" class=\"headerlink\" title=\"AUTO SPLITTING\"></a>AUTO SPLITTING</h2><hr>\n<p>Regardless of whether pre-splitting is used or not, once a region gets to a certain limit, it is automatically split into two regions. If you are using HBase 0.94 (which comes with HDP-1.2), you can configure when HBase decides to split a region, and how it calculates the split points via the pluggable RegionSplitPolicy API. There are a couple predefined region split policies: ConstantSizeRegionSplitPolicy, IncreasingToUpperBoundRegionSplitPolicy, and KeyPrefixRegionSplitPolicy.<br>无论是否使用pre-splitting，当一个region达到一定的限制，它会自动的分割成2个region。如果使用HBase0.94 （HDP-1.2），你可以配置什么时候HBase决定去切割一个region，和它怎么通过可插拔<a href=\"http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.html\" target=\"_blank\" rel=\"noopener\">RegionSplitPolicy API</a>来计算切割点。已经有一些预定义的切割策略：<a href=\"http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.html\" target=\"_blank\" rel=\"noopener\">ConstantSizeRegionSplitPolicy</a>，<a href=\"http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.html\" target=\"_blank\" rel=\"noopener\">IncreasingToUpperBoundRegionSplitPolicy</a>和<a href=\"http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/KeyPrefixRegionSplitPolicy.html\" target=\"_blank\" rel=\"noopener\">KeyPrefixRegionSplitPilicy</a>。</p>\n<p>The first one is the default and only split policy for HBase versions before 0.94. It splits the regions when the total data size for one of the stores (corresponding to a column-family) in the region gets bigger than configured “hbase.hregion.max.filesize”, which has a default value of 10GB. This split policy is ideal in cases, where you are have done pre-splitting, and are interested in getting lower number of regions per region server.<br>ConstantSizeRegionSplitPolicy是默认的切割策略，并且是HBase0.94之前的唯一策略。当一个region的一个store存储的数据大小大于配置的“hbase.hregion.max.filesize”值时，默认10GB，它就会自动的切割分区。这个切割策略是理想的，你已经预分区你的表，并且每个region server不会有太多的region。</p>\n<p>The default split policy for HBase 0.94 and trunk is IncreasingToUpperBoundRegionSplitPolicy, which does more aggressive splitting based on the number of regions hosted in the same region server. The split policy uses the max store file size based on Min (R^2 <em> “hbase.hregion.memstore.flush.size”, “hbase.hregion.max.filesize”), where R is the number of regions of the same table hosted on the same regionserver. So for example, with the default memstore flush size of 128MB and the default max store size of 10GB, the first region on the region server will be split just after the first flush at 128MB. As number of regions hosted in the region server increases, it will use increasing split sizes: 512MB, 1152MB, 2GB, 3.2GB, 4.6GB, 6.2GB, etc. After reaching 9 regions, the split size will go beyond the configured “hbase.hregion.max.filesize”, at which point, 10GB split size will be used from then on. For both of these algorithms, regardless of when splitting occurs, the split point used is the rowkey that corresponds to the mid point in the “block index” for the largest store file in the largest store.<br>HBase0.94默认的切割策略变成了IncreasingToUpperBoundRegionSplitPolicy，它会基于每个region server拥有的region个数对数据进行切割。这个切割策略使用最大store文件基于Min (R^2 </em> “hbase.hregion.memstore.flush.size”, “hbase.hregion.max.filesize”)，R是在一个region server上一个表拥有的region个数。例如，默认的内存存储是128MB，默认的最大store大小是10GB，只有在第一个flush达到128MB时，第一个region才会被切割。随着在region server上的region个数的增长，它将会增加分割大小：512MB, 1152MB, 2GB, 3.2GB, 4.6GB, 6.2GB,等待。当达到9个region时，分割大小将会超过参数 “hbase.hregion.max.filesize”，此时，10GB的切割大小将会被开始使用。对于所有的这些算法，无论何时发生拆分，所使用的拆分点都是对应于“块索引”中最大存储文件的中点的rowkey。</p>\n<p>KeyPrefixRegionSplitPolicy is a curious addition to the HBase arsenal. You can configure the length of the prefix for your row keys for grouping them, and this split policy ensures that the regions are not split in the middle of a group of rows having the same prefix. If you have set prefixes for your keys, then you can use this split policy to ensure that rows having the same rowkey prefix always end up in the same region. This grouping of records is sometimes referred to as “Entity Groups” or “Row Groups”. This is a key feature when considering use of the “local transactions” (alternative link) feature in your application design.<br>KeyPrefixRegionSplitPolicy是一个很好的补充。你可以配置你用来分组的row key的前缀的长度，这个分割策略可以确保有相同的前缀的一组数据不会被从中间切割。如果你设置了你的key的前缀，你可以使用这个切割策略来确保你所有有相同前缀的数据不会被分割到不同的region中。这个分组是指实例组或者行组。当在你的应用设计考虑使用本地处理时这是一个关键的功能。</p>\n<p>You can configure the default split policy to be used by setting the configuration “hbase.regionserver.region.split.policy”, or by configuring the table descriptor. For you brave souls, you can also implement your own custom split policy, and plug that in at table creation time, or by modifying an existing table:<br>你可以使用配置参数“hbase.regionserver.region.split.policy”来配置你的默认切割策略，或者通过配置表的描述信息。你也可以使用你自己的切割策略，并且在你创建表的时候使用它，或者修改已经存在的表。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">HTableDescriptor tableDesc = new HTableDescriptor(&quot;example-table&quot;);</span><br><span class=\"line\">tableDesc.setValue(HTableDescriptor.SPLIT_POLICY, AwesomeSplitPolicy.class.getName());</span><br><span class=\"line\">//add columns etc</span><br><span class=\"line\">admin.createTable(tableDesc);</span><br></pre></td></tr></table></figure></p>\n<p>If you are doing pre-splitting, and want to manually manage region splits, you can also disable region splits, by setting “hbase.hregion.max.filesize” to a high number and setting the split policy to ConstantSizeRegionSplitPolicy. However, you should use a safeguard value of like 100GB, so that regions does not grow beyond a region server’s capabilities. You can consider disabling automated splitting and rely on the initial set of regions from pre-splitting for example, if you are using uniform hashes for your key prefixes, and you can ensure that the read/write load to each region as well as its size is uniform across the regions in the table.<br>如果你正在做pre-splitting，并且向要手动的管理你的region切割，你可以通过设置“hbase.hregion.max.filesize”一个比较大的值和设置切割策略为ConstantSizeRegionSplitPolicy来禁用它。然后，你应该使用一个比较安全的值如100GB，这样region不会增长到超过一个region server的容量。你可以考虑禁用自动切割和通过pre-splitting的初始设置，例如，如果你对你的key的前缀使用统一的hash，你可以确保对每个区域的读/写负载以及表中各个region的大小都是统一的。</p>\n<h2 id=\"FORCED-SPLITS\"><a href=\"#FORCED-SPLITS\" class=\"headerlink\" title=\"FORCED SPLITS\"></a>FORCED SPLITS</h2><hr>\n<p>HBase also enables clients to force split an online table from the client side. For example, the HBase shell can be used to split all regions of the table, or split a region, optionally by supplying a split point.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):024:0&gt; split &apos;b07d0034cbe72cb040ae9cf66300a10c&apos;, &apos;b&apos;</span><br><span class=\"line\">0 row(s) in 0.1620 seconds</span><br></pre></td></tr></table></figure></p>\n<p>With careful monitoring of your HBase load distribution, if you see that some regions are getting uneven loads, you may consider manually splitting those regions to even-out the load and improve throughput. Another reason why you might want to do manual splits is when you see that the initial splits for the region turns out to be suboptimal, and you have disabled automated splits. That might happen for example, if the data distribution changes over time.<br>HBase也提供通过客户端去强制分割在线的表。例如，可以使用HBase shell去分割表的所有region或者一个region，也可以通过提供分割点去切割。<br>仔细监控你的HBase的加载分布，如果有些region变的不均匀，你需要考虑手分割这些region来提高图吞吐量。为什么你需要手动分割的另外一个原因是当你发现初始的region变的不标准或者你禁用了自分割。例如，你的数据分布随着时间一直在变化。</p>\n<h2 id=\"HOW-REGION-SPLITS-ARE-IMPLEMENTED\"><a href=\"#HOW-REGION-SPLITS-ARE-IMPLEMENTED\" class=\"headerlink\" title=\"HOW REGION SPLITS ARE IMPLEMENTED\"></a>HOW REGION SPLITS ARE IMPLEMENTED</h2><hr>\n<p>As write requests are handled by the region server, they accumulate in an in-memory storage system called the “memstore”. Once the memstore fills, its content are written to disk as additional store files. This event is called a “memstore flush”. As store files accumulate, the RegionServer will “compact” them into combined, larger files. After each flush or compaction finishes, a region split request is enqueued if the RegionSplitPolicy decides that the region should be split into two. Since all data files in HBase are immutable, when a split happens, the newly created daughter regions will not rewrite all the data into new files. Instead, they will create  small sym-link like files, named Reference files, which point to either top or bottom part of the parent store file according to the split point. The reference file will be used just like a regular data file, but only half of the records. The region can only be split if there are no more references to the immutable data files of the parent region. Those reference files are cleaned gradually by compactions, so that the region will stop referring to its parents files, and can be split further.<br>在HBase中，写的需求是由region server来处理的，他们被存储在内存存储系统“memstore”中。当memstore文件满了，它开始将数据写入存储在磁盘的存储文件中。这个过程叫做“memstore flush”。RegionServer将会把这些存储文件合并成大文件。每次flush或者合并完成后，如果RegionSplitPolicy决定这个region应该被分割成2个region，就会触发一个分割的请求。因为在HBase中所有的数据文件都是不可变的，当需要一个分割时，对于一个region的2个子region，他们不是重新将所有的数据写入新的文件，而是会先创建小的类似软连接的叫做引用文件的文件。一个region只有在没有引用指向它的父文件时才可以被分割。这些引用文件会被compaction清除，这样它就不会执行他们的父文件，然后可以被分割。</p>\n<p>Although splitting the region is a local decision made at the RegionServer, the split process itself must coordinate with many actors. The RegionServer notifies the Master before and after the split, updates the .META. table so that clients can discover the new daughter regions, and rearranges the directory structure and data files in HDFS. Split is a multi task process. To enable rollback in case of an error, the RegionServer keeps an in-memory journal about the execution state. The steps taken by the RegionServer to execute the split are illustrated by Figure 1. Each step is labeled with its step number. Actions from RegionServers or Master are shown in red, while actions from the clients are show in green.<br>尽管region分割是被在RegionServer本地处理的问题，但是分割过程要考虑很多动作。RegionServer在分割的前后会通知Master去更新.META.表，这样客户端可以发现新的子region和文件夹结构和HDFS的数据文件。分割是有多个任务的程序。为了再错误时能够回滚，RegionServer将执行的状态保存在内存中。RegionServer执行分割的过程如下图。每一步都标注了号码。RegionServer的操作或者Master操作用红色标注，客户端的操作是绿颜色。<br><img src=\"/img/DF2947BB-B48F-4C4E-B27A-C259D57EDA86.jpg\" alt=\"Alt text\"></p>\n<ol>\n<li>RegionServer decides locally to split the region, and prepares the split. As a first step, it creates a znode in zookeeper under /hbase/region-in-transition/region-name in SPLITTING state.<br>RegionServe决定在本地分割region，并且开始准备分割。第一步，它首先在zookeeper的/hbase/region-in-transition/region-name下创建一个SPLITTING状态的znode。</li>\n<li>The Master learns about this znode, since it has a watcher for the parent region-in-transition znode.<br>因为Master会监控父级znode region-in-transition，所以Master将获取到这个znode。</li>\n<li>RegionServer creates a sub-directory named “.splits” under the parent’s region directory in HDFS.<br>RegionServer在HDFS上的父region目录下创建一个名为“.splits”的子文件夹</li>\n<li>RegionServer closes the parent region, forces a flush of the cache and marks the region as offline in its local data structures. At this point, client requests coming to the parent region will throw NotServingRegionException. The client will retry with some backoff.<br>RegionServer关闭父region，强制执行一个缓存的flush并且在它本地的数据结构中标志这个region为offline。此时，客户端发送到父region的请求将会抛出NotServingRegionException，客户端重试一些回退。</li>\n<li>RegionServer create the region directories under .splits directory, for daughter regions A and B, and creates necessary data structures. Then it splits the store files, in the sense that it creates two Reference files per store file in the parent region. Those reference files will point to the parent regions files.<br>RegionServer在.splits文件夹下为子regionA和B创建region文件夹并创建需要的数据结构。然后开始分割存储文件，此时它会为每一个存储文件创建2个引用文件指向父region。这些引用文件将会指向父region的文件。</li>\n<li>RegionServer creates the actual region directory in HDFS, and moves the reference files for each daughter.<br>RegionServe在HDFS上创建真正的region文件夹并移动引用文件到每一个子region下。</li>\n<li>RegionServer sends a Put request to the .META. table, and sets the parent as offline in the .META. table and adds information about daughter regions. At this point, there won’t be individual entries in .META. for the daughters. Clients will see the parent region is split if they scan .META., but won’t know about the daughters until they appear in .META.. Also, if this Put to .META. succeeds, the parent will be effectively split. If the RegionServer fails before this RPC succeeds, Master and the next region server opening the region will clean dirty state about the region split. After the .META. update, though, the region split will be rolled-forward by Master.<br>RegionServer发送一个Put请求到.META.表，并在.META.表中将父region设置为offline，并添加子region的信息。客户端通过扫描.META.会发现父region正在做分割，但是并不知道子region知道在.META.表中有他们的信息。如果这个PUT请求成功了，父region会被分割。如果RegionServer在这个RPC成功之前失败了，Master和下一个访问region的region server将会清除region分割的垃圾状态。在更新.META.表后，Master会继续向前处理region的分割。</li>\n<li>RegionServer opens daughters in parallel to accept writes.<br>RegionServer打开子region并接受向其中写入数据。</li>\n<li>RegionServer adds the daughters A and B to .META. together with information that it hosts the regions. After this point, clients can discover the new regions, and issue requests to the new region. Clients cache the .META. entries locally, but when they make requests to the region server or .META., their caches will be invalidated, and they will learn about the new regions from .META..<br>RegionServer添加子region A和B和他们所在节点的信息到.META.。在这个操作完成后，客户端可以发现新的region，并发送请求到新的region。客户端本地缓存.META.，但是当他们发送请求到region server或者 .META.，他们的缓存将会验证，这时他们会更新他们的 .META.。</li>\n<li>RegionServer updates znode /hbase/region-in-transition/region-name in zookeeper to state SPLIT, so that the master can learn about it. The balancer can freely re-assign the daughter regions to other region servers if it chooses so.<br>RegionServer在zookeeper中更新znode /hbase/region-in-transition/region-name的状态为SPLIT，这样master就获取到这个状态。balancer可以将子region重新分配给其他的region server。</li>\n<li>After the split, meta and HDFS will still contain references to the parent region. Those references will be removed when compactions in daughter regions rewrite the data files. Garbage collection tasks in the master periodically checks whether the daughter regions still refer to parents files.  If not, the parent region will be removed.<br>在分割后，HDFS仍旧保留这执行父region的引用文件。这些引用在子region重写数据时将会别移除。Master垃圾回收任务也会检查子region是否还有引用文件执行父region，如果没有父region将会被移除。</li>\n</ol>\n<h2 id=\"REGION-MERGES\"><a href=\"#REGION-MERGES\" class=\"headerlink\" title=\"REGION MERGES\"></a>REGION MERGES</h2><hr>\n<p>Unlike region splitting, HBase at this point does not provide usable tools for merging regions. Although there are HMerge, and Merge tools, they are not very suited for general usage. There currently is no support for online tables, and auto-merging functionality. However, with issues like OnlineMerge, Master initiated automatic region merges, ZK-based Read/Write locks for table operations, we are working to stabilize region splits and enable better support for region merges. Stay tuned!</p>\n<p>CONCLUSION</p>\n<p>As you can see, under-the-hood HBase does a lot of housekeeping to manage regions splits and do automated sharding through regions. However, HBase also provides the necessary tools around region management, so that you can manage the splitting process. You can also control precisely when and how region splits are happening via a RegionSplitPolicy.<br>The number of regions in a table, and how those regions are split are crucial factors in understanding, and tuning your HBase cluster load. If you can estimate your key distribution, you should create the table with pre-splitting to get the optimum initial load performance. You can start with a lower multiple of number of region servers as a starting point for initial number of regions, and let automated splitting take over. If you cannot correctly estimate the initial split points, it is better to just create the table with one region, and start some initial load with automated splitting, and use IncreasingToUpperBoundRegionSplitPolicy. However, keep in mind that, the total number of regions will stabilize over time, and the current set of region split points will be determined from the data that the table has received so far. You may want to monitor the load distribution across the regions at all times, and if the load distribution changes over time, use manual splitting, or set more aggressive region split sizes. Lastly, you can try out the upcoming online merge feature and contribute your use case.</p>"},{"title":"在CentOS6上安装CDH6","date":"2018-09-10T01:41:00.000Z","_content":"\n## 1.   安装准备\n### 1.1.    环境\n操作系统: CentOS release 6.9 (Final)\nJDK: 1.8.0_121\nCM:6.0.0\nCDH:6.0.0\nMySQL:5.5.59\n### 1.2.    下载信息\nCM6.0下载地址: https://archive.cloudera.com/cm6/6.0.0/redhat6/yum/RPMS/x86_64/\nallkeys.asc文件下载地址: https://archive.cloudera.com/cm6/6.0.0/\nCDH6.0下载地址: https://archive.cloudera.com/cdh6/6.0.0/parcels/\nMySQL下载地址: https://dev.mysql.com/downloads/mysql/5.5.html#downloads\nPython27下载地址: https://www.python.org/ftp/python/2.7.13/\nPsycopg下载地址: http://initd.org/psycopg/tarballs/PSYCOPG-2-5/\n\n<!-- more -->\n### 1.3.    修改主机名\n执行如下命令对其内容进行修改\nvim /etc/sysconfig/network\n做类型如下的修改\n\n```\nNETWORKING=yes\nHOSTNAME=node1-c6.example.com\n```\n并在命令行执行\n\n```\nhostname node1-c6.example.com\n```\n修改/etc/hosts文件,内容如下:\n\n```\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.11.234 node1-c6.example.com node1-c6\n192.168.11.235 node2-c6.example.com node2-c6\n192.168.11.236 node3-c6.example.com node3-c6\n192.168.11.237 node4-c6.example.com node4-c6\n192.168.11.238 node5-c6.example.com node5-c6\n```\n\n### 1.4.    关闭防火墙\n执行命令:\nservice iptables stop\n并禁止开机启动\nchkconfig iptables off\n### 1.5.    安装JDK\n进入jdk的rpm包所在的目录执行命令:\n\n```\nrpm -ivh jdk-8u121-linux-x64.rpm\n```\n修改/etc/profile文件,在最后添加如下内容:\n\n```\nexport JAVA_HOME=/usr/java/latest\nexport PATH=$JAVA_HOME/bin:$PATH\n```\n### 1.6.    关闭交换内存\n在文件/etc/sysctl.conf文件中添加如下内容:\nvm.swappiness=0\n并执行命令使其生效\nsysctl -p\n\n### 1.7.    关闭Selinux\n修改文件/etc/selinux/config,将其中的SELINUX=enforcing修改为disabled\n注:此操作需要重启生效,可以使用命令setenforce 1 使其临时生效   \n### 1.8.    关闭透明大页\n在命令行执行如下命令:\necho never > /sys/kernel/mm/transparent_hugepage/defrag\necho never > /sys/kernel/mm/transparent_hugepage/defrag\n此命令只是临时生效,如想永久生效,可将其添加到文件/etc/rc.local中去.\n### 1.9.    创建本地yum源仓库\n* 安装httpd服务  \n执行命令 yum install -y httpd 来安装此服务\n* 修改配置文件  \n在配置文件/etc/httpd/conf/httpd.conf的779行附件将如下内容修改为  \n\n```\nAddType application/x-compress .Z  \nAddType application/x-gzip .gz .tgz   \n```\n修改后:\n\n```\nAddType application/x-compress .Z\nAddType application/x-gzip .gz .tgz .parcel\n```\n* 启动httpd 服务\nservice httpd start\n* 将下载的cm和cdh文件分别放置在/var/www/html/repos/cm6/和/var/www/html/repos/cdh6/下,格式类型如下:\n\nrepos\n├── cdh6\n│   ├── 6.0.0\n│   │   └── parcels\n│   │       ├── CDH-6.0.0-1.cdh6.0.0.p0.537114-el6.parcel\n│   │       ├── CDH-6.0.0-1.cdh6.0.0.p0.537114-el6.parcel.sha256\n│   │       ├── index.html\n│   │       └── manifest.json\n└── cm6\n    ├── 6.0.0\n    │   ├── cloudera-manager-agent-6.0.0-530873.el6.x86_64.rpm\n    │   ├── cloudera-manager-daemons-6.0.0-530873.el6.x86_64.rpm\n    │   ├── cloudera-manager-server-6.0.0-530873.el6.x86_64.rpm\n    │   ├── cloudera-manager-server-db-2-6.0.0-530873.el6.x86_64.rpm\n    │   ├── index.html\n    │   └── oracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpm\n    ├── allkeys.asc\n        └── repomd.xml\n\n* 安装createrepo软件\nyum install -y createrepo\n* 在/var/www/html/repos下执行命令\ncreaterepo cm6\ncreaterepo cdh6\n* 在浏览器查看\ncm6: http://192.168.11.234/repos/cm6/6.0.0/  \ncdh6: http://192.168.11.234/repos/cdh6/6.0.0/parcels/\n  \n### 1.10.   MySQL 安装\n* 卸载操作系统上已经存在的mysql-libs文件\n先用命令查询rpm -qa | grep mysql-libs,查出rpm文件,再使用如下命令进行卸载\n\n```\nrpm -e mysql-libs-5.1.73-8.el6_8.x86_64\n```\n* 安装MySQL的client,server等rpm包  \n\n```\nrpm -ivh MySQL-shared-5.5.59-1.el6.x86_64.rpm  \nrpm -ivh MySQL-shared-compat-5.5.59-1.el6.x86_64.rpm  \nrpm -ivh MySQL-client-5.5.59-1.el6.x86_64.rpm  \nrpm -ivh MySQL-server-5.5.59-1.el6.x86_64.rpm \n```\n* 配置\n执行初始化脚本/usr/bin/mysql_secure_installation,添加root用户的密码和移除root用户的远程登录权限\n* 启动mysqld并设置为开机启动\n\n```\nservice mysqld start\nchkconfig mysqld on\n```\n* 创建用户和数据库\n\n```\nmysql -u root --password='123456' -e 'create database metastore default character set utf8;'\nmysql -u root --password='123456' -e \"CREATE USER 'hive'@'%' IDENTIFIED BY '123456';\"\nmysql -u root --password='123456' -e \"GRANT ALL PRIVILEGES ON metastore. * TO 'hive'@'%';\"\nmysql -u root --password='123456' -e \"create user 'amon'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database amon default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on amon .* to 'amon'@'%'\"\nmysql -u root --password='123456' -e \"create user 'rman'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database rman default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on rman .* to 'rman'@'%'\"\nmysql -u root --password='123456' -e \"create user 'sentry'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database sentry default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on sentry.* to 'sentry'@'%'\"\nmysql -u root --password='123456' -e \"create user 'nav'@'%'identified by '123456'\"\nmysql -u root --password='123456' -e 'create database nav default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on nav.* to 'nav'@'%'\"\nmysql -u root --password='123456' -e \"create user 'navms'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database navms default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on navms.* to 'navms'@'%'\"\nmysql -u root --password='123456' -e \"create user 'cm'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database cm default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on cm.* to 'cm'@'%'\"\nmysql -u root --password='123456' -e \"create user 'oozie'@'%'identified by '123456'\"\nmysql -u root --password='123456' -e 'create database oozie default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on oozie.* to 'oozie'@'%'\"\nmysql -u root --password='123456' -e \"create user 'hue'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database hue default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on hue.* to 'hue'@'%'\"\nmysql -u root --password='123456' -e \"FLUSH PRIVILEGES;\"\n```\n### 1.11.   升级Python\n因为CDH6需要python2.7,而centos6默认安装的是python2.6\n如果服务器可以联网,则直接执行如下命令:\n\n```\nyum install centos-release-scl -y \nyum install scl-utils -y \nyum install python27 -y\n```\n并执行如下语句,且将其加入环境变量中去\n\n```\nsource /opt/rh/python27/enable\n```\n如果不能联网,则需要手动进行升级\n下载Python文件并解压\n\n```\ntar -zxvf Python-2.7.13.tgz\n```\n将解压的文件夹移动到/usr/local下,并重命名为python27\n\n```\nmv Python-2.7.13 /usr/local/python27\n```\n编译安装\n\n```\n./configure --prefix=/usr/local/python27\nmake\nmake install\n```\n将系统默认的python,从2.6的指向2.7的版本\n\n```\nmv /usr/bin/python /usr/bin/python_old\nln -s /usr/local/python27/python2.7 /usr/bin/python\n```\n测试是否修改正确\n\n```\npython -v\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture1.png?raw=true)\n\n### 1.12.   安装psycopg\n安装依赖包\n\n```\nyum install -y gcc postgresql postgresql-server postgresql-devel\n```\n将之前下载的psycopg文件解压并进入解压的psycopg文件夹执行命令\n\n```\npython setup.py build\npython setup.py install\n```\n\n### 1.13.   配置ntp\n参考其他网上对ntp的配置\n## 2.  Cloudera Manager安装\n### 2.1.    配置CM的yum源\n在要安装Cloudera Manager的主机上的/etc/yum.repos.d/下,创建文件cloudera-manager.local.repo,并在其中添加如下内容:\n\n```\n[cm-local]\nname = cm-local\nbaseurl = http://192.168.11.234/repos/cm6/\ngpgcheck = 0\n```\n其中的IP需要做对应的修改\n### 2.2.    安装\n执行命令:\n\n```\nyum install -y cloudera-manager-server\n```\n### 2.3.    配置数据库\n在命令行执行:\n\n```\n/opt/cloudera/cm/schema/scm_database_functions.sh mysql cm cm 123456\n```\n其中,mysql是数据库类型,第一个cm是数据库名称,第二个cm是用户名,123456是密码.\n### 2.4.    启动cloudera manager server\n执行命令\n\n```\nservice cloudera-scm-server start\n```\n并设置为开机启动\n\n```\nchkconfig cloudera-scm-server on\n```\n### 2.5.    检查\n执行命令:\n\n```\ntail -f /var/log/cloudera-scm-server/cloudera-scm-server.log\n```\n监控log,当log出现如下内容时,表示启动成功,可以进入下一步操作.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture2.png?raw=true)\n\n## 3.  CDH安装\n登录http://cloudera manager server:7180,用户名和密码都是admin\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture3.png?raw=true)\n同意License协议\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture4.png?raw=true)\n选择安装的版本,在CDH6中同CDH5一样,安装的时候可以进行3个版本的选择,Cloudera Express-免费版;Cloudera Enterprise Trial-企业60天试用版; Cloudera Enterprise-企业版.此处选择60天试用版本.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture5.png?raw=true)\n进入添加集群的安装界面\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture6.png?raw=true)\n在CDH6的版本中,建议用户启用TLS,在安装的过程中会出现如下界面,给出启用TLS的步骤,如果需要启用可以按照其步骤进行操作,如果不需要则直接跳过.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture7.png?raw=true)\n在主机搜索界面,选择要添加的主机,可以是ip也可以是域名.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture8.png?raw=true)\n在下一页面中,在CDH and other software选项中,点击more options,在Remote Parcel Repository URLs的配置项中,将默认的配置都删除,添加前面配置的本地源,如下图所示:\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture9.png?raw=true)\n在Cloudera Manager Agent的 Repository Location选项中选择Custom Repository,并将之前配置的本地cm的源地址填入其中\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture10.png?raw=true)\n如果自己手动安装过JDK,此处可以不用勾选.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture11.png?raw=true)\n填入用户名和密码或者使用公钥文件\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture12.png?raw=true)\n\n进入安装步骤\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture13.png?raw=true)\n如果出现类型如下错误,说明在cm的yum源下少了allkeys.asc文件\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture14.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture15.png?raw=true)\n进入分发CDH和激活界面\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture16.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture17.png?raw=true)\n在主机检查界面,尽量将所有的红色或者黄色的问题都修复掉,修复后再重新运行主机检查,所有的检查都通过后,再进入下一步\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture18.png?raw=true)\n\n选择安装的方式,不同的安装方式默认安装的CDH组件不同.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture19.png?raw=true)\n配置所有组件需要的数据库连接信息\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture20.png?raw=true)\n\n启动\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture21.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture22.png?raw=true)\n安装完成\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture23.png?raw=true)\n点击完成,会自动跳转到集群的home界面.如下图\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture24.png?raw=true)\n## 4.  验证\n### 4.1.    HDFS验证\n执行如下命令,进行验证\n\n```\nexport HADOOP_USER_NAME=hdfs\nhdfs dfs -ls /\n```\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture25.png?raw=true)\n### 4.2.    Hive验证\n\n```\nexport HADOOP_USER_NAME=hdfs\nbeeline -u \"jdbc:hive2://node1-c6.example.com:10000/default\"\nshow databases;\n```\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture26.png?raw=true)\n\n### 4.3.    Impala验证\n```\nimpala-shell  -i node3-c6.example.com\nshow databases;\n```\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture27.png?raw=true)\n### 4.4.    HBase验证\n```\nexport HADOOP_USER_NAME=hbase\nhbase shell\nlist_namespace\n```\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture28.png?raw=true)\n\n","source":"_posts/Install-CDH6-On-CentOS6.md","raw":"---\ntitle: 在CentOS6上安装CDH6\ndate: 2018-09-10 09:41:00\ntags: \n- CDH6\n- CDH\ncategories: CDH安装\n---\n\n## 1.   安装准备\n### 1.1.    环境\n操作系统: CentOS release 6.9 (Final)\nJDK: 1.8.0_121\nCM:6.0.0\nCDH:6.0.0\nMySQL:5.5.59\n### 1.2.    下载信息\nCM6.0下载地址: https://archive.cloudera.com/cm6/6.0.0/redhat6/yum/RPMS/x86_64/\nallkeys.asc文件下载地址: https://archive.cloudera.com/cm6/6.0.0/\nCDH6.0下载地址: https://archive.cloudera.com/cdh6/6.0.0/parcels/\nMySQL下载地址: https://dev.mysql.com/downloads/mysql/5.5.html#downloads\nPython27下载地址: https://www.python.org/ftp/python/2.7.13/\nPsycopg下载地址: http://initd.org/psycopg/tarballs/PSYCOPG-2-5/\n\n<!-- more -->\n### 1.3.    修改主机名\n执行如下命令对其内容进行修改\nvim /etc/sysconfig/network\n做类型如下的修改\n\n```\nNETWORKING=yes\nHOSTNAME=node1-c6.example.com\n```\n并在命令行执行\n\n```\nhostname node1-c6.example.com\n```\n修改/etc/hosts文件,内容如下:\n\n```\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.11.234 node1-c6.example.com node1-c6\n192.168.11.235 node2-c6.example.com node2-c6\n192.168.11.236 node3-c6.example.com node3-c6\n192.168.11.237 node4-c6.example.com node4-c6\n192.168.11.238 node5-c6.example.com node5-c6\n```\n\n### 1.4.    关闭防火墙\n执行命令:\nservice iptables stop\n并禁止开机启动\nchkconfig iptables off\n### 1.5.    安装JDK\n进入jdk的rpm包所在的目录执行命令:\n\n```\nrpm -ivh jdk-8u121-linux-x64.rpm\n```\n修改/etc/profile文件,在最后添加如下内容:\n\n```\nexport JAVA_HOME=/usr/java/latest\nexport PATH=$JAVA_HOME/bin:$PATH\n```\n### 1.6.    关闭交换内存\n在文件/etc/sysctl.conf文件中添加如下内容:\nvm.swappiness=0\n并执行命令使其生效\nsysctl -p\n\n### 1.7.    关闭Selinux\n修改文件/etc/selinux/config,将其中的SELINUX=enforcing修改为disabled\n注:此操作需要重启生效,可以使用命令setenforce 1 使其临时生效   \n### 1.8.    关闭透明大页\n在命令行执行如下命令:\necho never > /sys/kernel/mm/transparent_hugepage/defrag\necho never > /sys/kernel/mm/transparent_hugepage/defrag\n此命令只是临时生效,如想永久生效,可将其添加到文件/etc/rc.local中去.\n### 1.9.    创建本地yum源仓库\n* 安装httpd服务  \n执行命令 yum install -y httpd 来安装此服务\n* 修改配置文件  \n在配置文件/etc/httpd/conf/httpd.conf的779行附件将如下内容修改为  \n\n```\nAddType application/x-compress .Z  \nAddType application/x-gzip .gz .tgz   \n```\n修改后:\n\n```\nAddType application/x-compress .Z\nAddType application/x-gzip .gz .tgz .parcel\n```\n* 启动httpd 服务\nservice httpd start\n* 将下载的cm和cdh文件分别放置在/var/www/html/repos/cm6/和/var/www/html/repos/cdh6/下,格式类型如下:\n\nrepos\n├── cdh6\n│   ├── 6.0.0\n│   │   └── parcels\n│   │       ├── CDH-6.0.0-1.cdh6.0.0.p0.537114-el6.parcel\n│   │       ├── CDH-6.0.0-1.cdh6.0.0.p0.537114-el6.parcel.sha256\n│   │       ├── index.html\n│   │       └── manifest.json\n└── cm6\n    ├── 6.0.0\n    │   ├── cloudera-manager-agent-6.0.0-530873.el6.x86_64.rpm\n    │   ├── cloudera-manager-daemons-6.0.0-530873.el6.x86_64.rpm\n    │   ├── cloudera-manager-server-6.0.0-530873.el6.x86_64.rpm\n    │   ├── cloudera-manager-server-db-2-6.0.0-530873.el6.x86_64.rpm\n    │   ├── index.html\n    │   └── oracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpm\n    ├── allkeys.asc\n        └── repomd.xml\n\n* 安装createrepo软件\nyum install -y createrepo\n* 在/var/www/html/repos下执行命令\ncreaterepo cm6\ncreaterepo cdh6\n* 在浏览器查看\ncm6: http://192.168.11.234/repos/cm6/6.0.0/  \ncdh6: http://192.168.11.234/repos/cdh6/6.0.0/parcels/\n  \n### 1.10.   MySQL 安装\n* 卸载操作系统上已经存在的mysql-libs文件\n先用命令查询rpm -qa | grep mysql-libs,查出rpm文件,再使用如下命令进行卸载\n\n```\nrpm -e mysql-libs-5.1.73-8.el6_8.x86_64\n```\n* 安装MySQL的client,server等rpm包  \n\n```\nrpm -ivh MySQL-shared-5.5.59-1.el6.x86_64.rpm  \nrpm -ivh MySQL-shared-compat-5.5.59-1.el6.x86_64.rpm  \nrpm -ivh MySQL-client-5.5.59-1.el6.x86_64.rpm  \nrpm -ivh MySQL-server-5.5.59-1.el6.x86_64.rpm \n```\n* 配置\n执行初始化脚本/usr/bin/mysql_secure_installation,添加root用户的密码和移除root用户的远程登录权限\n* 启动mysqld并设置为开机启动\n\n```\nservice mysqld start\nchkconfig mysqld on\n```\n* 创建用户和数据库\n\n```\nmysql -u root --password='123456' -e 'create database metastore default character set utf8;'\nmysql -u root --password='123456' -e \"CREATE USER 'hive'@'%' IDENTIFIED BY '123456';\"\nmysql -u root --password='123456' -e \"GRANT ALL PRIVILEGES ON metastore. * TO 'hive'@'%';\"\nmysql -u root --password='123456' -e \"create user 'amon'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database amon default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on amon .* to 'amon'@'%'\"\nmysql -u root --password='123456' -e \"create user 'rman'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database rman default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on rman .* to 'rman'@'%'\"\nmysql -u root --password='123456' -e \"create user 'sentry'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database sentry default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on sentry.* to 'sentry'@'%'\"\nmysql -u root --password='123456' -e \"create user 'nav'@'%'identified by '123456'\"\nmysql -u root --password='123456' -e 'create database nav default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on nav.* to 'nav'@'%'\"\nmysql -u root --password='123456' -e \"create user 'navms'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database navms default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on navms.* to 'navms'@'%'\"\nmysql -u root --password='123456' -e \"create user 'cm'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database cm default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on cm.* to 'cm'@'%'\"\nmysql -u root --password='123456' -e \"create user 'oozie'@'%'identified by '123456'\"\nmysql -u root --password='123456' -e 'create database oozie default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on oozie.* to 'oozie'@'%'\"\nmysql -u root --password='123456' -e \"create user 'hue'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database hue default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on hue.* to 'hue'@'%'\"\nmysql -u root --password='123456' -e \"FLUSH PRIVILEGES;\"\n```\n### 1.11.   升级Python\n因为CDH6需要python2.7,而centos6默认安装的是python2.6\n如果服务器可以联网,则直接执行如下命令:\n\n```\nyum install centos-release-scl -y \nyum install scl-utils -y \nyum install python27 -y\n```\n并执行如下语句,且将其加入环境变量中去\n\n```\nsource /opt/rh/python27/enable\n```\n如果不能联网,则需要手动进行升级\n下载Python文件并解压\n\n```\ntar -zxvf Python-2.7.13.tgz\n```\n将解压的文件夹移动到/usr/local下,并重命名为python27\n\n```\nmv Python-2.7.13 /usr/local/python27\n```\n编译安装\n\n```\n./configure --prefix=/usr/local/python27\nmake\nmake install\n```\n将系统默认的python,从2.6的指向2.7的版本\n\n```\nmv /usr/bin/python /usr/bin/python_old\nln -s /usr/local/python27/python2.7 /usr/bin/python\n```\n测试是否修改正确\n\n```\npython -v\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture1.png?raw=true)\n\n### 1.12.   安装psycopg\n安装依赖包\n\n```\nyum install -y gcc postgresql postgresql-server postgresql-devel\n```\n将之前下载的psycopg文件解压并进入解压的psycopg文件夹执行命令\n\n```\npython setup.py build\npython setup.py install\n```\n\n### 1.13.   配置ntp\n参考其他网上对ntp的配置\n## 2.  Cloudera Manager安装\n### 2.1.    配置CM的yum源\n在要安装Cloudera Manager的主机上的/etc/yum.repos.d/下,创建文件cloudera-manager.local.repo,并在其中添加如下内容:\n\n```\n[cm-local]\nname = cm-local\nbaseurl = http://192.168.11.234/repos/cm6/\ngpgcheck = 0\n```\n其中的IP需要做对应的修改\n### 2.2.    安装\n执行命令:\n\n```\nyum install -y cloudera-manager-server\n```\n### 2.3.    配置数据库\n在命令行执行:\n\n```\n/opt/cloudera/cm/schema/scm_database_functions.sh mysql cm cm 123456\n```\n其中,mysql是数据库类型,第一个cm是数据库名称,第二个cm是用户名,123456是密码.\n### 2.4.    启动cloudera manager server\n执行命令\n\n```\nservice cloudera-scm-server start\n```\n并设置为开机启动\n\n```\nchkconfig cloudera-scm-server on\n```\n### 2.5.    检查\n执行命令:\n\n```\ntail -f /var/log/cloudera-scm-server/cloudera-scm-server.log\n```\n监控log,当log出现如下内容时,表示启动成功,可以进入下一步操作.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture2.png?raw=true)\n\n## 3.  CDH安装\n登录http://cloudera manager server:7180,用户名和密码都是admin\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture3.png?raw=true)\n同意License协议\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture4.png?raw=true)\n选择安装的版本,在CDH6中同CDH5一样,安装的时候可以进行3个版本的选择,Cloudera Express-免费版;Cloudera Enterprise Trial-企业60天试用版; Cloudera Enterprise-企业版.此处选择60天试用版本.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture5.png?raw=true)\n进入添加集群的安装界面\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture6.png?raw=true)\n在CDH6的版本中,建议用户启用TLS,在安装的过程中会出现如下界面,给出启用TLS的步骤,如果需要启用可以按照其步骤进行操作,如果不需要则直接跳过.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture7.png?raw=true)\n在主机搜索界面,选择要添加的主机,可以是ip也可以是域名.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture8.png?raw=true)\n在下一页面中,在CDH and other software选项中,点击more options,在Remote Parcel Repository URLs的配置项中,将默认的配置都删除,添加前面配置的本地源,如下图所示:\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture9.png?raw=true)\n在Cloudera Manager Agent的 Repository Location选项中选择Custom Repository,并将之前配置的本地cm的源地址填入其中\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture10.png?raw=true)\n如果自己手动安装过JDK,此处可以不用勾选.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture11.png?raw=true)\n填入用户名和密码或者使用公钥文件\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture12.png?raw=true)\n\n进入安装步骤\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture13.png?raw=true)\n如果出现类型如下错误,说明在cm的yum源下少了allkeys.asc文件\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture14.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture15.png?raw=true)\n进入分发CDH和激活界面\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture16.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture17.png?raw=true)\n在主机检查界面,尽量将所有的红色或者黄色的问题都修复掉,修复后再重新运行主机检查,所有的检查都通过后,再进入下一步\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture18.png?raw=true)\n\n选择安装的方式,不同的安装方式默认安装的CDH组件不同.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture19.png?raw=true)\n配置所有组件需要的数据库连接信息\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture20.png?raw=true)\n\n启动\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture21.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture22.png?raw=true)\n安装完成\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture23.png?raw=true)\n点击完成,会自动跳转到集群的home界面.如下图\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture24.png?raw=true)\n## 4.  验证\n### 4.1.    HDFS验证\n执行如下命令,进行验证\n\n```\nexport HADOOP_USER_NAME=hdfs\nhdfs dfs -ls /\n```\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture25.png?raw=true)\n### 4.2.    Hive验证\n\n```\nexport HADOOP_USER_NAME=hdfs\nbeeline -u \"jdbc:hive2://node1-c6.example.com:10000/default\"\nshow databases;\n```\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture26.png?raw=true)\n\n### 4.3.    Impala验证\n```\nimpala-shell  -i node3-c6.example.com\nshow databases;\n```\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture27.png?raw=true)\n### 4.4.    HBase验证\n```\nexport HADOOP_USER_NAME=hbase\nhbase shell\nlist_namespace\n```\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture28.png?raw=true)\n\n","slug":"Install-CDH6-On-CentOS6","published":1,"updated":"2018-11-02T03:21:07.762Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzw0014inamnvelrpbr","content":"<h2 id=\"1-安装准备\"><a href=\"#1-安装准备\" class=\"headerlink\" title=\"1.   安装准备\"></a>1.   安装准备</h2><h3 id=\"1-1-环境\"><a href=\"#1-1-环境\" class=\"headerlink\" title=\"1.1.    环境\"></a>1.1.    环境</h3><p>操作系统: CentOS release 6.9 (Final)<br>JDK: 1.8.0_121<br>CM:6.0.0<br>CDH:6.0.0<br>MySQL:5.5.59</p>\n<h3 id=\"1-2-下载信息\"><a href=\"#1-2-下载信息\" class=\"headerlink\" title=\"1.2.    下载信息\"></a>1.2.    下载信息</h3><p>CM6.0下载地址: <a href=\"https://archive.cloudera.com/cm6/6.0.0/redhat6/yum/RPMS/x86_64/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cm6/6.0.0/redhat6/yum/RPMS/x86_64/</a><br>allkeys.asc文件下载地址: <a href=\"https://archive.cloudera.com/cm6/6.0.0/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cm6/6.0.0/</a><br>CDH6.0下载地址: <a href=\"https://archive.cloudera.com/cdh6/6.0.0/parcels/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cdh6/6.0.0/parcels/</a><br>MySQL下载地址: <a href=\"https://dev.mysql.com/downloads/mysql/5.5.html#downloads\" target=\"_blank\" rel=\"noopener\">https://dev.mysql.com/downloads/mysql/5.5.html#downloads</a><br>Python27下载地址: <a href=\"https://www.python.org/ftp/python/2.7.13/\" target=\"_blank\" rel=\"noopener\">https://www.python.org/ftp/python/2.7.13/</a><br>Psycopg下载地址: <a href=\"http://initd.org/psycopg/tarballs/PSYCOPG-2-5/\" target=\"_blank\" rel=\"noopener\">http://initd.org/psycopg/tarballs/PSYCOPG-2-5/</a></p>\n<a id=\"more\"></a>\n<h3 id=\"1-3-修改主机名\"><a href=\"#1-3-修改主机名\" class=\"headerlink\" title=\"1.3.    修改主机名\"></a>1.3.    修改主机名</h3><p>执行如下命令对其内容进行修改<br>vim /etc/sysconfig/network<br>做类型如下的修改</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">NETWORKING=yes</span><br><span class=\"line\">HOSTNAME=node1-c6.example.com</span><br></pre></td></tr></table></figure>\n<p>并在命令行执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hostname node1-c6.example.com</span><br></pre></td></tr></table></figure>\n<p>修改/etc/hosts文件,内容如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class=\"line\">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class=\"line\">192.168.11.234 node1-c6.example.com node1-c6</span><br><span class=\"line\">192.168.11.235 node2-c6.example.com node2-c6</span><br><span class=\"line\">192.168.11.236 node3-c6.example.com node3-c6</span><br><span class=\"line\">192.168.11.237 node4-c6.example.com node4-c6</span><br><span class=\"line\">192.168.11.238 node5-c6.example.com node5-c6</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-4-关闭防火墙\"><a href=\"#1-4-关闭防火墙\" class=\"headerlink\" title=\"1.4.    关闭防火墙\"></a>1.4.    关闭防火墙</h3><p>执行命令:<br>service iptables stop<br>并禁止开机启动<br>chkconfig iptables off</p>\n<h3 id=\"1-5-安装JDK\"><a href=\"#1-5-安装JDK\" class=\"headerlink\" title=\"1.5.    安装JDK\"></a>1.5.    安装JDK</h3><p>进入jdk的rpm包所在的目录执行命令:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rpm -ivh jdk-8u121-linux-x64.rpm</span><br></pre></td></tr></table></figure>\n<p>修改/etc/profile文件,在最后添加如下内容:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export JAVA_HOME=/usr/java/latest</span><br><span class=\"line\">export PATH=$JAVA_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-6-关闭交换内存\"><a href=\"#1-6-关闭交换内存\" class=\"headerlink\" title=\"1.6.    关闭交换内存\"></a>1.6.    关闭交换内存</h3><p>在文件/etc/sysctl.conf文件中添加如下内容:<br>vm.swappiness=0<br>并执行命令使其生效<br>sysctl -p</p>\n<h3 id=\"1-7-关闭Selinux\"><a href=\"#1-7-关闭Selinux\" class=\"headerlink\" title=\"1.7.    关闭Selinux\"></a>1.7.    关闭Selinux</h3><p>修改文件/etc/selinux/config,将其中的SELINUX=enforcing修改为disabled<br>注:此操作需要重启生效,可以使用命令setenforce 1 使其临时生效   </p>\n<h3 id=\"1-8-关闭透明大页\"><a href=\"#1-8-关闭透明大页\" class=\"headerlink\" title=\"1.8.    关闭透明大页\"></a>1.8.    关闭透明大页</h3><p>在命令行执行如下命令:<br>echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag<br>echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag<br>此命令只是临时生效,如想永久生效,可将其添加到文件/etc/rc.local中去.</p>\n<h3 id=\"1-9-创建本地yum源仓库\"><a href=\"#1-9-创建本地yum源仓库\" class=\"headerlink\" title=\"1.9.    创建本地yum源仓库\"></a>1.9.    创建本地yum源仓库</h3><ul>\n<li>安装httpd服务<br>执行命令 yum install -y httpd 来安装此服务</li>\n<li>修改配置文件<br>在配置文件/etc/httpd/conf/httpd.conf的779行附件将如下内容修改为  </li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AddType application/x-compress .Z  </span><br><span class=\"line\">AddType application/x-gzip .gz .tgz</span><br></pre></td></tr></table></figure>\n<p>修改后:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AddType application/x-compress .Z</span><br><span class=\"line\">AddType application/x-gzip .gz .tgz .parcel</span><br></pre></td></tr></table></figure>\n<ul>\n<li>启动httpd 服务<br>service httpd start</li>\n<li>将下载的cm和cdh文件分别放置在/var/www/html/repos/cm6/和/var/www/html/repos/cdh6/下,格式类型如下:</li>\n</ul>\n<p>repos<br>├── cdh6<br>│   ├── 6.0.0<br>│   │   └── parcels<br>│   │       ├── CDH-6.0.0-1.cdh6.0.0.p0.537114-el6.parcel<br>│   │       ├── CDH-6.0.0-1.cdh6.0.0.p0.537114-el6.parcel.sha256<br>│   │       ├── index.html<br>│   │       └── manifest.json<br>└── cm6<br>    ├── 6.0.0<br>    │   ├── cloudera-manager-agent-6.0.0-530873.el6.x86_64.rpm<br>    │   ├── cloudera-manager-daemons-6.0.0-530873.el6.x86_64.rpm<br>    │   ├── cloudera-manager-server-6.0.0-530873.el6.x86_64.rpm<br>    │   ├── cloudera-manager-server-db-2-6.0.0-530873.el6.x86_64.rpm<br>    │   ├── index.html<br>    │   └── oracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpm<br>    ├── allkeys.asc<br>        └── repomd.xml</p>\n<ul>\n<li>安装createrepo软件<br>yum install -y createrepo</li>\n<li>在/var/www/html/repos下执行命令<br>createrepo cm6<br>createrepo cdh6</li>\n<li>在浏览器查看<br>cm6: <a href=\"http://192.168.11.234/repos/cm6/6.0.0/\" target=\"_blank\" rel=\"noopener\">http://192.168.11.234/repos/cm6/6.0.0/</a><br>cdh6: <a href=\"http://192.168.11.234/repos/cdh6/6.0.0/parcels/\" target=\"_blank\" rel=\"noopener\">http://192.168.11.234/repos/cdh6/6.0.0/parcels/</a></li>\n</ul>\n<h3 id=\"1-10-MySQL-安装\"><a href=\"#1-10-MySQL-安装\" class=\"headerlink\" title=\"1.10.   MySQL 安装\"></a>1.10.   MySQL 安装</h3><ul>\n<li>卸载操作系统上已经存在的mysql-libs文件<br>先用命令查询rpm -qa | grep mysql-libs,查出rpm文件,再使用如下命令进行卸载</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rpm -e mysql-libs-5.1.73-8.el6_8.x86_64</span><br></pre></td></tr></table></figure>\n<ul>\n<li>安装MySQL的client,server等rpm包  </li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rpm -ivh MySQL-shared-5.5.59-1.el6.x86_64.rpm  </span><br><span class=\"line\">rpm -ivh MySQL-shared-compat-5.5.59-1.el6.x86_64.rpm  </span><br><span class=\"line\">rpm -ivh MySQL-client-5.5.59-1.el6.x86_64.rpm  </span><br><span class=\"line\">rpm -ivh MySQL-server-5.5.59-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure>\n<ul>\n<li>配置<br>执行初始化脚本/usr/bin/mysql_secure_installation,添加root用户的密码和移除root用户的远程登录权限</li>\n<li>启动mysqld并设置为开机启动</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">service mysqld start</span><br><span class=\"line\">chkconfig mysqld on</span><br></pre></td></tr></table></figure>\n<ul>\n<li>创建用户和数据库</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database metastore default character set utf8;&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;CREATE USER &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;GRANT ALL PRIVILEGES ON metastore. * TO &apos;hive&apos;@&apos;%&apos;;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;amon&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database amon default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on amon .* to &apos;amon&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;rman&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database rman default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on rman .* to &apos;rman&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;sentry&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database sentry default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on sentry.* to &apos;sentry&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;nav&apos;@&apos;%&apos;identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database nav default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on nav.* to &apos;nav&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;navms&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database navms default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on navms.* to &apos;navms&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;cm&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database cm default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on cm.* to &apos;cm&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;oozie&apos;@&apos;%&apos;identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database oozie default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on oozie.* to &apos;oozie&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;hue&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database hue default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on hue.* to &apos;hue&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;FLUSH PRIVILEGES;&quot;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-11-升级Python\"><a href=\"#1-11-升级Python\" class=\"headerlink\" title=\"1.11.   升级Python\"></a>1.11.   升级Python</h3><p>因为CDH6需要python2.7,而centos6默认安装的是python2.6<br>如果服务器可以联网,则直接执行如下命令:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install centos-release-scl -y </span><br><span class=\"line\">yum install scl-utils -y </span><br><span class=\"line\">yum install python27 -y</span><br></pre></td></tr></table></figure>\n<p>并执行如下语句,且将其加入环境变量中去</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source /opt/rh/python27/enable</span><br></pre></td></tr></table></figure>\n<p>如果不能联网,则需要手动进行升级<br>下载Python文件并解压</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -zxvf Python-2.7.13.tgz</span><br></pre></td></tr></table></figure>\n<p>将解压的文件夹移动到/usr/local下,并重命名为python27</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mv Python-2.7.13 /usr/local/python27</span><br></pre></td></tr></table></figure>\n<p>编译安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./configure --prefix=/usr/local/python27</span><br><span class=\"line\">make</span><br><span class=\"line\">make install</span><br></pre></td></tr></table></figure>\n<p>将系统默认的python,从2.6的指向2.7的版本</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mv /usr/bin/python /usr/bin/python_old</span><br><span class=\"line\">ln -s /usr/local/python27/python2.7 /usr/bin/python</span><br></pre></td></tr></table></figure>\n<p>测试是否修改正确</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python -v</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture1.png?raw=true\" alt=\"\"></p>\n<h3 id=\"1-12-安装psycopg\"><a href=\"#1-12-安装psycopg\" class=\"headerlink\" title=\"1.12.   安装psycopg\"></a>1.12.   安装psycopg</h3><p>安装依赖包</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y gcc postgresql postgresql-server postgresql-devel</span><br></pre></td></tr></table></figure>\n<p>将之前下载的psycopg文件解压并进入解压的psycopg文件夹执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python setup.py build</span><br><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-13-配置ntp\"><a href=\"#1-13-配置ntp\" class=\"headerlink\" title=\"1.13.   配置ntp\"></a>1.13.   配置ntp</h3><p>参考其他网上对ntp的配置</p>\n<h2 id=\"2-Cloudera-Manager安装\"><a href=\"#2-Cloudera-Manager安装\" class=\"headerlink\" title=\"2.  Cloudera Manager安装\"></a>2.  Cloudera Manager安装</h2><h3 id=\"2-1-配置CM的yum源\"><a href=\"#2-1-配置CM的yum源\" class=\"headerlink\" title=\"2.1.    配置CM的yum源\"></a>2.1.    配置CM的yum源</h3><p>在要安装Cloudera Manager的主机上的/etc/yum.repos.d/下,创建文件cloudera-manager.local.repo,并在其中添加如下内容:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[cm-local]</span><br><span class=\"line\">name = cm-local</span><br><span class=\"line\">baseurl = http://192.168.11.234/repos/cm6/</span><br><span class=\"line\">gpgcheck = 0</span><br></pre></td></tr></table></figure>\n<p>其中的IP需要做对应的修改</p>\n<h3 id=\"2-2-安装\"><a href=\"#2-2-安装\" class=\"headerlink\" title=\"2.2.    安装\"></a>2.2.    安装</h3><p>执行命令:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y cloudera-manager-server</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-3-配置数据库\"><a href=\"#2-3-配置数据库\" class=\"headerlink\" title=\"2.3.    配置数据库\"></a>2.3.    配置数据库</h3><p>在命令行执行:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/opt/cloudera/cm/schema/scm_database_functions.sh mysql cm cm 123456</span><br></pre></td></tr></table></figure>\n<p>其中,mysql是数据库类型,第一个cm是数据库名称,第二个cm是用户名,123456是密码.</p>\n<h3 id=\"2-4-启动cloudera-manager-server\"><a href=\"#2-4-启动cloudera-manager-server\" class=\"headerlink\" title=\"2.4.    启动cloudera manager server\"></a>2.4.    启动cloudera manager server</h3><p>执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">service cloudera-scm-server start</span><br></pre></td></tr></table></figure>\n<p>并设置为开机启动</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chkconfig cloudera-scm-server on</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-5-检查\"><a href=\"#2-5-检查\" class=\"headerlink\" title=\"2.5.    检查\"></a>2.5.    检查</h3><p>执行命令:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log</span><br></pre></td></tr></table></figure>\n<p>监控log,当log出现如下内容时,表示启动成功,可以进入下一步操作.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture2.png?raw=true\" alt=\"\"></p>\n<h2 id=\"3-CDH安装\"><a href=\"#3-CDH安装\" class=\"headerlink\" title=\"3.  CDH安装\"></a>3.  CDH安装</h2><p>登录<a href=\"http://cloudera\" target=\"_blank\" rel=\"noopener\">http://cloudera</a> manager server:7180,用户名和密码都是admin<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture3.png?raw=true\" alt=\"\"><br>同意License协议<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture4.png?raw=true\" alt=\"\"><br>选择安装的版本,在CDH6中同CDH5一样,安装的时候可以进行3个版本的选择,Cloudera Express-免费版;Cloudera Enterprise Trial-企业60天试用版; Cloudera Enterprise-企业版.此处选择60天试用版本.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture5.png?raw=true\" alt=\"\"><br>进入添加集群的安装界面<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture6.png?raw=true\" alt=\"\"><br>在CDH6的版本中,建议用户启用TLS,在安装的过程中会出现如下界面,给出启用TLS的步骤,如果需要启用可以按照其步骤进行操作,如果不需要则直接跳过.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture7.png?raw=true\" alt=\"\"><br>在主机搜索界面,选择要添加的主机,可以是ip也可以是域名.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture8.png?raw=true\" alt=\"\"><br>在下一页面中,在CDH and other software选项中,点击more options,在Remote Parcel Repository URLs的配置项中,将默认的配置都删除,添加前面配置的本地源,如下图所示:<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture9.png?raw=true\" alt=\"\"><br>在Cloudera Manager Agent的 Repository Location选项中选择Custom Repository,并将之前配置的本地cm的源地址填入其中<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture10.png?raw=true\" alt=\"\"><br>如果自己手动安装过JDK,此处可以不用勾选.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture11.png?raw=true\" alt=\"\"><br>填入用户名和密码或者使用公钥文件<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture12.png?raw=true\" alt=\"\"></p>\n<p>进入安装步骤<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture13.png?raw=true\" alt=\"\"><br>如果出现类型如下错误,说明在cm的yum源下少了allkeys.asc文件<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture14.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture15.png?raw=true\" alt=\"\"><br>进入分发CDH和激活界面<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture16.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture17.png?raw=true\" alt=\"\"><br>在主机检查界面,尽量将所有的红色或者黄色的问题都修复掉,修复后再重新运行主机检查,所有的检查都通过后,再进入下一步<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture18.png?raw=true\" alt=\"\"></p>\n<p>选择安装的方式,不同的安装方式默认安装的CDH组件不同.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture19.png?raw=true\" alt=\"\"><br>配置所有组件需要的数据库连接信息<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture20.png?raw=true\" alt=\"\"></p>\n<p>启动<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture21.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture22.png?raw=true\" alt=\"\"><br>安装完成<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture23.png?raw=true\" alt=\"\"><br>点击完成,会自动跳转到集群的home界面.如下图<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture24.png?raw=true\" alt=\"\"></p>\n<h2 id=\"4-验证\"><a href=\"#4-验证\" class=\"headerlink\" title=\"4.  验证\"></a>4.  验证</h2><h3 id=\"4-1-HDFS验证\"><a href=\"#4-1-HDFS验证\" class=\"headerlink\" title=\"4.1.    HDFS验证\"></a>4.1.    HDFS验证</h3><p>执行如下命令,进行验证</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export HADOOP_USER_NAME=hdfs</span><br><span class=\"line\">hdfs dfs -ls /</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture25.png?raw=true\" alt=\"\"></p>\n<h3 id=\"4-2-Hive验证\"><a href=\"#4-2-Hive验证\" class=\"headerlink\" title=\"4.2.    Hive验证\"></a>4.2.    Hive验证</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export HADOOP_USER_NAME=hdfs</span><br><span class=\"line\">beeline -u &quot;jdbc:hive2://node1-c6.example.com:10000/default&quot;</span><br><span class=\"line\">show databases;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture26.png?raw=true\" alt=\"\"></p>\n<h3 id=\"4-3-Impala验证\"><a href=\"#4-3-Impala验证\" class=\"headerlink\" title=\"4.3.    Impala验证\"></a>4.3.    Impala验证</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">impala-shell  -i node3-c6.example.com</span><br><span class=\"line\">show databases;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture27.png?raw=true\" alt=\"\"></p>\n<h3 id=\"4-4-HBase验证\"><a href=\"#4-4-HBase验证\" class=\"headerlink\" title=\"4.4.    HBase验证\"></a>4.4.    HBase验证</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export HADOOP_USER_NAME=hbase</span><br><span class=\"line\">hbase shell</span><br><span class=\"line\">list_namespace</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture28.png?raw=true\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"1-安装准备\"><a href=\"#1-安装准备\" class=\"headerlink\" title=\"1.   安装准备\"></a>1.   安装准备</h2><h3 id=\"1-1-环境\"><a href=\"#1-1-环境\" class=\"headerlink\" title=\"1.1.    环境\"></a>1.1.    环境</h3><p>操作系统: CentOS release 6.9 (Final)<br>JDK: 1.8.0_121<br>CM:6.0.0<br>CDH:6.0.0<br>MySQL:5.5.59</p>\n<h3 id=\"1-2-下载信息\"><a href=\"#1-2-下载信息\" class=\"headerlink\" title=\"1.2.    下载信息\"></a>1.2.    下载信息</h3><p>CM6.0下载地址: <a href=\"https://archive.cloudera.com/cm6/6.0.0/redhat6/yum/RPMS/x86_64/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cm6/6.0.0/redhat6/yum/RPMS/x86_64/</a><br>allkeys.asc文件下载地址: <a href=\"https://archive.cloudera.com/cm6/6.0.0/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cm6/6.0.0/</a><br>CDH6.0下载地址: <a href=\"https://archive.cloudera.com/cdh6/6.0.0/parcels/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cdh6/6.0.0/parcels/</a><br>MySQL下载地址: <a href=\"https://dev.mysql.com/downloads/mysql/5.5.html#downloads\" target=\"_blank\" rel=\"noopener\">https://dev.mysql.com/downloads/mysql/5.5.html#downloads</a><br>Python27下载地址: <a href=\"https://www.python.org/ftp/python/2.7.13/\" target=\"_blank\" rel=\"noopener\">https://www.python.org/ftp/python/2.7.13/</a><br>Psycopg下载地址: <a href=\"http://initd.org/psycopg/tarballs/PSYCOPG-2-5/\" target=\"_blank\" rel=\"noopener\">http://initd.org/psycopg/tarballs/PSYCOPG-2-5/</a></p>","more":"<h3 id=\"1-3-修改主机名\"><a href=\"#1-3-修改主机名\" class=\"headerlink\" title=\"1.3.    修改主机名\"></a>1.3.    修改主机名</h3><p>执行如下命令对其内容进行修改<br>vim /etc/sysconfig/network<br>做类型如下的修改</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">NETWORKING=yes</span><br><span class=\"line\">HOSTNAME=node1-c6.example.com</span><br></pre></td></tr></table></figure>\n<p>并在命令行执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hostname node1-c6.example.com</span><br></pre></td></tr></table></figure>\n<p>修改/etc/hosts文件,内容如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class=\"line\">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class=\"line\">192.168.11.234 node1-c6.example.com node1-c6</span><br><span class=\"line\">192.168.11.235 node2-c6.example.com node2-c6</span><br><span class=\"line\">192.168.11.236 node3-c6.example.com node3-c6</span><br><span class=\"line\">192.168.11.237 node4-c6.example.com node4-c6</span><br><span class=\"line\">192.168.11.238 node5-c6.example.com node5-c6</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-4-关闭防火墙\"><a href=\"#1-4-关闭防火墙\" class=\"headerlink\" title=\"1.4.    关闭防火墙\"></a>1.4.    关闭防火墙</h3><p>执行命令:<br>service iptables stop<br>并禁止开机启动<br>chkconfig iptables off</p>\n<h3 id=\"1-5-安装JDK\"><a href=\"#1-5-安装JDK\" class=\"headerlink\" title=\"1.5.    安装JDK\"></a>1.5.    安装JDK</h3><p>进入jdk的rpm包所在的目录执行命令:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rpm -ivh jdk-8u121-linux-x64.rpm</span><br></pre></td></tr></table></figure>\n<p>修改/etc/profile文件,在最后添加如下内容:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export JAVA_HOME=/usr/java/latest</span><br><span class=\"line\">export PATH=$JAVA_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-6-关闭交换内存\"><a href=\"#1-6-关闭交换内存\" class=\"headerlink\" title=\"1.6.    关闭交换内存\"></a>1.6.    关闭交换内存</h3><p>在文件/etc/sysctl.conf文件中添加如下内容:<br>vm.swappiness=0<br>并执行命令使其生效<br>sysctl -p</p>\n<h3 id=\"1-7-关闭Selinux\"><a href=\"#1-7-关闭Selinux\" class=\"headerlink\" title=\"1.7.    关闭Selinux\"></a>1.7.    关闭Selinux</h3><p>修改文件/etc/selinux/config,将其中的SELINUX=enforcing修改为disabled<br>注:此操作需要重启生效,可以使用命令setenforce 1 使其临时生效   </p>\n<h3 id=\"1-8-关闭透明大页\"><a href=\"#1-8-关闭透明大页\" class=\"headerlink\" title=\"1.8.    关闭透明大页\"></a>1.8.    关闭透明大页</h3><p>在命令行执行如下命令:<br>echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag<br>echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag<br>此命令只是临时生效,如想永久生效,可将其添加到文件/etc/rc.local中去.</p>\n<h3 id=\"1-9-创建本地yum源仓库\"><a href=\"#1-9-创建本地yum源仓库\" class=\"headerlink\" title=\"1.9.    创建本地yum源仓库\"></a>1.9.    创建本地yum源仓库</h3><ul>\n<li>安装httpd服务<br>执行命令 yum install -y httpd 来安装此服务</li>\n<li>修改配置文件<br>在配置文件/etc/httpd/conf/httpd.conf的779行附件将如下内容修改为  </li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AddType application/x-compress .Z  </span><br><span class=\"line\">AddType application/x-gzip .gz .tgz</span><br></pre></td></tr></table></figure>\n<p>修改后:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AddType application/x-compress .Z</span><br><span class=\"line\">AddType application/x-gzip .gz .tgz .parcel</span><br></pre></td></tr></table></figure>\n<ul>\n<li>启动httpd 服务<br>service httpd start</li>\n<li>将下载的cm和cdh文件分别放置在/var/www/html/repos/cm6/和/var/www/html/repos/cdh6/下,格式类型如下:</li>\n</ul>\n<p>repos<br>├── cdh6<br>│   ├── 6.0.0<br>│   │   └── parcels<br>│   │       ├── CDH-6.0.0-1.cdh6.0.0.p0.537114-el6.parcel<br>│   │       ├── CDH-6.0.0-1.cdh6.0.0.p0.537114-el6.parcel.sha256<br>│   │       ├── index.html<br>│   │       └── manifest.json<br>└── cm6<br>    ├── 6.0.0<br>    │   ├── cloudera-manager-agent-6.0.0-530873.el6.x86_64.rpm<br>    │   ├── cloudera-manager-daemons-6.0.0-530873.el6.x86_64.rpm<br>    │   ├── cloudera-manager-server-6.0.0-530873.el6.x86_64.rpm<br>    │   ├── cloudera-manager-server-db-2-6.0.0-530873.el6.x86_64.rpm<br>    │   ├── index.html<br>    │   └── oracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpm<br>    ├── allkeys.asc<br>        └── repomd.xml</p>\n<ul>\n<li>安装createrepo软件<br>yum install -y createrepo</li>\n<li>在/var/www/html/repos下执行命令<br>createrepo cm6<br>createrepo cdh6</li>\n<li>在浏览器查看<br>cm6: <a href=\"http://192.168.11.234/repos/cm6/6.0.0/\" target=\"_blank\" rel=\"noopener\">http://192.168.11.234/repos/cm6/6.0.0/</a><br>cdh6: <a href=\"http://192.168.11.234/repos/cdh6/6.0.0/parcels/\" target=\"_blank\" rel=\"noopener\">http://192.168.11.234/repos/cdh6/6.0.0/parcels/</a></li>\n</ul>\n<h3 id=\"1-10-MySQL-安装\"><a href=\"#1-10-MySQL-安装\" class=\"headerlink\" title=\"1.10.   MySQL 安装\"></a>1.10.   MySQL 安装</h3><ul>\n<li>卸载操作系统上已经存在的mysql-libs文件<br>先用命令查询rpm -qa | grep mysql-libs,查出rpm文件,再使用如下命令进行卸载</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rpm -e mysql-libs-5.1.73-8.el6_8.x86_64</span><br></pre></td></tr></table></figure>\n<ul>\n<li>安装MySQL的client,server等rpm包  </li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rpm -ivh MySQL-shared-5.5.59-1.el6.x86_64.rpm  </span><br><span class=\"line\">rpm -ivh MySQL-shared-compat-5.5.59-1.el6.x86_64.rpm  </span><br><span class=\"line\">rpm -ivh MySQL-client-5.5.59-1.el6.x86_64.rpm  </span><br><span class=\"line\">rpm -ivh MySQL-server-5.5.59-1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure>\n<ul>\n<li>配置<br>执行初始化脚本/usr/bin/mysql_secure_installation,添加root用户的密码和移除root用户的远程登录权限</li>\n<li>启动mysqld并设置为开机启动</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">service mysqld start</span><br><span class=\"line\">chkconfig mysqld on</span><br></pre></td></tr></table></figure>\n<ul>\n<li>创建用户和数据库</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database metastore default character set utf8;&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;CREATE USER &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;GRANT ALL PRIVILEGES ON metastore. * TO &apos;hive&apos;@&apos;%&apos;;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;amon&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database amon default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on amon .* to &apos;amon&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;rman&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database rman default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on rman .* to &apos;rman&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;sentry&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database sentry default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on sentry.* to &apos;sentry&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;nav&apos;@&apos;%&apos;identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database nav default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on nav.* to &apos;nav&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;navms&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database navms default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on navms.* to &apos;navms&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;cm&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database cm default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on cm.* to &apos;cm&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;oozie&apos;@&apos;%&apos;identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database oozie default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on oozie.* to &apos;oozie&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;hue&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database hue default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on hue.* to &apos;hue&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;FLUSH PRIVILEGES;&quot;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-11-升级Python\"><a href=\"#1-11-升级Python\" class=\"headerlink\" title=\"1.11.   升级Python\"></a>1.11.   升级Python</h3><p>因为CDH6需要python2.7,而centos6默认安装的是python2.6<br>如果服务器可以联网,则直接执行如下命令:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install centos-release-scl -y </span><br><span class=\"line\">yum install scl-utils -y </span><br><span class=\"line\">yum install python27 -y</span><br></pre></td></tr></table></figure>\n<p>并执行如下语句,且将其加入环境变量中去</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source /opt/rh/python27/enable</span><br></pre></td></tr></table></figure>\n<p>如果不能联网,则需要手动进行升级<br>下载Python文件并解压</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -zxvf Python-2.7.13.tgz</span><br></pre></td></tr></table></figure>\n<p>将解压的文件夹移动到/usr/local下,并重命名为python27</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mv Python-2.7.13 /usr/local/python27</span><br></pre></td></tr></table></figure>\n<p>编译安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./configure --prefix=/usr/local/python27</span><br><span class=\"line\">make</span><br><span class=\"line\">make install</span><br></pre></td></tr></table></figure>\n<p>将系统默认的python,从2.6的指向2.7的版本</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mv /usr/bin/python /usr/bin/python_old</span><br><span class=\"line\">ln -s /usr/local/python27/python2.7 /usr/bin/python</span><br></pre></td></tr></table></figure>\n<p>测试是否修改正确</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python -v</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture1.png?raw=true\" alt=\"\"></p>\n<h3 id=\"1-12-安装psycopg\"><a href=\"#1-12-安装psycopg\" class=\"headerlink\" title=\"1.12.   安装psycopg\"></a>1.12.   安装psycopg</h3><p>安装依赖包</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y gcc postgresql postgresql-server postgresql-devel</span><br></pre></td></tr></table></figure>\n<p>将之前下载的psycopg文件解压并进入解压的psycopg文件夹执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python setup.py build</span><br><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-13-配置ntp\"><a href=\"#1-13-配置ntp\" class=\"headerlink\" title=\"1.13.   配置ntp\"></a>1.13.   配置ntp</h3><p>参考其他网上对ntp的配置</p>\n<h2 id=\"2-Cloudera-Manager安装\"><a href=\"#2-Cloudera-Manager安装\" class=\"headerlink\" title=\"2.  Cloudera Manager安装\"></a>2.  Cloudera Manager安装</h2><h3 id=\"2-1-配置CM的yum源\"><a href=\"#2-1-配置CM的yum源\" class=\"headerlink\" title=\"2.1.    配置CM的yum源\"></a>2.1.    配置CM的yum源</h3><p>在要安装Cloudera Manager的主机上的/etc/yum.repos.d/下,创建文件cloudera-manager.local.repo,并在其中添加如下内容:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[cm-local]</span><br><span class=\"line\">name = cm-local</span><br><span class=\"line\">baseurl = http://192.168.11.234/repos/cm6/</span><br><span class=\"line\">gpgcheck = 0</span><br></pre></td></tr></table></figure>\n<p>其中的IP需要做对应的修改</p>\n<h3 id=\"2-2-安装\"><a href=\"#2-2-安装\" class=\"headerlink\" title=\"2.2.    安装\"></a>2.2.    安装</h3><p>执行命令:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y cloudera-manager-server</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-3-配置数据库\"><a href=\"#2-3-配置数据库\" class=\"headerlink\" title=\"2.3.    配置数据库\"></a>2.3.    配置数据库</h3><p>在命令行执行:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/opt/cloudera/cm/schema/scm_database_functions.sh mysql cm cm 123456</span><br></pre></td></tr></table></figure>\n<p>其中,mysql是数据库类型,第一个cm是数据库名称,第二个cm是用户名,123456是密码.</p>\n<h3 id=\"2-4-启动cloudera-manager-server\"><a href=\"#2-4-启动cloudera-manager-server\" class=\"headerlink\" title=\"2.4.    启动cloudera manager server\"></a>2.4.    启动cloudera manager server</h3><p>执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">service cloudera-scm-server start</span><br></pre></td></tr></table></figure>\n<p>并设置为开机启动</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chkconfig cloudera-scm-server on</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-5-检查\"><a href=\"#2-5-检查\" class=\"headerlink\" title=\"2.5.    检查\"></a>2.5.    检查</h3><p>执行命令:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log</span><br></pre></td></tr></table></figure>\n<p>监控log,当log出现如下内容时,表示启动成功,可以进入下一步操作.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture2.png?raw=true\" alt=\"\"></p>\n<h2 id=\"3-CDH安装\"><a href=\"#3-CDH安装\" class=\"headerlink\" title=\"3.  CDH安装\"></a>3.  CDH安装</h2><p>登录<a href=\"http://cloudera\" target=\"_blank\" rel=\"noopener\">http://cloudera</a> manager server:7180,用户名和密码都是admin<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture3.png?raw=true\" alt=\"\"><br>同意License协议<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture4.png?raw=true\" alt=\"\"><br>选择安装的版本,在CDH6中同CDH5一样,安装的时候可以进行3个版本的选择,Cloudera Express-免费版;Cloudera Enterprise Trial-企业60天试用版; Cloudera Enterprise-企业版.此处选择60天试用版本.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture5.png?raw=true\" alt=\"\"><br>进入添加集群的安装界面<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture6.png?raw=true\" alt=\"\"><br>在CDH6的版本中,建议用户启用TLS,在安装的过程中会出现如下界面,给出启用TLS的步骤,如果需要启用可以按照其步骤进行操作,如果不需要则直接跳过.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture7.png?raw=true\" alt=\"\"><br>在主机搜索界面,选择要添加的主机,可以是ip也可以是域名.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture8.png?raw=true\" alt=\"\"><br>在下一页面中,在CDH and other software选项中,点击more options,在Remote Parcel Repository URLs的配置项中,将默认的配置都删除,添加前面配置的本地源,如下图所示:<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture9.png?raw=true\" alt=\"\"><br>在Cloudera Manager Agent的 Repository Location选项中选择Custom Repository,并将之前配置的本地cm的源地址填入其中<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture10.png?raw=true\" alt=\"\"><br>如果自己手动安装过JDK,此处可以不用勾选.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture11.png?raw=true\" alt=\"\"><br>填入用户名和密码或者使用公钥文件<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture12.png?raw=true\" alt=\"\"></p>\n<p>进入安装步骤<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture13.png?raw=true\" alt=\"\"><br>如果出现类型如下错误,说明在cm的yum源下少了allkeys.asc文件<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture14.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture15.png?raw=true\" alt=\"\"><br>进入分发CDH和激活界面<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture16.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture17.png?raw=true\" alt=\"\"><br>在主机检查界面,尽量将所有的红色或者黄色的问题都修复掉,修复后再重新运行主机检查,所有的检查都通过后,再进入下一步<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture18.png?raw=true\" alt=\"\"></p>\n<p>选择安装的方式,不同的安装方式默认安装的CDH组件不同.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture19.png?raw=true\" alt=\"\"><br>配置所有组件需要的数据库连接信息<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture20.png?raw=true\" alt=\"\"></p>\n<p>启动<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture21.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture22.png?raw=true\" alt=\"\"><br>安装完成<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture23.png?raw=true\" alt=\"\"><br>点击完成,会自动跳转到集群的home界面.如下图<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture24.png?raw=true\" alt=\"\"></p>\n<h2 id=\"4-验证\"><a href=\"#4-验证\" class=\"headerlink\" title=\"4.  验证\"></a>4.  验证</h2><h3 id=\"4-1-HDFS验证\"><a href=\"#4-1-HDFS验证\" class=\"headerlink\" title=\"4.1.    HDFS验证\"></a>4.1.    HDFS验证</h3><p>执行如下命令,进行验证</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export HADOOP_USER_NAME=hdfs</span><br><span class=\"line\">hdfs dfs -ls /</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture25.png?raw=true\" alt=\"\"></p>\n<h3 id=\"4-2-Hive验证\"><a href=\"#4-2-Hive验证\" class=\"headerlink\" title=\"4.2.    Hive验证\"></a>4.2.    Hive验证</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export HADOOP_USER_NAME=hdfs</span><br><span class=\"line\">beeline -u &quot;jdbc:hive2://node1-c6.example.com:10000/default&quot;</span><br><span class=\"line\">show databases;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture26.png?raw=true\" alt=\"\"></p>\n<h3 id=\"4-3-Impala验证\"><a href=\"#4-3-Impala验证\" class=\"headerlink\" title=\"4.3.    Impala验证\"></a>4.3.    Impala验证</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">impala-shell  -i node3-c6.example.com</span><br><span class=\"line\">show databases;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture27.png?raw=true\" alt=\"\"></p>\n<h3 id=\"4-4-HBase验证\"><a href=\"#4-4-HBase验证\" class=\"headerlink\" title=\"4.4.    HBase验证\"></a>4.4.    HBase验证</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export HADOOP_USER_NAME=hbase</span><br><span class=\"line\">hbase shell</span><br><span class=\"line\">list_namespace</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/centos6/Picture28.png?raw=true\" alt=\"\"></p>"},{"title":"PARQUET FILE FORMAT","date":"2018-05-13T03:59:26.000Z","_content":"# PARQUET FILE FORMAT\n\n来源 https://www.cloudera.com/documentation/enterprise/latest/topics/impala_parquet.html#parquet_ddl\n\nParquet是一个列式存储文件格式，可用于Hadoop生态系统的各个组件之上。\n- 列式存储：对单独一个列的查询可以只查询数据的一部分数据。\n- 灵活的压缩方式：数据可以使用多种压缩方式进行压缩\n- 创新的编码方案：相同的、相似的或相关的数据值的序列可以以节省磁盘空间和内存的方式表示。\n- 大文件存储:Parquet的文件存储格式就是为大文件而设计的，文件范围从M-G。\n\n## Parquet文件的压缩\n对于大多数CDH的组件，Parquet文件默认是没有压缩的，CDH建议启用压缩以减少硬盘的使用和提高文件的读写效率\n在读取压缩格式的Parquet文件时，你不需要指定压缩类型，但是当你写一个压缩的Parquet文件时，你必须指定压缩格式。\n\n<!-- more -->\n## HBase使用Parquet\n\nTBD\n\n## Hive中使用Parquet表\n使用如下的命令创建一个Parquet格式的Hive表。\n\n```\nCREATE TABLE parquet_table_name (x INT, y STRING) STORED AS PARQUET;\n```\n当你创建完成Parquet格式的hive表， 你可以使用Impala或者Spark去访问它。\n\n如果数据文件是由外部导入的，你也可以创建外部表\n```\ncreate external table parquet_table_name (x INT, y STRING)\nROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe'\nSTORED AS\nINPUTFORMAT \"parquet.hive.DeprecatedParquetInputFormat\"\nOUTPUTFORMAT \"parquet.hive.DeprecatedParquetOutputFormat\"\nLOCATION '/test-warehouse/tinytable';\n```\n写入数据时，设置压缩方式。\n```\nset parquet.compression=GZIP;\nINSERT OVERWRITE TABLE tinytable SELECT * FROM texttable;\n```\n支持的压缩方式有UNCOMPRESSED、GZIP和SNAPPY。\n\n## Impala中使用Parquet表\nImpala只要在创建表的时候使用STORED AS PARQUET关键字指定存储格式，就能在插入和读写时使用它了。\n\n    create table parquet_table (x int, y string) stored as parquet;\n    insert into parquet_table select x, y from some_other_table;\n    Inserted 50000000 rows in 33.52s\n    select y from parquet_table where x between 70 and 100;\n\n使用这种方法创建的表，可以使用Impala或者Hive进行查询。\n在Impala2.0 默认Parquet文件的大小是256M，低版本是1G。避免使用INSERT ... VALUES这样的语法或者使用颗粒度比较大的字段进行分区。那样会降低Parquet的性能。\nImpala的数据插入是内存密集型操作，因为每个数据文件都需要一些内存区域来保存数据。有时这样的操作可能会超过HDFS同时打开文件的限制，在这种情况下，可以考虑将插入操作拆分为每个分区一insert语句。\n如果使用Sqoop将RDBMS的数据转换为Parquet文件，请注意 DATE, DATETIME, or TIMESTAMP类型的列，Parquet底层使用INT64表示这些值，Parquet使用毫秒表示这些值，但是Impala编译BIGINT作为秒的为单位的时间，因此如果你有BIGINT类型的Time类型，在转换为Parquet文件时需要除以1000 。\n\n## Parquet文件结构\n使用parquet-tools 查看Parquet文件的内容，在CDH中包含这个命令。\ncat: 使用标准输出Parquet格式的文件\nhead: 输出文件的前几行\nschema: 输出文件的schema\nmeta: 输出文件的元数据信息，包括key-value属性，压缩率，编码，压缩方式和行组信息。\ndump: 输出所有的数据和元数据信息。\n\n## Parquet表查询优化\nParquet表的查询依赖需要查询的列的个数和where条件。Parquet将数据根据固定块的大小切分成大的文件。\n例如：下面的命令就是一个高效的查询\n\n\tselect avg(income) from census_data where state = 'CA';\n这个查询语句只处理了2列数据，如果表是根据state进行分区的，那将会更加高效，因为它只需要读取分区‘CA’文件夹下文件的部分数据。\n下面的命令就比较低效\n\t\n\tselect * from census_data;\n\n如果对表进行大量数据的插入后，执行 COMPUTE STATS 更新meta信息。\nhttps://www.cloudera.com/documentation/enterprise/latest/topics/impala_compute_stats.html#compute_stats\n\n```\t\nCOMPUTE STATS [db_name.]table_name\nCOMPUTE INCREMENTAL STATS [db_name.]table_name [PARTITION (partition_spec)]\n\npartition_spec ::= partition_col=constant_value\n\npartition_spec ::= simple_partition_spec | complex_partition_spec\n\nsimple_partition_spec ::= partition_col=constant_value\n\ncomplex_partition_spec ::= comparison_expression_on_partition_col\n```\n\n注：对于一个表，不要将 COMPUTE STATS 和COMPUTE INCREMENTAL STATS 混合使用，如果想要切换，需要先使用 DROP STATS 和 DROP INCREMENTAL STATS。\n对于一个比较大的表需要 400 bytes的metedata信息。如果所有表的metedata超过了2G，可能会失败几次。\n\n优化查询的第一步就是对所有的表执行COMPUTE STATS。或者在out-of-memory错误的时候\n- 准确的统计数据有助于Impala构建一个高效的查询计划，用于查询、提高性能和减少内存使用。\n- 准确的统计数据帮助Impala将工作有效地分配到Parquet表中，提高性能和减少内存使用。\n- 准确的统计数据有助于Impala估计每个查询所需的内存，当您使用资源管理特性时，例如权限控制和YARN资源管理框架，这一点非常重要。统计数据帮助Impala实现高并发性，充分利用可用内存，并避免与其他Hadoop组件的工作负载争用。\n- 在CDH 5.10 / Impala 2.8 或者更高的版本, 当您运行计算统计数据或在Parquet表上计算增量统计语句时，Impala会自动应用查询选项设置MT_DOP = 4，以增加在此cpu密集型操作期间内节点并行度的增加。\n\n在CDH 5.10 / Impala 2.8 或者更高的版本,可以使用如下的命令计算多个分区的信息\n\t\n\tcompute incremental stats int_partitions partition (x < 100);\n\n查询表的状态\n\n\tshow table stats t1;\n\n查询列的状态\n\t\n\tshow column stats t1;\n\n## Impala支持的文件格式\nhttps://www.cloudera.com/documentation/enterprise/latest/topics/impala_file_formats.html#file_formats\n\n| 文件类型 | 结构 |压缩方式|Impala能否创建|Impala能否插入|\n| ---| ---|\n|Parquet|有结构的|Snappy, gzip; 默认 Snappy|能|是: 创建表, 插入, Load数据和查询|\n|Text|无结构的|LZO, gzip, bzip2, Snappy|是. 在创建表的时候不带STORED AS 语法, 默认文件格式是没有压缩的text, 使用ASCII 0x01 做分隔符 (通常点表Ctrl-A).|是: 创建表, 插入, Load数据和查询. 如果使用LZO压缩, 你必须在hive里面创建表和load数据. 如果使用了其他的压缩翻身,你必须load数据通过LOAD DATA, Hive,或者手动在 HDFS上操作.|\n|Avro|有结构的|\tSnappy, gzip, deflate, bzip2|是, 在Impala 1.4.0和更高的版本. 之前的版本使用hive创建表|不能, 导入数据时，使用正确格式的数据文件加载数据，或者在Hive中使用INSERT，然后在Impala中使用REFRESH table_name。|\n|RCFile|有结构的| Snappy, gzip, deflate, bzip2|是|不能, 导入数据时，使用正确格式的数据文件加载数据，或者在Hive中使用INSERT，然后在Impala中使用REFRESH table_name。|\n|SequenceFile|有结构的|Snappy, gzip, deflate, bzip2|是|不能,导入数据时，使用正确格式的数据文件加载数据，或者在Hive中使用INSERT，然后在Impala中使用REFRESH table_name。|\n\nImpala 只支持以上的文件格式，尤其要指出，Impala不支持ORC格式的文件。\n\nImpala 支持以下的压缩格式\n- Snappy. 因为其压缩率和解压速度是优先被推荐的格式 Snappy 压缩速度非常快,但是gzip更节省空间. 在 Impala 2.0 和更高的版本支持text使用这种压缩方式。\n- Gzip. 当需要更高level的压缩时，Gzip是被推荐的 (以为其更节省空间). 在 Impala 2.0 和更高的版本支持text使用这种压缩方式\n- Deflate. text 文件不支持使用此种压缩方式.\n- Bzip2. 在 Impala 2.0 和更高的版本支持text使用这种压缩方式\n- LZO, 只能为text文件使用. Impala 只能查询 LZO压缩的 text表,但是还不能创建和插入; 在Hive中去创建表和插入数据。\n\n## 为一个表选择文件格式\n\n\n- 如果您正在处理的文件格式是已经被支持的，请使用与实际的Impala表相同的格式。如果原始格式不产生可接受的查询性能或资源使用情况，可以考虑使用不同的文件格式或压缩特性创建一个新的Impala表，并使用INSERT语句将数据复制到新表进行一次性转换。根据文件格式，您可以在impala - shell中或在Hive中运行INSERT语句。\n- 许多不同的工具都可以很方便的生产文本文件，而且是便于验证和调试的。这些特性就是为什么文本是Impala创建表语句的默认格式。当性能和资源使用是主要考虑因素时，使用另一种文件格式，并考虑使用压缩。一个典型的工作流可能包括将数据复制到一个Impala表中，将CSV或TSV文件复制到适当的数据目录中，然后使用insert...select语法转换到适当的表中\n- 如果您的体系结构需要在内存中存储数据，那么不要压缩数据。由于数据不需要从磁盘移动，所以没有I/O成本，但是要解压数据，需要消耗CPU成本。\n\n## Impala数据类型\nhttps://www.cloudera.com/documentation/enterprise/latest/topics/impala_datatypes.html\nNote: 当前Impala只支持标量的数据类型，不支持复合和嵌套类型。\n\nARRAY类型：\n语法\n\n\tcolumn_name ARRAY < type >\n\n\ttype ::= primitive_type | complex_type\n","source":"_posts/Parquet-File-Format.md","raw":"---\ntitle: PARQUET FILE FORMAT\ndate: 2018-05-13 11:59:26\ntags: \n  - Parquet\n  - CDH\n---\n# PARQUET FILE FORMAT\n\n来源 https://www.cloudera.com/documentation/enterprise/latest/topics/impala_parquet.html#parquet_ddl\n\nParquet是一个列式存储文件格式，可用于Hadoop生态系统的各个组件之上。\n- 列式存储：对单独一个列的查询可以只查询数据的一部分数据。\n- 灵活的压缩方式：数据可以使用多种压缩方式进行压缩\n- 创新的编码方案：相同的、相似的或相关的数据值的序列可以以节省磁盘空间和内存的方式表示。\n- 大文件存储:Parquet的文件存储格式就是为大文件而设计的，文件范围从M-G。\n\n## Parquet文件的压缩\n对于大多数CDH的组件，Parquet文件默认是没有压缩的，CDH建议启用压缩以减少硬盘的使用和提高文件的读写效率\n在读取压缩格式的Parquet文件时，你不需要指定压缩类型，但是当你写一个压缩的Parquet文件时，你必须指定压缩格式。\n\n<!-- more -->\n## HBase使用Parquet\n\nTBD\n\n## Hive中使用Parquet表\n使用如下的命令创建一个Parquet格式的Hive表。\n\n```\nCREATE TABLE parquet_table_name (x INT, y STRING) STORED AS PARQUET;\n```\n当你创建完成Parquet格式的hive表， 你可以使用Impala或者Spark去访问它。\n\n如果数据文件是由外部导入的，你也可以创建外部表\n```\ncreate external table parquet_table_name (x INT, y STRING)\nROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe'\nSTORED AS\nINPUTFORMAT \"parquet.hive.DeprecatedParquetInputFormat\"\nOUTPUTFORMAT \"parquet.hive.DeprecatedParquetOutputFormat\"\nLOCATION '/test-warehouse/tinytable';\n```\n写入数据时，设置压缩方式。\n```\nset parquet.compression=GZIP;\nINSERT OVERWRITE TABLE tinytable SELECT * FROM texttable;\n```\n支持的压缩方式有UNCOMPRESSED、GZIP和SNAPPY。\n\n## Impala中使用Parquet表\nImpala只要在创建表的时候使用STORED AS PARQUET关键字指定存储格式，就能在插入和读写时使用它了。\n\n    create table parquet_table (x int, y string) stored as parquet;\n    insert into parquet_table select x, y from some_other_table;\n    Inserted 50000000 rows in 33.52s\n    select y from parquet_table where x between 70 and 100;\n\n使用这种方法创建的表，可以使用Impala或者Hive进行查询。\n在Impala2.0 默认Parquet文件的大小是256M，低版本是1G。避免使用INSERT ... VALUES这样的语法或者使用颗粒度比较大的字段进行分区。那样会降低Parquet的性能。\nImpala的数据插入是内存密集型操作，因为每个数据文件都需要一些内存区域来保存数据。有时这样的操作可能会超过HDFS同时打开文件的限制，在这种情况下，可以考虑将插入操作拆分为每个分区一insert语句。\n如果使用Sqoop将RDBMS的数据转换为Parquet文件，请注意 DATE, DATETIME, or TIMESTAMP类型的列，Parquet底层使用INT64表示这些值，Parquet使用毫秒表示这些值，但是Impala编译BIGINT作为秒的为单位的时间，因此如果你有BIGINT类型的Time类型，在转换为Parquet文件时需要除以1000 。\n\n## Parquet文件结构\n使用parquet-tools 查看Parquet文件的内容，在CDH中包含这个命令。\ncat: 使用标准输出Parquet格式的文件\nhead: 输出文件的前几行\nschema: 输出文件的schema\nmeta: 输出文件的元数据信息，包括key-value属性，压缩率，编码，压缩方式和行组信息。\ndump: 输出所有的数据和元数据信息。\n\n## Parquet表查询优化\nParquet表的查询依赖需要查询的列的个数和where条件。Parquet将数据根据固定块的大小切分成大的文件。\n例如：下面的命令就是一个高效的查询\n\n\tselect avg(income) from census_data where state = 'CA';\n这个查询语句只处理了2列数据，如果表是根据state进行分区的，那将会更加高效，因为它只需要读取分区‘CA’文件夹下文件的部分数据。\n下面的命令就比较低效\n\t\n\tselect * from census_data;\n\n如果对表进行大量数据的插入后，执行 COMPUTE STATS 更新meta信息。\nhttps://www.cloudera.com/documentation/enterprise/latest/topics/impala_compute_stats.html#compute_stats\n\n```\t\nCOMPUTE STATS [db_name.]table_name\nCOMPUTE INCREMENTAL STATS [db_name.]table_name [PARTITION (partition_spec)]\n\npartition_spec ::= partition_col=constant_value\n\npartition_spec ::= simple_partition_spec | complex_partition_spec\n\nsimple_partition_spec ::= partition_col=constant_value\n\ncomplex_partition_spec ::= comparison_expression_on_partition_col\n```\n\n注：对于一个表，不要将 COMPUTE STATS 和COMPUTE INCREMENTAL STATS 混合使用，如果想要切换，需要先使用 DROP STATS 和 DROP INCREMENTAL STATS。\n对于一个比较大的表需要 400 bytes的metedata信息。如果所有表的metedata超过了2G，可能会失败几次。\n\n优化查询的第一步就是对所有的表执行COMPUTE STATS。或者在out-of-memory错误的时候\n- 准确的统计数据有助于Impala构建一个高效的查询计划，用于查询、提高性能和减少内存使用。\n- 准确的统计数据帮助Impala将工作有效地分配到Parquet表中，提高性能和减少内存使用。\n- 准确的统计数据有助于Impala估计每个查询所需的内存，当您使用资源管理特性时，例如权限控制和YARN资源管理框架，这一点非常重要。统计数据帮助Impala实现高并发性，充分利用可用内存，并避免与其他Hadoop组件的工作负载争用。\n- 在CDH 5.10 / Impala 2.8 或者更高的版本, 当您运行计算统计数据或在Parquet表上计算增量统计语句时，Impala会自动应用查询选项设置MT_DOP = 4，以增加在此cpu密集型操作期间内节点并行度的增加。\n\n在CDH 5.10 / Impala 2.8 或者更高的版本,可以使用如下的命令计算多个分区的信息\n\t\n\tcompute incremental stats int_partitions partition (x < 100);\n\n查询表的状态\n\n\tshow table stats t1;\n\n查询列的状态\n\t\n\tshow column stats t1;\n\n## Impala支持的文件格式\nhttps://www.cloudera.com/documentation/enterprise/latest/topics/impala_file_formats.html#file_formats\n\n| 文件类型 | 结构 |压缩方式|Impala能否创建|Impala能否插入|\n| ---| ---|\n|Parquet|有结构的|Snappy, gzip; 默认 Snappy|能|是: 创建表, 插入, Load数据和查询|\n|Text|无结构的|LZO, gzip, bzip2, Snappy|是. 在创建表的时候不带STORED AS 语法, 默认文件格式是没有压缩的text, 使用ASCII 0x01 做分隔符 (通常点表Ctrl-A).|是: 创建表, 插入, Load数据和查询. 如果使用LZO压缩, 你必须在hive里面创建表和load数据. 如果使用了其他的压缩翻身,你必须load数据通过LOAD DATA, Hive,或者手动在 HDFS上操作.|\n|Avro|有结构的|\tSnappy, gzip, deflate, bzip2|是, 在Impala 1.4.0和更高的版本. 之前的版本使用hive创建表|不能, 导入数据时，使用正确格式的数据文件加载数据，或者在Hive中使用INSERT，然后在Impala中使用REFRESH table_name。|\n|RCFile|有结构的| Snappy, gzip, deflate, bzip2|是|不能, 导入数据时，使用正确格式的数据文件加载数据，或者在Hive中使用INSERT，然后在Impala中使用REFRESH table_name。|\n|SequenceFile|有结构的|Snappy, gzip, deflate, bzip2|是|不能,导入数据时，使用正确格式的数据文件加载数据，或者在Hive中使用INSERT，然后在Impala中使用REFRESH table_name。|\n\nImpala 只支持以上的文件格式，尤其要指出，Impala不支持ORC格式的文件。\n\nImpala 支持以下的压缩格式\n- Snappy. 因为其压缩率和解压速度是优先被推荐的格式 Snappy 压缩速度非常快,但是gzip更节省空间. 在 Impala 2.0 和更高的版本支持text使用这种压缩方式。\n- Gzip. 当需要更高level的压缩时，Gzip是被推荐的 (以为其更节省空间). 在 Impala 2.0 和更高的版本支持text使用这种压缩方式\n- Deflate. text 文件不支持使用此种压缩方式.\n- Bzip2. 在 Impala 2.0 和更高的版本支持text使用这种压缩方式\n- LZO, 只能为text文件使用. Impala 只能查询 LZO压缩的 text表,但是还不能创建和插入; 在Hive中去创建表和插入数据。\n\n## 为一个表选择文件格式\n\n\n- 如果您正在处理的文件格式是已经被支持的，请使用与实际的Impala表相同的格式。如果原始格式不产生可接受的查询性能或资源使用情况，可以考虑使用不同的文件格式或压缩特性创建一个新的Impala表，并使用INSERT语句将数据复制到新表进行一次性转换。根据文件格式，您可以在impala - shell中或在Hive中运行INSERT语句。\n- 许多不同的工具都可以很方便的生产文本文件，而且是便于验证和调试的。这些特性就是为什么文本是Impala创建表语句的默认格式。当性能和资源使用是主要考虑因素时，使用另一种文件格式，并考虑使用压缩。一个典型的工作流可能包括将数据复制到一个Impala表中，将CSV或TSV文件复制到适当的数据目录中，然后使用insert...select语法转换到适当的表中\n- 如果您的体系结构需要在内存中存储数据，那么不要压缩数据。由于数据不需要从磁盘移动，所以没有I/O成本，但是要解压数据，需要消耗CPU成本。\n\n## Impala数据类型\nhttps://www.cloudera.com/documentation/enterprise/latest/topics/impala_datatypes.html\nNote: 当前Impala只支持标量的数据类型，不支持复合和嵌套类型。\n\nARRAY类型：\n语法\n\n\tcolumn_name ARRAY < type >\n\n\ttype ::= primitive_type | complex_type\n","slug":"Parquet-File-Format","published":1,"updated":"2018-09-14T01:22:31.907Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzx0016inamlb6s07hn","content":"<h1 id=\"PARQUET-FILE-FORMAT\"><a href=\"#PARQUET-FILE-FORMAT\" class=\"headerlink\" title=\"PARQUET FILE FORMAT\"></a>PARQUET FILE FORMAT</h1><p>来源 <a href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/impala_parquet.html#parquet_ddl\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/documentation/enterprise/latest/topics/impala_parquet.html#parquet_ddl</a></p>\n<p>Parquet是一个列式存储文件格式，可用于Hadoop生态系统的各个组件之上。</p>\n<ul>\n<li>列式存储：对单独一个列的查询可以只查询数据的一部分数据。</li>\n<li>灵活的压缩方式：数据可以使用多种压缩方式进行压缩</li>\n<li>创新的编码方案：相同的、相似的或相关的数据值的序列可以以节省磁盘空间和内存的方式表示。</li>\n<li>大文件存储:Parquet的文件存储格式就是为大文件而设计的，文件范围从M-G。</li>\n</ul>\n<h2 id=\"Parquet文件的压缩\"><a href=\"#Parquet文件的压缩\" class=\"headerlink\" title=\"Parquet文件的压缩\"></a>Parquet文件的压缩</h2><p>对于大多数CDH的组件，Parquet文件默认是没有压缩的，CDH建议启用压缩以减少硬盘的使用和提高文件的读写效率<br>在读取压缩格式的Parquet文件时，你不需要指定压缩类型，但是当你写一个压缩的Parquet文件时，你必须指定压缩格式。</p>\n<a id=\"more\"></a>\n<h2 id=\"HBase使用Parquet\"><a href=\"#HBase使用Parquet\" class=\"headerlink\" title=\"HBase使用Parquet\"></a>HBase使用Parquet</h2><p>TBD</p>\n<h2 id=\"Hive中使用Parquet表\"><a href=\"#Hive中使用Parquet表\" class=\"headerlink\" title=\"Hive中使用Parquet表\"></a>Hive中使用Parquet表</h2><p>使用如下的命令创建一个Parquet格式的Hive表。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE parquet_table_name (x INT, y STRING) STORED AS PARQUET;</span><br></pre></td></tr></table></figure>\n<p>当你创建完成Parquet格式的hive表， 你可以使用Impala或者Spark去访问它。</p>\n<p>如果数据文件是由外部导入的，你也可以创建外部表<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create external table parquet_table_name (x INT, y STRING)</span><br><span class=\"line\">ROW FORMAT SERDE &apos;parquet.hive.serde.ParquetHiveSerDe&apos;</span><br><span class=\"line\">STORED AS</span><br><span class=\"line\">INPUTFORMAT &quot;parquet.hive.DeprecatedParquetInputFormat&quot;</span><br><span class=\"line\">OUTPUTFORMAT &quot;parquet.hive.DeprecatedParquetOutputFormat&quot;</span><br><span class=\"line\">LOCATION &apos;/test-warehouse/tinytable&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>写入数据时，设置压缩方式。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set parquet.compression=GZIP;</span><br><span class=\"line\">INSERT OVERWRITE TABLE tinytable SELECT * FROM texttable;</span><br></pre></td></tr></table></figure></p>\n<p>支持的压缩方式有UNCOMPRESSED、GZIP和SNAPPY。</p>\n<h2 id=\"Impala中使用Parquet表\"><a href=\"#Impala中使用Parquet表\" class=\"headerlink\" title=\"Impala中使用Parquet表\"></a>Impala中使用Parquet表</h2><p>Impala只要在创建表的时候使用STORED AS PARQUET关键字指定存储格式，就能在插入和读写时使用它了。</p>\n<pre><code>create table parquet_table (x int, y string) stored as parquet;\ninsert into parquet_table select x, y from some_other_table;\nInserted 50000000 rows in 33.52s\nselect y from parquet_table where x between 70 and 100;\n</code></pre><p>使用这种方法创建的表，可以使用Impala或者Hive进行查询。<br>在Impala2.0 默认Parquet文件的大小是256M，低版本是1G。避免使用INSERT … VALUES这样的语法或者使用颗粒度比较大的字段进行分区。那样会降低Parquet的性能。<br>Impala的数据插入是内存密集型操作，因为每个数据文件都需要一些内存区域来保存数据。有时这样的操作可能会超过HDFS同时打开文件的限制，在这种情况下，可以考虑将插入操作拆分为每个分区一insert语句。<br>如果使用Sqoop将RDBMS的数据转换为Parquet文件，请注意 DATE, DATETIME, or TIMESTAMP类型的列，Parquet底层使用INT64表示这些值，Parquet使用毫秒表示这些值，但是Impala编译BIGINT作为秒的为单位的时间，因此如果你有BIGINT类型的Time类型，在转换为Parquet文件时需要除以1000 。</p>\n<h2 id=\"Parquet文件结构\"><a href=\"#Parquet文件结构\" class=\"headerlink\" title=\"Parquet文件结构\"></a>Parquet文件结构</h2><p>使用parquet-tools 查看Parquet文件的内容，在CDH中包含这个命令。<br>cat: 使用标准输出Parquet格式的文件<br>head: 输出文件的前几行<br>schema: 输出文件的schema<br>meta: 输出文件的元数据信息，包括key-value属性，压缩率，编码，压缩方式和行组信息。<br>dump: 输出所有的数据和元数据信息。</p>\n<h2 id=\"Parquet表查询优化\"><a href=\"#Parquet表查询优化\" class=\"headerlink\" title=\"Parquet表查询优化\"></a>Parquet表查询优化</h2><p>Parquet表的查询依赖需要查询的列的个数和where条件。Parquet将数据根据固定块的大小切分成大的文件。<br>例如：下面的命令就是一个高效的查询</p>\n<pre><code>select avg(income) from census_data where state = &apos;CA&apos;;\n</code></pre><p>这个查询语句只处理了2列数据，如果表是根据state进行分区的，那将会更加高效，因为它只需要读取分区‘CA’文件夹下文件的部分数据。<br>下面的命令就比较低效</p>\n<pre><code>select * from census_data;\n</code></pre><p>如果对表进行大量数据的插入后，执行 COMPUTE STATS 更新meta信息。<br><a href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/impala_compute_stats.html#compute_stats\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/documentation/enterprise/latest/topics/impala_compute_stats.html#compute_stats</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">COMPUTE STATS [db_name.]table_name</span><br><span class=\"line\">COMPUTE INCREMENTAL STATS [db_name.]table_name [PARTITION (partition_spec)]</span><br><span class=\"line\"></span><br><span class=\"line\">partition_spec ::= partition_col=constant_value</span><br><span class=\"line\"></span><br><span class=\"line\">partition_spec ::= simple_partition_spec | complex_partition_spec</span><br><span class=\"line\"></span><br><span class=\"line\">simple_partition_spec ::= partition_col=constant_value</span><br><span class=\"line\"></span><br><span class=\"line\">complex_partition_spec ::= comparison_expression_on_partition_col</span><br></pre></td></tr></table></figure>\n<p>注：对于一个表，不要将 COMPUTE STATS 和COMPUTE INCREMENTAL STATS 混合使用，如果想要切换，需要先使用 DROP STATS 和 DROP INCREMENTAL STATS。<br>对于一个比较大的表需要 400 bytes的metedata信息。如果所有表的metedata超过了2G，可能会失败几次。</p>\n<p>优化查询的第一步就是对所有的表执行COMPUTE STATS。或者在out-of-memory错误的时候</p>\n<ul>\n<li>准确的统计数据有助于Impala构建一个高效的查询计划，用于查询、提高性能和减少内存使用。</li>\n<li>准确的统计数据帮助Impala将工作有效地分配到Parquet表中，提高性能和减少内存使用。</li>\n<li>准确的统计数据有助于Impala估计每个查询所需的内存，当您使用资源管理特性时，例如权限控制和YARN资源管理框架，这一点非常重要。统计数据帮助Impala实现高并发性，充分利用可用内存，并避免与其他Hadoop组件的工作负载争用。</li>\n<li>在CDH 5.10 / Impala 2.8 或者更高的版本, 当您运行计算统计数据或在Parquet表上计算增量统计语句时，Impala会自动应用查询选项设置MT_DOP = 4，以增加在此cpu密集型操作期间内节点并行度的增加。</li>\n</ul>\n<p>在CDH 5.10 / Impala 2.8 或者更高的版本,可以使用如下的命令计算多个分区的信息</p>\n<pre><code>compute incremental stats int_partitions partition (x &lt; 100);\n</code></pre><p>查询表的状态</p>\n<pre><code>show table stats t1;\n</code></pre><p>查询列的状态</p>\n<pre><code>show column stats t1;\n</code></pre><h2 id=\"Impala支持的文件格式\"><a href=\"#Impala支持的文件格式\" class=\"headerlink\" title=\"Impala支持的文件格式\"></a>Impala支持的文件格式</h2><p><a href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/impala_file_formats.html#file_formats\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/documentation/enterprise/latest/topics/impala_file_formats.html#file_formats</a></p>\n<table>\n<thead>\n<tr>\n<th>文件类型</th>\n<th>结构</th>\n<th>压缩方式</th>\n<th>Impala能否创建</th>\n<th>Impala能否插入</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Parquet</td>\n<td>有结构的</td>\n<td>Snappy, gzip; 默认 Snappy</td>\n<td>能</td>\n<td>是: 创建表, 插入, Load数据和查询</td>\n</tr>\n<tr>\n<td>Text</td>\n<td>无结构的</td>\n<td>LZO, gzip, bzip2, Snappy</td>\n<td>是. 在创建表的时候不带STORED AS 语法, 默认文件格式是没有压缩的text, 使用ASCII 0x01 做分隔符 (通常点表Ctrl-A).</td>\n<td>是: 创建表, 插入, Load数据和查询. 如果使用LZO压缩, 你必须在hive里面创建表和load数据. 如果使用了其他的压缩翻身,你必须load数据通过LOAD DATA, Hive,或者手动在 HDFS上操作.</td>\n</tr>\n<tr>\n<td>Avro</td>\n<td>有结构的</td>\n<td>Snappy, gzip, deflate, bzip2</td>\n<td>是, 在Impala 1.4.0和更高的版本. 之前的版本使用hive创建表</td>\n<td>不能, 导入数据时，使用正确格式的数据文件加载数据，或者在Hive中使用INSERT，然后在Impala中使用REFRESH table_name。</td>\n</tr>\n<tr>\n<td>RCFile</td>\n<td>有结构的</td>\n<td>Snappy, gzip, deflate, bzip2</td>\n<td>是</td>\n<td>不能, 导入数据时，使用正确格式的数据文件加载数据，或者在Hive中使用INSERT，然后在Impala中使用REFRESH table_name。</td>\n</tr>\n<tr>\n<td>SequenceFile</td>\n<td>有结构的</td>\n<td>Snappy, gzip, deflate, bzip2</td>\n<td>是</td>\n<td>不能,导入数据时，使用正确格式的数据文件加载数据，或者在Hive中使用INSERT，然后在Impala中使用REFRESH table_name。</td>\n</tr>\n</tbody>\n</table>\n<p>Impala 只支持以上的文件格式，尤其要指出，Impala不支持ORC格式的文件。</p>\n<p>Impala 支持以下的压缩格式</p>\n<ul>\n<li>Snappy. 因为其压缩率和解压速度是优先被推荐的格式 Snappy 压缩速度非常快,但是gzip更节省空间. 在 Impala 2.0 和更高的版本支持text使用这种压缩方式。</li>\n<li>Gzip. 当需要更高level的压缩时，Gzip是被推荐的 (以为其更节省空间). 在 Impala 2.0 和更高的版本支持text使用这种压缩方式</li>\n<li>Deflate. text 文件不支持使用此种压缩方式.</li>\n<li>Bzip2. 在 Impala 2.0 和更高的版本支持text使用这种压缩方式</li>\n<li>LZO, 只能为text文件使用. Impala 只能查询 LZO压缩的 text表,但是还不能创建和插入; 在Hive中去创建表和插入数据。</li>\n</ul>\n<h2 id=\"为一个表选择文件格式\"><a href=\"#为一个表选择文件格式\" class=\"headerlink\" title=\"为一个表选择文件格式\"></a>为一个表选择文件格式</h2><ul>\n<li>如果您正在处理的文件格式是已经被支持的，请使用与实际的Impala表相同的格式。如果原始格式不产生可接受的查询性能或资源使用情况，可以考虑使用不同的文件格式或压缩特性创建一个新的Impala表，并使用INSERT语句将数据复制到新表进行一次性转换。根据文件格式，您可以在impala - shell中或在Hive中运行INSERT语句。</li>\n<li>许多不同的工具都可以很方便的生产文本文件，而且是便于验证和调试的。这些特性就是为什么文本是Impala创建表语句的默认格式。当性能和资源使用是主要考虑因素时，使用另一种文件格式，并考虑使用压缩。一个典型的工作流可能包括将数据复制到一个Impala表中，将CSV或TSV文件复制到适当的数据目录中，然后使用insert…select语法转换到适当的表中</li>\n<li>如果您的体系结构需要在内存中存储数据，那么不要压缩数据。由于数据不需要从磁盘移动，所以没有I/O成本，但是要解压数据，需要消耗CPU成本。</li>\n</ul>\n<h2 id=\"Impala数据类型\"><a href=\"#Impala数据类型\" class=\"headerlink\" title=\"Impala数据类型\"></a>Impala数据类型</h2><p><a href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/impala_datatypes.html\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/documentation/enterprise/latest/topics/impala_datatypes.html</a><br>Note: 当前Impala只支持标量的数据类型，不支持复合和嵌套类型。</p>\n<p>ARRAY类型：<br>语法</p>\n<pre><code>column_name ARRAY &lt; type &gt;\n\ntype ::= primitive_type | complex_type\n</code></pre>","site":{"data":{}},"excerpt":"<h1 id=\"PARQUET-FILE-FORMAT\"><a href=\"#PARQUET-FILE-FORMAT\" class=\"headerlink\" title=\"PARQUET FILE FORMAT\"></a>PARQUET FILE FORMAT</h1><p>来源 <a href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/impala_parquet.html#parquet_ddl\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/documentation/enterprise/latest/topics/impala_parquet.html#parquet_ddl</a></p>\n<p>Parquet是一个列式存储文件格式，可用于Hadoop生态系统的各个组件之上。</p>\n<ul>\n<li>列式存储：对单独一个列的查询可以只查询数据的一部分数据。</li>\n<li>灵活的压缩方式：数据可以使用多种压缩方式进行压缩</li>\n<li>创新的编码方案：相同的、相似的或相关的数据值的序列可以以节省磁盘空间和内存的方式表示。</li>\n<li>大文件存储:Parquet的文件存储格式就是为大文件而设计的，文件范围从M-G。</li>\n</ul>\n<h2 id=\"Parquet文件的压缩\"><a href=\"#Parquet文件的压缩\" class=\"headerlink\" title=\"Parquet文件的压缩\"></a>Parquet文件的压缩</h2><p>对于大多数CDH的组件，Parquet文件默认是没有压缩的，CDH建议启用压缩以减少硬盘的使用和提高文件的读写效率<br>在读取压缩格式的Parquet文件时，你不需要指定压缩类型，但是当你写一个压缩的Parquet文件时，你必须指定压缩格式。</p>","more":"<h2 id=\"HBase使用Parquet\"><a href=\"#HBase使用Parquet\" class=\"headerlink\" title=\"HBase使用Parquet\"></a>HBase使用Parquet</h2><p>TBD</p>\n<h2 id=\"Hive中使用Parquet表\"><a href=\"#Hive中使用Parquet表\" class=\"headerlink\" title=\"Hive中使用Parquet表\"></a>Hive中使用Parquet表</h2><p>使用如下的命令创建一个Parquet格式的Hive表。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE parquet_table_name (x INT, y STRING) STORED AS PARQUET;</span><br></pre></td></tr></table></figure>\n<p>当你创建完成Parquet格式的hive表， 你可以使用Impala或者Spark去访问它。</p>\n<p>如果数据文件是由外部导入的，你也可以创建外部表<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create external table parquet_table_name (x INT, y STRING)</span><br><span class=\"line\">ROW FORMAT SERDE &apos;parquet.hive.serde.ParquetHiveSerDe&apos;</span><br><span class=\"line\">STORED AS</span><br><span class=\"line\">INPUTFORMAT &quot;parquet.hive.DeprecatedParquetInputFormat&quot;</span><br><span class=\"line\">OUTPUTFORMAT &quot;parquet.hive.DeprecatedParquetOutputFormat&quot;</span><br><span class=\"line\">LOCATION &apos;/test-warehouse/tinytable&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>写入数据时，设置压缩方式。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set parquet.compression=GZIP;</span><br><span class=\"line\">INSERT OVERWRITE TABLE tinytable SELECT * FROM texttable;</span><br></pre></td></tr></table></figure></p>\n<p>支持的压缩方式有UNCOMPRESSED、GZIP和SNAPPY。</p>\n<h2 id=\"Impala中使用Parquet表\"><a href=\"#Impala中使用Parquet表\" class=\"headerlink\" title=\"Impala中使用Parquet表\"></a>Impala中使用Parquet表</h2><p>Impala只要在创建表的时候使用STORED AS PARQUET关键字指定存储格式，就能在插入和读写时使用它了。</p>\n<pre><code>create table parquet_table (x int, y string) stored as parquet;\ninsert into parquet_table select x, y from some_other_table;\nInserted 50000000 rows in 33.52s\nselect y from parquet_table where x between 70 and 100;\n</code></pre><p>使用这种方法创建的表，可以使用Impala或者Hive进行查询。<br>在Impala2.0 默认Parquet文件的大小是256M，低版本是1G。避免使用INSERT … VALUES这样的语法或者使用颗粒度比较大的字段进行分区。那样会降低Parquet的性能。<br>Impala的数据插入是内存密集型操作，因为每个数据文件都需要一些内存区域来保存数据。有时这样的操作可能会超过HDFS同时打开文件的限制，在这种情况下，可以考虑将插入操作拆分为每个分区一insert语句。<br>如果使用Sqoop将RDBMS的数据转换为Parquet文件，请注意 DATE, DATETIME, or TIMESTAMP类型的列，Parquet底层使用INT64表示这些值，Parquet使用毫秒表示这些值，但是Impala编译BIGINT作为秒的为单位的时间，因此如果你有BIGINT类型的Time类型，在转换为Parquet文件时需要除以1000 。</p>\n<h2 id=\"Parquet文件结构\"><a href=\"#Parquet文件结构\" class=\"headerlink\" title=\"Parquet文件结构\"></a>Parquet文件结构</h2><p>使用parquet-tools 查看Parquet文件的内容，在CDH中包含这个命令。<br>cat: 使用标准输出Parquet格式的文件<br>head: 输出文件的前几行<br>schema: 输出文件的schema<br>meta: 输出文件的元数据信息，包括key-value属性，压缩率，编码，压缩方式和行组信息。<br>dump: 输出所有的数据和元数据信息。</p>\n<h2 id=\"Parquet表查询优化\"><a href=\"#Parquet表查询优化\" class=\"headerlink\" title=\"Parquet表查询优化\"></a>Parquet表查询优化</h2><p>Parquet表的查询依赖需要查询的列的个数和where条件。Parquet将数据根据固定块的大小切分成大的文件。<br>例如：下面的命令就是一个高效的查询</p>\n<pre><code>select avg(income) from census_data where state = &apos;CA&apos;;\n</code></pre><p>这个查询语句只处理了2列数据，如果表是根据state进行分区的，那将会更加高效，因为它只需要读取分区‘CA’文件夹下文件的部分数据。<br>下面的命令就比较低效</p>\n<pre><code>select * from census_data;\n</code></pre><p>如果对表进行大量数据的插入后，执行 COMPUTE STATS 更新meta信息。<br><a href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/impala_compute_stats.html#compute_stats\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/documentation/enterprise/latest/topics/impala_compute_stats.html#compute_stats</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">COMPUTE STATS [db_name.]table_name</span><br><span class=\"line\">COMPUTE INCREMENTAL STATS [db_name.]table_name [PARTITION (partition_spec)]</span><br><span class=\"line\"></span><br><span class=\"line\">partition_spec ::= partition_col=constant_value</span><br><span class=\"line\"></span><br><span class=\"line\">partition_spec ::= simple_partition_spec | complex_partition_spec</span><br><span class=\"line\"></span><br><span class=\"line\">simple_partition_spec ::= partition_col=constant_value</span><br><span class=\"line\"></span><br><span class=\"line\">complex_partition_spec ::= comparison_expression_on_partition_col</span><br></pre></td></tr></table></figure>\n<p>注：对于一个表，不要将 COMPUTE STATS 和COMPUTE INCREMENTAL STATS 混合使用，如果想要切换，需要先使用 DROP STATS 和 DROP INCREMENTAL STATS。<br>对于一个比较大的表需要 400 bytes的metedata信息。如果所有表的metedata超过了2G，可能会失败几次。</p>\n<p>优化查询的第一步就是对所有的表执行COMPUTE STATS。或者在out-of-memory错误的时候</p>\n<ul>\n<li>准确的统计数据有助于Impala构建一个高效的查询计划，用于查询、提高性能和减少内存使用。</li>\n<li>准确的统计数据帮助Impala将工作有效地分配到Parquet表中，提高性能和减少内存使用。</li>\n<li>准确的统计数据有助于Impala估计每个查询所需的内存，当您使用资源管理特性时，例如权限控制和YARN资源管理框架，这一点非常重要。统计数据帮助Impala实现高并发性，充分利用可用内存，并避免与其他Hadoop组件的工作负载争用。</li>\n<li>在CDH 5.10 / Impala 2.8 或者更高的版本, 当您运行计算统计数据或在Parquet表上计算增量统计语句时，Impala会自动应用查询选项设置MT_DOP = 4，以增加在此cpu密集型操作期间内节点并行度的增加。</li>\n</ul>\n<p>在CDH 5.10 / Impala 2.8 或者更高的版本,可以使用如下的命令计算多个分区的信息</p>\n<pre><code>compute incremental stats int_partitions partition (x &lt; 100);\n</code></pre><p>查询表的状态</p>\n<pre><code>show table stats t1;\n</code></pre><p>查询列的状态</p>\n<pre><code>show column stats t1;\n</code></pre><h2 id=\"Impala支持的文件格式\"><a href=\"#Impala支持的文件格式\" class=\"headerlink\" title=\"Impala支持的文件格式\"></a>Impala支持的文件格式</h2><p><a href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/impala_file_formats.html#file_formats\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/documentation/enterprise/latest/topics/impala_file_formats.html#file_formats</a></p>\n<table>\n<thead>\n<tr>\n<th>文件类型</th>\n<th>结构</th>\n<th>压缩方式</th>\n<th>Impala能否创建</th>\n<th>Impala能否插入</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Parquet</td>\n<td>有结构的</td>\n<td>Snappy, gzip; 默认 Snappy</td>\n<td>能</td>\n<td>是: 创建表, 插入, Load数据和查询</td>\n</tr>\n<tr>\n<td>Text</td>\n<td>无结构的</td>\n<td>LZO, gzip, bzip2, Snappy</td>\n<td>是. 在创建表的时候不带STORED AS 语法, 默认文件格式是没有压缩的text, 使用ASCII 0x01 做分隔符 (通常点表Ctrl-A).</td>\n<td>是: 创建表, 插入, Load数据和查询. 如果使用LZO压缩, 你必须在hive里面创建表和load数据. 如果使用了其他的压缩翻身,你必须load数据通过LOAD DATA, Hive,或者手动在 HDFS上操作.</td>\n</tr>\n<tr>\n<td>Avro</td>\n<td>有结构的</td>\n<td>Snappy, gzip, deflate, bzip2</td>\n<td>是, 在Impala 1.4.0和更高的版本. 之前的版本使用hive创建表</td>\n<td>不能, 导入数据时，使用正确格式的数据文件加载数据，或者在Hive中使用INSERT，然后在Impala中使用REFRESH table_name。</td>\n</tr>\n<tr>\n<td>RCFile</td>\n<td>有结构的</td>\n<td>Snappy, gzip, deflate, bzip2</td>\n<td>是</td>\n<td>不能, 导入数据时，使用正确格式的数据文件加载数据，或者在Hive中使用INSERT，然后在Impala中使用REFRESH table_name。</td>\n</tr>\n<tr>\n<td>SequenceFile</td>\n<td>有结构的</td>\n<td>Snappy, gzip, deflate, bzip2</td>\n<td>是</td>\n<td>不能,导入数据时，使用正确格式的数据文件加载数据，或者在Hive中使用INSERT，然后在Impala中使用REFRESH table_name。</td>\n</tr>\n</tbody>\n</table>\n<p>Impala 只支持以上的文件格式，尤其要指出，Impala不支持ORC格式的文件。</p>\n<p>Impala 支持以下的压缩格式</p>\n<ul>\n<li>Snappy. 因为其压缩率和解压速度是优先被推荐的格式 Snappy 压缩速度非常快,但是gzip更节省空间. 在 Impala 2.0 和更高的版本支持text使用这种压缩方式。</li>\n<li>Gzip. 当需要更高level的压缩时，Gzip是被推荐的 (以为其更节省空间). 在 Impala 2.0 和更高的版本支持text使用这种压缩方式</li>\n<li>Deflate. text 文件不支持使用此种压缩方式.</li>\n<li>Bzip2. 在 Impala 2.0 和更高的版本支持text使用这种压缩方式</li>\n<li>LZO, 只能为text文件使用. Impala 只能查询 LZO压缩的 text表,但是还不能创建和插入; 在Hive中去创建表和插入数据。</li>\n</ul>\n<h2 id=\"为一个表选择文件格式\"><a href=\"#为一个表选择文件格式\" class=\"headerlink\" title=\"为一个表选择文件格式\"></a>为一个表选择文件格式</h2><ul>\n<li>如果您正在处理的文件格式是已经被支持的，请使用与实际的Impala表相同的格式。如果原始格式不产生可接受的查询性能或资源使用情况，可以考虑使用不同的文件格式或压缩特性创建一个新的Impala表，并使用INSERT语句将数据复制到新表进行一次性转换。根据文件格式，您可以在impala - shell中或在Hive中运行INSERT语句。</li>\n<li>许多不同的工具都可以很方便的生产文本文件，而且是便于验证和调试的。这些特性就是为什么文本是Impala创建表语句的默认格式。当性能和资源使用是主要考虑因素时，使用另一种文件格式，并考虑使用压缩。一个典型的工作流可能包括将数据复制到一个Impala表中，将CSV或TSV文件复制到适当的数据目录中，然后使用insert…select语法转换到适当的表中</li>\n<li>如果您的体系结构需要在内存中存储数据，那么不要压缩数据。由于数据不需要从磁盘移动，所以没有I/O成本，但是要解压数据，需要消耗CPU成本。</li>\n</ul>\n<h2 id=\"Impala数据类型\"><a href=\"#Impala数据类型\" class=\"headerlink\" title=\"Impala数据类型\"></a>Impala数据类型</h2><p><a href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/impala_datatypes.html\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/documentation/enterprise/latest/topics/impala_datatypes.html</a><br>Note: 当前Impala只支持标量的数据类型，不支持复合和嵌套类型。</p>\n<p>ARRAY类型：<br>语法</p>\n<pre><code>column_name ARRAY &lt; type &gt;\n\ntype ::= primitive_type | complex_type\n</code></pre>"},{"title":"Kerberos 认证流程图","date":"2018-10-23T11:29:50.000Z","_content":"\n![Alt text](img/kerberos.png)\n","source":"_posts/Kerberos.md","raw":"---\ntitle: Kerberos 认证流程图\ndate: 2018-10-23 19:29:50\ntags:\n- Kerberos\n---\n\n![Alt text](img/kerberos.png)\n","slug":"Kerberos","published":1,"updated":"2018-10-23T11:38:25.700Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzy0018inamiwbxifyc","content":"<p><img src=\"img/kerberos.png\" alt=\"Alt text\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"img/kerberos.png\" alt=\"Alt text\"></p>\n"},{"title":"Install CDH6 On CentOS7","date":"2018-09-13T09:58:15.000Z","_content":"## 1.   安装准备\n### 1.1.    环境\n操作系统: CentOS Linux release 7.3.1611 (Core)\nJDK: 1.8.0_181\nCM:6.0.0\nCDH:6.0.0\nmariadb:5.5\n### 1.2.    下载信息\nCM6.0下载地址: https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPMS/x86_64/\nallkeys.asc文件下载地址: https://archive.cloudera.com/cm6/6.0.0/\nCDH6.0下载地址: https://archive.cloudera.com/cdh6/6.0.0/parcels/\nmariadb下载地址: https://downloads.mariadb.org\nPsycopg下载地址: http://initd.org/psycopg/tarballs/PSYCOPG-2-5/\n\n<!-- more -->\n\n### 1.3. 关闭IPv6\n在文件/etc/sysctl.conf文件中如下内容\n\n```\nnet.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\n```\n并在命令行执行\n\n```\nsysctl -p\n```\n### 1.4. 关闭交换内存\n在文件/etc/sysctl.conf文件中如下内容\n\n```\nvm.swappiness=0\n```\n并在命令行执行\n\n```\nsysctl -p\n```\n### 1.5. 修改主机名\n在CentOS7上可以直接执行如下命令修改主机名称\n```\n[root@localhost ~]# hostnamectl set-hostname node1.example.com\n[root@localhost ~]# hostname\nnode1.example.com\n[root@localhost ~]#\n```\n修改/etc/hosts 文件\n\n```\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.11.224 node1-c7.example.com node1\n192.168.11.225 node2-c7.example.com node2\n192.168.11.226 node3-c7.example.com node3\n192.168.11.227 node4-c7.example.com node4\n192.168.11.228 node5-c7.example.com node5\n```\n### 1.6. 关闭防火墙\n在CentOS7上使用如下命令关闭防火墙并禁止开机启动。\n\n```\n[root@localhost java]# systemctl stop firewalld\n[root@localhost java]# systemctl disable firewalld\nRemoved symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.\nRemoved symlink /etc/systemd/system/basic.target.wants/firewalld.service.\n[root@localhost java]#\n```\n\n### 1.7. 关闭SELINUX\n修改文件/etc/selinux/config 将其中的SELINUX=enforcing修改为disabled\n```\nSELINUX=disabled\n```\n\n### 1.8. 关闭透明大页\n在命令行执行\n\n```\n[root@localhost ~]# echo never > /sys/kernel/mm/transparent_hugepage/enabled\n[root@localhost ~]# echo never > /sys/kernel/mm/transparent_hugepage/defrag\n[root@localhost ~]#\n```\n为使其永久生效，将其添加到文件/ect/rc.local\n\n```\ntouch /var/lock/subsys/local\necho never > /sys/kernel/mm/transparent_hugepage/enabled\necho never > /sys/kernel/mm/transparent_hugepage/defrag\n```\n\n### 1.9. ntp服务\n参考其他网上对ntp的配置\n\n### 1.10. 创建本地repository\n参考：https://www.cloudera.com/documentation/enterprise/upgrade/topics/cm_ig_create_local_package_repo.html\n\n#### 1.10.1. 安装Httpd和createrepo\n执行如下命令安装http和createrepo\n\n```\n[root@localhost ~]# yum install -y httpd createrepo\n```\n修改配置文件/etc/httpd/conf/httpd.conf，修改284行附件将\n\n```\nAddType application/x-compress .Z\nAddType application/x-gzip .gz .tgz\n```\n修改为\n\n```\nAddType application/x-compress .Z\nAddType application/x-gzip .gz .tgz .parcel\n```\n启动Http\n\n```\nsystemctl start httpd\n```\n在/var/www/html/下创建文件夹repos，在repos下创建文件夹cm6和cdh6\n\n```\nmkdir -p /var/www/html/repos/cm6\nmkdir -p /var/www/html/repos/cdh6\n```\n将下载的文件分别上传到对应的文件夹\n\n```\n/var/www/html/repos/\n|-- cdh6\n|   |-- CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel\n|   |-- CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha256\n|    `--manifest.json\n`-- cm6\n    |-- allkeys.asc\n    |-- cloudera-manager-agent-6.0.0-530873.el7.x86_64.rpm\n    |-- cloudera-manager-daemons-6.0.0-530873.el7.x86_64.rpm\n    |-- cloudera-manager-server-6.0.0-530873.el7.x86_64.rpm\n    |-- cloudera-manager-server-db-2-6.0.0-530873.el7.x86_64.rpm\n    |-- index.html\n    `--  oracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpm\n```\n在repos文件夹下执行命令\n\n```\ncreaterepo cm6\ncreaterepo cdh6\n```\n\n### 1.11. 安装数据库\n使用命令行安装mariadb，因为当前版本的Linux镜像中自带的就是mariadb5.5\n\n```\nyum install -y mariadb-server\n```\n启动mariadb\n\n```\nsystemctl start mariadb\nsystemctl enable mariadb\n```\n创建数据库和用户\n\n```\nmysql -u root --password='123456' -e 'create database metastore default character set utf8;'\nmysql -u root --password='123456' -e \"CREATE USER 'hive'@'%' IDENTIFIED BY '123456';\"\nmysql -u root --password='123456' -e \"GRANT ALL PRIVILEGES ON metastore. * TO 'hive'@'%';\"\nmysql -u root --password='123456' -e \"create user 'amon'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database amon default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on amon .* to 'amon'@'%'\"\nmysql -u root --password='123456' -e \"create user 'rman'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database rman default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on rman .* to 'rman'@'%'\"\nmysql -u root --password='123456' -e \"create user 'sentry'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database sentry default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on sentry.* to 'sentry'@'%'\"\nmysql -u root --password='123456' -e \"create user 'nav'@'%'identified by '123456'\"\nmysql -u root --password='123456' -e 'create database nav default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on nav.* to 'nav'@'%'\"\nmysql -u root --password='123456' -e \"create user 'navms'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database navms default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on navms.* to 'navms'@'%'\"\nmysql -u root --password='123456' -e \"create user 'cm'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database cm default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on cm.* to 'cm'@'%'\"\nmysql -u root --password='123456' -e \"create user 'oozie'@'%'identified by '123456'\"\nmysql -u root --password='123456' -e 'create database oozie default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on oozie.* to 'oozie'@'%'\"\nmysql -u root --password='123456' -e \"create user 'hue'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database hue default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on hue.* to 'hue'@'%'\"\nmysql -u root --password='123456' -e \"FLUSH PRIVILEGES;\n```\n\n### 1.12.    安装JDK\n进入jdk的rpm包所在的目录执行命令:\n\n```\nrpm -ivh jdk-8u181-linux-x64.rpm\n```\n修改/etc/profile文件,在最后添加如下内容:\n\n```\nexport JAVA_HOME=/usr/java/latest\nexport PATH=$JAVA_HOME/bin:$PATH\n```\n### 1.13.   安装psycopg\n安装依赖包\n\n```\nyum install -y gcc postgresql postgresql-server postgresql-devel\n```\n将之前下载的psycopg文件解压并进入解压的psycopg文件夹执行命令\n\n```\npython setup.py build\npython setup.py install\n```\n## 2. 安装Cloudera Manager\n使用yum 命令安装cloudera manager\n\n```\nyum install -y cloudera-manager-server\n```\n安装完成后，配置cm数据库。\n\n```\n[root@node1 java]# /opt/cloudera/cm/schema/scm_prepare_database.sh  mysql cm cm 123456\nJAVA_HOME=/usr/java/latest\nVerifying that we can write to /etc/cloudera-scm-server\nCreating SCM configuration file in /etc/cloudera-scm-server\nExecuting:  /usr/java/latest/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/java/postgresql-connector-java.jar:/opt/cloudera/cm/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.\n[                          main] DbCommandExecutor              INFO  Successfully connected to database.\nAll done, your SCM database is configured correctly!\n[root@node1 java]#\n```\n\n## 3.页面安装CDH\n打开网页http://cloudera manager host:7180,用户名和密码都是admin\n![初始界面](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/1.png?raw=true)\n同意License\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/2.png?raw=true)\n选择安装的版本,在CDH6中同CDH5一样,安装的时候可以进行3个版本的选择,Cloudera Express-免费版;Cloudera Enterprise Trial-企业60天试用版; Cloudera Enterprise-企业版.此处选择60天试用版本.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/3.png?raw=true)\n进入集群安装界面\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/4.png?raw=true)\n在CDH6的版本中,建议用户启用TLS,在安装的过程中会出现如下界面,给出启用TLS的步骤,如果需要启用可以按照其步骤进行操作,如果不需要则直接跳过.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/5.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/6.png?raw=true)\n填入需要安装的主机IP地址\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/7.png?raw=true)\n在Custom Repository的输入框中填入之前创建的本地CM的yum源地址\n在CDH and other software的More Options的选项中填入CDH的yum源的地址，当CDH Version自动选择的CDH-6.0才继续下一步\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/8.png?raw=true)\n如果安装过了JDK，此处不勾选。\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/9.png?raw=true)\n填入root用户的密码\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/10.png?raw=true)\n进入安装cloudera agent的步骤\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/11.png?raw=true)\n进入分发parcels的步骤\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/12.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/13.png?raw=true)\n主机检查，尽量修复所有的警告问题。\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/14.png?raw=true)\n选择安装的方式,不同的安装方式默认安装的CDH组件不同.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/15.png?raw=true)\n配置所有组件需要的数据库连接信息\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/16.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/17.png?raw=true)\n启动\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/18.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/19.png?raw=true)\n安装完成\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/20.png?raw=true)\n","source":"_posts/Install-CDH6-on-CentOS7.md","raw":"---\ntitle: Install CDH6 On CentOS7\ndate: 2018-09-13 17:58:15\ntags:\n- CDH6\n- CDH\n---\n## 1.   安装准备\n### 1.1.    环境\n操作系统: CentOS Linux release 7.3.1611 (Core)\nJDK: 1.8.0_181\nCM:6.0.0\nCDH:6.0.0\nmariadb:5.5\n### 1.2.    下载信息\nCM6.0下载地址: https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPMS/x86_64/\nallkeys.asc文件下载地址: https://archive.cloudera.com/cm6/6.0.0/\nCDH6.0下载地址: https://archive.cloudera.com/cdh6/6.0.0/parcels/\nmariadb下载地址: https://downloads.mariadb.org\nPsycopg下载地址: http://initd.org/psycopg/tarballs/PSYCOPG-2-5/\n\n<!-- more -->\n\n### 1.3. 关闭IPv6\n在文件/etc/sysctl.conf文件中如下内容\n\n```\nnet.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\n```\n并在命令行执行\n\n```\nsysctl -p\n```\n### 1.4. 关闭交换内存\n在文件/etc/sysctl.conf文件中如下内容\n\n```\nvm.swappiness=0\n```\n并在命令行执行\n\n```\nsysctl -p\n```\n### 1.5. 修改主机名\n在CentOS7上可以直接执行如下命令修改主机名称\n```\n[root@localhost ~]# hostnamectl set-hostname node1.example.com\n[root@localhost ~]# hostname\nnode1.example.com\n[root@localhost ~]#\n```\n修改/etc/hosts 文件\n\n```\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.11.224 node1-c7.example.com node1\n192.168.11.225 node2-c7.example.com node2\n192.168.11.226 node3-c7.example.com node3\n192.168.11.227 node4-c7.example.com node4\n192.168.11.228 node5-c7.example.com node5\n```\n### 1.6. 关闭防火墙\n在CentOS7上使用如下命令关闭防火墙并禁止开机启动。\n\n```\n[root@localhost java]# systemctl stop firewalld\n[root@localhost java]# systemctl disable firewalld\nRemoved symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.\nRemoved symlink /etc/systemd/system/basic.target.wants/firewalld.service.\n[root@localhost java]#\n```\n\n### 1.7. 关闭SELINUX\n修改文件/etc/selinux/config 将其中的SELINUX=enforcing修改为disabled\n```\nSELINUX=disabled\n```\n\n### 1.8. 关闭透明大页\n在命令行执行\n\n```\n[root@localhost ~]# echo never > /sys/kernel/mm/transparent_hugepage/enabled\n[root@localhost ~]# echo never > /sys/kernel/mm/transparent_hugepage/defrag\n[root@localhost ~]#\n```\n为使其永久生效，将其添加到文件/ect/rc.local\n\n```\ntouch /var/lock/subsys/local\necho never > /sys/kernel/mm/transparent_hugepage/enabled\necho never > /sys/kernel/mm/transparent_hugepage/defrag\n```\n\n### 1.9. ntp服务\n参考其他网上对ntp的配置\n\n### 1.10. 创建本地repository\n参考：https://www.cloudera.com/documentation/enterprise/upgrade/topics/cm_ig_create_local_package_repo.html\n\n#### 1.10.1. 安装Httpd和createrepo\n执行如下命令安装http和createrepo\n\n```\n[root@localhost ~]# yum install -y httpd createrepo\n```\n修改配置文件/etc/httpd/conf/httpd.conf，修改284行附件将\n\n```\nAddType application/x-compress .Z\nAddType application/x-gzip .gz .tgz\n```\n修改为\n\n```\nAddType application/x-compress .Z\nAddType application/x-gzip .gz .tgz .parcel\n```\n启动Http\n\n```\nsystemctl start httpd\n```\n在/var/www/html/下创建文件夹repos，在repos下创建文件夹cm6和cdh6\n\n```\nmkdir -p /var/www/html/repos/cm6\nmkdir -p /var/www/html/repos/cdh6\n```\n将下载的文件分别上传到对应的文件夹\n\n```\n/var/www/html/repos/\n|-- cdh6\n|   |-- CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel\n|   |-- CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha256\n|    `--manifest.json\n`-- cm6\n    |-- allkeys.asc\n    |-- cloudera-manager-agent-6.0.0-530873.el7.x86_64.rpm\n    |-- cloudera-manager-daemons-6.0.0-530873.el7.x86_64.rpm\n    |-- cloudera-manager-server-6.0.0-530873.el7.x86_64.rpm\n    |-- cloudera-manager-server-db-2-6.0.0-530873.el7.x86_64.rpm\n    |-- index.html\n    `--  oracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpm\n```\n在repos文件夹下执行命令\n\n```\ncreaterepo cm6\ncreaterepo cdh6\n```\n\n### 1.11. 安装数据库\n使用命令行安装mariadb，因为当前版本的Linux镜像中自带的就是mariadb5.5\n\n```\nyum install -y mariadb-server\n```\n启动mariadb\n\n```\nsystemctl start mariadb\nsystemctl enable mariadb\n```\n创建数据库和用户\n\n```\nmysql -u root --password='123456' -e 'create database metastore default character set utf8;'\nmysql -u root --password='123456' -e \"CREATE USER 'hive'@'%' IDENTIFIED BY '123456';\"\nmysql -u root --password='123456' -e \"GRANT ALL PRIVILEGES ON metastore. * TO 'hive'@'%';\"\nmysql -u root --password='123456' -e \"create user 'amon'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database amon default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on amon .* to 'amon'@'%'\"\nmysql -u root --password='123456' -e \"create user 'rman'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database rman default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on rman .* to 'rman'@'%'\"\nmysql -u root --password='123456' -e \"create user 'sentry'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database sentry default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on sentry.* to 'sentry'@'%'\"\nmysql -u root --password='123456' -e \"create user 'nav'@'%'identified by '123456'\"\nmysql -u root --password='123456' -e 'create database nav default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on nav.* to 'nav'@'%'\"\nmysql -u root --password='123456' -e \"create user 'navms'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database navms default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on navms.* to 'navms'@'%'\"\nmysql -u root --password='123456' -e \"create user 'cm'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database cm default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on cm.* to 'cm'@'%'\"\nmysql -u root --password='123456' -e \"create user 'oozie'@'%'identified by '123456'\"\nmysql -u root --password='123456' -e 'create database oozie default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on oozie.* to 'oozie'@'%'\"\nmysql -u root --password='123456' -e \"create user 'hue'@'%' identified by '123456'\"\nmysql -u root --password='123456' -e 'create database hue default character set utf8'\nmysql -u root --password='123456' -e \"grant all privileges on hue.* to 'hue'@'%'\"\nmysql -u root --password='123456' -e \"FLUSH PRIVILEGES;\n```\n\n### 1.12.    安装JDK\n进入jdk的rpm包所在的目录执行命令:\n\n```\nrpm -ivh jdk-8u181-linux-x64.rpm\n```\n修改/etc/profile文件,在最后添加如下内容:\n\n```\nexport JAVA_HOME=/usr/java/latest\nexport PATH=$JAVA_HOME/bin:$PATH\n```\n### 1.13.   安装psycopg\n安装依赖包\n\n```\nyum install -y gcc postgresql postgresql-server postgresql-devel\n```\n将之前下载的psycopg文件解压并进入解压的psycopg文件夹执行命令\n\n```\npython setup.py build\npython setup.py install\n```\n## 2. 安装Cloudera Manager\n使用yum 命令安装cloudera manager\n\n```\nyum install -y cloudera-manager-server\n```\n安装完成后，配置cm数据库。\n\n```\n[root@node1 java]# /opt/cloudera/cm/schema/scm_prepare_database.sh  mysql cm cm 123456\nJAVA_HOME=/usr/java/latest\nVerifying that we can write to /etc/cloudera-scm-server\nCreating SCM configuration file in /etc/cloudera-scm-server\nExecuting:  /usr/java/latest/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/java/postgresql-connector-java.jar:/opt/cloudera/cm/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.\n[                          main] DbCommandExecutor              INFO  Successfully connected to database.\nAll done, your SCM database is configured correctly!\n[root@node1 java]#\n```\n\n## 3.页面安装CDH\n打开网页http://cloudera manager host:7180,用户名和密码都是admin\n![初始界面](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/1.png?raw=true)\n同意License\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/2.png?raw=true)\n选择安装的版本,在CDH6中同CDH5一样,安装的时候可以进行3个版本的选择,Cloudera Express-免费版;Cloudera Enterprise Trial-企业60天试用版; Cloudera Enterprise-企业版.此处选择60天试用版本.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/3.png?raw=true)\n进入集群安装界面\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/4.png?raw=true)\n在CDH6的版本中,建议用户启用TLS,在安装的过程中会出现如下界面,给出启用TLS的步骤,如果需要启用可以按照其步骤进行操作,如果不需要则直接跳过.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/5.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/6.png?raw=true)\n填入需要安装的主机IP地址\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/7.png?raw=true)\n在Custom Repository的输入框中填入之前创建的本地CM的yum源地址\n在CDH and other software的More Options的选项中填入CDH的yum源的地址，当CDH Version自动选择的CDH-6.0才继续下一步\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/8.png?raw=true)\n如果安装过了JDK，此处不勾选。\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/9.png?raw=true)\n填入root用户的密码\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/10.png?raw=true)\n进入安装cloudera agent的步骤\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/11.png?raw=true)\n进入分发parcels的步骤\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/12.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/13.png?raw=true)\n主机检查，尽量修复所有的警告问题。\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/14.png?raw=true)\n选择安装的方式,不同的安装方式默认安装的CDH组件不同.\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/15.png?raw=true)\n配置所有组件需要的数据库连接信息\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/16.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/17.png?raw=true)\n启动\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/18.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/19.png?raw=true)\n安装完成\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/20.png?raw=true)\n","slug":"Install-CDH6-on-CentOS7","published":1,"updated":"2018-09-14T01:15:17.423Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avnzz001ainamkzy56o6a","content":"<h2 id=\"1-安装准备\"><a href=\"#1-安装准备\" class=\"headerlink\" title=\"1.   安装准备\"></a>1.   安装准备</h2><h3 id=\"1-1-环境\"><a href=\"#1-1-环境\" class=\"headerlink\" title=\"1.1.    环境\"></a>1.1.    环境</h3><p>操作系统: CentOS Linux release 7.3.1611 (Core)<br>JDK: 1.8.0_181<br>CM:6.0.0<br>CDH:6.0.0<br>mariadb:5.5</p>\n<h3 id=\"1-2-下载信息\"><a href=\"#1-2-下载信息\" class=\"headerlink\" title=\"1.2.    下载信息\"></a>1.2.    下载信息</h3><p>CM6.0下载地址: <a href=\"https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPMS/x86_64/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPMS/x86_64/</a><br>allkeys.asc文件下载地址: <a href=\"https://archive.cloudera.com/cm6/6.0.0/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cm6/6.0.0/</a><br>CDH6.0下载地址: <a href=\"https://archive.cloudera.com/cdh6/6.0.0/parcels/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cdh6/6.0.0/parcels/</a><br>mariadb下载地址: <a href=\"https://downloads.mariadb.org\" target=\"_blank\" rel=\"noopener\">https://downloads.mariadb.org</a><br>Psycopg下载地址: <a href=\"http://initd.org/psycopg/tarballs/PSYCOPG-2-5/\" target=\"_blank\" rel=\"noopener\">http://initd.org/psycopg/tarballs/PSYCOPG-2-5/</a></p>\n<a id=\"more\"></a>\n<h3 id=\"1-3-关闭IPv6\"><a href=\"#1-3-关闭IPv6\" class=\"headerlink\" title=\"1.3. 关闭IPv6\"></a>1.3. 关闭IPv6</h3><p>在文件/etc/sysctl.conf文件中如下内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net.ipv6.conf.all.disable_ipv6 = 1</span><br><span class=\"line\">net.ipv6.conf.default.disable_ipv6 = 1</span><br></pre></td></tr></table></figure>\n<p>并在命令行执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sysctl -p</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-4-关闭交换内存\"><a href=\"#1-4-关闭交换内存\" class=\"headerlink\" title=\"1.4. 关闭交换内存\"></a>1.4. 关闭交换内存</h3><p>在文件/etc/sysctl.conf文件中如下内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vm.swappiness=0</span><br></pre></td></tr></table></figure>\n<p>并在命令行执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sysctl -p</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-5-修改主机名\"><a href=\"#1-5-修改主机名\" class=\"headerlink\" title=\"1.5. 修改主机名\"></a>1.5. 修改主机名</h3><p>在CentOS7上可以直接执行如下命令修改主机名称<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@localhost ~]# hostnamectl set-hostname node1.example.com</span><br><span class=\"line\">[root@localhost ~]# hostname</span><br><span class=\"line\">node1.example.com</span><br><span class=\"line\">[root@localhost ~]#</span><br></pre></td></tr></table></figure></p>\n<p>修改/etc/hosts 文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class=\"line\">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class=\"line\">192.168.11.224 node1-c7.example.com node1</span><br><span class=\"line\">192.168.11.225 node2-c7.example.com node2</span><br><span class=\"line\">192.168.11.226 node3-c7.example.com node3</span><br><span class=\"line\">192.168.11.227 node4-c7.example.com node4</span><br><span class=\"line\">192.168.11.228 node5-c7.example.com node5</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-6-关闭防火墙\"><a href=\"#1-6-关闭防火墙\" class=\"headerlink\" title=\"1.6. 关闭防火墙\"></a>1.6. 关闭防火墙</h3><p>在CentOS7上使用如下命令关闭防火墙并禁止开机启动。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@localhost java]# systemctl stop firewalld</span><br><span class=\"line\">[root@localhost java]# systemctl disable firewalld</span><br><span class=\"line\">Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.</span><br><span class=\"line\">Removed symlink /etc/systemd/system/basic.target.wants/firewalld.service.</span><br><span class=\"line\">[root@localhost java]#</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-7-关闭SELINUX\"><a href=\"#1-7-关闭SELINUX\" class=\"headerlink\" title=\"1.7. 关闭SELINUX\"></a>1.7. 关闭SELINUX</h3><p>修改文件/etc/selinux/config 将其中的SELINUX=enforcing修改为disabled<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELINUX=disabled</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"1-8-关闭透明大页\"><a href=\"#1-8-关闭透明大页\" class=\"headerlink\" title=\"1.8. 关闭透明大页\"></a>1.8. 关闭透明大页</h3><p>在命令行执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@localhost ~]# echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class=\"line\">[root@localhost ~]# echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class=\"line\">[root@localhost ~]#</span><br></pre></td></tr></table></figure>\n<p>为使其永久生效，将其添加到文件/ect/rc.local</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">touch /var/lock/subsys/local</span><br><span class=\"line\">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class=\"line\">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-9-ntp服务\"><a href=\"#1-9-ntp服务\" class=\"headerlink\" title=\"1.9. ntp服务\"></a>1.9. ntp服务</h3><p>参考其他网上对ntp的配置</p>\n<h3 id=\"1-10-创建本地repository\"><a href=\"#1-10-创建本地repository\" class=\"headerlink\" title=\"1.10. 创建本地repository\"></a>1.10. 创建本地repository</h3><p>参考：<a href=\"https://www.cloudera.com/documentation/enterprise/upgrade/topics/cm_ig_create_local_package_repo.html\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/documentation/enterprise/upgrade/topics/cm_ig_create_local_package_repo.html</a></p>\n<h4 id=\"1-10-1-安装Httpd和createrepo\"><a href=\"#1-10-1-安装Httpd和createrepo\" class=\"headerlink\" title=\"1.10.1. 安装Httpd和createrepo\"></a>1.10.1. 安装Httpd和createrepo</h4><p>执行如下命令安装http和createrepo</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@localhost ~]# yum install -y httpd createrepo</span><br></pre></td></tr></table></figure>\n<p>修改配置文件/etc/httpd/conf/httpd.conf，修改284行附件将</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AddType application/x-compress .Z</span><br><span class=\"line\">AddType application/x-gzip .gz .tgz</span><br></pre></td></tr></table></figure>\n<p>修改为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AddType application/x-compress .Z</span><br><span class=\"line\">AddType application/x-gzip .gz .tgz .parcel</span><br></pre></td></tr></table></figure>\n<p>启动Http</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl start httpd</span><br></pre></td></tr></table></figure>\n<p>在/var/www/html/下创建文件夹repos，在repos下创建文件夹cm6和cdh6</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /var/www/html/repos/cm6</span><br><span class=\"line\">mkdir -p /var/www/html/repos/cdh6</span><br></pre></td></tr></table></figure>\n<p>将下载的文件分别上传到对应的文件夹</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/var/www/html/repos/</span><br><span class=\"line\">|-- cdh6</span><br><span class=\"line\">|   |-- CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel</span><br><span class=\"line\">|   |-- CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha256</span><br><span class=\"line\">|    `--manifest.json</span><br><span class=\"line\">`-- cm6</span><br><span class=\"line\">    |-- allkeys.asc</span><br><span class=\"line\">    |-- cloudera-manager-agent-6.0.0-530873.el7.x86_64.rpm</span><br><span class=\"line\">    |-- cloudera-manager-daemons-6.0.0-530873.el7.x86_64.rpm</span><br><span class=\"line\">    |-- cloudera-manager-server-6.0.0-530873.el7.x86_64.rpm</span><br><span class=\"line\">    |-- cloudera-manager-server-db-2-6.0.0-530873.el7.x86_64.rpm</span><br><span class=\"line\">    |-- index.html</span><br><span class=\"line\">    `--  oracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpm</span><br></pre></td></tr></table></figure>\n<p>在repos文件夹下执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">createrepo cm6</span><br><span class=\"line\">createrepo cdh6</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-11-安装数据库\"><a href=\"#1-11-安装数据库\" class=\"headerlink\" title=\"1.11. 安装数据库\"></a>1.11. 安装数据库</h3><p>使用命令行安装mariadb，因为当前版本的Linux镜像中自带的就是mariadb5.5</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y mariadb-server</span><br></pre></td></tr></table></figure>\n<p>启动mariadb</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl start mariadb</span><br><span class=\"line\">systemctl enable mariadb</span><br></pre></td></tr></table></figure>\n<p>创建数据库和用户</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database metastore default character set utf8;&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;CREATE USER &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;GRANT ALL PRIVILEGES ON metastore. * TO &apos;hive&apos;@&apos;%&apos;;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;amon&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database amon default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on amon .* to &apos;amon&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;rman&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database rman default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on rman .* to &apos;rman&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;sentry&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database sentry default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on sentry.* to &apos;sentry&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;nav&apos;@&apos;%&apos;identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database nav default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on nav.* to &apos;nav&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;navms&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database navms default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on navms.* to &apos;navms&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;cm&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database cm default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on cm.* to &apos;cm&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;oozie&apos;@&apos;%&apos;identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database oozie default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on oozie.* to &apos;oozie&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;hue&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database hue default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on hue.* to &apos;hue&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-12-安装JDK\"><a href=\"#1-12-安装JDK\" class=\"headerlink\" title=\"1.12.    安装JDK\"></a>1.12.    安装JDK</h3><p>进入jdk的rpm包所在的目录执行命令:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rpm -ivh jdk-8u181-linux-x64.rpm</span><br></pre></td></tr></table></figure>\n<p>修改/etc/profile文件,在最后添加如下内容:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export JAVA_HOME=/usr/java/latest</span><br><span class=\"line\">export PATH=$JAVA_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-13-安装psycopg\"><a href=\"#1-13-安装psycopg\" class=\"headerlink\" title=\"1.13.   安装psycopg\"></a>1.13.   安装psycopg</h3><p>安装依赖包</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y gcc postgresql postgresql-server postgresql-devel</span><br></pre></td></tr></table></figure>\n<p>将之前下载的psycopg文件解压并进入解压的psycopg文件夹执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python setup.py build</span><br><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-安装Cloudera-Manager\"><a href=\"#2-安装Cloudera-Manager\" class=\"headerlink\" title=\"2. 安装Cloudera Manager\"></a>2. 安装Cloudera Manager</h2><p>使用yum 命令安装cloudera manager</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y cloudera-manager-server</span><br></pre></td></tr></table></figure>\n<p>安装完成后，配置cm数据库。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@node1 java]# /opt/cloudera/cm/schema/scm_prepare_database.sh  mysql cm cm 123456</span><br><span class=\"line\">JAVA_HOME=/usr/java/latest</span><br><span class=\"line\">Verifying that we can write to /etc/cloudera-scm-server</span><br><span class=\"line\">Creating SCM configuration file in /etc/cloudera-scm-server</span><br><span class=\"line\">Executing:  /usr/java/latest/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/java/postgresql-connector-java.jar:/opt/cloudera/cm/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.</span><br><span class=\"line\">[                          main] DbCommandExecutor              INFO  Successfully connected to database.</span><br><span class=\"line\">All done, your SCM database is configured correctly!</span><br><span class=\"line\">[root@node1 java]#</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-页面安装CDH\"><a href=\"#3-页面安装CDH\" class=\"headerlink\" title=\"3.页面安装CDH\"></a>3.页面安装CDH</h2><p>打开网页<a href=\"http://cloudera\" target=\"_blank\" rel=\"noopener\">http://cloudera</a> manager host:7180,用户名和密码都是admin<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/1.png?raw=true\" alt=\"初始界面\"><br>同意License<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/2.png?raw=true\" alt=\"\"><br>选择安装的版本,在CDH6中同CDH5一样,安装的时候可以进行3个版本的选择,Cloudera Express-免费版;Cloudera Enterprise Trial-企业60天试用版; Cloudera Enterprise-企业版.此处选择60天试用版本.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/3.png?raw=true\" alt=\"\"><br>进入集群安装界面<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/4.png?raw=true\" alt=\"\"><br>在CDH6的版本中,建议用户启用TLS,在安装的过程中会出现如下界面,给出启用TLS的步骤,如果需要启用可以按照其步骤进行操作,如果不需要则直接跳过.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/5.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/6.png?raw=true\" alt=\"\"><br>填入需要安装的主机IP地址<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/7.png?raw=true\" alt=\"\"><br>在Custom Repository的输入框中填入之前创建的本地CM的yum源地址<br>在CDH and other software的More Options的选项中填入CDH的yum源的地址，当CDH Version自动选择的CDH-6.0才继续下一步<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/8.png?raw=true\" alt=\"\"><br>如果安装过了JDK，此处不勾选。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/9.png?raw=true\" alt=\"\"><br>填入root用户的密码<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/10.png?raw=true\" alt=\"\"><br>进入安装cloudera agent的步骤<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/11.png?raw=true\" alt=\"\"><br>进入分发parcels的步骤<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/12.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/13.png?raw=true\" alt=\"\"><br>主机检查，尽量修复所有的警告问题。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/14.png?raw=true\" alt=\"\"><br>选择安装的方式,不同的安装方式默认安装的CDH组件不同.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/15.png?raw=true\" alt=\"\"><br>配置所有组件需要的数据库连接信息<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/16.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/17.png?raw=true\" alt=\"\"><br>启动<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/18.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/19.png?raw=true\" alt=\"\"><br>安装完成<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/20.png?raw=true\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"1-安装准备\"><a href=\"#1-安装准备\" class=\"headerlink\" title=\"1.   安装准备\"></a>1.   安装准备</h2><h3 id=\"1-1-环境\"><a href=\"#1-1-环境\" class=\"headerlink\" title=\"1.1.    环境\"></a>1.1.    环境</h3><p>操作系统: CentOS Linux release 7.3.1611 (Core)<br>JDK: 1.8.0_181<br>CM:6.0.0<br>CDH:6.0.0<br>mariadb:5.5</p>\n<h3 id=\"1-2-下载信息\"><a href=\"#1-2-下载信息\" class=\"headerlink\" title=\"1.2.    下载信息\"></a>1.2.    下载信息</h3><p>CM6.0下载地址: <a href=\"https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPMS/x86_64/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPMS/x86_64/</a><br>allkeys.asc文件下载地址: <a href=\"https://archive.cloudera.com/cm6/6.0.0/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cm6/6.0.0/</a><br>CDH6.0下载地址: <a href=\"https://archive.cloudera.com/cdh6/6.0.0/parcels/\" target=\"_blank\" rel=\"noopener\">https://archive.cloudera.com/cdh6/6.0.0/parcels/</a><br>mariadb下载地址: <a href=\"https://downloads.mariadb.org\" target=\"_blank\" rel=\"noopener\">https://downloads.mariadb.org</a><br>Psycopg下载地址: <a href=\"http://initd.org/psycopg/tarballs/PSYCOPG-2-5/\" target=\"_blank\" rel=\"noopener\">http://initd.org/psycopg/tarballs/PSYCOPG-2-5/</a></p>","more":"<h3 id=\"1-3-关闭IPv6\"><a href=\"#1-3-关闭IPv6\" class=\"headerlink\" title=\"1.3. 关闭IPv6\"></a>1.3. 关闭IPv6</h3><p>在文件/etc/sysctl.conf文件中如下内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net.ipv6.conf.all.disable_ipv6 = 1</span><br><span class=\"line\">net.ipv6.conf.default.disable_ipv6 = 1</span><br></pre></td></tr></table></figure>\n<p>并在命令行执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sysctl -p</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-4-关闭交换内存\"><a href=\"#1-4-关闭交换内存\" class=\"headerlink\" title=\"1.4. 关闭交换内存\"></a>1.4. 关闭交换内存</h3><p>在文件/etc/sysctl.conf文件中如下内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vm.swappiness=0</span><br></pre></td></tr></table></figure>\n<p>并在命令行执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sysctl -p</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-5-修改主机名\"><a href=\"#1-5-修改主机名\" class=\"headerlink\" title=\"1.5. 修改主机名\"></a>1.5. 修改主机名</h3><p>在CentOS7上可以直接执行如下命令修改主机名称<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@localhost ~]# hostnamectl set-hostname node1.example.com</span><br><span class=\"line\">[root@localhost ~]# hostname</span><br><span class=\"line\">node1.example.com</span><br><span class=\"line\">[root@localhost ~]#</span><br></pre></td></tr></table></figure></p>\n<p>修改/etc/hosts 文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class=\"line\">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class=\"line\">192.168.11.224 node1-c7.example.com node1</span><br><span class=\"line\">192.168.11.225 node2-c7.example.com node2</span><br><span class=\"line\">192.168.11.226 node3-c7.example.com node3</span><br><span class=\"line\">192.168.11.227 node4-c7.example.com node4</span><br><span class=\"line\">192.168.11.228 node5-c7.example.com node5</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-6-关闭防火墙\"><a href=\"#1-6-关闭防火墙\" class=\"headerlink\" title=\"1.6. 关闭防火墙\"></a>1.6. 关闭防火墙</h3><p>在CentOS7上使用如下命令关闭防火墙并禁止开机启动。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@localhost java]# systemctl stop firewalld</span><br><span class=\"line\">[root@localhost java]# systemctl disable firewalld</span><br><span class=\"line\">Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.</span><br><span class=\"line\">Removed symlink /etc/systemd/system/basic.target.wants/firewalld.service.</span><br><span class=\"line\">[root@localhost java]#</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-7-关闭SELINUX\"><a href=\"#1-7-关闭SELINUX\" class=\"headerlink\" title=\"1.7. 关闭SELINUX\"></a>1.7. 关闭SELINUX</h3><p>修改文件/etc/selinux/config 将其中的SELINUX=enforcing修改为disabled<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELINUX=disabled</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"1-8-关闭透明大页\"><a href=\"#1-8-关闭透明大页\" class=\"headerlink\" title=\"1.8. 关闭透明大页\"></a>1.8. 关闭透明大页</h3><p>在命令行执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@localhost ~]# echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class=\"line\">[root@localhost ~]# echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class=\"line\">[root@localhost ~]#</span><br></pre></td></tr></table></figure>\n<p>为使其永久生效，将其添加到文件/ect/rc.local</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">touch /var/lock/subsys/local</span><br><span class=\"line\">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class=\"line\">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-9-ntp服务\"><a href=\"#1-9-ntp服务\" class=\"headerlink\" title=\"1.9. ntp服务\"></a>1.9. ntp服务</h3><p>参考其他网上对ntp的配置</p>\n<h3 id=\"1-10-创建本地repository\"><a href=\"#1-10-创建本地repository\" class=\"headerlink\" title=\"1.10. 创建本地repository\"></a>1.10. 创建本地repository</h3><p>参考：<a href=\"https://www.cloudera.com/documentation/enterprise/upgrade/topics/cm_ig_create_local_package_repo.html\" target=\"_blank\" rel=\"noopener\">https://www.cloudera.com/documentation/enterprise/upgrade/topics/cm_ig_create_local_package_repo.html</a></p>\n<h4 id=\"1-10-1-安装Httpd和createrepo\"><a href=\"#1-10-1-安装Httpd和createrepo\" class=\"headerlink\" title=\"1.10.1. 安装Httpd和createrepo\"></a>1.10.1. 安装Httpd和createrepo</h4><p>执行如下命令安装http和createrepo</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@localhost ~]# yum install -y httpd createrepo</span><br></pre></td></tr></table></figure>\n<p>修改配置文件/etc/httpd/conf/httpd.conf，修改284行附件将</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AddType application/x-compress .Z</span><br><span class=\"line\">AddType application/x-gzip .gz .tgz</span><br></pre></td></tr></table></figure>\n<p>修改为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">AddType application/x-compress .Z</span><br><span class=\"line\">AddType application/x-gzip .gz .tgz .parcel</span><br></pre></td></tr></table></figure>\n<p>启动Http</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl start httpd</span><br></pre></td></tr></table></figure>\n<p>在/var/www/html/下创建文件夹repos，在repos下创建文件夹cm6和cdh6</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /var/www/html/repos/cm6</span><br><span class=\"line\">mkdir -p /var/www/html/repos/cdh6</span><br></pre></td></tr></table></figure>\n<p>将下载的文件分别上传到对应的文件夹</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/var/www/html/repos/</span><br><span class=\"line\">|-- cdh6</span><br><span class=\"line\">|   |-- CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel</span><br><span class=\"line\">|   |-- CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha256</span><br><span class=\"line\">|    `--manifest.json</span><br><span class=\"line\">`-- cm6</span><br><span class=\"line\">    |-- allkeys.asc</span><br><span class=\"line\">    |-- cloudera-manager-agent-6.0.0-530873.el7.x86_64.rpm</span><br><span class=\"line\">    |-- cloudera-manager-daemons-6.0.0-530873.el7.x86_64.rpm</span><br><span class=\"line\">    |-- cloudera-manager-server-6.0.0-530873.el7.x86_64.rpm</span><br><span class=\"line\">    |-- cloudera-manager-server-db-2-6.0.0-530873.el7.x86_64.rpm</span><br><span class=\"line\">    |-- index.html</span><br><span class=\"line\">    `--  oracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpm</span><br></pre></td></tr></table></figure>\n<p>在repos文件夹下执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">createrepo cm6</span><br><span class=\"line\">createrepo cdh6</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-11-安装数据库\"><a href=\"#1-11-安装数据库\" class=\"headerlink\" title=\"1.11. 安装数据库\"></a>1.11. 安装数据库</h3><p>使用命令行安装mariadb，因为当前版本的Linux镜像中自带的就是mariadb5.5</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y mariadb-server</span><br></pre></td></tr></table></figure>\n<p>启动mariadb</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl start mariadb</span><br><span class=\"line\">systemctl enable mariadb</span><br></pre></td></tr></table></figure>\n<p>创建数据库和用户</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database metastore default character set utf8;&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;CREATE USER &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;GRANT ALL PRIVILEGES ON metastore. * TO &apos;hive&apos;@&apos;%&apos;;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;amon&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database amon default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on amon .* to &apos;amon&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;rman&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database rman default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on rman .* to &apos;rman&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;sentry&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database sentry default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on sentry.* to &apos;sentry&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;nav&apos;@&apos;%&apos;identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database nav default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on nav.* to &apos;nav&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;navms&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database navms default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on navms.* to &apos;navms&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;cm&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database cm default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on cm.* to &apos;cm&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;oozie&apos;@&apos;%&apos;identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database oozie default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on oozie.* to &apos;oozie&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;hue&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &apos;create database hue default character set utf8&apos;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on hue.* to &apos;hue&apos;@&apos;%&apos;&quot;</span><br><span class=\"line\">mysql -u root --password=&apos;123456&apos; -e &quot;FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-12-安装JDK\"><a href=\"#1-12-安装JDK\" class=\"headerlink\" title=\"1.12.    安装JDK\"></a>1.12.    安装JDK</h3><p>进入jdk的rpm包所在的目录执行命令:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rpm -ivh jdk-8u181-linux-x64.rpm</span><br></pre></td></tr></table></figure>\n<p>修改/etc/profile文件,在最后添加如下内容:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export JAVA_HOME=/usr/java/latest</span><br><span class=\"line\">export PATH=$JAVA_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-13-安装psycopg\"><a href=\"#1-13-安装psycopg\" class=\"headerlink\" title=\"1.13.   安装psycopg\"></a>1.13.   安装psycopg</h3><p>安装依赖包</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y gcc postgresql postgresql-server postgresql-devel</span><br></pre></td></tr></table></figure>\n<p>将之前下载的psycopg文件解压并进入解压的psycopg文件夹执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python setup.py build</span><br><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-安装Cloudera-Manager\"><a href=\"#2-安装Cloudera-Manager\" class=\"headerlink\" title=\"2. 安装Cloudera Manager\"></a>2. 安装Cloudera Manager</h2><p>使用yum 命令安装cloudera manager</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y cloudera-manager-server</span><br></pre></td></tr></table></figure>\n<p>安装完成后，配置cm数据库。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@node1 java]# /opt/cloudera/cm/schema/scm_prepare_database.sh  mysql cm cm 123456</span><br><span class=\"line\">JAVA_HOME=/usr/java/latest</span><br><span class=\"line\">Verifying that we can write to /etc/cloudera-scm-server</span><br><span class=\"line\">Creating SCM configuration file in /etc/cloudera-scm-server</span><br><span class=\"line\">Executing:  /usr/java/latest/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/java/postgresql-connector-java.jar:/opt/cloudera/cm/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.</span><br><span class=\"line\">[                          main] DbCommandExecutor              INFO  Successfully connected to database.</span><br><span class=\"line\">All done, your SCM database is configured correctly!</span><br><span class=\"line\">[root@node1 java]#</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-页面安装CDH\"><a href=\"#3-页面安装CDH\" class=\"headerlink\" title=\"3.页面安装CDH\"></a>3.页面安装CDH</h2><p>打开网页<a href=\"http://cloudera\" target=\"_blank\" rel=\"noopener\">http://cloudera</a> manager host:7180,用户名和密码都是admin<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/1.png?raw=true\" alt=\"初始界面\"><br>同意License<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/2.png?raw=true\" alt=\"\"><br>选择安装的版本,在CDH6中同CDH5一样,安装的时候可以进行3个版本的选择,Cloudera Express-免费版;Cloudera Enterprise Trial-企业60天试用版; Cloudera Enterprise-企业版.此处选择60天试用版本.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/3.png?raw=true\" alt=\"\"><br>进入集群安装界面<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/4.png?raw=true\" alt=\"\"><br>在CDH6的版本中,建议用户启用TLS,在安装的过程中会出现如下界面,给出启用TLS的步骤,如果需要启用可以按照其步骤进行操作,如果不需要则直接跳过.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/5.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/6.png?raw=true\" alt=\"\"><br>填入需要安装的主机IP地址<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/7.png?raw=true\" alt=\"\"><br>在Custom Repository的输入框中填入之前创建的本地CM的yum源地址<br>在CDH and other software的More Options的选项中填入CDH的yum源的地址，当CDH Version自动选择的CDH-6.0才继续下一步<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/8.png?raw=true\" alt=\"\"><br>如果安装过了JDK，此处不勾选。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/9.png?raw=true\" alt=\"\"><br>填入root用户的密码<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/10.png?raw=true\" alt=\"\"><br>进入安装cloudera agent的步骤<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/11.png?raw=true\" alt=\"\"><br>进入分发parcels的步骤<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/12.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/13.png?raw=true\" alt=\"\"><br>主机检查，尽量修复所有的警告问题。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/14.png?raw=true\" alt=\"\"><br>选择安装的方式,不同的安装方式默认安装的CDH组件不同.<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/15.png?raw=true\" alt=\"\"><br>配置所有组件需要的数据库连接信息<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/16.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/17.png?raw=true\" alt=\"\"><br>启动<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/18.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/19.png?raw=true\" alt=\"\"><br>安装完成<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/clouderainstall/cdh6/20.png?raw=true\" alt=\"\"></p>"},{"title":"Test 随笔","date":"2018-05-13T00:33:26.000Z","_content":"\n\n\nTTTTTeeeeeeeeeeSSSSTTT\n","source":"_posts/test.md","raw":"---\ntitle: Test 随笔\ndate: 2018-05-13 08:33:26\ntags: 随笔\n---\n\n\n\nTTTTTeeeeeeeeeeSSSSTTT\n","slug":"test","published":1,"updated":"2018-05-13T01:29:07.225Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avo00001cinam8po5zbv4","content":"<p>TTTTTeeeeeeeeeeSSSSTTT</p>\n","site":{"data":{}},"excerpt":"","more":"<p>TTTTTeeeeeeeeeeSSSSTTT</p>\n"},{"title":"Test hive-overwrite and delete-target-dir in sqoop","date":"2018-05-16T06:15:39.000Z","_content":"\n# 测试Sqoop 中hive-overwrite和delete-target-dir是否冲突\n\n- 指定 –hive-import –hive-table –target-dir –delete-target-dir ,并且target-dir和hive表所在的路径一致。\n\n```\nsqoop import --hive-import --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --table orders --hive-table orders --target-dir /user/hive/warehouse/orders/ --delete-target-dir --hive-overwrite\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite1.png?raw=true)\n\n<!-- more -->\n- 指定 –hive-import –hive-table –target-dir –delete-target-dir ,并且target-dir和hive表所在的路径不一致。\n\n```\nsqoop import --hive-import --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --table orders --hive-table orders  --target-dir /user/hive/warehouse/order-1/ --delete-target-dir --hive-overwrite\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite2.png?raw=true)\n\n从结果来看，数据没有导入到—target-dir 指定的HDFS路径上，而是导入到表中去了。\n\n- 如果不指定target-dir和—delete-target-dir\n\n```\nsqoop import --hive-import --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --table orders --hive-table orders --hive-overwrite\n```\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite3.png?raw=true)\n\n- 调整下 –hive-overwrite的位置\n\n```\nsqoop import --hive-import --hive-overwrite --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --table orders --hive-table orders\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite4.png?raw=true)\n原因是默认在当前用户的hdfs home目录下创建一个和表名一致的文件夹，先手动删除这个文件夹然后再执行命令，在执行的Log中有如下的内容，因为不是使用hive进行数据导入的。\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite5.png?raw=true)\n\n检查结果\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite6.png?raw=true)\n\n再次执行，即使没有delete-target-dir 仍能执行成功，并且在执行的过程中，发现确实创建了 /user/training/orders文件夹，然后在任务执行完成后将文件移动到表的路径下，如下面第二张图所示。\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite7.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite8.png?raw=true)\n\n\n\n- 测试-query的情况\n\n```\nsqoop import --hive-import --hive-overwrite --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --query \"select * from orders WHERE \\$CONDITIONS\" --hive-table orders\n```\n没有指定—target-dir  报如下错误\n<img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite9.png?raw=true\" alt=\"drawing\" style=\"width: 500px;\"/>\n\n```\nsqoop import --hive-import --hive-overwrite --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --query \"select * from orders WHERE \\$CONDITIONS\" --hive-table orders –target-dir /user/training/orders/\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite10.png?raw=true)\n必须指定split-by\n\n```\nsqoop import --hive-import --hive-overwrite --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --query \"select * from orders WHERE \\$CONDITIONS\" --hive-table orders  --target-dir /user/training/orders-1/ --split-by order_id\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite11.png?raw=true)\n\n\n在导入的过程中在target-dir生成临时文件，完成后将文件移动到表的路径下。所以此时不需要—delete-target-dir\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite12.png?raw=true)\n\n- 如果即使用--hive-table  --hive-overwrite 、--target-dir、 --delete-target-dir 并且 target-dir和表的路径一致。\n\n```\nsqoop import --hive-import --hive-overwrite --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --query \"select * from orders WHERE \\$CONDITIONS\" --hive-table orders  --target-dir /user/hive/warehouse/orders/ --split-by order_id --delete-target-dir\n```\n\n报如下错误\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite13.png?raw=true)\n\n```\nsqoop import --hive-import --hive-overwrite --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --query \"select * from orders WHERE \\$CONDITIONS\" --hive-table test.orders  --target-dir /user/hive/warehouse/test.db/orders/ --split-by order_id --delete-target-dir\n```\n即使是加上db name也是一样的错误。\n\n\n\n\n\n\n\n使用如下格式的代码导入也是一样的错误，但是如果是Oracle数据库，则可以执行成功但是没有数据。（暂时无法解释）\n\n```\nsqoop import -D \\\nmapreduce.job.queuename=hdfs \\\n --hive-import \\\n --hive-overwrite \\\n--connect \"jdbc:mysql://lion:3306/retail_db\" \\\n--username retaildba --password retaildba \\\n --fetch-size 5000 \\\n--query \"select * from orders WHERE \\$CONDITIONS\" \\\n --target-dir '/user/hive/warehouse/test.db/orders/' \\\n --delete-target-dir  \\\n --hive-table test.orders \\\n --hive-delims-replacement ' ' \\\n --null-string '\\\\N' --null-non-string '\\\\N'\\\n --split-by order_id \\\n```\n\n结论： --target-dir 在有--hive-import的情况下只是一个临时的存储文件夹，而且不要和表的路径设置一样。\n\n\n--hive-overwrite\n\n首先创建一个表test.order\n<img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite14.png?raw=true\"  style=\"width: 300px; display\"/>\n执行下面的sqoop\n\n```\nsqoop import --hive-import --hive-overwrite --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --query \"select * from orders WHERE \\$CONDITIONS\" --hive-table test.orders  --target-dir /user/hive/orders/ --split-by order_id --delete-target-dir\n```\n再次查询表结构\n<p align = \"center\">\n<img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite15.png?raw=true\" alt=\"drawing\" style=\"width: 600px; \"/>\n</p>\n结论：不要轻易使用 –hive-overwrite\n\n","source":"_posts/Test-hive-overwrite-and-delete-target-dir-in-sqoop.md","raw":"---\ntitle: Test hive-overwrite and delete-target-dir in sqoop\ndate: 2018-05-16 14:15:39\ntags: sqoop\n---\n\n# 测试Sqoop 中hive-overwrite和delete-target-dir是否冲突\n\n- 指定 –hive-import –hive-table –target-dir –delete-target-dir ,并且target-dir和hive表所在的路径一致。\n\n```\nsqoop import --hive-import --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --table orders --hive-table orders --target-dir /user/hive/warehouse/orders/ --delete-target-dir --hive-overwrite\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite1.png?raw=true)\n\n<!-- more -->\n- 指定 –hive-import –hive-table –target-dir –delete-target-dir ,并且target-dir和hive表所在的路径不一致。\n\n```\nsqoop import --hive-import --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --table orders --hive-table orders  --target-dir /user/hive/warehouse/order-1/ --delete-target-dir --hive-overwrite\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite2.png?raw=true)\n\n从结果来看，数据没有导入到—target-dir 指定的HDFS路径上，而是导入到表中去了。\n\n- 如果不指定target-dir和—delete-target-dir\n\n```\nsqoop import --hive-import --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --table orders --hive-table orders --hive-overwrite\n```\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite3.png?raw=true)\n\n- 调整下 –hive-overwrite的位置\n\n```\nsqoop import --hive-import --hive-overwrite --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --table orders --hive-table orders\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite4.png?raw=true)\n原因是默认在当前用户的hdfs home目录下创建一个和表名一致的文件夹，先手动删除这个文件夹然后再执行命令，在执行的Log中有如下的内容，因为不是使用hive进行数据导入的。\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite5.png?raw=true)\n\n检查结果\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite6.png?raw=true)\n\n再次执行，即使没有delete-target-dir 仍能执行成功，并且在执行的过程中，发现确实创建了 /user/training/orders文件夹，然后在任务执行完成后将文件移动到表的路径下，如下面第二张图所示。\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite7.png?raw=true)\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite8.png?raw=true)\n\n\n\n- 测试-query的情况\n\n```\nsqoop import --hive-import --hive-overwrite --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --query \"select * from orders WHERE \\$CONDITIONS\" --hive-table orders\n```\n没有指定—target-dir  报如下错误\n<img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite9.png?raw=true\" alt=\"drawing\" style=\"width: 500px;\"/>\n\n```\nsqoop import --hive-import --hive-overwrite --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --query \"select * from orders WHERE \\$CONDITIONS\" --hive-table orders –target-dir /user/training/orders/\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite10.png?raw=true)\n必须指定split-by\n\n```\nsqoop import --hive-import --hive-overwrite --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --query \"select * from orders WHERE \\$CONDITIONS\" --hive-table orders  --target-dir /user/training/orders-1/ --split-by order_id\n```\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite11.png?raw=true)\n\n\n在导入的过程中在target-dir生成临时文件，完成后将文件移动到表的路径下。所以此时不需要—delete-target-dir\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite12.png?raw=true)\n\n- 如果即使用--hive-table  --hive-overwrite 、--target-dir、 --delete-target-dir 并且 target-dir和表的路径一致。\n\n```\nsqoop import --hive-import --hive-overwrite --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --query \"select * from orders WHERE \\$CONDITIONS\" --hive-table orders  --target-dir /user/hive/warehouse/orders/ --split-by order_id --delete-target-dir\n```\n\n报如下错误\n\n![](https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite13.png?raw=true)\n\n```\nsqoop import --hive-import --hive-overwrite --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --query \"select * from orders WHERE \\$CONDITIONS\" --hive-table test.orders  --target-dir /user/hive/warehouse/test.db/orders/ --split-by order_id --delete-target-dir\n```\n即使是加上db name也是一样的错误。\n\n\n\n\n\n\n\n使用如下格式的代码导入也是一样的错误，但是如果是Oracle数据库，则可以执行成功但是没有数据。（暂时无法解释）\n\n```\nsqoop import -D \\\nmapreduce.job.queuename=hdfs \\\n --hive-import \\\n --hive-overwrite \\\n--connect \"jdbc:mysql://lion:3306/retail_db\" \\\n--username retaildba --password retaildba \\\n --fetch-size 5000 \\\n--query \"select * from orders WHERE \\$CONDITIONS\" \\\n --target-dir '/user/hive/warehouse/test.db/orders/' \\\n --delete-target-dir  \\\n --hive-table test.orders \\\n --hive-delims-replacement ' ' \\\n --null-string '\\\\N' --null-non-string '\\\\N'\\\n --split-by order_id \\\n```\n\n结论： --target-dir 在有--hive-import的情况下只是一个临时的存储文件夹，而且不要和表的路径设置一样。\n\n\n--hive-overwrite\n\n首先创建一个表test.order\n<img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite14.png?raw=true\"  style=\"width: 300px; display\"/>\n执行下面的sqoop\n\n```\nsqoop import --hive-import --hive-overwrite --connect \"jdbc:mysql://lion:3306/retail_db\" --username retaildba --password retaildba --query \"select * from orders WHERE \\$CONDITIONS\" --hive-table test.orders  --target-dir /user/hive/orders/ --split-by order_id --delete-target-dir\n```\n再次查询表结构\n<p align = \"center\">\n<img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite15.png?raw=true\" alt=\"drawing\" style=\"width: 600px; \"/>\n</p>\n结论：不要轻易使用 –hive-overwrite\n\n","slug":"Test-hive-overwrite-and-delete-target-dir-in-sqoop","published":1,"updated":"2018-09-14T01:20:53.876Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avo01001finam3t9u4mjz","content":"<h1 id=\"测试Sqoop-中hive-overwrite和delete-target-dir是否冲突\"><a href=\"#测试Sqoop-中hive-overwrite和delete-target-dir是否冲突\" class=\"headerlink\" title=\"测试Sqoop 中hive-overwrite和delete-target-dir是否冲突\"></a>测试Sqoop 中hive-overwrite和delete-target-dir是否冲突</h1><ul>\n<li>指定 –hive-import –hive-table –target-dir –delete-target-dir ,并且target-dir和hive表所在的路径一致。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --table orders --hive-table orders --target-dir /user/hive/warehouse/orders/ --delete-target-dir --hive-overwrite</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite1.png?raw=true\" alt=\"\"></p>\n<a id=\"more\"></a>\n<ul>\n<li>指定 –hive-import –hive-table –target-dir –delete-target-dir ,并且target-dir和hive表所在的路径不一致。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --table orders --hive-table orders  --target-dir /user/hive/warehouse/order-1/ --delete-target-dir --hive-overwrite</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite2.png?raw=true\" alt=\"\"></p>\n<p>从结果来看，数据没有导入到—target-dir 指定的HDFS路径上，而是导入到表中去了。</p>\n<ul>\n<li>如果不指定target-dir和—delete-target-dir</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --table orders --hive-table orders --hive-overwrite</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite3.png?raw=true\" alt=\"\"></p>\n<ul>\n<li>调整下 –hive-overwrite的位置</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --table orders --hive-table orders</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite4.png?raw=true\" alt=\"\"><br>原因是默认在当前用户的hdfs home目录下创建一个和表名一致的文件夹，先手动删除这个文件夹然后再执行命令，在执行的Log中有如下的内容，因为不是使用hive进行数据导入的。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite5.png?raw=true\" alt=\"\"></p>\n<p>检查结果<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite6.png?raw=true\" alt=\"\"></p>\n<p>再次执行，即使没有delete-target-dir 仍能执行成功，并且在执行的过程中，发现确实创建了 /user/training/orders文件夹，然后在任务执行完成后将文件移动到表的路径下，如下面第二张图所示。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite7.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite8.png?raw=true\" alt=\"\"></p>\n<ul>\n<li>测试-query的情况</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table orders</span><br></pre></td></tr></table></figure>\n<p>没有指定—target-dir  报如下错误<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite9.png?raw=true\" alt=\"drawing\" style=\"width: 500px;\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table orders –target-dir /user/training/orders/</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite10.png?raw=true\" alt=\"\"><br>必须指定split-by</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table orders  --target-dir /user/training/orders-1/ --split-by order_id</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite11.png?raw=true\" alt=\"\"></p>\n<p>在导入的过程中在target-dir生成临时文件，完成后将文件移动到表的路径下。所以此时不需要—delete-target-dir<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite12.png?raw=true\" alt=\"\"></p>\n<ul>\n<li>如果即使用–hive-table  –hive-overwrite 、–target-dir、 –delete-target-dir 并且 target-dir和表的路径一致。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table orders  --target-dir /user/hive/warehouse/orders/ --split-by order_id --delete-target-dir</span><br></pre></td></tr></table></figure>\n<p>报如下错误</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite13.png?raw=true\" alt=\"\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table test.orders  --target-dir /user/hive/warehouse/test.db/orders/ --split-by order_id --delete-target-dir</span><br></pre></td></tr></table></figure>\n<p>即使是加上db name也是一样的错误。</p>\n<p>使用如下格式的代码导入也是一样的错误，但是如果是Oracle数据库，则可以执行成功但是没有数据。（暂时无法解释）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import -D \\</span><br><span class=\"line\">mapreduce.job.queuename=hdfs \\</span><br><span class=\"line\"> --hive-import \\</span><br><span class=\"line\"> --hive-overwrite \\</span><br><span class=\"line\">--connect &quot;jdbc:mysql://lion:3306/retail_db&quot; \\</span><br><span class=\"line\">--username retaildba --password retaildba \\</span><br><span class=\"line\"> --fetch-size 5000 \\</span><br><span class=\"line\">--query &quot;select * from orders WHERE \\$CONDITIONS&quot; \\</span><br><span class=\"line\"> --target-dir &apos;/user/hive/warehouse/test.db/orders/&apos; \\</span><br><span class=\"line\"> --delete-target-dir  \\</span><br><span class=\"line\"> --hive-table test.orders \\</span><br><span class=\"line\"> --hive-delims-replacement &apos; &apos; \\</span><br><span class=\"line\"> --null-string &apos;\\\\N&apos; --null-non-string &apos;\\\\N&apos;\\</span><br><span class=\"line\"> --split-by order_id \\</span><br></pre></td></tr></table></figure>\n<p>结论： –target-dir 在有–hive-import的情况下只是一个临时的存储文件夹，而且不要和表的路径设置一样。</p>\n<p>–hive-overwrite</p>\n<p>首先创建一个表test.order<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite14.png?raw=true\" style=\"width: 300px; display\"><br>执行下面的sqoop</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table test.orders  --target-dir /user/hive/orders/ --split-by order_id --delete-target-dir</span><br></pre></td></tr></table></figure>\n<p>再次查询表结构</p>\n<p></p><p align=\"center\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite15.png?raw=true\" alt=\"drawing\" style=\"width: 600px; \"><br></p><br>结论：不要轻易使用 –hive-overwrite<p></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"测试Sqoop-中hive-overwrite和delete-target-dir是否冲突\"><a href=\"#测试Sqoop-中hive-overwrite和delete-target-dir是否冲突\" class=\"headerlink\" title=\"测试Sqoop 中hive-overwrite和delete-target-dir是否冲突\"></a>测试Sqoop 中hive-overwrite和delete-target-dir是否冲突</h1><ul>\n<li>指定 –hive-import –hive-table –target-dir –delete-target-dir ,并且target-dir和hive表所在的路径一致。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --table orders --hive-table orders --target-dir /user/hive/warehouse/orders/ --delete-target-dir --hive-overwrite</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite1.png?raw=true\" alt=\"\"></p>","more":"<ul>\n<li>指定 –hive-import –hive-table –target-dir –delete-target-dir ,并且target-dir和hive表所在的路径不一致。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --table orders --hive-table orders  --target-dir /user/hive/warehouse/order-1/ --delete-target-dir --hive-overwrite</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite2.png?raw=true\" alt=\"\"></p>\n<p>从结果来看，数据没有导入到—target-dir 指定的HDFS路径上，而是导入到表中去了。</p>\n<ul>\n<li>如果不指定target-dir和—delete-target-dir</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --table orders --hive-table orders --hive-overwrite</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite3.png?raw=true\" alt=\"\"></p>\n<ul>\n<li>调整下 –hive-overwrite的位置</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --table orders --hive-table orders</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite4.png?raw=true\" alt=\"\"><br>原因是默认在当前用户的hdfs home目录下创建一个和表名一致的文件夹，先手动删除这个文件夹然后再执行命令，在执行的Log中有如下的内容，因为不是使用hive进行数据导入的。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite5.png?raw=true\" alt=\"\"></p>\n<p>检查结果<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite6.png?raw=true\" alt=\"\"></p>\n<p>再次执行，即使没有delete-target-dir 仍能执行成功，并且在执行的过程中，发现确实创建了 /user/training/orders文件夹，然后在任务执行完成后将文件移动到表的路径下，如下面第二张图所示。<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite7.png?raw=true\" alt=\"\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite8.png?raw=true\" alt=\"\"></p>\n<ul>\n<li>测试-query的情况</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table orders</span><br></pre></td></tr></table></figure>\n<p>没有指定—target-dir  报如下错误<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite9.png?raw=true\" alt=\"drawing\" style=\"width: 500px;\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table orders –target-dir /user/training/orders/</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite10.png?raw=true\" alt=\"\"><br>必须指定split-by</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table orders  --target-dir /user/training/orders-1/ --split-by order_id</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite11.png?raw=true\" alt=\"\"></p>\n<p>在导入的过程中在target-dir生成临时文件，完成后将文件移动到表的路径下。所以此时不需要—delete-target-dir<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite12.png?raw=true\" alt=\"\"></p>\n<ul>\n<li>如果即使用–hive-table  –hive-overwrite 、–target-dir、 –delete-target-dir 并且 target-dir和表的路径一致。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table orders  --target-dir /user/hive/warehouse/orders/ --split-by order_id --delete-target-dir</span><br></pre></td></tr></table></figure>\n<p>报如下错误</p>\n<p><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite13.png?raw=true\" alt=\"\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table test.orders  --target-dir /user/hive/warehouse/test.db/orders/ --split-by order_id --delete-target-dir</span><br></pre></td></tr></table></figure>\n<p>即使是加上db name也是一样的错误。</p>\n<p>使用如下格式的代码导入也是一样的错误，但是如果是Oracle数据库，则可以执行成功但是没有数据。（暂时无法解释）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import -D \\</span><br><span class=\"line\">mapreduce.job.queuename=hdfs \\</span><br><span class=\"line\"> --hive-import \\</span><br><span class=\"line\"> --hive-overwrite \\</span><br><span class=\"line\">--connect &quot;jdbc:mysql://lion:3306/retail_db&quot; \\</span><br><span class=\"line\">--username retaildba --password retaildba \\</span><br><span class=\"line\"> --fetch-size 5000 \\</span><br><span class=\"line\">--query &quot;select * from orders WHERE \\$CONDITIONS&quot; \\</span><br><span class=\"line\"> --target-dir &apos;/user/hive/warehouse/test.db/orders/&apos; \\</span><br><span class=\"line\"> --delete-target-dir  \\</span><br><span class=\"line\"> --hive-table test.orders \\</span><br><span class=\"line\"> --hive-delims-replacement &apos; &apos; \\</span><br><span class=\"line\"> --null-string &apos;\\\\N&apos; --null-non-string &apos;\\\\N&apos;\\</span><br><span class=\"line\"> --split-by order_id \\</span><br></pre></td></tr></table></figure>\n<p>结论： –target-dir 在有–hive-import的情况下只是一个临时的存储文件夹，而且不要和表的路径设置一样。</p>\n<p>–hive-overwrite</p>\n<p>首先创建一个表test.order<br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite14.png?raw=true\" style=\"width: 300px; display\"><br>执行下面的sqoop</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table test.orders  --target-dir /user/hive/orders/ --split-by order_id --delete-target-dir</span><br></pre></td></tr></table></figure>\n<p>再次查询表结构</p>\n<p></p><p align=\"center\"><br><img src=\"https://github.com/FrommyMind/MarkDownPhotos/blob/master/screenshot/Sqoop/testhiveoverwrite/testhiveoverwrite15.png?raw=true\" alt=\"drawing\" style=\"width: 600px; \"><br></p><br>结论：不要轻易使用 –hive-overwrite<p></p>"},{"title":"Sqoop 导入数据","date":"2018-05-12T06:27:44.000Z","_content":"# Sqoop 导入数据\n\n## sqoop help\n\n```\nAvailable commands:\n  codegen            Generate code to interact with database records\n  create-hive-table  Import a table definition into Hive\n  eval               Evaluate a SQL statement and display the results\n  export             Export an HDFS directory to a database table\n  help               List available commands\n  import             Import a table from a database to HDFS\n  import-all-tables  Import tables from a database to HDFS\n  import-mainframe   Import datasets from a mainframe server to HDFS\n  job                Work with saved jobs\n  list-databases     List available databases on a server\n  list-tables        List available tables in a database\n  merge              Merge results of incremental imports\n  metastore          Run a standalone Sqoop metastore\n  version            Display version information\n\n```\n<!-- more -->\n### sqoop codegen\n\ncodegen 命令用来生成Java代码\n例如：\n\n```\nsqoop codegen --connect jdbc:mysql://quickstart:3306/retail_db  \\\n--username retail_dba --password cloudera -e 'select * from orders  \\\nwhere $CONDITIONS limit 10'\n```\n\n\n### sqoop create-hive-table\n\n根据源数据库表结构在hive中创建表,可以创建别名的表，但是只能创建textfile格式的表\n\n```\nsqoop create-hive-table --connect jdbc:mysql://quickstart:3306/retail_db \\\n --username retail_dba --password cloudera --table orders --hive-table orders_2\n```\n在hive中查看创建的表的结构\n\n```\nCREATE TABLE `orders_2`(                           \n   `order_id` int,                                  \n   `order_date` string,                             \n   `order_customer_id` int,                         \n   `order_status` string)                           \n COMMENT 'Imported by sqoop on 2017/10/21 02:27:38' \n ROW FORMAT SERDE                                   \n   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'  \n WITH SERDEPROPERTIES (                             \n   'field.delim'='\\u0001',                          \n   'line.delim'='\\n',                               \n   'serialization.format'='\\u0001')                 \n STORED AS INPUTFORMAT                              \n   'org.apache.hadoop.mapred.TextInputFormat'       \n OUTPUTFORMAT                                       \n   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' \n LOCATION                                           \n   'hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders_2' \n TBLPROPERTIES (                                    \n   'transient_lastDdlTime'='1508578063')            \n\n```\n### sqoop eval\n执行一段sql\n\n```\nsqoop eval --connect jdbc:mysql://quickstart:3306/retail_db  \\\n--username retail_dba --password cloudera --query 'select count(1) from orders'\n```\n结果\n\n```\n------------------------\n count(1)              \n------------------------\n 68883                 \n------------------------\n```\n\n### sqoop export\n\t将hive文件夹中的数据导入到数据库表中\nParquet 表\n\n```\nsqoop export --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba \\\n --password cloudera  --hcatalog-table \"products\" --hcatalog-database \"default\"  \\\n --table products_1 -m 1\n```\nText表\n\n```\nsqoop export --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba  \\\n--password cloudera  --export-dir /user/hive/warehouse/products_2 \\\n --input-fields-terminated-by '\\001' --table products_2 -m 1\n```\n\t\n### Sqoop  import\n\n导入一张表到HDFS\n\n```\nsqoop import --connect jdbc:mysql://quickstart:3306/retail_db  \\\n--username=retail_dba --password=cloudera   \\\n--target-dir /usr/hive/warehouse/orders --table orders --hive-import\n```\nHive 表结构\n\n```\n CREATE TABLE `orders`(                             \n   `order_id` int,                                  \n   `order_date` string,                             \n   `order_customer_id` int,                         \n   `order_status` string)                           \n COMMENT 'Imported by sqoop on 2017/10/26 05:50:14' \n ROW FORMAT SERDE                                   \n   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'  \n WITH SERDEPROPERTIES (                             \n   'field.delim'='\\u0001',                          \n   'line.delim'='\\n',                               \n   'serialization.format'='\\u0001')                 \n STORED AS INPUTFORMAT                              \n   'org.apache.hadoop.mapred.TextInputFormat'       \n OUTPUTFORMAT                                       \n   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' \n LOCATION                                           \n   'hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders' \n TBLPROPERTIES (                                    \n   'COLUMN_STATS_ACCURATE'='true',                  \n   'numFiles'='1',                                  \n   'numRows'='0',                                   \n   'rawDataSize'='0',                               \n   'totalSize'='2999944',                           \n   'transient_lastDdlTime'='1509022219')            \n```\nHDFS 文件\n\n```\n[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders\nFound 2 items\n-rw-r--r--   1 cloudera supergroup          0 2017-10-26 06:20 /user/hive/warehouse/orders/_SUCCESS\n-rwxr-xr-x   1 cloudera supergroup    2999944 2017-10-26 06:20 /user/hive/warehouse/orders/part-m-00000\n```\n\n导入parquet格式表\n\n```\nsqoop import --connect jdbc:mysql://quickstart:3306/retail_db  \\\n--username=retail_dba --password=cloudera   \\\n--target-dir /usr/hive/warehouse/orders --table orders --hive-import --as-parquetfile\n```\n\nHDFS文件\n```\n[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders/\nFound 5 items\ndrwxr-xr-x   - cloudera supergroup          0 2017-10-26 05:53 /user/hive/warehouse/orders/.metadata\ndrwxr-xr-x   - cloudera supergroup          0 2017-10-26 05:54 /user/hive/warehouse/orders/.signals\n-rw-r--r--   1 cloudera supergroup     488257 2017-10-26 05:54 /user/hive/warehouse/orders/5e708d3e-a273-48de-8a3d-37efcdaf8d62.parquet\n-rw-r--r--   1 cloudera supergroup          0 2017-10-26 05:50 /user/hive/warehouse/orders/_SUCCESS\n-rwxr-xr-x   1 cloudera supergroup    2999944 2017-10-26 05:50 /user/hive/warehouse/orders/part-m-00000\n```\n\n加上Snappy 压缩\n\n```\n[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders/\nFound 3 items\ndrwxr-xr-x   - cloudera supergroup          0 2017-10-26 06:04 /user/hive/warehouse/orders/.metadata\ndrwxr-xr-x   - cloudera supergroup          0 2017-10-26 06:05 /user/hive/warehouse/orders/.signals\n-rw-r--r--   1 cloudera supergroup     488257 2017-10-26 06:05 /user/hive/warehouse/orders/bfa9265e-2711-494e-a924-bd8c6591954e.parquet\n```\n\n### Sqoop import-all-tables\n\n导入数据库所有表到hdfs：创建表并将数据导入HDFS\n\n```\nsqoop import-all-tables --connect jdbc:mysql://quickstart:3306/retail_db  \\\n--username=retail_dba --password=cloudera --compress-codec=snappy \\ \n--as-parquetfile --warehouse-dir=/user/hive/warehouse --hive-import\n```\n\n检查其中一张表到hdfs文件\n\n```\n[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders\nFound 3 items\ndrwxr-xr-x   - cloudera supergroup          0 2017-10-20 18:57 /user/hive/warehouse/orders/.metadata\ndrwxr-xr-x   - cloudera supergroup          0 2017-10-20 18:58 /user/hive/warehouse/orders/.signals\n-rw-r--r--   1 cloudera supergroup     488257 2017-10-20 18:58 /user/hive/warehouse/orders/a851e80b-1238-465d-93b7-a8ac11c53697.parquet\n\n```\n\n检查hive表结构\n\n```\nshow create table orders\n```\n\n```\n CREATE TABLE `orders`(                             \n   `order_id` int,                                  \n   `order_date` bigint,                             \n   `order_customer_id` int,                         \n   `order_status` string)                           \n ROW FORMAT SERDE                                   \n   'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'  \n STORED AS INPUTFORMAT                              \n   'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'  \n OUTPUTFORMAT                                       \n   'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' \n LOCATION                                           \n   'hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders' \n TBLPROPERTIES (                                    \n   'COLUMN_STATS_ACCURATE'='false',                 \n   'avro.schema.url'='hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders/.metadata/schemas/1.avsc',  \n   'kite.compression.type'='snappy',                \n   'numFiles'='0',                                  \n   'numRows'='-1',                                  \n   'rawDataSize'='-1',                              \n   'totalSize'='0',                                 \n   'transient_lastDdlTime'='1508551065')   \n           \n```\n\n\n### Sqoop list-databases\n\n查看数据库列表\n\n```\nsqoop list-databases --connect jdbc:mysql://quickstart:3306  \\\n--username retail_dba --password cloudera\n```\n结果\n\n```\ninformation_schema\nretail_db\n```\n\n\n### Sqoop list-tables\n\n查看数据库中表到列表\n\n```\nsqoop list-tables --connect jdbc:mysql://quickstart:3306/retail_db  \\\n--username retail_dba --password cloudera\n\n```\n结果：\n\n```\ncategories\ncustomers\ndepartments\norder_items\norders\nproducts\n```\n","source":"_posts/Sqoop-导入数据库到hive.md","raw":"---\ntitle: Sqoop 导入数据\ndate: 2018-05-12 14:27:44\ntags: \n    - sqoop\n---\n# Sqoop 导入数据\n\n## sqoop help\n\n```\nAvailable commands:\n  codegen            Generate code to interact with database records\n  create-hive-table  Import a table definition into Hive\n  eval               Evaluate a SQL statement and display the results\n  export             Export an HDFS directory to a database table\n  help               List available commands\n  import             Import a table from a database to HDFS\n  import-all-tables  Import tables from a database to HDFS\n  import-mainframe   Import datasets from a mainframe server to HDFS\n  job                Work with saved jobs\n  list-databases     List available databases on a server\n  list-tables        List available tables in a database\n  merge              Merge results of incremental imports\n  metastore          Run a standalone Sqoop metastore\n  version            Display version information\n\n```\n<!-- more -->\n### sqoop codegen\n\ncodegen 命令用来生成Java代码\n例如：\n\n```\nsqoop codegen --connect jdbc:mysql://quickstart:3306/retail_db  \\\n--username retail_dba --password cloudera -e 'select * from orders  \\\nwhere $CONDITIONS limit 10'\n```\n\n\n### sqoop create-hive-table\n\n根据源数据库表结构在hive中创建表,可以创建别名的表，但是只能创建textfile格式的表\n\n```\nsqoop create-hive-table --connect jdbc:mysql://quickstart:3306/retail_db \\\n --username retail_dba --password cloudera --table orders --hive-table orders_2\n```\n在hive中查看创建的表的结构\n\n```\nCREATE TABLE `orders_2`(                           \n   `order_id` int,                                  \n   `order_date` string,                             \n   `order_customer_id` int,                         \n   `order_status` string)                           \n COMMENT 'Imported by sqoop on 2017/10/21 02:27:38' \n ROW FORMAT SERDE                                   \n   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'  \n WITH SERDEPROPERTIES (                             \n   'field.delim'='\\u0001',                          \n   'line.delim'='\\n',                               \n   'serialization.format'='\\u0001')                 \n STORED AS INPUTFORMAT                              \n   'org.apache.hadoop.mapred.TextInputFormat'       \n OUTPUTFORMAT                                       \n   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' \n LOCATION                                           \n   'hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders_2' \n TBLPROPERTIES (                                    \n   'transient_lastDdlTime'='1508578063')            \n\n```\n### sqoop eval\n执行一段sql\n\n```\nsqoop eval --connect jdbc:mysql://quickstart:3306/retail_db  \\\n--username retail_dba --password cloudera --query 'select count(1) from orders'\n```\n结果\n\n```\n------------------------\n count(1)              \n------------------------\n 68883                 \n------------------------\n```\n\n### sqoop export\n\t将hive文件夹中的数据导入到数据库表中\nParquet 表\n\n```\nsqoop export --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba \\\n --password cloudera  --hcatalog-table \"products\" --hcatalog-database \"default\"  \\\n --table products_1 -m 1\n```\nText表\n\n```\nsqoop export --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba  \\\n--password cloudera  --export-dir /user/hive/warehouse/products_2 \\\n --input-fields-terminated-by '\\001' --table products_2 -m 1\n```\n\t\n### Sqoop  import\n\n导入一张表到HDFS\n\n```\nsqoop import --connect jdbc:mysql://quickstart:3306/retail_db  \\\n--username=retail_dba --password=cloudera   \\\n--target-dir /usr/hive/warehouse/orders --table orders --hive-import\n```\nHive 表结构\n\n```\n CREATE TABLE `orders`(                             \n   `order_id` int,                                  \n   `order_date` string,                             \n   `order_customer_id` int,                         \n   `order_status` string)                           \n COMMENT 'Imported by sqoop on 2017/10/26 05:50:14' \n ROW FORMAT SERDE                                   \n   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'  \n WITH SERDEPROPERTIES (                             \n   'field.delim'='\\u0001',                          \n   'line.delim'='\\n',                               \n   'serialization.format'='\\u0001')                 \n STORED AS INPUTFORMAT                              \n   'org.apache.hadoop.mapred.TextInputFormat'       \n OUTPUTFORMAT                                       \n   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' \n LOCATION                                           \n   'hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders' \n TBLPROPERTIES (                                    \n   'COLUMN_STATS_ACCURATE'='true',                  \n   'numFiles'='1',                                  \n   'numRows'='0',                                   \n   'rawDataSize'='0',                               \n   'totalSize'='2999944',                           \n   'transient_lastDdlTime'='1509022219')            \n```\nHDFS 文件\n\n```\n[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders\nFound 2 items\n-rw-r--r--   1 cloudera supergroup          0 2017-10-26 06:20 /user/hive/warehouse/orders/_SUCCESS\n-rwxr-xr-x   1 cloudera supergroup    2999944 2017-10-26 06:20 /user/hive/warehouse/orders/part-m-00000\n```\n\n导入parquet格式表\n\n```\nsqoop import --connect jdbc:mysql://quickstart:3306/retail_db  \\\n--username=retail_dba --password=cloudera   \\\n--target-dir /usr/hive/warehouse/orders --table orders --hive-import --as-parquetfile\n```\n\nHDFS文件\n```\n[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders/\nFound 5 items\ndrwxr-xr-x   - cloudera supergroup          0 2017-10-26 05:53 /user/hive/warehouse/orders/.metadata\ndrwxr-xr-x   - cloudera supergroup          0 2017-10-26 05:54 /user/hive/warehouse/orders/.signals\n-rw-r--r--   1 cloudera supergroup     488257 2017-10-26 05:54 /user/hive/warehouse/orders/5e708d3e-a273-48de-8a3d-37efcdaf8d62.parquet\n-rw-r--r--   1 cloudera supergroup          0 2017-10-26 05:50 /user/hive/warehouse/orders/_SUCCESS\n-rwxr-xr-x   1 cloudera supergroup    2999944 2017-10-26 05:50 /user/hive/warehouse/orders/part-m-00000\n```\n\n加上Snappy 压缩\n\n```\n[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders/\nFound 3 items\ndrwxr-xr-x   - cloudera supergroup          0 2017-10-26 06:04 /user/hive/warehouse/orders/.metadata\ndrwxr-xr-x   - cloudera supergroup          0 2017-10-26 06:05 /user/hive/warehouse/orders/.signals\n-rw-r--r--   1 cloudera supergroup     488257 2017-10-26 06:05 /user/hive/warehouse/orders/bfa9265e-2711-494e-a924-bd8c6591954e.parquet\n```\n\n### Sqoop import-all-tables\n\n导入数据库所有表到hdfs：创建表并将数据导入HDFS\n\n```\nsqoop import-all-tables --connect jdbc:mysql://quickstart:3306/retail_db  \\\n--username=retail_dba --password=cloudera --compress-codec=snappy \\ \n--as-parquetfile --warehouse-dir=/user/hive/warehouse --hive-import\n```\n\n检查其中一张表到hdfs文件\n\n```\n[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders\nFound 3 items\ndrwxr-xr-x   - cloudera supergroup          0 2017-10-20 18:57 /user/hive/warehouse/orders/.metadata\ndrwxr-xr-x   - cloudera supergroup          0 2017-10-20 18:58 /user/hive/warehouse/orders/.signals\n-rw-r--r--   1 cloudera supergroup     488257 2017-10-20 18:58 /user/hive/warehouse/orders/a851e80b-1238-465d-93b7-a8ac11c53697.parquet\n\n```\n\n检查hive表结构\n\n```\nshow create table orders\n```\n\n```\n CREATE TABLE `orders`(                             \n   `order_id` int,                                  \n   `order_date` bigint,                             \n   `order_customer_id` int,                         \n   `order_status` string)                           \n ROW FORMAT SERDE                                   \n   'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'  \n STORED AS INPUTFORMAT                              \n   'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'  \n OUTPUTFORMAT                                       \n   'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' \n LOCATION                                           \n   'hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders' \n TBLPROPERTIES (                                    \n   'COLUMN_STATS_ACCURATE'='false',                 \n   'avro.schema.url'='hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders/.metadata/schemas/1.avsc',  \n   'kite.compression.type'='snappy',                \n   'numFiles'='0',                                  \n   'numRows'='-1',                                  \n   'rawDataSize'='-1',                              \n   'totalSize'='0',                                 \n   'transient_lastDdlTime'='1508551065')   \n           \n```\n\n\n### Sqoop list-databases\n\n查看数据库列表\n\n```\nsqoop list-databases --connect jdbc:mysql://quickstart:3306  \\\n--username retail_dba --password cloudera\n```\n结果\n\n```\ninformation_schema\nretail_db\n```\n\n\n### Sqoop list-tables\n\n查看数据库中表到列表\n\n```\nsqoop list-tables --connect jdbc:mysql://quickstart:3306/retail_db  \\\n--username retail_dba --password cloudera\n\n```\n结果：\n\n```\ncategories\ncustomers\ndepartments\norder_items\norders\nproducts\n```\n","slug":"Sqoop-导入数据库到hive","published":1,"updated":"2018-09-14T01:20:41.645Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avo02001hinam1u0jak39","content":"<h1 id=\"Sqoop-导入数据\"><a href=\"#Sqoop-导入数据\" class=\"headerlink\" title=\"Sqoop 导入数据\"></a>Sqoop 导入数据</h1><h2 id=\"sqoop-help\"><a href=\"#sqoop-help\" class=\"headerlink\" title=\"sqoop help\"></a>sqoop help</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Available commands:</span><br><span class=\"line\">  codegen            Generate code to interact with database records</span><br><span class=\"line\">  create-hive-table  Import a table definition into Hive</span><br><span class=\"line\">  eval               Evaluate a SQL statement and display the results</span><br><span class=\"line\">  export             Export an HDFS directory to a database table</span><br><span class=\"line\">  help               List available commands</span><br><span class=\"line\">  import             Import a table from a database to HDFS</span><br><span class=\"line\">  import-all-tables  Import tables from a database to HDFS</span><br><span class=\"line\">  import-mainframe   Import datasets from a mainframe server to HDFS</span><br><span class=\"line\">  job                Work with saved jobs</span><br><span class=\"line\">  list-databases     List available databases on a server</span><br><span class=\"line\">  list-tables        List available tables in a database</span><br><span class=\"line\">  merge              Merge results of incremental imports</span><br><span class=\"line\">  metastore          Run a standalone Sqoop metastore</span><br><span class=\"line\">  version            Display version information</span><br></pre></td></tr></table></figure>\n<a id=\"more\"></a>\n<h3 id=\"sqoop-codegen\"><a href=\"#sqoop-codegen\" class=\"headerlink\" title=\"sqoop codegen\"></a>sqoop codegen</h3><p>codegen 命令用来生成Java代码<br>例如：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop codegen --connect jdbc:mysql://quickstart:3306/retail_db  \\</span><br><span class=\"line\">--username retail_dba --password cloudera -e &apos;select * from orders  \\</span><br><span class=\"line\">where $CONDITIONS limit 10&apos;</span><br></pre></td></tr></table></figure>\n<h3 id=\"sqoop-create-hive-table\"><a href=\"#sqoop-create-hive-table\" class=\"headerlink\" title=\"sqoop create-hive-table\"></a>sqoop create-hive-table</h3><p>根据源数据库表结构在hive中创建表,可以创建别名的表，但是只能创建textfile格式的表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop create-hive-table --connect jdbc:mysql://quickstart:3306/retail_db \\</span><br><span class=\"line\"> --username retail_dba --password cloudera --table orders --hive-table orders_2</span><br></pre></td></tr></table></figure>\n<p>在hive中查看创建的表的结构</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE `orders_2`(                           </span><br><span class=\"line\">   `order_id` int,                                  </span><br><span class=\"line\">   `order_date` string,                             </span><br><span class=\"line\">   `order_customer_id` int,                         </span><br><span class=\"line\">   `order_status` string)                           </span><br><span class=\"line\"> COMMENT &apos;Imported by sqoop on 2017/10/21 02:27:38&apos; </span><br><span class=\"line\"> ROW FORMAT SERDE                                   </span><br><span class=\"line\">   &apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&apos;  </span><br><span class=\"line\"> WITH SERDEPROPERTIES (                             </span><br><span class=\"line\">   &apos;field.delim&apos;=&apos;\\u0001&apos;,                          </span><br><span class=\"line\">   &apos;line.delim&apos;=&apos;\\n&apos;,                               </span><br><span class=\"line\">   &apos;serialization.format&apos;=&apos;\\u0001&apos;)                 </span><br><span class=\"line\"> STORED AS INPUTFORMAT                              </span><br><span class=\"line\">   &apos;org.apache.hadoop.mapred.TextInputFormat&apos;       </span><br><span class=\"line\"> OUTPUTFORMAT                                       </span><br><span class=\"line\">   &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos; </span><br><span class=\"line\"> LOCATION                                           </span><br><span class=\"line\">   &apos;hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders_2&apos; </span><br><span class=\"line\"> TBLPROPERTIES (                                    </span><br><span class=\"line\">   &apos;transient_lastDdlTime&apos;=&apos;1508578063&apos;)</span><br></pre></td></tr></table></figure>\n<h3 id=\"sqoop-eval\"><a href=\"#sqoop-eval\" class=\"headerlink\" title=\"sqoop eval\"></a>sqoop eval</h3><p>执行一段sql</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop eval --connect jdbc:mysql://quickstart:3306/retail_db  \\</span><br><span class=\"line\">--username retail_dba --password cloudera --query &apos;select count(1) from orders&apos;</span><br></pre></td></tr></table></figure>\n<p>结果</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">------------------------</span><br><span class=\"line\"> count(1)              </span><br><span class=\"line\">------------------------</span><br><span class=\"line\"> 68883                 </span><br><span class=\"line\">------------------------</span><br></pre></td></tr></table></figure>\n<h3 id=\"sqoop-export\"><a href=\"#sqoop-export\" class=\"headerlink\" title=\"sqoop export\"></a>sqoop export</h3><pre><code>将hive文件夹中的数据导入到数据库表中\n</code></pre><p>Parquet 表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop export --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba \\</span><br><span class=\"line\"> --password cloudera  --hcatalog-table &quot;products&quot; --hcatalog-database &quot;default&quot;  \\</span><br><span class=\"line\"> --table products_1 -m 1</span><br></pre></td></tr></table></figure>\n<p>Text表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop export --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba  \\</span><br><span class=\"line\">--password cloudera  --export-dir /user/hive/warehouse/products_2 \\</span><br><span class=\"line\"> --input-fields-terminated-by &apos;\\001&apos; --table products_2 -m 1</span><br></pre></td></tr></table></figure>\n<h3 id=\"Sqoop-import\"><a href=\"#Sqoop-import\" class=\"headerlink\" title=\"Sqoop  import\"></a>Sqoop  import</h3><p>导入一张表到HDFS</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --connect jdbc:mysql://quickstart:3306/retail_db  \\</span><br><span class=\"line\">--username=retail_dba --password=cloudera   \\</span><br><span class=\"line\">--target-dir /usr/hive/warehouse/orders --table orders --hive-import</span><br></pre></td></tr></table></figure>\n<p>Hive 表结构</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE `orders`(                             </span><br><span class=\"line\">  `order_id` int,                                  </span><br><span class=\"line\">  `order_date` string,                             </span><br><span class=\"line\">  `order_customer_id` int,                         </span><br><span class=\"line\">  `order_status` string)                           </span><br><span class=\"line\">COMMENT &apos;Imported by sqoop on 2017/10/26 05:50:14&apos; </span><br><span class=\"line\">ROW FORMAT SERDE                                   </span><br><span class=\"line\">  &apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&apos;  </span><br><span class=\"line\">WITH SERDEPROPERTIES (                             </span><br><span class=\"line\">  &apos;field.delim&apos;=&apos;\\u0001&apos;,                          </span><br><span class=\"line\">  &apos;line.delim&apos;=&apos;\\n&apos;,                               </span><br><span class=\"line\">  &apos;serialization.format&apos;=&apos;\\u0001&apos;)                 </span><br><span class=\"line\">STORED AS INPUTFORMAT                              </span><br><span class=\"line\">  &apos;org.apache.hadoop.mapred.TextInputFormat&apos;       </span><br><span class=\"line\">OUTPUTFORMAT                                       </span><br><span class=\"line\">  &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos; </span><br><span class=\"line\">LOCATION                                           </span><br><span class=\"line\">  &apos;hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders&apos; </span><br><span class=\"line\">TBLPROPERTIES (                                    </span><br><span class=\"line\">  &apos;COLUMN_STATS_ACCURATE&apos;=&apos;true&apos;,                  </span><br><span class=\"line\">  &apos;numFiles&apos;=&apos;1&apos;,                                  </span><br><span class=\"line\">  &apos;numRows&apos;=&apos;0&apos;,                                   </span><br><span class=\"line\">  &apos;rawDataSize&apos;=&apos;0&apos;,                               </span><br><span class=\"line\">  &apos;totalSize&apos;=&apos;2999944&apos;,                           </span><br><span class=\"line\">  &apos;transient_lastDdlTime&apos;=&apos;1509022219&apos;)</span><br></pre></td></tr></table></figure>\n<p>HDFS 文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders</span><br><span class=\"line\">Found 2 items</span><br><span class=\"line\">-rw-r--r--   1 cloudera supergroup          0 2017-10-26 06:20 /user/hive/warehouse/orders/_SUCCESS</span><br><span class=\"line\">-rwxr-xr-x   1 cloudera supergroup    2999944 2017-10-26 06:20 /user/hive/warehouse/orders/part-m-00000</span><br></pre></td></tr></table></figure>\n<p>导入parquet格式表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --connect jdbc:mysql://quickstart:3306/retail_db  \\</span><br><span class=\"line\">--username=retail_dba --password=cloudera   \\</span><br><span class=\"line\">--target-dir /usr/hive/warehouse/orders --table orders --hive-import --as-parquetfile</span><br></pre></td></tr></table></figure>\n<p>HDFS文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders/</span><br><span class=\"line\">Found 5 items</span><br><span class=\"line\">drwxr-xr-x   - cloudera supergroup          0 2017-10-26 05:53 /user/hive/warehouse/orders/.metadata</span><br><span class=\"line\">drwxr-xr-x   - cloudera supergroup          0 2017-10-26 05:54 /user/hive/warehouse/orders/.signals</span><br><span class=\"line\">-rw-r--r--   1 cloudera supergroup     488257 2017-10-26 05:54 /user/hive/warehouse/orders/5e708d3e-a273-48de-8a3d-37efcdaf8d62.parquet</span><br><span class=\"line\">-rw-r--r--   1 cloudera supergroup          0 2017-10-26 05:50 /user/hive/warehouse/orders/_SUCCESS</span><br><span class=\"line\">-rwxr-xr-x   1 cloudera supergroup    2999944 2017-10-26 05:50 /user/hive/warehouse/orders/part-m-00000</span><br></pre></td></tr></table></figure></p>\n<p>加上Snappy 压缩</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders/</span><br><span class=\"line\">Found 3 items</span><br><span class=\"line\">drwxr-xr-x   - cloudera supergroup          0 2017-10-26 06:04 /user/hive/warehouse/orders/.metadata</span><br><span class=\"line\">drwxr-xr-x   - cloudera supergroup          0 2017-10-26 06:05 /user/hive/warehouse/orders/.signals</span><br><span class=\"line\">-rw-r--r--   1 cloudera supergroup     488257 2017-10-26 06:05 /user/hive/warehouse/orders/bfa9265e-2711-494e-a924-bd8c6591954e.parquet</span><br></pre></td></tr></table></figure>\n<h3 id=\"Sqoop-import-all-tables\"><a href=\"#Sqoop-import-all-tables\" class=\"headerlink\" title=\"Sqoop import-all-tables\"></a>Sqoop import-all-tables</h3><p>导入数据库所有表到hdfs：创建表并将数据导入HDFS</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import-all-tables --connect jdbc:mysql://quickstart:3306/retail_db  \\</span><br><span class=\"line\">--username=retail_dba --password=cloudera --compress-codec=snappy \\ </span><br><span class=\"line\">--as-parquetfile --warehouse-dir=/user/hive/warehouse --hive-import</span><br></pre></td></tr></table></figure>\n<p>检查其中一张表到hdfs文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders</span><br><span class=\"line\">Found 3 items</span><br><span class=\"line\">drwxr-xr-x   - cloudera supergroup          0 2017-10-20 18:57 /user/hive/warehouse/orders/.metadata</span><br><span class=\"line\">drwxr-xr-x   - cloudera supergroup          0 2017-10-20 18:58 /user/hive/warehouse/orders/.signals</span><br><span class=\"line\">-rw-r--r--   1 cloudera supergroup     488257 2017-10-20 18:58 /user/hive/warehouse/orders/a851e80b-1238-465d-93b7-a8ac11c53697.parquet</span><br></pre></td></tr></table></figure>\n<p>检查hive表结构</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show create table orders</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE `orders`(                             </span><br><span class=\"line\">  `order_id` int,                                  </span><br><span class=\"line\">  `order_date` bigint,                             </span><br><span class=\"line\">  `order_customer_id` int,                         </span><br><span class=\"line\">  `order_status` string)                           </span><br><span class=\"line\">ROW FORMAT SERDE                                   </span><br><span class=\"line\">  &apos;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe&apos;  </span><br><span class=\"line\">STORED AS INPUTFORMAT                              </span><br><span class=\"line\">  &apos;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat&apos;  </span><br><span class=\"line\">OUTPUTFORMAT                                       </span><br><span class=\"line\">  &apos;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat&apos; </span><br><span class=\"line\">LOCATION                                           </span><br><span class=\"line\">  &apos;hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders&apos; </span><br><span class=\"line\">TBLPROPERTIES (                                    </span><br><span class=\"line\">  &apos;COLUMN_STATS_ACCURATE&apos;=&apos;false&apos;,                 </span><br><span class=\"line\">  &apos;avro.schema.url&apos;=&apos;hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders/.metadata/schemas/1.avsc&apos;,  </span><br><span class=\"line\">  &apos;kite.compression.type&apos;=&apos;snappy&apos;,                </span><br><span class=\"line\">  &apos;numFiles&apos;=&apos;0&apos;,                                  </span><br><span class=\"line\">  &apos;numRows&apos;=&apos;-1&apos;,                                  </span><br><span class=\"line\">  &apos;rawDataSize&apos;=&apos;-1&apos;,                              </span><br><span class=\"line\">  &apos;totalSize&apos;=&apos;0&apos;,                                 </span><br><span class=\"line\">  &apos;transient_lastDdlTime&apos;=&apos;1508551065&apos;)</span><br></pre></td></tr></table></figure>\n<h3 id=\"Sqoop-list-databases\"><a href=\"#Sqoop-list-databases\" class=\"headerlink\" title=\"Sqoop list-databases\"></a>Sqoop list-databases</h3><p>查看数据库列表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop list-databases --connect jdbc:mysql://quickstart:3306  \\</span><br><span class=\"line\">--username retail_dba --password cloudera</span><br></pre></td></tr></table></figure>\n<p>结果</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">information_schema</span><br><span class=\"line\">retail_db</span><br></pre></td></tr></table></figure>\n<h3 id=\"Sqoop-list-tables\"><a href=\"#Sqoop-list-tables\" class=\"headerlink\" title=\"Sqoop list-tables\"></a>Sqoop list-tables</h3><p>查看数据库中表到列表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop list-tables --connect jdbc:mysql://quickstart:3306/retail_db  \\</span><br><span class=\"line\">--username retail_dba --password cloudera</span><br></pre></td></tr></table></figure>\n<p>结果：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">categories</span><br><span class=\"line\">customers</span><br><span class=\"line\">departments</span><br><span class=\"line\">order_items</span><br><span class=\"line\">orders</span><br><span class=\"line\">products</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<h1 id=\"Sqoop-导入数据\"><a href=\"#Sqoop-导入数据\" class=\"headerlink\" title=\"Sqoop 导入数据\"></a>Sqoop 导入数据</h1><h2 id=\"sqoop-help\"><a href=\"#sqoop-help\" class=\"headerlink\" title=\"sqoop help\"></a>sqoop help</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Available commands:</span><br><span class=\"line\">  codegen            Generate code to interact with database records</span><br><span class=\"line\">  create-hive-table  Import a table definition into Hive</span><br><span class=\"line\">  eval               Evaluate a SQL statement and display the results</span><br><span class=\"line\">  export             Export an HDFS directory to a database table</span><br><span class=\"line\">  help               List available commands</span><br><span class=\"line\">  import             Import a table from a database to HDFS</span><br><span class=\"line\">  import-all-tables  Import tables from a database to HDFS</span><br><span class=\"line\">  import-mainframe   Import datasets from a mainframe server to HDFS</span><br><span class=\"line\">  job                Work with saved jobs</span><br><span class=\"line\">  list-databases     List available databases on a server</span><br><span class=\"line\">  list-tables        List available tables in a database</span><br><span class=\"line\">  merge              Merge results of incremental imports</span><br><span class=\"line\">  metastore          Run a standalone Sqoop metastore</span><br><span class=\"line\">  version            Display version information</span><br></pre></td></tr></table></figure>","more":"<h3 id=\"sqoop-codegen\"><a href=\"#sqoop-codegen\" class=\"headerlink\" title=\"sqoop codegen\"></a>sqoop codegen</h3><p>codegen 命令用来生成Java代码<br>例如：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop codegen --connect jdbc:mysql://quickstart:3306/retail_db  \\</span><br><span class=\"line\">--username retail_dba --password cloudera -e &apos;select * from orders  \\</span><br><span class=\"line\">where $CONDITIONS limit 10&apos;</span><br></pre></td></tr></table></figure>\n<h3 id=\"sqoop-create-hive-table\"><a href=\"#sqoop-create-hive-table\" class=\"headerlink\" title=\"sqoop create-hive-table\"></a>sqoop create-hive-table</h3><p>根据源数据库表结构在hive中创建表,可以创建别名的表，但是只能创建textfile格式的表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop create-hive-table --connect jdbc:mysql://quickstart:3306/retail_db \\</span><br><span class=\"line\"> --username retail_dba --password cloudera --table orders --hive-table orders_2</span><br></pre></td></tr></table></figure>\n<p>在hive中查看创建的表的结构</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE `orders_2`(                           </span><br><span class=\"line\">   `order_id` int,                                  </span><br><span class=\"line\">   `order_date` string,                             </span><br><span class=\"line\">   `order_customer_id` int,                         </span><br><span class=\"line\">   `order_status` string)                           </span><br><span class=\"line\"> COMMENT &apos;Imported by sqoop on 2017/10/21 02:27:38&apos; </span><br><span class=\"line\"> ROW FORMAT SERDE                                   </span><br><span class=\"line\">   &apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&apos;  </span><br><span class=\"line\"> WITH SERDEPROPERTIES (                             </span><br><span class=\"line\">   &apos;field.delim&apos;=&apos;\\u0001&apos;,                          </span><br><span class=\"line\">   &apos;line.delim&apos;=&apos;\\n&apos;,                               </span><br><span class=\"line\">   &apos;serialization.format&apos;=&apos;\\u0001&apos;)                 </span><br><span class=\"line\"> STORED AS INPUTFORMAT                              </span><br><span class=\"line\">   &apos;org.apache.hadoop.mapred.TextInputFormat&apos;       </span><br><span class=\"line\"> OUTPUTFORMAT                                       </span><br><span class=\"line\">   &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos; </span><br><span class=\"line\"> LOCATION                                           </span><br><span class=\"line\">   &apos;hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders_2&apos; </span><br><span class=\"line\"> TBLPROPERTIES (                                    </span><br><span class=\"line\">   &apos;transient_lastDdlTime&apos;=&apos;1508578063&apos;)</span><br></pre></td></tr></table></figure>\n<h3 id=\"sqoop-eval\"><a href=\"#sqoop-eval\" class=\"headerlink\" title=\"sqoop eval\"></a>sqoop eval</h3><p>执行一段sql</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop eval --connect jdbc:mysql://quickstart:3306/retail_db  \\</span><br><span class=\"line\">--username retail_dba --password cloudera --query &apos;select count(1) from orders&apos;</span><br></pre></td></tr></table></figure>\n<p>结果</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">------------------------</span><br><span class=\"line\"> count(1)              </span><br><span class=\"line\">------------------------</span><br><span class=\"line\"> 68883                 </span><br><span class=\"line\">------------------------</span><br></pre></td></tr></table></figure>\n<h3 id=\"sqoop-export\"><a href=\"#sqoop-export\" class=\"headerlink\" title=\"sqoop export\"></a>sqoop export</h3><pre><code>将hive文件夹中的数据导入到数据库表中\n</code></pre><p>Parquet 表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop export --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba \\</span><br><span class=\"line\"> --password cloudera  --hcatalog-table &quot;products&quot; --hcatalog-database &quot;default&quot;  \\</span><br><span class=\"line\"> --table products_1 -m 1</span><br></pre></td></tr></table></figure>\n<p>Text表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop export --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba  \\</span><br><span class=\"line\">--password cloudera  --export-dir /user/hive/warehouse/products_2 \\</span><br><span class=\"line\"> --input-fields-terminated-by &apos;\\001&apos; --table products_2 -m 1</span><br></pre></td></tr></table></figure>\n<h3 id=\"Sqoop-import\"><a href=\"#Sqoop-import\" class=\"headerlink\" title=\"Sqoop  import\"></a>Sqoop  import</h3><p>导入一张表到HDFS</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --connect jdbc:mysql://quickstart:3306/retail_db  \\</span><br><span class=\"line\">--username=retail_dba --password=cloudera   \\</span><br><span class=\"line\">--target-dir /usr/hive/warehouse/orders --table orders --hive-import</span><br></pre></td></tr></table></figure>\n<p>Hive 表结构</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE `orders`(                             </span><br><span class=\"line\">  `order_id` int,                                  </span><br><span class=\"line\">  `order_date` string,                             </span><br><span class=\"line\">  `order_customer_id` int,                         </span><br><span class=\"line\">  `order_status` string)                           </span><br><span class=\"line\">COMMENT &apos;Imported by sqoop on 2017/10/26 05:50:14&apos; </span><br><span class=\"line\">ROW FORMAT SERDE                                   </span><br><span class=\"line\">  &apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&apos;  </span><br><span class=\"line\">WITH SERDEPROPERTIES (                             </span><br><span class=\"line\">  &apos;field.delim&apos;=&apos;\\u0001&apos;,                          </span><br><span class=\"line\">  &apos;line.delim&apos;=&apos;\\n&apos;,                               </span><br><span class=\"line\">  &apos;serialization.format&apos;=&apos;\\u0001&apos;)                 </span><br><span class=\"line\">STORED AS INPUTFORMAT                              </span><br><span class=\"line\">  &apos;org.apache.hadoop.mapred.TextInputFormat&apos;       </span><br><span class=\"line\">OUTPUTFORMAT                                       </span><br><span class=\"line\">  &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos; </span><br><span class=\"line\">LOCATION                                           </span><br><span class=\"line\">  &apos;hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders&apos; </span><br><span class=\"line\">TBLPROPERTIES (                                    </span><br><span class=\"line\">  &apos;COLUMN_STATS_ACCURATE&apos;=&apos;true&apos;,                  </span><br><span class=\"line\">  &apos;numFiles&apos;=&apos;1&apos;,                                  </span><br><span class=\"line\">  &apos;numRows&apos;=&apos;0&apos;,                                   </span><br><span class=\"line\">  &apos;rawDataSize&apos;=&apos;0&apos;,                               </span><br><span class=\"line\">  &apos;totalSize&apos;=&apos;2999944&apos;,                           </span><br><span class=\"line\">  &apos;transient_lastDdlTime&apos;=&apos;1509022219&apos;)</span><br></pre></td></tr></table></figure>\n<p>HDFS 文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders</span><br><span class=\"line\">Found 2 items</span><br><span class=\"line\">-rw-r--r--   1 cloudera supergroup          0 2017-10-26 06:20 /user/hive/warehouse/orders/_SUCCESS</span><br><span class=\"line\">-rwxr-xr-x   1 cloudera supergroup    2999944 2017-10-26 06:20 /user/hive/warehouse/orders/part-m-00000</span><br></pre></td></tr></table></figure>\n<p>导入parquet格式表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import --connect jdbc:mysql://quickstart:3306/retail_db  \\</span><br><span class=\"line\">--username=retail_dba --password=cloudera   \\</span><br><span class=\"line\">--target-dir /usr/hive/warehouse/orders --table orders --hive-import --as-parquetfile</span><br></pre></td></tr></table></figure>\n<p>HDFS文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders/</span><br><span class=\"line\">Found 5 items</span><br><span class=\"line\">drwxr-xr-x   - cloudera supergroup          0 2017-10-26 05:53 /user/hive/warehouse/orders/.metadata</span><br><span class=\"line\">drwxr-xr-x   - cloudera supergroup          0 2017-10-26 05:54 /user/hive/warehouse/orders/.signals</span><br><span class=\"line\">-rw-r--r--   1 cloudera supergroup     488257 2017-10-26 05:54 /user/hive/warehouse/orders/5e708d3e-a273-48de-8a3d-37efcdaf8d62.parquet</span><br><span class=\"line\">-rw-r--r--   1 cloudera supergroup          0 2017-10-26 05:50 /user/hive/warehouse/orders/_SUCCESS</span><br><span class=\"line\">-rwxr-xr-x   1 cloudera supergroup    2999944 2017-10-26 05:50 /user/hive/warehouse/orders/part-m-00000</span><br></pre></td></tr></table></figure></p>\n<p>加上Snappy 压缩</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders/</span><br><span class=\"line\">Found 3 items</span><br><span class=\"line\">drwxr-xr-x   - cloudera supergroup          0 2017-10-26 06:04 /user/hive/warehouse/orders/.metadata</span><br><span class=\"line\">drwxr-xr-x   - cloudera supergroup          0 2017-10-26 06:05 /user/hive/warehouse/orders/.signals</span><br><span class=\"line\">-rw-r--r--   1 cloudera supergroup     488257 2017-10-26 06:05 /user/hive/warehouse/orders/bfa9265e-2711-494e-a924-bd8c6591954e.parquet</span><br></pre></td></tr></table></figure>\n<h3 id=\"Sqoop-import-all-tables\"><a href=\"#Sqoop-import-all-tables\" class=\"headerlink\" title=\"Sqoop import-all-tables\"></a>Sqoop import-all-tables</h3><p>导入数据库所有表到hdfs：创建表并将数据导入HDFS</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import-all-tables --connect jdbc:mysql://quickstart:3306/retail_db  \\</span><br><span class=\"line\">--username=retail_dba --password=cloudera --compress-codec=snappy \\ </span><br><span class=\"line\">--as-parquetfile --warehouse-dir=/user/hive/warehouse --hive-import</span><br></pre></td></tr></table></figure>\n<p>检查其中一张表到hdfs文件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders</span><br><span class=\"line\">Found 3 items</span><br><span class=\"line\">drwxr-xr-x   - cloudera supergroup          0 2017-10-20 18:57 /user/hive/warehouse/orders/.metadata</span><br><span class=\"line\">drwxr-xr-x   - cloudera supergroup          0 2017-10-20 18:58 /user/hive/warehouse/orders/.signals</span><br><span class=\"line\">-rw-r--r--   1 cloudera supergroup     488257 2017-10-20 18:58 /user/hive/warehouse/orders/a851e80b-1238-465d-93b7-a8ac11c53697.parquet</span><br></pre></td></tr></table></figure>\n<p>检查hive表结构</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show create table orders</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE `orders`(                             </span><br><span class=\"line\">  `order_id` int,                                  </span><br><span class=\"line\">  `order_date` bigint,                             </span><br><span class=\"line\">  `order_customer_id` int,                         </span><br><span class=\"line\">  `order_status` string)                           </span><br><span class=\"line\">ROW FORMAT SERDE                                   </span><br><span class=\"line\">  &apos;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe&apos;  </span><br><span class=\"line\">STORED AS INPUTFORMAT                              </span><br><span class=\"line\">  &apos;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat&apos;  </span><br><span class=\"line\">OUTPUTFORMAT                                       </span><br><span class=\"line\">  &apos;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat&apos; </span><br><span class=\"line\">LOCATION                                           </span><br><span class=\"line\">  &apos;hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders&apos; </span><br><span class=\"line\">TBLPROPERTIES (                                    </span><br><span class=\"line\">  &apos;COLUMN_STATS_ACCURATE&apos;=&apos;false&apos;,                 </span><br><span class=\"line\">  &apos;avro.schema.url&apos;=&apos;hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders/.metadata/schemas/1.avsc&apos;,  </span><br><span class=\"line\">  &apos;kite.compression.type&apos;=&apos;snappy&apos;,                </span><br><span class=\"line\">  &apos;numFiles&apos;=&apos;0&apos;,                                  </span><br><span class=\"line\">  &apos;numRows&apos;=&apos;-1&apos;,                                  </span><br><span class=\"line\">  &apos;rawDataSize&apos;=&apos;-1&apos;,                              </span><br><span class=\"line\">  &apos;totalSize&apos;=&apos;0&apos;,                                 </span><br><span class=\"line\">  &apos;transient_lastDdlTime&apos;=&apos;1508551065&apos;)</span><br></pre></td></tr></table></figure>\n<h3 id=\"Sqoop-list-databases\"><a href=\"#Sqoop-list-databases\" class=\"headerlink\" title=\"Sqoop list-databases\"></a>Sqoop list-databases</h3><p>查看数据库列表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop list-databases --connect jdbc:mysql://quickstart:3306  \\</span><br><span class=\"line\">--username retail_dba --password cloudera</span><br></pre></td></tr></table></figure>\n<p>结果</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">information_schema</span><br><span class=\"line\">retail_db</span><br></pre></td></tr></table></figure>\n<h3 id=\"Sqoop-list-tables\"><a href=\"#Sqoop-list-tables\" class=\"headerlink\" title=\"Sqoop list-tables\"></a>Sqoop list-tables</h3><p>查看数据库中表到列表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop list-tables --connect jdbc:mysql://quickstart:3306/retail_db  \\</span><br><span class=\"line\">--username retail_dba --password cloudera</span><br></pre></td></tr></table></figure>\n<p>结果：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">categories</span><br><span class=\"line\">customers</span><br><span class=\"line\">departments</span><br><span class=\"line\">order_items</span><br><span class=\"line\">orders</span><br><span class=\"line\">products</span><br></pre></td></tr></table></figure>"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/help.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"help","published":1,"date":"2017-12-23T05:53:16.740Z","updated":"2017-12-23T05:53:16.740Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avo04001linamk16uleti","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n"},{"title":"测试Hive权限级别","date":"2018-05-12T06:34:44.000Z","_content":"\n# 测试Hive权限级别\n\n## Hive权限分类\n\n|名称\t|解释\n|-----|-----\r|ALL\t |所有权限\r|ALTER|\t允许修改元数据（modify metadata data of object）---表信息数据 \r|UPDATE|\t允许修改物理数据（modify physical data of object）---实际数据 \r|CREATE|\t允许进行Create操作 \r|DROP|\t允许进行DROP操作 \r|INDEX|\t允许建索引（目前还没有实现）\r|LOCK|\t当出现并发的使用允许用户进行LOCK和UNLOCK操作 \r|SELECT|\t允许用户进行SELECT操作\r|SHOW_DATABASE|\t允许用户查看可用的数据库\n\n\n**Sentry能管理的权限**\n\nSELECT、INSERT、ALL 即Sentry只能分这三种权限进行赋值。\n\n## Hive User、Group、Role\n\n**Role**\n\n可以使用Grant语法，赋权限给Role。\n\n**User**\n\n操作系统上的用户，Hive权限里无法创建用户\n\n**Group**\n\n操作系统上的组，Hive权限里如法创建组\n\n**关系**：\n\n* 一个用户就是操作系统上的用户\n* 可以把一个用户分配到一个组或者多个组上\n* 一个组就是操作系统上的组\n* 一个组可以有多个角色\n* 角色只能被赋值给组，不能赋值给用户\n* 一个角色可以被赋值多种权限\n* 一个用户，继承它所在的所有组的所有权限\n\n## 权限管理与分类\n\n不同的部门、应用在操作系统上属于不同的组 \n角色和角色的名称应该是有标准的\n一个部门的用户，在操作系统上属于一个组，用户名不相同 可以有不同的权限\n\n如:   \n部门 IT_DEV 有读取数据库内容的权限  \nManager 有写数据库的权限   \n如果user1 同时属于2个组，那么user1 具有读写数据库的权限   \n如果user2 只属于ITDEV4组，那么user2 只有读取数据库内容的权限\n\n## 测试准备\n创建一个测试库\n\n```\ncreate database test;\n```\n\n创建表并插入数据\n\n```\nuse test;\ncreate table test.test_partition (id int, name string) partitioned by (dt int);\ninsert into test.test_partition partition(dt=20180101) select '1','daniel';\nselect * from test_partition;\n+--------------------+----------------------+--------------------+--+\n| test_partition.id  | test_partition.name  | test_partition.dt  |\n+--------------------+----------------------+--------------------+--+\n| 1                  | daniel               | 20180101           |\n+--------------------+----------------------+--------------------+--+\n```\n\n创建一个测试用户\n\n```\ncreate role test_auto;\n```\n在每个操作系统上都创建这个用户\n添加一个组\n\n```\ngroupadd it_dev    ## 创建一个用户组\nuseradd test_auto  ## 创建一个用户\nusermod -g it_dev test_auto  ## 将这个用户只加入 itdev4 组\n## 检查用户所属的组\n[root@dn3 ~]# id test_auto;\nuid=1008(test_auto) gid=1011(itdev4) groups=1011(itdev4)\n[root@dn3 ~]#\n```\n\n在Hive用户下，beeline执行，添加角色并分配权限\n\n```\ncreate role reader;\ngrant select on database test to role reader;\ncreate role writer;\ngrant insert on database test to role writer;\n```\n\n将只读角色赋到组上\n\n```\ngrant role reader to group itdev4;\n\nshow role grant  group  itdev4;\n+---------+---------------+-------------+----------+--+\n|  role   | grant_option  | grant_time  | grantor  |\n+---------+---------------+-------------+----------+--+\n| reader  | false         | NULL        | --       |\n+---------+---------------+-------------+----------+--+\n```\n\n使用用户test_auto连接hive\n\n```\nbeeline -n test_auto -u jdbc:hive2://nn2.htsec.com:10000/test\nshow tables; ## 执行成功\n```\n将写的权限赋到组上\n\n```\ngrant role writer to group itdev4;\nshow role grant  group  itdev4;\n+---------+---------------+-------------+----------+--+\n|  role   | grant_option  | grant_time  | grantor  |\n+---------+---------------+-------------+----------+--+\n| reader  | false         | NULL        | --       |\n| writer  | false         | NULL        | --       |\n+---------+---------------+-------------+----------+--+\n```\n\n向表中插入数据，执行成功\n\n```\ninsert into test_partition partition(dt=20180101) select '2','tom';\nselect * from test_partition;\n+--------------------+----------------------+--------------------+--+\n| test_partition.id  | test_partition.name  | test_partition.dt  |\n+--------------------+----------------------+--------------------+--+\n| 1                  | daniel               | 20180101           |\n| 2                  | tom                  | 20180101           |\n+--------------------+----------------------+--------------------+--+\n```\n\n创建用户test2，并添加到it_dev 下\n\n```\nuseradd -G it_dev test2\n[root@nn1 ~]# id test2\nuid=1009(test2) gid=1010(itdev4) groups=1010(itdev4)\n```\n使用beeline连接并测试\n\n```\nbeeline -n test2 -u jdbc:hive2://nn2.htsec.com:10000/test\nselect * from test_partition;\n+--------------------+----------------------+--------------------+--+\n| test_partition.id  | test_partition.name  | test_partition.dt  |\n+--------------------+----------------------+--------------------+--+\n| 1                  | daniel               | 20180101           |\n| 2                  | tom                  | 20180101           |\n+--------------------+----------------------+--------------------+--+\ninsert into test_partition partition(dt=20180101) select '3','Monkey';\n\nselect * from test_partition;\n+--------------------+----------------------+--------------------+--+\n| test_partition.id  | test_partition.name  | test_partition.dt  |\n+--------------------+----------------------+--------------------+--+\n| 1                  | daniel               | 20180101           |\n| 2                  | tom                  | 20180101           |\n| 3                  | Monkey               | 20180101           |\n+--------------------+----------------------+--------------------+--+\n```\n\n## 测试\n\n### 测试SELECT\n\n\n使用test_auto连接hiveserver2,并测试\n\n```\nbeeline -n test_auto -u jdbc:hive2://xxx:10000\n\nshow databases;   ##看不到test数据库\n+----------------+--+\n| database_name  |\n+----------------+--+\n| default        |\n+----------------+--+\n\nuse test; ## 报错，无法切换数据库\nUser test_auto does not have privileges for SWITCHDATABASE\n```\n\n```\nbeeline -n test_auto -u jdbc:hive2://xxx:10000/test\nshow tables;  ## 无法查看任何表。\n+-----------+--+\n| tab_name  |\n+-----------+--+\n+-----------+--+\n```\n在hive用户下，查看test_auto是否在test_auto 组里。\n\n```\nshow role grant  group test_auto;  ## test_auto组下没有任何的role；\n+-------+---------------+-------------+----------+--+\n| role  | grant_option  | grant_time  | grantor  |\n+-------+---------------+-------------+----------+--+\n+-------+---------------+-------------+----------+--+\n\ngrant role test_auto to group test_auto;\nshow role grant  group test_auto;\n+------------+---------------+-------------+----------+--+\n|    role    | grant_option  | grant_time  | grantor  |\n+------------+---------------+-------------+----------+--+\n| test_auto  | false         | NULL        | --       |\n+------------+---------------+-------------+----------+--+\n```\n再在test_auto用户下，执行命令\n\n```\nshow databases;\n+----------------+--+\n| database_name  |\n+----------------+--+\n| default        |\n| test           |\n+----------------+--+\n\nshow tables;  ##查看到表\n+------------------------+--+\n|        tab_name        |\n+------------------------+--+\n| test_partition         |\n+------------------------+--+\n\nselect * from test_partition;\n+--------------------+----------------------+--------------------+--+\n| test_partition.id  | test_partition.name  | test_partition.dt  |\n+--------------------+----------------------+--------------------+--+\n| 1                  | daniel               | 20180101           |\n+--------------------+----------------------+--------------------+--+\n\n```\n查看表的表结构\n**DESCRIBE TABLE**\n\n```\ndesc test_table; ## 报错,因为test_table是一个Kudu的表。\n FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.ClassNotFoundException Class  not found (state=08S01,code=1)\n\ndesc test_partition; \n+--------------------------+-----------------------+-----------------------+--+\n|         col_name         |       data_type       |        comment        |\n+--------------------------+-----------------------+-----------------------+--+\n| id                       | int                   |                       |\n| name                     | string                |                       |\n| dt                       | int                   |                       |\n|                          | NULL                  | NULL                  |\n| # Partition Information  | NULL                  | NULL                  |\n| # col_name               | data_type             | comment               |\n|                          | NULL                  | NULL                  |\n| dt                       | int                   |                       |\n+--------------------------+-----------------------+-----------------------+--+\n```\n\n查看分区\n**SHOW PARTITIONS**\n\n```\nshow partitions test_partition; ## 有查看表分区的权限\n+--------------+--+\n|  partition   |\n+--------------+--+\n| dt=20180101  |\n+--------------+--+\n```\n\n创建视图的权限 **CREATE VIEW**\n\n```\ncreate view test_view as select * from test_partition;\nser test_auto does not have privileges for CREATEVIEW\n The required privileges: Server=server1->Db=test->action=*; (state=42000,code=40000)\n```\n\n\n分析表 **ANALYZE TABLE**\n\n```\nanalyze table test_partition partition(dt=20180101) COMPUTE STATISTICS; ## 需要Insert 权限\nError: Error while compiling statement: FAILED: SemanticException No valid privileges\n User test_auto does not have privileges for QUERY\n The required privileges: Server=server1->Db=test->Table=test_partition->action=insert; (state=42000,code=40000)\n \nanalyze table test_partition compute statistics for columns; ## 执行成功\n```\n\n\r查看表中的列 **SHOW COLUMNS**\n\n```\nshow columns in test_partition;\n+--------+--+\n| field  |\n+--------+--+\n| id     |\n| name   |\n| dt     |\n+--------+--+\n\n```\r查看表的统计信息 **SHOW TABLE STATUS**\n\n```\nDESCRIBE EXTENDED test_partition;\n\n+-----------------------------+----------------------------------------------------+-----------------------+--+\n|          col_name           |                     data_type                      |        comment        |\n+-----------------------------+----------------------------------------------------+-----------------------+--+\n| id                          | int                                                |                       |\n| name                        | string                                             |                       |\n| dt                          | int                                                |                       |\n|                             | NULL                                               | NULL                  |\n| # Partition Information     | NULL                                               | NULL                  |\n| # col_name                  | data_type                                          | comment               |\n|                             | NULL                                               | NULL                  |\n| dt                          | int                                                |                       |\n|                             | NULL                                               | NULL                  |\n| Detailed Table Information  | Table(tableName:test_partition, dbName:test, owner:hive, createTime:1516768367, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:null), FieldSchema(name:name, type:string, comment:null), FieldSchema(name:dt, type:int, comment:null)], location:hdfs://xxx/user/hive/dw/test.db/test_partition, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:dt, type:int, comment:null)], parameters:{numPartitions=1, transient_lastDdlTime=1516768367}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE) |\n\n\nDESCRIBE EXTENDED test_partition.id partition(dt=20180101);\n\n+-----------+------------+--------------------+--+\n| col_name  | data_type  |      comment       |\n+-----------+------------+--------------------+--+\n| id        | int        | from deserializer  |\n+-----------+------------+--------------------+--+\n\ndescribe formatted test_partition.id; ## 执行成功\n```\r查看表的属性信息 **SHOW TABLE PROPERTIES**\r\r```\nshow tblproperties test_partition;\n+------------------------+-------------+--+\n|       prpt_name        | prpt_value  |\n+------------------------+-------------+--+\n| transient_lastDdlTime  | 1516768367  |\n+------------------------+-------------+--+\n\nshow tblproperties test_partition('transient_lastDdlTime');\n\n+-------------+-------------+--+\n|  prpt_name  | prpt_value  |\n+-------------+-------------+--+\n| 1516768367  | NULL        |\n+-------------+-------------+--+\n```\n\r复制表 **CREATE TABLE AS SELECT**\n\n```\ncreate table createas_test as select * from test_partition;\nError: Error while compiling statement: FAILED: SemanticException No valid privileges\n User test_auto does not have privileges for CREATETABLE_AS_SELECT\n The required privileges: Server=server1->Db=test->action=*; (state=42000,code=40000)\n```\n查看表的创建语句 **SHOW CREATE TABLE**\n\n```\nshow create table test_partition;\n+----------------------------------------------------+--+\n|                   createtab_stmt                   |\n+----------------------------------------------------+--+\n| CREATE TABLE `test_partition`(                     |\n|   `id` int,                                        |\n|   `name` string)                                   |\n| PARTITIONED BY (                                   |\n|   `dt` int)                                        |\n| ROW FORMAT SERDE                                   |\n|   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'  |\n| STORED AS INPUTFORMAT                              |\n|   'org.apache.hadoop.mapred.TextInputFormat'       |\n| OUTPUTFORMAT                                       |\n|   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' |\n| LOCATION                                           |\n|   'hdfs://xxx/user/hive/dw/test.db/test_partition' |\n| TBLPROPERTIES (                                    |\n|   'transient_lastDdlTime'='1516768367')            |\n+----------------------------------------------------+--+\n\n```\n\n分析 **EXPLAIN**\n\n```\nEXPLAIN select * from test_partition;\n\n+----------------------------------------------------+--+\n|                      Explain                       |\n+----------------------------------------------------+--+\n| STAGE DEPENDENCIES:                                |\n|   Stage-0 is a root stage                          |\n|                                                    |\n| STAGE PLANS:                                       |\n|   Stage: Stage-0                                   |\n|     Fetch Operator                                 |\n|       limit: -1                                    |\n|       Processor Tree:                              |\n|         TableScan                                  |\n|           alias: test_partition                    |\n|           Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |\n|           Select Operator                          |\n|             expressions: id (type: int), name (type: string), dt (type: int) |\n|             outputColumnNames: _col0, _col1, _col2 |\n|             Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |\n|             ListSink                               |\n|                                                    |\n+----------------------------------------------------+--+\n```\n\n### 测试 INSERT\n\n在hive用户下\n\n```\nshow grant role test_auto;\n+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n| database  | table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |    grant_time     | grantor  |\n+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n| test      |        |            |         | test_auto       | ROLE            | select     | false         | 1516762810118000  | --       |\n+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n\n## 回收权限\nrevoke select on database test from role test_auto;\n\n## 赋予Insert权限\ngrant insert on database test to role test_auto;\nshow grant role test_auto;\n+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n| database  | table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |    grant_time     | grantor  |\n+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n| test      |        |            |         | test_auto       | ROLE            | insert     | false         | 1516771174805000  | --       |\n+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n```\n\n在只有INSERT权限，测试如下几个在SELECT命令成功，而现在执行失败或者有问题的仍无法成功\n\n```\nselect * from test_partition;\n\nError: Error while compiling statement: FAILED: SemanticException No valid privileges\n User test_auto does not have privileges for QUERY\n The required privileges: Server=server1->Db=test->Table=test_partition->Column=dt->action=select; (state=42000,code=40000)\n \nanalyze table test_partition partition(dt=20180101) COMPUTE STATISTICS;\n\nError: Error while compiling statement: FAILED: SemanticException No valid privileges\n User test_auto does not have privileges for QUERY\n The required privileges: Server=server1->Db=test->Table=test_partition->Column=RAW__DATA__SIZE->action=select; (state=42000,code=40000)\n\ncreate table createas_test as select * from test_partition;\n\nError: Error while compiling statement: FAILED: SemanticException No valid privileges\n User test_auto does not have privileges for CREATETABLE_AS_SELECT\n The required privileges: Server=server1->Db=test->Table=test_partition->Column=dt->action=select; (state=42000,code=40000)\n```\n\n插入 **INSERT**\n\n```\ninsert into table test_partition partition(dt=20180102) select '2','tom' ; ## 执行成功\n\n```\n\n导入文件 **LOAD**\n\n```\nload data local inpath  '/tmp/hosts' into table test_partition partition(dt=20180103); ## 此时/tmp/hosts文件的所有者是test_auto,但仍旧报错。\nError: Error while compiling statement: FAILED: SemanticException No valid privileges\n User test_auto does not have privileges for LOAD\n The required privileges: Server=server1->URI=file:///tmp/hosts->action=*; (state=42000,code=40000)\n```\n\n更新 **UPDATE**\n\n```\nupdate  test_pratition set id =3 where id=1 ;\nError: Error while compiling statement: FAILED: SemanticException [Error 10294]: Attempt to do update or delete using transaction manager that does not support these operations. (state=42000,code=10294)\n```\n\n在hive用户下，给用户赋予SELECT 权限测试以上几个失败的命令\n\n```\ngrant select on base test from role test_auto;\n```\n\n```\nselect * from test_partition;\n+--------------------+----------------------+--------------------+--+\n| test_partition.id  | test_partition.name  | test_partition.dt  |\n+--------------------+----------------------+--------------------+--+\n| 1                  | daniel               | 20180101           |\n| 2                  | tom                  | 20180102           |\n+--------------------+----------------------+--------------------+--+\nanalyze table test_partition partition(dt=20180101) COMPUTE STATISTICS; ## 执行成功\n\ncreate table createas_test as select * from test_partition; \nError: Error while compiling statement: FAILED: SemanticException No valid privileges\n User test_auto does not have privileges for CREATETABLE_AS_SELECT\n The required privileges: Server=server1->Db=test->action=*; (state=42000,code=40000)\n```\n\n### 测试UPDATE\n不支持Update权限的赋值\n\n```\nrevoke select on database test from role test_auto;\nrevoke insert on database test from role test_auto;\n\ngrant update on database test to role test_auto;\nError: Error while compiling statement: FAILED: SemanticException Sentry does not support privilege: Update (state=42000,code=40000)\n```\n\n### 测试DELETE\n不支持Delete权限的赋值\n\n```\ngrant delete on database test to role test_auto;\nError: Error while compiling statement: FAILED: SemanticException Sentry does not support privilege: Delete (state=42000,code=40000)\n```\n\n### URI \n\n```\nGRANT ALL ON URI 'hdfs://tmp/hosts' to role test_auto; ## 成功\nload data  inpath  '/tmp/hosts' into table test_partition partition(dt=20180103); \n\n\nrevoke all on uri 'hdfs://tmp/hosts' from role test_auto;\nGRANT select ON URI 'hdfs://tmp/hosts' to role test_auto; ## 成功\nload data  inpath  '/tmp/hosts' into table test_partition partition(dt=20180103);  \n\n grant all on uri 'hdfs:/tmp/' to role test_auto;\n\n insert overwrite directory \"/user/test_auto/result/test_partition\" row format delimited fields terminated by \"\\t\"   select * from test_partition;\n \n insert overwrite directory \"/user/test_auto/result/test_partition\"   select * from test_partition;\n \n INSERT OVERWRITE DIRECTORY '/tmp/test_partition' SELECT * FROM test_partition; ## 执行成功\n\n需要hive能在这个路径下创建文件夹，并且test_auto也有权限、\n\n```\n","source":"_posts/测试Hive权限级别.md","raw":"---\ntitle: 测试Hive权限级别\ndate: 2018-05-12 14:34:44\ntags: \n    - Hive\n---\n\n# 测试Hive权限级别\n\n## Hive权限分类\n\n|名称\t|解释\n|-----|-----\r|ALL\t |所有权限\r|ALTER|\t允许修改元数据（modify metadata data of object）---表信息数据 \r|UPDATE|\t允许修改物理数据（modify physical data of object）---实际数据 \r|CREATE|\t允许进行Create操作 \r|DROP|\t允许进行DROP操作 \r|INDEX|\t允许建索引（目前还没有实现）\r|LOCK|\t当出现并发的使用允许用户进行LOCK和UNLOCK操作 \r|SELECT|\t允许用户进行SELECT操作\r|SHOW_DATABASE|\t允许用户查看可用的数据库\n\n\n**Sentry能管理的权限**\n\nSELECT、INSERT、ALL 即Sentry只能分这三种权限进行赋值。\n\n## Hive User、Group、Role\n\n**Role**\n\n可以使用Grant语法，赋权限给Role。\n\n**User**\n\n操作系统上的用户，Hive权限里无法创建用户\n\n**Group**\n\n操作系统上的组，Hive权限里如法创建组\n\n**关系**：\n\n* 一个用户就是操作系统上的用户\n* 可以把一个用户分配到一个组或者多个组上\n* 一个组就是操作系统上的组\n* 一个组可以有多个角色\n* 角色只能被赋值给组，不能赋值给用户\n* 一个角色可以被赋值多种权限\n* 一个用户，继承它所在的所有组的所有权限\n\n## 权限管理与分类\n\n不同的部门、应用在操作系统上属于不同的组 \n角色和角色的名称应该是有标准的\n一个部门的用户，在操作系统上属于一个组，用户名不相同 可以有不同的权限\n\n如:   \n部门 IT_DEV 有读取数据库内容的权限  \nManager 有写数据库的权限   \n如果user1 同时属于2个组，那么user1 具有读写数据库的权限   \n如果user2 只属于ITDEV4组，那么user2 只有读取数据库内容的权限\n\n## 测试准备\n创建一个测试库\n\n```\ncreate database test;\n```\n\n创建表并插入数据\n\n```\nuse test;\ncreate table test.test_partition (id int, name string) partitioned by (dt int);\ninsert into test.test_partition partition(dt=20180101) select '1','daniel';\nselect * from test_partition;\n+--------------------+----------------------+--------------------+--+\n| test_partition.id  | test_partition.name  | test_partition.dt  |\n+--------------------+----------------------+--------------------+--+\n| 1                  | daniel               | 20180101           |\n+--------------------+----------------------+--------------------+--+\n```\n\n创建一个测试用户\n\n```\ncreate role test_auto;\n```\n在每个操作系统上都创建这个用户\n添加一个组\n\n```\ngroupadd it_dev    ## 创建一个用户组\nuseradd test_auto  ## 创建一个用户\nusermod -g it_dev test_auto  ## 将这个用户只加入 itdev4 组\n## 检查用户所属的组\n[root@dn3 ~]# id test_auto;\nuid=1008(test_auto) gid=1011(itdev4) groups=1011(itdev4)\n[root@dn3 ~]#\n```\n\n在Hive用户下，beeline执行，添加角色并分配权限\n\n```\ncreate role reader;\ngrant select on database test to role reader;\ncreate role writer;\ngrant insert on database test to role writer;\n```\n\n将只读角色赋到组上\n\n```\ngrant role reader to group itdev4;\n\nshow role grant  group  itdev4;\n+---------+---------------+-------------+----------+--+\n|  role   | grant_option  | grant_time  | grantor  |\n+---------+---------------+-------------+----------+--+\n| reader  | false         | NULL        | --       |\n+---------+---------------+-------------+----------+--+\n```\n\n使用用户test_auto连接hive\n\n```\nbeeline -n test_auto -u jdbc:hive2://nn2.htsec.com:10000/test\nshow tables; ## 执行成功\n```\n将写的权限赋到组上\n\n```\ngrant role writer to group itdev4;\nshow role grant  group  itdev4;\n+---------+---------------+-------------+----------+--+\n|  role   | grant_option  | grant_time  | grantor  |\n+---------+---------------+-------------+----------+--+\n| reader  | false         | NULL        | --       |\n| writer  | false         | NULL        | --       |\n+---------+---------------+-------------+----------+--+\n```\n\n向表中插入数据，执行成功\n\n```\ninsert into test_partition partition(dt=20180101) select '2','tom';\nselect * from test_partition;\n+--------------------+----------------------+--------------------+--+\n| test_partition.id  | test_partition.name  | test_partition.dt  |\n+--------------------+----------------------+--------------------+--+\n| 1                  | daniel               | 20180101           |\n| 2                  | tom                  | 20180101           |\n+--------------------+----------------------+--------------------+--+\n```\n\n创建用户test2，并添加到it_dev 下\n\n```\nuseradd -G it_dev test2\n[root@nn1 ~]# id test2\nuid=1009(test2) gid=1010(itdev4) groups=1010(itdev4)\n```\n使用beeline连接并测试\n\n```\nbeeline -n test2 -u jdbc:hive2://nn2.htsec.com:10000/test\nselect * from test_partition;\n+--------------------+----------------------+--------------------+--+\n| test_partition.id  | test_partition.name  | test_partition.dt  |\n+--------------------+----------------------+--------------------+--+\n| 1                  | daniel               | 20180101           |\n| 2                  | tom                  | 20180101           |\n+--------------------+----------------------+--------------------+--+\ninsert into test_partition partition(dt=20180101) select '3','Monkey';\n\nselect * from test_partition;\n+--------------------+----------------------+--------------------+--+\n| test_partition.id  | test_partition.name  | test_partition.dt  |\n+--------------------+----------------------+--------------------+--+\n| 1                  | daniel               | 20180101           |\n| 2                  | tom                  | 20180101           |\n| 3                  | Monkey               | 20180101           |\n+--------------------+----------------------+--------------------+--+\n```\n\n## 测试\n\n### 测试SELECT\n\n\n使用test_auto连接hiveserver2,并测试\n\n```\nbeeline -n test_auto -u jdbc:hive2://xxx:10000\n\nshow databases;   ##看不到test数据库\n+----------------+--+\n| database_name  |\n+----------------+--+\n| default        |\n+----------------+--+\n\nuse test; ## 报错，无法切换数据库\nUser test_auto does not have privileges for SWITCHDATABASE\n```\n\n```\nbeeline -n test_auto -u jdbc:hive2://xxx:10000/test\nshow tables;  ## 无法查看任何表。\n+-----------+--+\n| tab_name  |\n+-----------+--+\n+-----------+--+\n```\n在hive用户下，查看test_auto是否在test_auto 组里。\n\n```\nshow role grant  group test_auto;  ## test_auto组下没有任何的role；\n+-------+---------------+-------------+----------+--+\n| role  | grant_option  | grant_time  | grantor  |\n+-------+---------------+-------------+----------+--+\n+-------+---------------+-------------+----------+--+\n\ngrant role test_auto to group test_auto;\nshow role grant  group test_auto;\n+------------+---------------+-------------+----------+--+\n|    role    | grant_option  | grant_time  | grantor  |\n+------------+---------------+-------------+----------+--+\n| test_auto  | false         | NULL        | --       |\n+------------+---------------+-------------+----------+--+\n```\n再在test_auto用户下，执行命令\n\n```\nshow databases;\n+----------------+--+\n| database_name  |\n+----------------+--+\n| default        |\n| test           |\n+----------------+--+\n\nshow tables;  ##查看到表\n+------------------------+--+\n|        tab_name        |\n+------------------------+--+\n| test_partition         |\n+------------------------+--+\n\nselect * from test_partition;\n+--------------------+----------------------+--------------------+--+\n| test_partition.id  | test_partition.name  | test_partition.dt  |\n+--------------------+----------------------+--------------------+--+\n| 1                  | daniel               | 20180101           |\n+--------------------+----------------------+--------------------+--+\n\n```\n查看表的表结构\n**DESCRIBE TABLE**\n\n```\ndesc test_table; ## 报错,因为test_table是一个Kudu的表。\n FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.ClassNotFoundException Class  not found (state=08S01,code=1)\n\ndesc test_partition; \n+--------------------------+-----------------------+-----------------------+--+\n|         col_name         |       data_type       |        comment        |\n+--------------------------+-----------------------+-----------------------+--+\n| id                       | int                   |                       |\n| name                     | string                |                       |\n| dt                       | int                   |                       |\n|                          | NULL                  | NULL                  |\n| # Partition Information  | NULL                  | NULL                  |\n| # col_name               | data_type             | comment               |\n|                          | NULL                  | NULL                  |\n| dt                       | int                   |                       |\n+--------------------------+-----------------------+-----------------------+--+\n```\n\n查看分区\n**SHOW PARTITIONS**\n\n```\nshow partitions test_partition; ## 有查看表分区的权限\n+--------------+--+\n|  partition   |\n+--------------+--+\n| dt=20180101  |\n+--------------+--+\n```\n\n创建视图的权限 **CREATE VIEW**\n\n```\ncreate view test_view as select * from test_partition;\nser test_auto does not have privileges for CREATEVIEW\n The required privileges: Server=server1->Db=test->action=*; (state=42000,code=40000)\n```\n\n\n分析表 **ANALYZE TABLE**\n\n```\nanalyze table test_partition partition(dt=20180101) COMPUTE STATISTICS; ## 需要Insert 权限\nError: Error while compiling statement: FAILED: SemanticException No valid privileges\n User test_auto does not have privileges for QUERY\n The required privileges: Server=server1->Db=test->Table=test_partition->action=insert; (state=42000,code=40000)\n \nanalyze table test_partition compute statistics for columns; ## 执行成功\n```\n\n\r查看表中的列 **SHOW COLUMNS**\n\n```\nshow columns in test_partition;\n+--------+--+\n| field  |\n+--------+--+\n| id     |\n| name   |\n| dt     |\n+--------+--+\n\n```\r查看表的统计信息 **SHOW TABLE STATUS**\n\n```\nDESCRIBE EXTENDED test_partition;\n\n+-----------------------------+----------------------------------------------------+-----------------------+--+\n|          col_name           |                     data_type                      |        comment        |\n+-----------------------------+----------------------------------------------------+-----------------------+--+\n| id                          | int                                                |                       |\n| name                        | string                                             |                       |\n| dt                          | int                                                |                       |\n|                             | NULL                                               | NULL                  |\n| # Partition Information     | NULL                                               | NULL                  |\n| # col_name                  | data_type                                          | comment               |\n|                             | NULL                                               | NULL                  |\n| dt                          | int                                                |                       |\n|                             | NULL                                               | NULL                  |\n| Detailed Table Information  | Table(tableName:test_partition, dbName:test, owner:hive, createTime:1516768367, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:null), FieldSchema(name:name, type:string, comment:null), FieldSchema(name:dt, type:int, comment:null)], location:hdfs://xxx/user/hive/dw/test.db/test_partition, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:dt, type:int, comment:null)], parameters:{numPartitions=1, transient_lastDdlTime=1516768367}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE) |\n\n\nDESCRIBE EXTENDED test_partition.id partition(dt=20180101);\n\n+-----------+------------+--------------------+--+\n| col_name  | data_type  |      comment       |\n+-----------+------------+--------------------+--+\n| id        | int        | from deserializer  |\n+-----------+------------+--------------------+--+\n\ndescribe formatted test_partition.id; ## 执行成功\n```\r查看表的属性信息 **SHOW TABLE PROPERTIES**\r\r```\nshow tblproperties test_partition;\n+------------------------+-------------+--+\n|       prpt_name        | prpt_value  |\n+------------------------+-------------+--+\n| transient_lastDdlTime  | 1516768367  |\n+------------------------+-------------+--+\n\nshow tblproperties test_partition('transient_lastDdlTime');\n\n+-------------+-------------+--+\n|  prpt_name  | prpt_value  |\n+-------------+-------------+--+\n| 1516768367  | NULL        |\n+-------------+-------------+--+\n```\n\r复制表 **CREATE TABLE AS SELECT**\n\n```\ncreate table createas_test as select * from test_partition;\nError: Error while compiling statement: FAILED: SemanticException No valid privileges\n User test_auto does not have privileges for CREATETABLE_AS_SELECT\n The required privileges: Server=server1->Db=test->action=*; (state=42000,code=40000)\n```\n查看表的创建语句 **SHOW CREATE TABLE**\n\n```\nshow create table test_partition;\n+----------------------------------------------------+--+\n|                   createtab_stmt                   |\n+----------------------------------------------------+--+\n| CREATE TABLE `test_partition`(                     |\n|   `id` int,                                        |\n|   `name` string)                                   |\n| PARTITIONED BY (                                   |\n|   `dt` int)                                        |\n| ROW FORMAT SERDE                                   |\n|   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'  |\n| STORED AS INPUTFORMAT                              |\n|   'org.apache.hadoop.mapred.TextInputFormat'       |\n| OUTPUTFORMAT                                       |\n|   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' |\n| LOCATION                                           |\n|   'hdfs://xxx/user/hive/dw/test.db/test_partition' |\n| TBLPROPERTIES (                                    |\n|   'transient_lastDdlTime'='1516768367')            |\n+----------------------------------------------------+--+\n\n```\n\n分析 **EXPLAIN**\n\n```\nEXPLAIN select * from test_partition;\n\n+----------------------------------------------------+--+\n|                      Explain                       |\n+----------------------------------------------------+--+\n| STAGE DEPENDENCIES:                                |\n|   Stage-0 is a root stage                          |\n|                                                    |\n| STAGE PLANS:                                       |\n|   Stage: Stage-0                                   |\n|     Fetch Operator                                 |\n|       limit: -1                                    |\n|       Processor Tree:                              |\n|         TableScan                                  |\n|           alias: test_partition                    |\n|           Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |\n|           Select Operator                          |\n|             expressions: id (type: int), name (type: string), dt (type: int) |\n|             outputColumnNames: _col0, _col1, _col2 |\n|             Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |\n|             ListSink                               |\n|                                                    |\n+----------------------------------------------------+--+\n```\n\n### 测试 INSERT\n\n在hive用户下\n\n```\nshow grant role test_auto;\n+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n| database  | table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |    grant_time     | grantor  |\n+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n| test      |        |            |         | test_auto       | ROLE            | select     | false         | 1516762810118000  | --       |\n+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n\n## 回收权限\nrevoke select on database test from role test_auto;\n\n## 赋予Insert权限\ngrant insert on database test to role test_auto;\nshow grant role test_auto;\n+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n| database  | table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |    grant_time     | grantor  |\n+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n| test      |        |            |         | test_auto       | ROLE            | insert     | false         | 1516771174805000  | --       |\n+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n```\n\n在只有INSERT权限，测试如下几个在SELECT命令成功，而现在执行失败或者有问题的仍无法成功\n\n```\nselect * from test_partition;\n\nError: Error while compiling statement: FAILED: SemanticException No valid privileges\n User test_auto does not have privileges for QUERY\n The required privileges: Server=server1->Db=test->Table=test_partition->Column=dt->action=select; (state=42000,code=40000)\n \nanalyze table test_partition partition(dt=20180101) COMPUTE STATISTICS;\n\nError: Error while compiling statement: FAILED: SemanticException No valid privileges\n User test_auto does not have privileges for QUERY\n The required privileges: Server=server1->Db=test->Table=test_partition->Column=RAW__DATA__SIZE->action=select; (state=42000,code=40000)\n\ncreate table createas_test as select * from test_partition;\n\nError: Error while compiling statement: FAILED: SemanticException No valid privileges\n User test_auto does not have privileges for CREATETABLE_AS_SELECT\n The required privileges: Server=server1->Db=test->Table=test_partition->Column=dt->action=select; (state=42000,code=40000)\n```\n\n插入 **INSERT**\n\n```\ninsert into table test_partition partition(dt=20180102) select '2','tom' ; ## 执行成功\n\n```\n\n导入文件 **LOAD**\n\n```\nload data local inpath  '/tmp/hosts' into table test_partition partition(dt=20180103); ## 此时/tmp/hosts文件的所有者是test_auto,但仍旧报错。\nError: Error while compiling statement: FAILED: SemanticException No valid privileges\n User test_auto does not have privileges for LOAD\n The required privileges: Server=server1->URI=file:///tmp/hosts->action=*; (state=42000,code=40000)\n```\n\n更新 **UPDATE**\n\n```\nupdate  test_pratition set id =3 where id=1 ;\nError: Error while compiling statement: FAILED: SemanticException [Error 10294]: Attempt to do update or delete using transaction manager that does not support these operations. (state=42000,code=10294)\n```\n\n在hive用户下，给用户赋予SELECT 权限测试以上几个失败的命令\n\n```\ngrant select on base test from role test_auto;\n```\n\n```\nselect * from test_partition;\n+--------------------+----------------------+--------------------+--+\n| test_partition.id  | test_partition.name  | test_partition.dt  |\n+--------------------+----------------------+--------------------+--+\n| 1                  | daniel               | 20180101           |\n| 2                  | tom                  | 20180102           |\n+--------------------+----------------------+--------------------+--+\nanalyze table test_partition partition(dt=20180101) COMPUTE STATISTICS; ## 执行成功\n\ncreate table createas_test as select * from test_partition; \nError: Error while compiling statement: FAILED: SemanticException No valid privileges\n User test_auto does not have privileges for CREATETABLE_AS_SELECT\n The required privileges: Server=server1->Db=test->action=*; (state=42000,code=40000)\n```\n\n### 测试UPDATE\n不支持Update权限的赋值\n\n```\nrevoke select on database test from role test_auto;\nrevoke insert on database test from role test_auto;\n\ngrant update on database test to role test_auto;\nError: Error while compiling statement: FAILED: SemanticException Sentry does not support privilege: Update (state=42000,code=40000)\n```\n\n### 测试DELETE\n不支持Delete权限的赋值\n\n```\ngrant delete on database test to role test_auto;\nError: Error while compiling statement: FAILED: SemanticException Sentry does not support privilege: Delete (state=42000,code=40000)\n```\n\n### URI \n\n```\nGRANT ALL ON URI 'hdfs://tmp/hosts' to role test_auto; ## 成功\nload data  inpath  '/tmp/hosts' into table test_partition partition(dt=20180103); \n\n\nrevoke all on uri 'hdfs://tmp/hosts' from role test_auto;\nGRANT select ON URI 'hdfs://tmp/hosts' to role test_auto; ## 成功\nload data  inpath  '/tmp/hosts' into table test_partition partition(dt=20180103);  \n\n grant all on uri 'hdfs:/tmp/' to role test_auto;\n\n insert overwrite directory \"/user/test_auto/result/test_partition\" row format delimited fields terminated by \"\\t\"   select * from test_partition;\n \n insert overwrite directory \"/user/test_auto/result/test_partition\"   select * from test_partition;\n \n INSERT OVERWRITE DIRECTORY '/tmp/test_partition' SELECT * FROM test_partition; ## 执行成功\n\n需要hive能在这个路径下创建文件夹，并且test_auto也有权限、\n\n```\n","slug":"测试Hive权限级别","published":1,"updated":"2018-05-13T06:40:00.661Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjp0avo05001ninamno28tdqn","content":"<h1 id=\"测试Hive权限级别\"><a href=\"#测试Hive权限级别\" class=\"headerlink\" title=\"测试Hive权限级别\"></a>测试Hive权限级别</h1><h2 id=\"Hive权限分类\"><a href=\"#Hive权限分类\" class=\"headerlink\" title=\"Hive权限分类\"></a>Hive权限分类</h2><table>\n<thead>\n<tr>\n<th>名称</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ALL</td>\n<td>所有权限</td>\n</tr>\n<tr>\n<td>ALTER</td>\n<td>允许修改元数据（modify metadata data of object）—表信息数据 </td>\n</tr>\n<tr>\n<td>UPDATE</td>\n<td>允许修改物理数据（modify physical data of object）—实际数据 </td>\n</tr>\n<tr>\n<td>CREATE</td>\n<td>允许进行Create操作 </td>\n</tr>\n<tr>\n<td>DROP</td>\n<td>允许进行DROP操作 </td>\n</tr>\n<tr>\n<td>INDEX</td>\n<td>允许建索引（目前还没有实现）</td>\n</tr>\n<tr>\n<td>LOCK</td>\n<td>当出现并发的使用允许用户进行LOCK和UNLOCK操作 </td>\n</tr>\n<tr>\n<td>SELECT</td>\n<td>允许用户进行SELECT操作</td>\n</tr>\n<tr>\n<td>SHOW_DATABASE</td>\n<td>允许用户查看可用的数据库</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Sentry能管理的权限</strong></p>\n<p>SELECT、INSERT、ALL 即Sentry只能分这三种权限进行赋值。</p>\n<h2 id=\"Hive-User、Group、Role\"><a href=\"#Hive-User、Group、Role\" class=\"headerlink\" title=\"Hive User、Group、Role\"></a>Hive User、Group、Role</h2><p><strong>Role</strong></p>\n<p>可以使用Grant语法，赋权限给Role。</p>\n<p><strong>User</strong></p>\n<p>操作系统上的用户，Hive权限里无法创建用户</p>\n<p><strong>Group</strong></p>\n<p>操作系统上的组，Hive权限里如法创建组</p>\n<p><strong>关系</strong>：</p>\n<ul>\n<li>一个用户就是操作系统上的用户</li>\n<li>可以把一个用户分配到一个组或者多个组上</li>\n<li>一个组就是操作系统上的组</li>\n<li>一个组可以有多个角色</li>\n<li>角色只能被赋值给组，不能赋值给用户</li>\n<li>一个角色可以被赋值多种权限</li>\n<li>一个用户，继承它所在的所有组的所有权限</li>\n</ul>\n<h2 id=\"权限管理与分类\"><a href=\"#权限管理与分类\" class=\"headerlink\" title=\"权限管理与分类\"></a>权限管理与分类</h2><p>不同的部门、应用在操作系统上属于不同的组<br>角色和角色的名称应该是有标准的<br>一个部门的用户，在操作系统上属于一个组，用户名不相同 可以有不同的权限</p>\n<p>如:<br>部门 IT_DEV 有读取数据库内容的权限<br>Manager 有写数据库的权限<br>如果user1 同时属于2个组，那么user1 具有读写数据库的权限<br>如果user2 只属于ITDEV4组，那么user2 只有读取数据库内容的权限</p>\n<h2 id=\"测试准备\"><a href=\"#测试准备\" class=\"headerlink\" title=\"测试准备\"></a>测试准备</h2><p>创建一个测试库</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create database test;</span><br></pre></td></tr></table></figure>\n<p>创建表并插入数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">use test;</span><br><span class=\"line\">create table test.test_partition (id int, name string) partitioned by (dt int);</span><br><span class=\"line\">insert into test.test_partition partition(dt=20180101) select &apos;1&apos;,&apos;daniel&apos;;</span><br><span class=\"line\">select * from test_partition;</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| test_partition.id  | test_partition.name  | test_partition.dt  |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| 1                  | daniel               | 20180101           |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br></pre></td></tr></table></figure>\n<p>创建一个测试用户</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create role test_auto;</span><br></pre></td></tr></table></figure>\n<p>在每个操作系统上都创建这个用户<br>添加一个组</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">groupadd it_dev    ## 创建一个用户组</span><br><span class=\"line\">useradd test_auto  ## 创建一个用户</span><br><span class=\"line\">usermod -g it_dev test_auto  ## 将这个用户只加入 itdev4 组</span><br><span class=\"line\">## 检查用户所属的组</span><br><span class=\"line\">[root@dn3 ~]# id test_auto;</span><br><span class=\"line\">uid=1008(test_auto) gid=1011(itdev4) groups=1011(itdev4)</span><br><span class=\"line\">[root@dn3 ~]#</span><br></pre></td></tr></table></figure>\n<p>在Hive用户下，beeline执行，添加角色并分配权限</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create role reader;</span><br><span class=\"line\">grant select on database test to role reader;</span><br><span class=\"line\">create role writer;</span><br><span class=\"line\">grant insert on database test to role writer;</span><br></pre></td></tr></table></figure>\n<p>将只读角色赋到组上</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">grant role reader to group itdev4;</span><br><span class=\"line\"></span><br><span class=\"line\">show role grant  group  itdev4;</span><br><span class=\"line\">+---------+---------------+-------------+----------+--+</span><br><span class=\"line\">|  role   | grant_option  | grant_time  | grantor  |</span><br><span class=\"line\">+---------+---------------+-------------+----------+--+</span><br><span class=\"line\">| reader  | false         | NULL        | --       |</span><br><span class=\"line\">+---------+---------------+-------------+----------+--+</span><br></pre></td></tr></table></figure>\n<p>使用用户test_auto连接hive</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">beeline -n test_auto -u jdbc:hive2://nn2.htsec.com:10000/test</span><br><span class=\"line\">show tables; ## 执行成功</span><br></pre></td></tr></table></figure>\n<p>将写的权限赋到组上</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">grant role writer to group itdev4;</span><br><span class=\"line\">show role grant  group  itdev4;</span><br><span class=\"line\">+---------+---------------+-------------+----------+--+</span><br><span class=\"line\">|  role   | grant_option  | grant_time  | grantor  |</span><br><span class=\"line\">+---------+---------------+-------------+----------+--+</span><br><span class=\"line\">| reader  | false         | NULL        | --       |</span><br><span class=\"line\">| writer  | false         | NULL        | --       |</span><br><span class=\"line\">+---------+---------------+-------------+----------+--+</span><br></pre></td></tr></table></figure>\n<p>向表中插入数据，执行成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">insert into test_partition partition(dt=20180101) select &apos;2&apos;,&apos;tom&apos;;</span><br><span class=\"line\">select * from test_partition;</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| test_partition.id  | test_partition.name  | test_partition.dt  |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| 1                  | daniel               | 20180101           |</span><br><span class=\"line\">| 2                  | tom                  | 20180101           |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br></pre></td></tr></table></figure>\n<p>创建用户test2，并添加到it_dev 下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">useradd -G it_dev test2</span><br><span class=\"line\">[root@nn1 ~]# id test2</span><br><span class=\"line\">uid=1009(test2) gid=1010(itdev4) groups=1010(itdev4)</span><br></pre></td></tr></table></figure>\n<p>使用beeline连接并测试</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">beeline -n test2 -u jdbc:hive2://nn2.htsec.com:10000/test</span><br><span class=\"line\">select * from test_partition;</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| test_partition.id  | test_partition.name  | test_partition.dt  |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| 1                  | daniel               | 20180101           |</span><br><span class=\"line\">| 2                  | tom                  | 20180101           |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">insert into test_partition partition(dt=20180101) select &apos;3&apos;,&apos;Monkey&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">select * from test_partition;</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| test_partition.id  | test_partition.name  | test_partition.dt  |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| 1                  | daniel               | 20180101           |</span><br><span class=\"line\">| 2                  | tom                  | 20180101           |</span><br><span class=\"line\">| 3                  | Monkey               | 20180101           |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br></pre></td></tr></table></figure>\n<h2 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h2><h3 id=\"测试SELECT\"><a href=\"#测试SELECT\" class=\"headerlink\" title=\"测试SELECT\"></a>测试SELECT</h3><p>使用test_auto连接hiveserver2,并测试</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">beeline -n test_auto -u jdbc:hive2://xxx:10000</span><br><span class=\"line\"></span><br><span class=\"line\">show databases;   ##看不到test数据库</span><br><span class=\"line\">+----------------+--+</span><br><span class=\"line\">| database_name  |</span><br><span class=\"line\">+----------------+--+</span><br><span class=\"line\">| default        |</span><br><span class=\"line\">+----------------+--+</span><br><span class=\"line\"></span><br><span class=\"line\">use test; ## 报错，无法切换数据库</span><br><span class=\"line\">User test_auto does not have privileges for SWITCHDATABASE</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">beeline -n test_auto -u jdbc:hive2://xxx:10000/test</span><br><span class=\"line\">show tables;  ## 无法查看任何表。</span><br><span class=\"line\">+-----------+--+</span><br><span class=\"line\">| tab_name  |</span><br><span class=\"line\">+-----------+--+</span><br><span class=\"line\">+-----------+--+</span><br></pre></td></tr></table></figure>\n<p>在hive用户下，查看test_auto是否在test_auto 组里。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show role grant  group test_auto;  ## test_auto组下没有任何的role；</span><br><span class=\"line\">+-------+---------------+-------------+----------+--+</span><br><span class=\"line\">| role  | grant_option  | grant_time  | grantor  |</span><br><span class=\"line\">+-------+---------------+-------------+----------+--+</span><br><span class=\"line\">+-------+---------------+-------------+----------+--+</span><br><span class=\"line\"></span><br><span class=\"line\">grant role test_auto to group test_auto;</span><br><span class=\"line\">show role grant  group test_auto;</span><br><span class=\"line\">+------------+---------------+-------------+----------+--+</span><br><span class=\"line\">|    role    | grant_option  | grant_time  | grantor  |</span><br><span class=\"line\">+------------+---------------+-------------+----------+--+</span><br><span class=\"line\">| test_auto  | false         | NULL        | --       |</span><br><span class=\"line\">+------------+---------------+-------------+----------+--+</span><br></pre></td></tr></table></figure>\n<p>再在test_auto用户下，执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show databases;</span><br><span class=\"line\">+----------------+--+</span><br><span class=\"line\">| database_name  |</span><br><span class=\"line\">+----------------+--+</span><br><span class=\"line\">| default        |</span><br><span class=\"line\">| test           |</span><br><span class=\"line\">+----------------+--+</span><br><span class=\"line\"></span><br><span class=\"line\">show tables;  ##查看到表</span><br><span class=\"line\">+------------------------+--+</span><br><span class=\"line\">|        tab_name        |</span><br><span class=\"line\">+------------------------+--+</span><br><span class=\"line\">| test_partition         |</span><br><span class=\"line\">+------------------------+--+</span><br><span class=\"line\"></span><br><span class=\"line\">select * from test_partition;</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| test_partition.id  | test_partition.name  | test_partition.dt  |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| 1                  | daniel               | 20180101           |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br></pre></td></tr></table></figure>\n<p>查看表的表结构<br><strong>DESCRIBE TABLE</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">desc test_table; ## 报错,因为test_table是一个Kudu的表。</span><br><span class=\"line\"> FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.ClassNotFoundException Class  not found (state=08S01,code=1)</span><br><span class=\"line\"></span><br><span class=\"line\">desc test_partition; </span><br><span class=\"line\">+--------------------------+-----------------------+-----------------------+--+</span><br><span class=\"line\">|         col_name         |       data_type       |        comment        |</span><br><span class=\"line\">+--------------------------+-----------------------+-----------------------+--+</span><br><span class=\"line\">| id                       | int                   |                       |</span><br><span class=\"line\">| name                     | string                |                       |</span><br><span class=\"line\">| dt                       | int                   |                       |</span><br><span class=\"line\">|                          | NULL                  | NULL                  |</span><br><span class=\"line\">| # Partition Information  | NULL                  | NULL                  |</span><br><span class=\"line\">| # col_name               | data_type             | comment               |</span><br><span class=\"line\">|                          | NULL                  | NULL                  |</span><br><span class=\"line\">| dt                       | int                   |                       |</span><br><span class=\"line\">+--------------------------+-----------------------+-----------------------+--+</span><br></pre></td></tr></table></figure>\n<p>查看分区<br><strong>SHOW PARTITIONS</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show partitions test_partition; ## 有查看表分区的权限</span><br><span class=\"line\">+--------------+--+</span><br><span class=\"line\">|  partition   |</span><br><span class=\"line\">+--------------+--+</span><br><span class=\"line\">| dt=20180101  |</span><br><span class=\"line\">+--------------+--+</span><br></pre></td></tr></table></figure>\n<p>创建视图的权限 <strong>CREATE VIEW</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create view test_view as select * from test_partition;</span><br><span class=\"line\">ser test_auto does not have privileges for CREATEVIEW</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;Db=test-&gt;action=*; (state=42000,code=40000)</span><br></pre></td></tr></table></figure>\n<p>分析表 <strong>ANALYZE TABLE</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">analyze table test_partition partition(dt=20180101) COMPUTE STATISTICS; ## 需要Insert 权限</span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException No valid privileges</span><br><span class=\"line\"> User test_auto does not have privileges for QUERY</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;Db=test-&gt;Table=test_partition-&gt;action=insert; (state=42000,code=40000)</span><br><span class=\"line\"> </span><br><span class=\"line\">analyze table test_partition compute statistics for columns; ## 执行成功</span><br></pre></td></tr></table></figure>\n<p>查看表中的列 <strong>SHOW COLUMNS</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show columns in test_partition;</span><br><span class=\"line\">+--------+--+</span><br><span class=\"line\">| field  |</span><br><span class=\"line\">+--------+--+</span><br><span class=\"line\">| id     |</span><br><span class=\"line\">| name   |</span><br><span class=\"line\">| dt     |</span><br><span class=\"line\">+--------+--+</span><br><span class=\"line\"></span><br><span class=\"line\">```\r查看表的统计信息 **SHOW TABLE STATUS**</span><br></pre></td></tr></table></figure>\n<p>DESCRIBE EXTENDED test_partition;</p>\n<p>+—————————–+—————————————————-+———————–+–+<br>|          col_name           |                     data_type                      |        comment        |<br>+—————————–+—————————————————-+———————–+–+<br>| id                          | int                                                |                       |<br>| name                        | string                                             |                       |<br>| dt                          | int                                                |                       |<br>|                             | NULL                                               | NULL                  |<br>| # Partition Information     | NULL                                               | NULL                  |<br>| # col_name                  | data_type                                          | comment               |<br>|                             | NULL                                               | NULL                  |<br>| dt                          | int                                                |                       |<br>|                             | NULL                                               | NULL                  |<br>| Detailed Table Information  | Table(tableName:test_partition, dbName:test, owner:hive, createTime:1516768367, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:null), FieldSchema(name:name, type:string, comment:null), FieldSchema(name:dt, type:int, comment:null)], location:hdfs://xxx/user/hive/dw/test.db/test_partition, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:dt, type:int, comment:null)], parameters:{numPartitions=1, transient_lastDdlTime=1516768367}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE) |</p>\n<p>DESCRIBE EXTENDED test_partition.id partition(dt=20180101);</p>\n<p>+———–+————+——————–+–+<br>| col_name  | data_type  |      comment       |<br>+———–+————+——————–+–+<br>| id        | int        | from deserializer  |<br>+———–+————+——————–+–+</p>\n<p>describe formatted test_partition.id; ## 执行成功<br>```<br>查看表的属性信息 <strong>SHOW TABLE PROPERTIES</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show tblproperties test_partition;</span><br><span class=\"line\">+------------------------+-------------+--+</span><br><span class=\"line\">|       prpt_name        | prpt_value  |</span><br><span class=\"line\">+------------------------+-------------+--+</span><br><span class=\"line\">| transient_lastDdlTime  | 1516768367  |</span><br><span class=\"line\">+------------------------+-------------+--+</span><br><span class=\"line\"></span><br><span class=\"line\">show tblproperties test_partition(&apos;transient_lastDdlTime&apos;);</span><br><span class=\"line\"></span><br><span class=\"line\">+-------------+-------------+--+</span><br><span class=\"line\">|  prpt_name  | prpt_value  |</span><br><span class=\"line\">+-------------+-------------+--+</span><br><span class=\"line\">| 1516768367  | NULL        |</span><br><span class=\"line\">+-------------+-------------+--+</span><br></pre></td></tr></table></figure>\n<p>复制表 <strong>CREATE TABLE AS SELECT</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create table createas_test as select * from test_partition;</span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException No valid privileges</span><br><span class=\"line\"> User test_auto does not have privileges for CREATETABLE_AS_SELECT</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;Db=test-&gt;action=*; (state=42000,code=40000)</span><br></pre></td></tr></table></figure>\n<p>查看表的创建语句 <strong>SHOW CREATE TABLE</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show create table test_partition;</span><br><span class=\"line\">+----------------------------------------------------+--+</span><br><span class=\"line\">|                   createtab_stmt                   |</span><br><span class=\"line\">+----------------------------------------------------+--+</span><br><span class=\"line\">| CREATE TABLE `test_partition`(                     |</span><br><span class=\"line\">|   `id` int,                                        |</span><br><span class=\"line\">|   `name` string)                                   |</span><br><span class=\"line\">| PARTITIONED BY (                                   |</span><br><span class=\"line\">|   `dt` int)                                        |</span><br><span class=\"line\">| ROW FORMAT SERDE                                   |</span><br><span class=\"line\">|   &apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&apos;  |</span><br><span class=\"line\">| STORED AS INPUTFORMAT                              |</span><br><span class=\"line\">|   &apos;org.apache.hadoop.mapred.TextInputFormat&apos;       |</span><br><span class=\"line\">| OUTPUTFORMAT                                       |</span><br><span class=\"line\">|   &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos; |</span><br><span class=\"line\">| LOCATION                                           |</span><br><span class=\"line\">|   &apos;hdfs://xxx/user/hive/dw/test.db/test_partition&apos; |</span><br><span class=\"line\">| TBLPROPERTIES (                                    |</span><br><span class=\"line\">|   &apos;transient_lastDdlTime&apos;=&apos;1516768367&apos;)            |</span><br><span class=\"line\">+----------------------------------------------------+--+</span><br></pre></td></tr></table></figure>\n<p>分析 <strong>EXPLAIN</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">EXPLAIN select * from test_partition;</span><br><span class=\"line\"></span><br><span class=\"line\">+----------------------------------------------------+--+</span><br><span class=\"line\">|                      Explain                       |</span><br><span class=\"line\">+----------------------------------------------------+--+</span><br><span class=\"line\">| STAGE DEPENDENCIES:                                |</span><br><span class=\"line\">|   Stage-0 is a root stage                          |</span><br><span class=\"line\">|                                                    |</span><br><span class=\"line\">| STAGE PLANS:                                       |</span><br><span class=\"line\">|   Stage: Stage-0                                   |</span><br><span class=\"line\">|     Fetch Operator                                 |</span><br><span class=\"line\">|       limit: -1                                    |</span><br><span class=\"line\">|       Processor Tree:                              |</span><br><span class=\"line\">|         TableScan                                  |</span><br><span class=\"line\">|           alias: test_partition                    |</span><br><span class=\"line\">|           Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |</span><br><span class=\"line\">|           Select Operator                          |</span><br><span class=\"line\">|             expressions: id (type: int), name (type: string), dt (type: int) |</span><br><span class=\"line\">|             outputColumnNames: _col0, _col1, _col2 |</span><br><span class=\"line\">|             Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |</span><br><span class=\"line\">|             ListSink                               |</span><br><span class=\"line\">|                                                    |</span><br><span class=\"line\">+----------------------------------------------------+--+</span><br></pre></td></tr></table></figure>\n<h3 id=\"测试-INSERT\"><a href=\"#测试-INSERT\" class=\"headerlink\" title=\"测试 INSERT\"></a>测试 INSERT</h3><p>在hive用户下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show grant role test_auto;</span><br><span class=\"line\">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</span><br><span class=\"line\">| database  | table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |    grant_time     | grantor  |</span><br><span class=\"line\">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</span><br><span class=\"line\">| test      |        |            |         | test_auto       | ROLE            | select     | false         | 1516762810118000  | --       |</span><br><span class=\"line\">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</span><br><span class=\"line\"></span><br><span class=\"line\">## 回收权限</span><br><span class=\"line\">revoke select on database test from role test_auto;</span><br><span class=\"line\"></span><br><span class=\"line\">## 赋予Insert权限</span><br><span class=\"line\">grant insert on database test to role test_auto;</span><br><span class=\"line\">show grant role test_auto;</span><br><span class=\"line\">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</span><br><span class=\"line\">| database  | table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |    grant_time     | grantor  |</span><br><span class=\"line\">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</span><br><span class=\"line\">| test      |        |            |         | test_auto       | ROLE            | insert     | false         | 1516771174805000  | --       |</span><br><span class=\"line\">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</span><br></pre></td></tr></table></figure>\n<p>在只有INSERT权限，测试如下几个在SELECT命令成功，而现在执行失败或者有问题的仍无法成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select * from test_partition;</span><br><span class=\"line\"></span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException No valid privileges</span><br><span class=\"line\"> User test_auto does not have privileges for QUERY</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;Db=test-&gt;Table=test_partition-&gt;Column=dt-&gt;action=select; (state=42000,code=40000)</span><br><span class=\"line\"> </span><br><span class=\"line\">analyze table test_partition partition(dt=20180101) COMPUTE STATISTICS;</span><br><span class=\"line\"></span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException No valid privileges</span><br><span class=\"line\"> User test_auto does not have privileges for QUERY</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;Db=test-&gt;Table=test_partition-&gt;Column=RAW__DATA__SIZE-&gt;action=select; (state=42000,code=40000)</span><br><span class=\"line\"></span><br><span class=\"line\">create table createas_test as select * from test_partition;</span><br><span class=\"line\"></span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException No valid privileges</span><br><span class=\"line\"> User test_auto does not have privileges for CREATETABLE_AS_SELECT</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;Db=test-&gt;Table=test_partition-&gt;Column=dt-&gt;action=select; (state=42000,code=40000)</span><br></pre></td></tr></table></figure>\n<p>插入 <strong>INSERT</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">insert into table test_partition partition(dt=20180102) select &apos;2&apos;,&apos;tom&apos; ; ## 执行成功</span><br></pre></td></tr></table></figure>\n<p>导入文件 <strong>LOAD</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">load data local inpath  &apos;/tmp/hosts&apos; into table test_partition partition(dt=20180103); ## 此时/tmp/hosts文件的所有者是test_auto,但仍旧报错。</span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException No valid privileges</span><br><span class=\"line\"> User test_auto does not have privileges for LOAD</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;URI=file:///tmp/hosts-&gt;action=*; (state=42000,code=40000)</span><br></pre></td></tr></table></figure>\n<p>更新 <strong>UPDATE</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">update  test_pratition set id =3 where id=1 ;</span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException [Error 10294]: Attempt to do update or delete using transaction manager that does not support these operations. (state=42000,code=10294)</span><br></pre></td></tr></table></figure>\n<p>在hive用户下，给用户赋予SELECT 权限测试以上几个失败的命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">grant select on base test from role test_auto;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select * from test_partition;</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| test_partition.id  | test_partition.name  | test_partition.dt  |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| 1                  | daniel               | 20180101           |</span><br><span class=\"line\">| 2                  | tom                  | 20180102           |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">analyze table test_partition partition(dt=20180101) COMPUTE STATISTICS; ## 执行成功</span><br><span class=\"line\"></span><br><span class=\"line\">create table createas_test as select * from test_partition; </span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException No valid privileges</span><br><span class=\"line\"> User test_auto does not have privileges for CREATETABLE_AS_SELECT</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;Db=test-&gt;action=*; (state=42000,code=40000)</span><br></pre></td></tr></table></figure>\n<h3 id=\"测试UPDATE\"><a href=\"#测试UPDATE\" class=\"headerlink\" title=\"测试UPDATE\"></a>测试UPDATE</h3><p>不支持Update权限的赋值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">revoke select on database test from role test_auto;</span><br><span class=\"line\">revoke insert on database test from role test_auto;</span><br><span class=\"line\"></span><br><span class=\"line\">grant update on database test to role test_auto;</span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException Sentry does not support privilege: Update (state=42000,code=40000)</span><br></pre></td></tr></table></figure>\n<h3 id=\"测试DELETE\"><a href=\"#测试DELETE\" class=\"headerlink\" title=\"测试DELETE\"></a>测试DELETE</h3><p>不支持Delete权限的赋值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">grant delete on database test to role test_auto;</span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException Sentry does not support privilege: Delete (state=42000,code=40000)</span><br></pre></td></tr></table></figure>\n<h3 id=\"URI\"><a href=\"#URI\" class=\"headerlink\" title=\"URI\"></a>URI</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GRANT ALL ON URI &apos;hdfs://tmp/hosts&apos; to role test_auto; ## 成功</span><br><span class=\"line\">load data  inpath  &apos;/tmp/hosts&apos; into table test_partition partition(dt=20180103); </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">revoke all on uri &apos;hdfs://tmp/hosts&apos; from role test_auto;</span><br><span class=\"line\">GRANT select ON URI &apos;hdfs://tmp/hosts&apos; to role test_auto; ## 成功</span><br><span class=\"line\">load data  inpath  &apos;/tmp/hosts&apos; into table test_partition partition(dt=20180103);  </span><br><span class=\"line\"></span><br><span class=\"line\"> grant all on uri &apos;hdfs:/tmp/&apos; to role test_auto;</span><br><span class=\"line\"></span><br><span class=\"line\"> insert overwrite directory &quot;/user/test_auto/result/test_partition&quot; row format delimited fields terminated by &quot;\\t&quot;   select * from test_partition;</span><br><span class=\"line\"> </span><br><span class=\"line\"> insert overwrite directory &quot;/user/test_auto/result/test_partition&quot;   select * from test_partition;</span><br><span class=\"line\"> </span><br><span class=\"line\"> INSERT OVERWRITE DIRECTORY &apos;/tmp/test_partition&apos; SELECT * FROM test_partition; ## 执行成功</span><br><span class=\"line\"></span><br><span class=\"line\">需要hive能在这个路径下创建文件夹，并且test_auto也有权限、</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"测试Hive权限级别\"><a href=\"#测试Hive权限级别\" class=\"headerlink\" title=\"测试Hive权限级别\"></a>测试Hive权限级别</h1><h2 id=\"Hive权限分类\"><a href=\"#Hive权限分类\" class=\"headerlink\" title=\"Hive权限分类\"></a>Hive权限分类</h2><table>\n<thead>\n<tr>\n<th>名称</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ALL</td>\n<td>所有权限</td>\n</tr>\n<tr>\n<td>ALTER</td>\n<td>允许修改元数据（modify metadata data of object）—表信息数据 </td>\n</tr>\n<tr>\n<td>UPDATE</td>\n<td>允许修改物理数据（modify physical data of object）—实际数据 </td>\n</tr>\n<tr>\n<td>CREATE</td>\n<td>允许进行Create操作 </td>\n</tr>\n<tr>\n<td>DROP</td>\n<td>允许进行DROP操作 </td>\n</tr>\n<tr>\n<td>INDEX</td>\n<td>允许建索引（目前还没有实现）</td>\n</tr>\n<tr>\n<td>LOCK</td>\n<td>当出现并发的使用允许用户进行LOCK和UNLOCK操作 </td>\n</tr>\n<tr>\n<td>SELECT</td>\n<td>允许用户进行SELECT操作</td>\n</tr>\n<tr>\n<td>SHOW_DATABASE</td>\n<td>允许用户查看可用的数据库</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Sentry能管理的权限</strong></p>\n<p>SELECT、INSERT、ALL 即Sentry只能分这三种权限进行赋值。</p>\n<h2 id=\"Hive-User、Group、Role\"><a href=\"#Hive-User、Group、Role\" class=\"headerlink\" title=\"Hive User、Group、Role\"></a>Hive User、Group、Role</h2><p><strong>Role</strong></p>\n<p>可以使用Grant语法，赋权限给Role。</p>\n<p><strong>User</strong></p>\n<p>操作系统上的用户，Hive权限里无法创建用户</p>\n<p><strong>Group</strong></p>\n<p>操作系统上的组，Hive权限里如法创建组</p>\n<p><strong>关系</strong>：</p>\n<ul>\n<li>一个用户就是操作系统上的用户</li>\n<li>可以把一个用户分配到一个组或者多个组上</li>\n<li>一个组就是操作系统上的组</li>\n<li>一个组可以有多个角色</li>\n<li>角色只能被赋值给组，不能赋值给用户</li>\n<li>一个角色可以被赋值多种权限</li>\n<li>一个用户，继承它所在的所有组的所有权限</li>\n</ul>\n<h2 id=\"权限管理与分类\"><a href=\"#权限管理与分类\" class=\"headerlink\" title=\"权限管理与分类\"></a>权限管理与分类</h2><p>不同的部门、应用在操作系统上属于不同的组<br>角色和角色的名称应该是有标准的<br>一个部门的用户，在操作系统上属于一个组，用户名不相同 可以有不同的权限</p>\n<p>如:<br>部门 IT_DEV 有读取数据库内容的权限<br>Manager 有写数据库的权限<br>如果user1 同时属于2个组，那么user1 具有读写数据库的权限<br>如果user2 只属于ITDEV4组，那么user2 只有读取数据库内容的权限</p>\n<h2 id=\"测试准备\"><a href=\"#测试准备\" class=\"headerlink\" title=\"测试准备\"></a>测试准备</h2><p>创建一个测试库</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create database test;</span><br></pre></td></tr></table></figure>\n<p>创建表并插入数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">use test;</span><br><span class=\"line\">create table test.test_partition (id int, name string) partitioned by (dt int);</span><br><span class=\"line\">insert into test.test_partition partition(dt=20180101) select &apos;1&apos;,&apos;daniel&apos;;</span><br><span class=\"line\">select * from test_partition;</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| test_partition.id  | test_partition.name  | test_partition.dt  |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| 1                  | daniel               | 20180101           |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br></pre></td></tr></table></figure>\n<p>创建一个测试用户</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create role test_auto;</span><br></pre></td></tr></table></figure>\n<p>在每个操作系统上都创建这个用户<br>添加一个组</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">groupadd it_dev    ## 创建一个用户组</span><br><span class=\"line\">useradd test_auto  ## 创建一个用户</span><br><span class=\"line\">usermod -g it_dev test_auto  ## 将这个用户只加入 itdev4 组</span><br><span class=\"line\">## 检查用户所属的组</span><br><span class=\"line\">[root@dn3 ~]# id test_auto;</span><br><span class=\"line\">uid=1008(test_auto) gid=1011(itdev4) groups=1011(itdev4)</span><br><span class=\"line\">[root@dn3 ~]#</span><br></pre></td></tr></table></figure>\n<p>在Hive用户下，beeline执行，添加角色并分配权限</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create role reader;</span><br><span class=\"line\">grant select on database test to role reader;</span><br><span class=\"line\">create role writer;</span><br><span class=\"line\">grant insert on database test to role writer;</span><br></pre></td></tr></table></figure>\n<p>将只读角色赋到组上</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">grant role reader to group itdev4;</span><br><span class=\"line\"></span><br><span class=\"line\">show role grant  group  itdev4;</span><br><span class=\"line\">+---------+---------------+-------------+----------+--+</span><br><span class=\"line\">|  role   | grant_option  | grant_time  | grantor  |</span><br><span class=\"line\">+---------+---------------+-------------+----------+--+</span><br><span class=\"line\">| reader  | false         | NULL        | --       |</span><br><span class=\"line\">+---------+---------------+-------------+----------+--+</span><br></pre></td></tr></table></figure>\n<p>使用用户test_auto连接hive</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">beeline -n test_auto -u jdbc:hive2://nn2.htsec.com:10000/test</span><br><span class=\"line\">show tables; ## 执行成功</span><br></pre></td></tr></table></figure>\n<p>将写的权限赋到组上</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">grant role writer to group itdev4;</span><br><span class=\"line\">show role grant  group  itdev4;</span><br><span class=\"line\">+---------+---------------+-------------+----------+--+</span><br><span class=\"line\">|  role   | grant_option  | grant_time  | grantor  |</span><br><span class=\"line\">+---------+---------------+-------------+----------+--+</span><br><span class=\"line\">| reader  | false         | NULL        | --       |</span><br><span class=\"line\">| writer  | false         | NULL        | --       |</span><br><span class=\"line\">+---------+---------------+-------------+----------+--+</span><br></pre></td></tr></table></figure>\n<p>向表中插入数据，执行成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">insert into test_partition partition(dt=20180101) select &apos;2&apos;,&apos;tom&apos;;</span><br><span class=\"line\">select * from test_partition;</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| test_partition.id  | test_partition.name  | test_partition.dt  |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| 1                  | daniel               | 20180101           |</span><br><span class=\"line\">| 2                  | tom                  | 20180101           |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br></pre></td></tr></table></figure>\n<p>创建用户test2，并添加到it_dev 下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">useradd -G it_dev test2</span><br><span class=\"line\">[root@nn1 ~]# id test2</span><br><span class=\"line\">uid=1009(test2) gid=1010(itdev4) groups=1010(itdev4)</span><br></pre></td></tr></table></figure>\n<p>使用beeline连接并测试</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">beeline -n test2 -u jdbc:hive2://nn2.htsec.com:10000/test</span><br><span class=\"line\">select * from test_partition;</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| test_partition.id  | test_partition.name  | test_partition.dt  |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| 1                  | daniel               | 20180101           |</span><br><span class=\"line\">| 2                  | tom                  | 20180101           |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">insert into test_partition partition(dt=20180101) select &apos;3&apos;,&apos;Monkey&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">select * from test_partition;</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| test_partition.id  | test_partition.name  | test_partition.dt  |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| 1                  | daniel               | 20180101           |</span><br><span class=\"line\">| 2                  | tom                  | 20180101           |</span><br><span class=\"line\">| 3                  | Monkey               | 20180101           |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br></pre></td></tr></table></figure>\n<h2 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h2><h3 id=\"测试SELECT\"><a href=\"#测试SELECT\" class=\"headerlink\" title=\"测试SELECT\"></a>测试SELECT</h3><p>使用test_auto连接hiveserver2,并测试</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">beeline -n test_auto -u jdbc:hive2://xxx:10000</span><br><span class=\"line\"></span><br><span class=\"line\">show databases;   ##看不到test数据库</span><br><span class=\"line\">+----------------+--+</span><br><span class=\"line\">| database_name  |</span><br><span class=\"line\">+----------------+--+</span><br><span class=\"line\">| default        |</span><br><span class=\"line\">+----------------+--+</span><br><span class=\"line\"></span><br><span class=\"line\">use test; ## 报错，无法切换数据库</span><br><span class=\"line\">User test_auto does not have privileges for SWITCHDATABASE</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">beeline -n test_auto -u jdbc:hive2://xxx:10000/test</span><br><span class=\"line\">show tables;  ## 无法查看任何表。</span><br><span class=\"line\">+-----------+--+</span><br><span class=\"line\">| tab_name  |</span><br><span class=\"line\">+-----------+--+</span><br><span class=\"line\">+-----------+--+</span><br></pre></td></tr></table></figure>\n<p>在hive用户下，查看test_auto是否在test_auto 组里。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show role grant  group test_auto;  ## test_auto组下没有任何的role；</span><br><span class=\"line\">+-------+---------------+-------------+----------+--+</span><br><span class=\"line\">| role  | grant_option  | grant_time  | grantor  |</span><br><span class=\"line\">+-------+---------------+-------------+----------+--+</span><br><span class=\"line\">+-------+---------------+-------------+----------+--+</span><br><span class=\"line\"></span><br><span class=\"line\">grant role test_auto to group test_auto;</span><br><span class=\"line\">show role grant  group test_auto;</span><br><span class=\"line\">+------------+---------------+-------------+----------+--+</span><br><span class=\"line\">|    role    | grant_option  | grant_time  | grantor  |</span><br><span class=\"line\">+------------+---------------+-------------+----------+--+</span><br><span class=\"line\">| test_auto  | false         | NULL        | --       |</span><br><span class=\"line\">+------------+---------------+-------------+----------+--+</span><br></pre></td></tr></table></figure>\n<p>再在test_auto用户下，执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show databases;</span><br><span class=\"line\">+----------------+--+</span><br><span class=\"line\">| database_name  |</span><br><span class=\"line\">+----------------+--+</span><br><span class=\"line\">| default        |</span><br><span class=\"line\">| test           |</span><br><span class=\"line\">+----------------+--+</span><br><span class=\"line\"></span><br><span class=\"line\">show tables;  ##查看到表</span><br><span class=\"line\">+------------------------+--+</span><br><span class=\"line\">|        tab_name        |</span><br><span class=\"line\">+------------------------+--+</span><br><span class=\"line\">| test_partition         |</span><br><span class=\"line\">+------------------------+--+</span><br><span class=\"line\"></span><br><span class=\"line\">select * from test_partition;</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| test_partition.id  | test_partition.name  | test_partition.dt  |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| 1                  | daniel               | 20180101           |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br></pre></td></tr></table></figure>\n<p>查看表的表结构<br><strong>DESCRIBE TABLE</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">desc test_table; ## 报错,因为test_table是一个Kudu的表。</span><br><span class=\"line\"> FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.ClassNotFoundException Class  not found (state=08S01,code=1)</span><br><span class=\"line\"></span><br><span class=\"line\">desc test_partition; </span><br><span class=\"line\">+--------------------------+-----------------------+-----------------------+--+</span><br><span class=\"line\">|         col_name         |       data_type       |        comment        |</span><br><span class=\"line\">+--------------------------+-----------------------+-----------------------+--+</span><br><span class=\"line\">| id                       | int                   |                       |</span><br><span class=\"line\">| name                     | string                |                       |</span><br><span class=\"line\">| dt                       | int                   |                       |</span><br><span class=\"line\">|                          | NULL                  | NULL                  |</span><br><span class=\"line\">| # Partition Information  | NULL                  | NULL                  |</span><br><span class=\"line\">| # col_name               | data_type             | comment               |</span><br><span class=\"line\">|                          | NULL                  | NULL                  |</span><br><span class=\"line\">| dt                       | int                   |                       |</span><br><span class=\"line\">+--------------------------+-----------------------+-----------------------+--+</span><br></pre></td></tr></table></figure>\n<p>查看分区<br><strong>SHOW PARTITIONS</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show partitions test_partition; ## 有查看表分区的权限</span><br><span class=\"line\">+--------------+--+</span><br><span class=\"line\">|  partition   |</span><br><span class=\"line\">+--------------+--+</span><br><span class=\"line\">| dt=20180101  |</span><br><span class=\"line\">+--------------+--+</span><br></pre></td></tr></table></figure>\n<p>创建视图的权限 <strong>CREATE VIEW</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create view test_view as select * from test_partition;</span><br><span class=\"line\">ser test_auto does not have privileges for CREATEVIEW</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;Db=test-&gt;action=*; (state=42000,code=40000)</span><br></pre></td></tr></table></figure>\n<p>分析表 <strong>ANALYZE TABLE</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">analyze table test_partition partition(dt=20180101) COMPUTE STATISTICS; ## 需要Insert 权限</span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException No valid privileges</span><br><span class=\"line\"> User test_auto does not have privileges for QUERY</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;Db=test-&gt;Table=test_partition-&gt;action=insert; (state=42000,code=40000)</span><br><span class=\"line\"> </span><br><span class=\"line\">analyze table test_partition compute statistics for columns; ## 执行成功</span><br></pre></td></tr></table></figure>\n<p>查看表中的列 <strong>SHOW COLUMNS</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show columns in test_partition;</span><br><span class=\"line\">+--------+--+</span><br><span class=\"line\">| field  |</span><br><span class=\"line\">+--------+--+</span><br><span class=\"line\">| id     |</span><br><span class=\"line\">| name   |</span><br><span class=\"line\">| dt     |</span><br><span class=\"line\">+--------+--+</span><br><span class=\"line\"></span><br><span class=\"line\">```\r查看表的统计信息 **SHOW TABLE STATUS**</span><br></pre></td></tr></table></figure>\n<p>DESCRIBE EXTENDED test_partition;</p>\n<p>+—————————–+—————————————————-+———————–+–+<br>|          col_name           |                     data_type                      |        comment        |<br>+—————————–+—————————————————-+———————–+–+<br>| id                          | int                                                |                       |<br>| name                        | string                                             |                       |<br>| dt                          | int                                                |                       |<br>|                             | NULL                                               | NULL                  |<br>| # Partition Information     | NULL                                               | NULL                  |<br>| # col_name                  | data_type                                          | comment               |<br>|                             | NULL                                               | NULL                  |<br>| dt                          | int                                                |                       |<br>|                             | NULL                                               | NULL                  |<br>| Detailed Table Information  | Table(tableName:test_partition, dbName:test, owner:hive, createTime:1516768367, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:null), FieldSchema(name:name, type:string, comment:null), FieldSchema(name:dt, type:int, comment:null)], location:hdfs://xxx/user/hive/dw/test.db/test_partition, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:dt, type:int, comment:null)], parameters:{numPartitions=1, transient_lastDdlTime=1516768367}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE) |</p>\n<p>DESCRIBE EXTENDED test_partition.id partition(dt=20180101);</p>\n<p>+———–+————+——————–+–+<br>| col_name  | data_type  |      comment       |<br>+———–+————+——————–+–+<br>| id        | int        | from deserializer  |<br>+———–+————+——————–+–+</p>\n<p>describe formatted test_partition.id; ## 执行成功<br>```<br>查看表的属性信息 <strong>SHOW TABLE PROPERTIES</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show tblproperties test_partition;</span><br><span class=\"line\">+------------------------+-------------+--+</span><br><span class=\"line\">|       prpt_name        | prpt_value  |</span><br><span class=\"line\">+------------------------+-------------+--+</span><br><span class=\"line\">| transient_lastDdlTime  | 1516768367  |</span><br><span class=\"line\">+------------------------+-------------+--+</span><br><span class=\"line\"></span><br><span class=\"line\">show tblproperties test_partition(&apos;transient_lastDdlTime&apos;);</span><br><span class=\"line\"></span><br><span class=\"line\">+-------------+-------------+--+</span><br><span class=\"line\">|  prpt_name  | prpt_value  |</span><br><span class=\"line\">+-------------+-------------+--+</span><br><span class=\"line\">| 1516768367  | NULL        |</span><br><span class=\"line\">+-------------+-------------+--+</span><br></pre></td></tr></table></figure>\n<p>复制表 <strong>CREATE TABLE AS SELECT</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create table createas_test as select * from test_partition;</span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException No valid privileges</span><br><span class=\"line\"> User test_auto does not have privileges for CREATETABLE_AS_SELECT</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;Db=test-&gt;action=*; (state=42000,code=40000)</span><br></pre></td></tr></table></figure>\n<p>查看表的创建语句 <strong>SHOW CREATE TABLE</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show create table test_partition;</span><br><span class=\"line\">+----------------------------------------------------+--+</span><br><span class=\"line\">|                   createtab_stmt                   |</span><br><span class=\"line\">+----------------------------------------------------+--+</span><br><span class=\"line\">| CREATE TABLE `test_partition`(                     |</span><br><span class=\"line\">|   `id` int,                                        |</span><br><span class=\"line\">|   `name` string)                                   |</span><br><span class=\"line\">| PARTITIONED BY (                                   |</span><br><span class=\"line\">|   `dt` int)                                        |</span><br><span class=\"line\">| ROW FORMAT SERDE                                   |</span><br><span class=\"line\">|   &apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&apos;  |</span><br><span class=\"line\">| STORED AS INPUTFORMAT                              |</span><br><span class=\"line\">|   &apos;org.apache.hadoop.mapred.TextInputFormat&apos;       |</span><br><span class=\"line\">| OUTPUTFORMAT                                       |</span><br><span class=\"line\">|   &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos; |</span><br><span class=\"line\">| LOCATION                                           |</span><br><span class=\"line\">|   &apos;hdfs://xxx/user/hive/dw/test.db/test_partition&apos; |</span><br><span class=\"line\">| TBLPROPERTIES (                                    |</span><br><span class=\"line\">|   &apos;transient_lastDdlTime&apos;=&apos;1516768367&apos;)            |</span><br><span class=\"line\">+----------------------------------------------------+--+</span><br></pre></td></tr></table></figure>\n<p>分析 <strong>EXPLAIN</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">EXPLAIN select * from test_partition;</span><br><span class=\"line\"></span><br><span class=\"line\">+----------------------------------------------------+--+</span><br><span class=\"line\">|                      Explain                       |</span><br><span class=\"line\">+----------------------------------------------------+--+</span><br><span class=\"line\">| STAGE DEPENDENCIES:                                |</span><br><span class=\"line\">|   Stage-0 is a root stage                          |</span><br><span class=\"line\">|                                                    |</span><br><span class=\"line\">| STAGE PLANS:                                       |</span><br><span class=\"line\">|   Stage: Stage-0                                   |</span><br><span class=\"line\">|     Fetch Operator                                 |</span><br><span class=\"line\">|       limit: -1                                    |</span><br><span class=\"line\">|       Processor Tree:                              |</span><br><span class=\"line\">|         TableScan                                  |</span><br><span class=\"line\">|           alias: test_partition                    |</span><br><span class=\"line\">|           Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |</span><br><span class=\"line\">|           Select Operator                          |</span><br><span class=\"line\">|             expressions: id (type: int), name (type: string), dt (type: int) |</span><br><span class=\"line\">|             outputColumnNames: _col0, _col1, _col2 |</span><br><span class=\"line\">|             Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |</span><br><span class=\"line\">|             ListSink                               |</span><br><span class=\"line\">|                                                    |</span><br><span class=\"line\">+----------------------------------------------------+--+</span><br></pre></td></tr></table></figure>\n<h3 id=\"测试-INSERT\"><a href=\"#测试-INSERT\" class=\"headerlink\" title=\"测试 INSERT\"></a>测试 INSERT</h3><p>在hive用户下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show grant role test_auto;</span><br><span class=\"line\">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</span><br><span class=\"line\">| database  | table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |    grant_time     | grantor  |</span><br><span class=\"line\">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</span><br><span class=\"line\">| test      |        |            |         | test_auto       | ROLE            | select     | false         | 1516762810118000  | --       |</span><br><span class=\"line\">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</span><br><span class=\"line\"></span><br><span class=\"line\">## 回收权限</span><br><span class=\"line\">revoke select on database test from role test_auto;</span><br><span class=\"line\"></span><br><span class=\"line\">## 赋予Insert权限</span><br><span class=\"line\">grant insert on database test to role test_auto;</span><br><span class=\"line\">show grant role test_auto;</span><br><span class=\"line\">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</span><br><span class=\"line\">| database  | table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |    grant_time     | grantor  |</span><br><span class=\"line\">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</span><br><span class=\"line\">| test      |        |            |         | test_auto       | ROLE            | insert     | false         | 1516771174805000  | --       |</span><br><span class=\"line\">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</span><br></pre></td></tr></table></figure>\n<p>在只有INSERT权限，测试如下几个在SELECT命令成功，而现在执行失败或者有问题的仍无法成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select * from test_partition;</span><br><span class=\"line\"></span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException No valid privileges</span><br><span class=\"line\"> User test_auto does not have privileges for QUERY</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;Db=test-&gt;Table=test_partition-&gt;Column=dt-&gt;action=select; (state=42000,code=40000)</span><br><span class=\"line\"> </span><br><span class=\"line\">analyze table test_partition partition(dt=20180101) COMPUTE STATISTICS;</span><br><span class=\"line\"></span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException No valid privileges</span><br><span class=\"line\"> User test_auto does not have privileges for QUERY</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;Db=test-&gt;Table=test_partition-&gt;Column=RAW__DATA__SIZE-&gt;action=select; (state=42000,code=40000)</span><br><span class=\"line\"></span><br><span class=\"line\">create table createas_test as select * from test_partition;</span><br><span class=\"line\"></span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException No valid privileges</span><br><span class=\"line\"> User test_auto does not have privileges for CREATETABLE_AS_SELECT</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;Db=test-&gt;Table=test_partition-&gt;Column=dt-&gt;action=select; (state=42000,code=40000)</span><br></pre></td></tr></table></figure>\n<p>插入 <strong>INSERT</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">insert into table test_partition partition(dt=20180102) select &apos;2&apos;,&apos;tom&apos; ; ## 执行成功</span><br></pre></td></tr></table></figure>\n<p>导入文件 <strong>LOAD</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">load data local inpath  &apos;/tmp/hosts&apos; into table test_partition partition(dt=20180103); ## 此时/tmp/hosts文件的所有者是test_auto,但仍旧报错。</span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException No valid privileges</span><br><span class=\"line\"> User test_auto does not have privileges for LOAD</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;URI=file:///tmp/hosts-&gt;action=*; (state=42000,code=40000)</span><br></pre></td></tr></table></figure>\n<p>更新 <strong>UPDATE</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">update  test_pratition set id =3 where id=1 ;</span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException [Error 10294]: Attempt to do update or delete using transaction manager that does not support these operations. (state=42000,code=10294)</span><br></pre></td></tr></table></figure>\n<p>在hive用户下，给用户赋予SELECT 权限测试以上几个失败的命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">grant select on base test from role test_auto;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select * from test_partition;</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| test_partition.id  | test_partition.name  | test_partition.dt  |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">| 1                  | daniel               | 20180101           |</span><br><span class=\"line\">| 2                  | tom                  | 20180102           |</span><br><span class=\"line\">+--------------------+----------------------+--------------------+--+</span><br><span class=\"line\">analyze table test_partition partition(dt=20180101) COMPUTE STATISTICS; ## 执行成功</span><br><span class=\"line\"></span><br><span class=\"line\">create table createas_test as select * from test_partition; </span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException No valid privileges</span><br><span class=\"line\"> User test_auto does not have privileges for CREATETABLE_AS_SELECT</span><br><span class=\"line\"> The required privileges: Server=server1-&gt;Db=test-&gt;action=*; (state=42000,code=40000)</span><br></pre></td></tr></table></figure>\n<h3 id=\"测试UPDATE\"><a href=\"#测试UPDATE\" class=\"headerlink\" title=\"测试UPDATE\"></a>测试UPDATE</h3><p>不支持Update权限的赋值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">revoke select on database test from role test_auto;</span><br><span class=\"line\">revoke insert on database test from role test_auto;</span><br><span class=\"line\"></span><br><span class=\"line\">grant update on database test to role test_auto;</span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException Sentry does not support privilege: Update (state=42000,code=40000)</span><br></pre></td></tr></table></figure>\n<h3 id=\"测试DELETE\"><a href=\"#测试DELETE\" class=\"headerlink\" title=\"测试DELETE\"></a>测试DELETE</h3><p>不支持Delete权限的赋值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">grant delete on database test to role test_auto;</span><br><span class=\"line\">Error: Error while compiling statement: FAILED: SemanticException Sentry does not support privilege: Delete (state=42000,code=40000)</span><br></pre></td></tr></table></figure>\n<h3 id=\"URI\"><a href=\"#URI\" class=\"headerlink\" title=\"URI\"></a>URI</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GRANT ALL ON URI &apos;hdfs://tmp/hosts&apos; to role test_auto; ## 成功</span><br><span class=\"line\">load data  inpath  &apos;/tmp/hosts&apos; into table test_partition partition(dt=20180103); </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">revoke all on uri &apos;hdfs://tmp/hosts&apos; from role test_auto;</span><br><span class=\"line\">GRANT select ON URI &apos;hdfs://tmp/hosts&apos; to role test_auto; ## 成功</span><br><span class=\"line\">load data  inpath  &apos;/tmp/hosts&apos; into table test_partition partition(dt=20180103);  </span><br><span class=\"line\"></span><br><span class=\"line\"> grant all on uri &apos;hdfs:/tmp/&apos; to role test_auto;</span><br><span class=\"line\"></span><br><span class=\"line\"> insert overwrite directory &quot;/user/test_auto/result/test_partition&quot; row format delimited fields terminated by &quot;\\t&quot;   select * from test_partition;</span><br><span class=\"line\"> </span><br><span class=\"line\"> insert overwrite directory &quot;/user/test_auto/result/test_partition&quot;   select * from test_partition;</span><br><span class=\"line\"> </span><br><span class=\"line\"> INSERT OVERWRITE DIRECTORY &apos;/tmp/test_partition&apos; SELECT * FROM test_partition; ## 执行成功</span><br><span class=\"line\"></span><br><span class=\"line\">需要hive能在这个路径下创建文件夹，并且test_auto也有权限、</span><br></pre></td></tr></table></figure>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjp0avnzw0014inamnvelrpbr","category_id":"cjp0avnzy0017inam98kd8ein","_id":"cjp0avo03001iinamvqemgfcw"}],"PostTag":[{"post_id":"cjp0avnzc0006inamdehsh4hp","tag_id":"cjp0avnz90004inamgt0rar5i","_id":"cjp0avnzf0009inami4y4fuzd"},{"post_id":"cjp0avnz20001inamtbjhbohr","tag_id":"cjp0avnz90004inamgt0rar5i","_id":"cjp0avnzg000binamg7k9wje9"},{"post_id":"cjp0avnzd0007inamkwsen9pv","tag_id":"cjp0avnz90004inamgt0rar5i","_id":"cjp0avnzj000einamqvfqs341"},{"post_id":"cjp0avnzf000ainam3gdhmgr9","tag_id":"cjp0avnz90004inamgt0rar5i","_id":"cjp0avnzk000ginam1tbvn8dz"},{"post_id":"cjp0avnz60003inamxxf8dsdi","tag_id":"cjp0avnz90004inamgt0rar5i","_id":"cjp0avnzl000jinam0n1a2rih"},{"post_id":"cjp0avnzj000finam4uxdzjyg","tag_id":"cjp0avnz90004inamgt0rar5i","_id":"cjp0avnzm000linamk3rymja3"},{"post_id":"cjp0avnza0005inamxd9fq06d","tag_id":"cjp0avnz90004inamgt0rar5i","_id":"cjp0avnzn000oinamuzlr3bx1"},{"post_id":"cjp0avnzh000cinamzqsntyfc","tag_id":"cjp0avnzl000iinamn0el4r2l","_id":"cjp0avnzo000qinam2es47d4b"},{"post_id":"cjp0avnzk000hinam9z2k62r4","tag_id":"cjp0avnzn000ninam9441u1w5","_id":"cjp0avnzq000uinamnrtk6hfq"},{"post_id":"cjp0avnzp000tinammpigqg8r","tag_id":"cjp0avnzn000ninam9441u1w5","_id":"cjp0avnzr000winami3hujvya"},{"post_id":"cjp0avnzl000kinam3my8s3cm","tag_id":"cjp0avnzp000sinamm1cvajyf","_id":"cjp0avnzt000zinam5k5d7lzj"},{"post_id":"cjp0avnzm000minamicibc82a","tag_id":"cjp0avnzp000sinamm1cvajyf","_id":"cjp0avo00001binamnqcft9ov"},{"post_id":"cjp0avnzm000minamicibc82a","tag_id":"cjp0avnzu0011inamuaf5dhe6","_id":"cjp0avo01001dinamca8god45"},{"post_id":"cjp0avnzm000minamicibc82a","tag_id":"cjp0avnzw0015inam0j6e9j78","_id":"cjp0avo02001ginamq33ra8xe"},{"post_id":"cjp0avnzy0018inamiwbxifyc","tag_id":"cjp0avnzw0015inam0j6e9j78","_id":"cjp0avo03001jinam1vieoetp"},{"post_id":"cjp0avnzn000pinamp4u04s7f","tag_id":"cjp0avnzp000sinamm1cvajyf","_id":"cjp0avo05001minam0weu7wwz"},{"post_id":"cjp0avnzn000pinamp4u04s7f","tag_id":"cjp0avo01001einamvin8w349","_id":"cjp0avo06001oinamh5l5nocs"},{"post_id":"cjp0avo05001ninamno28tdqn","tag_id":"cjp0avnzu0011inamuaf5dhe6","_id":"cjp0avo06001qinam13lry08r"},{"post_id":"cjp0avnzo000rinamaucitr73","tag_id":"cjp0avo03001kinamwft0bohf","_id":"cjp0avo07001sinamxwuby1yp"},{"post_id":"cjp0avnzo000rinamaucitr73","tag_id":"cjp0avnzp000sinamm1cvajyf","_id":"cjp0avo08001tinamvxoldzcg"},{"post_id":"cjp0avnzq000vinamnsnvs99z","tag_id":"cjp0avnzu0011inamuaf5dhe6","_id":"cjp0avo08001winam1maps5tn"},{"post_id":"cjp0avnzq000vinamnsnvs99z","tag_id":"cjp0avo08001uinam2i91lmo0","_id":"cjp0avo08001xinam47ml7oag"},{"post_id":"cjp0avnzr000yinamlifirviw","tag_id":"cjp0avnzu0011inamuaf5dhe6","_id":"cjp0avo090020inamh92sbt3k"},{"post_id":"cjp0avnzr000yinamlifirviw","tag_id":"cjp0avo08001yinamb4sf5qnv","_id":"cjp0avo090021inamw3wctway"},{"post_id":"cjp0avnzt0010inamm9ywsk94","tag_id":"cjp0avnzu0011inamuaf5dhe6","_id":"cjp0avo0a0024inamxpi2600h"},{"post_id":"cjp0avnzt0010inamm9ywsk94","tag_id":"cjp0avo090022inamipjdd1ad","_id":"cjp0avo0a0025inam5gi05nzo"},{"post_id":"cjp0avnzu0012inamd3n0en5m","tag_id":"cjp0avnzu0011inamuaf5dhe6","_id":"cjp0avo0b0028inamvw255ght"},{"post_id":"cjp0avnzu0012inamd3n0en5m","tag_id":"cjp0avo0a0026inamgcdngsgv","_id":"cjp0avo0b0029inambd5lhdqk"},{"post_id":"cjp0avnzv0013inamkuwqi8g9","tag_id":"cjp0avo0a0027inamnp3sw8dv","_id":"cjp0avo0b002binamh9dlj9cv"},{"post_id":"cjp0avnzw0014inamnvelrpbr","tag_id":"cjp0avo0b002ainamqqebrysl","_id":"cjp0avo0c002dinaml11j7vxh"},{"post_id":"cjp0avnzw0014inamnvelrpbr","tag_id":"cjp0avnzp000sinamm1cvajyf","_id":"cjp0avo0c002einamcv2xk0mc"},{"post_id":"cjp0avnzx0016inamlb6s07hn","tag_id":"cjp0avo0b002cinam34lhhfjr","_id":"cjp0avo0c002ginam1g1q1brh"},{"post_id":"cjp0avnzx0016inamlb6s07hn","tag_id":"cjp0avnzp000sinamm1cvajyf","_id":"cjp0avo0c002hinamjm1zj5cd"},{"post_id":"cjp0avnzz001ainamkzy56o6a","tag_id":"cjp0avo0b002ainamqqebrysl","_id":"cjp0avo0d002jinambffz7yxl"},{"post_id":"cjp0avnzz001ainamkzy56o6a","tag_id":"cjp0avnzp000sinamm1cvajyf","_id":"cjp0avo0d002kinam4sj9mi29"},{"post_id":"cjp0avo00001cinam8po5zbv4","tag_id":"cjp0avo0d002iinamajeavjl1","_id":"cjp0avo0d002minamgct45n04"},{"post_id":"cjp0avo01001finam3t9u4mjz","tag_id":"cjp0avo0d002linamu8sx79kn","_id":"cjp0avo0e002oinam6wq16ycp"},{"post_id":"cjp0avo02001hinam1u0jak39","tag_id":"cjp0avo0d002linamu8sx79kn","_id":"cjp0avo0e002pinamjmwl7mto"}],"Tag":[{"name":"CCAH-131","_id":"cjp0avnz90004inamgt0rar5i"},{"name":"CM","_id":"cjp0avnzl000iinamn0el4r2l"},{"name":"CDH安装","_id":"cjp0avnzn000ninam9441u1w5"},{"name":"CDH","_id":"cjp0avnzp000sinamm1cvajyf"},{"name":"Hive","_id":"cjp0avnzu0011inamuaf5dhe6"},{"name":"Kerberos","_id":"cjp0avnzw0015inam0j6e9j78"},{"name":"User","_id":"cjp0avo01001einamvin8w349"},{"name":"Navigator","_id":"cjp0avo03001kinamwft0bohf"},{"name":"Udf","_id":"cjp0avo08001uinam2i91lmo0"},{"name":"metastore","_id":"cjp0avo08001yinamb4sf5qnv"},{"name":"函数","_id":"cjp0avo090022inamipjdd1ad"},{"name":"Data Type","_id":"cjp0avo0a0026inamgcdngsgv"},{"name":"HBase","_id":"cjp0avo0a0027inamnp3sw8dv"},{"name":"CDH6","_id":"cjp0avo0b002ainamqqebrysl"},{"name":"Parquet","_id":"cjp0avo0b002cinam34lhhfjr"},{"name":"随笔","_id":"cjp0avo0d002iinamajeavjl1"},{"name":"sqoop","_id":"cjp0avo0d002linamu8sx79kn"}]}}