[{"title":"HBase Region Splitting and Merging","date":"2018-11-27T22:01:03.000Z","path":"2018/11/28/HBase-Region-Splitting-and-Merging/","text":"源文：https://hortonworks.com/blog/apache-hbase-region-splitting-and-merging/ For this post, we take a technical deep-dive into one of the core areas of HBase. Specifically, we will look at how Apache HBase distributes load through regions, and manages region splitting. HBase stores rows of data in tables. Tables are split into chunks of rows called “regions”. Those regions are distributed across the cluster, hosted and made available to client processes by the RegionServer process. A region is a continuous range within the key space, meaning all rows in the table that sort between the region’s start key and end key are stored in the same region. Regions are non-overlapping, i.e. a single row key belongs to exactly one region at any point in time. A region is only served by a single region server at any point in time, which is how HBase guarantees strong consistency within a single row#. Together with the -ROOT- and .META. regions, a table’s regions effectively form a 3 level B-Tree for the purposes of locating a row within a table. 在这篇文章中，我们将深入到HBase的一块核心内容。尤其是我们将要看下Apache Hbase怎么通过regions做负载的，怎么管理region的切分。HBase将数据存储在表中。表被切分为数据块即regions。这些regions被分配在整个集群，可以被客户端程序调用RegionServer来使用。一个region就是一个连续的key空间，意味着在一个表中，所有在region的开始key和结束key之间的数据都被存储在一个region里。Regions是不重叠的，例如，在任何一个时间点上一个row key都只属于一个特定的region，一个region只会存在一个region server上。这样HBase就能保证数据的强一致性。算上-ROOT-和.META这2个region，一个表的region可以有效地通过一个3层B-树获取一行数据在一个表中的位置。A Region in turn, consists of many “Stores”, which correspond to column families. A store contains one memstore and zero or more store files. The data for each column family is stored and accessed separately.一个Region反过来，由许多列簇的存储组成。一个列簇的存储包含一个内存存储和零到多个存储文件。数据的每一个列簇都被分开存储和使用。 A table typically consists of many regions, which are in turn hosted by many region servers. Thus, regions are the physical mechanism used to distribute the write and query load across region servers. When a table is first created, HBase, by default, will allocate only one region for the table. This means that initially, all requests will go to a single region server, regardless of the number of region servers. This is the primary reason why initial phases of loading data into an empty table cannot utilize the whole capacity of the cluster.一个表通常由多个region组成，这些region分布在多个region server上。因此，region被分布式的region server在物理上进行读写和查询等操作。当一个表被初始创建时，HBase默认会为其分配一个region。这就意味着，在开始时所有的请求都会发送到一个单独的region servers上，无论还有多少其他的region server。这就在初始加载数据进入一个空表时，不能利用整个集群的性能的主要原因。 PRE-SPLITTING The reason HBase creates only one region for the table is that it cannot possibly know how to create the split points within the row key space. Making such decisions is based highly on the distribution of the keys in your data. Rather than taking a guess and leaving you to deal with the consequences, HBase does provide you with tools to manage this from the client. With a process called pre-splitting, you can create a table with many regions by supplying the split points at the table creation time. Since pre-splitting will ensure that the initial load is more evenly distributed throughout the cluster, you should always consider using it if you know your key distribution beforehand. However, pre-splitting also has a risk of creating regions, that do not truly distribute the load evenly because of data skew, or in the presence of very hot or large rows. If the initial set of region split points is chosen poorly, you may end up with heterogeneous load distribution, which will in turn limit your clusters performance.HBase在初始时只创建一个region的原因是它不知道怎么去创建row key空间的分割点。需要根据数据的分布才能做出这个决定。HBase提供了一个工具帮助你从客户端管理它，而不是随便猜测一个值或者留给你去处理。通过一个叫做pre-splitting的程序，一个可以在创建表的时候通过提供切割点来创建多个region。因为pre-splitting将确保在初始加载数据时会更均匀的分布在整个集群。所以如果你只要你的key的分布情况，就应该尽量的考虑使用它。然而，pre-splitting在创建表时也有一个风险，就是由于数据倾斜或者热冷数据或者某些行的数据非常大使它不能完全的平均分布在集群上。如果初始的region切割点设置的不好，可能会产生各种各样的加载，这可能会影响你集群的性能。 There is no short answer for the optimal number of regions for a given load, but you can start with a lower multiple of the number of region servers as number of splits, then let automated splitting take care of the rest.对于一个load任务使用多少region适合并没有一个很直观的答案，但是你可以在开始时设置较为region server的几倍数，然后随着数据的增加让它自动切分。 One issue with pre-splitting is calculating the split points for the table. You can use the RegionSplitter utility. RegionSplitter creates the split points, by using a pluggable SplitAlgorithm. HexStringSplit and UniformSplit are two predefined algorithms. The former can be used if the row keys have a prefix for hexadecimal strings (like if you are using hashes as prefixes). The latter divides up the key space evenly assuming they are random byte arrays. You can also implement your custom SplitAlgorithm and use it from the RegionSplitter utility. pre-splitting的一个问题就是计算表的切割点。你可以使用RegionSplitter工具。RegionSplitter使用一个可插拔的算法来创建切割点。HexStringSplit和UniformSplit是2个预定义的算法。当row key有一个16进制字符串的前缀时，可以使用HexStringSplit算法。UniformSplit算法假设key是随机的字节数组，然后去切割key空间。你也可以自己写Split算法并通过RegionSplitter工具来使用它。 1$ hbase org.apache.hadoop.hbase.util.RegionSplitter test_table HexStringSplit -c 10 -f f1 -c 10,指定需要的region是10个，-f 指定列簇, 分隔符是“:”. 这个工具将会创建一个有10个region的名为 “test_table”的表。12313/01/18 18:49:32 DEBUG hbase.HRegionInfo: Current INFO from scan results = &#123;NAME =&gt; &apos;test_table,,1358563771069.acc1ad1b7962564fc3a43e5907e8db33.&apos;, STARTKEY =&gt; &apos;&apos;, ENDKEY =&gt; &apos;19999999&apos;, ENCODED =&gt; acc1ad1b7962564fc3a43e5907e8db33,&#125;13/01/18 18:49:32 DEBUG hbase.HRegionInfo: Current INFO from scan results = &#123;NAME =&gt; &apos;test_table,19999999,1358563771096.37ec12df6bd0078f5573565af415c91b.&apos;, STARTKEY =&gt; &apos;19999999&apos;, ENDKEY =&gt; &apos;33333332&apos;, ENCODED =&gt; 37ec12df6bd0078f5573565af415c91b,&#125;... If you have split points at hand, you can also use the HBase shell, to create the table with the desired split points.如果你知道切割点，你也可以使用HBase shell在创建表的时候指定切割点。1hbase(main):015:0&gt; create &apos;test_table&apos;, &apos;f1&apos;, SPLITS=&gt; [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;] or12$ echo -e &quot;anbnc&quot; &gt;/tmp/splitshbase(main):015:0&gt; create &apos;test_table&apos;, &apos;f1&apos;, SPLITSFILE=&gt;&apos;/tmp/splits&apos; For optimum load distribution, you should think about your data model, and key distribution for choosing the correct split algorithm or split points. Regardless of the method you chose to create the table with pre determined number of regions, you can now start loading the data into the table, and see that the load is distributed throughout your cluster. You can let automated splitting take over once data ingest starts, and continuously monitor the total number of regions for the table.为了更好的分布加载数据，你应该思考下你的数据模型和key的分布情况，然后选择合适的分割算法或者分割点。无论你选择哪种方法创建你的表，你可以开始导入数据进入你的表，并查看加载是否分布在你的集群上。当你已经开始导入数据了，你也可以让它自动切割，然后监控表的region的个数。 AUTO SPLITTING Regardless of whether pre-splitting is used or not, once a region gets to a certain limit, it is automatically split into two regions. If you are using HBase 0.94 (which comes with HDP-1.2), you can configure when HBase decides to split a region, and how it calculates the split points via the pluggable RegionSplitPolicy API. There are a couple predefined region split policies: ConstantSizeRegionSplitPolicy, IncreasingToUpperBoundRegionSplitPolicy, and KeyPrefixRegionSplitPolicy.无论是否使用pre-splitting，当一个region达到一定的限制，它会自动的分割成2个region。如果使用HBase0.94 （HDP-1.2），你可以配置什么时候HBase决定去切割一个region，和它怎么通过可插拔RegionSplitPolicy API来计算切割点。已经有一些预定义的切割策略：ConstantSizeRegionSplitPolicy，IncreasingToUpperBoundRegionSplitPolicy和KeyPrefixRegionSplitPilicy。 The first one is the default and only split policy for HBase versions before 0.94. It splits the regions when the total data size for one of the stores (corresponding to a column-family) in the region gets bigger than configured “hbase.hregion.max.filesize”, which has a default value of 10GB. This split policy is ideal in cases, where you are have done pre-splitting, and are interested in getting lower number of regions per region server.ConstantSizeRegionSplitPolicy是默认的切割策略，并且是HBase0.94之前的唯一策略。当一个region的一个store存储的数据大小大于配置的“hbase.hregion.max.filesize”值时，默认10GB，它就会自动的切割分区。这个切割策略是理想的，你已经预分区你的表，并且每个region server不会有太多的region。 The default split policy for HBase 0.94 and trunk is IncreasingToUpperBoundRegionSplitPolicy, which does more aggressive splitting based on the number of regions hosted in the same region server. The split policy uses the max store file size based on Min (R^2 “hbase.hregion.memstore.flush.size”, “hbase.hregion.max.filesize”), where R is the number of regions of the same table hosted on the same regionserver. So for example, with the default memstore flush size of 128MB and the default max store size of 10GB, the first region on the region server will be split just after the first flush at 128MB. As number of regions hosted in the region server increases, it will use increasing split sizes: 512MB, 1152MB, 2GB, 3.2GB, 4.6GB, 6.2GB, etc. After reaching 9 regions, the split size will go beyond the configured “hbase.hregion.max.filesize”, at which point, 10GB split size will be used from then on. For both of these algorithms, regardless of when splitting occurs, the split point used is the rowkey that corresponds to the mid point in the “block index” for the largest store file in the largest store.HBase0.94默认的切割策略变成了IncreasingToUpperBoundRegionSplitPolicy，它会基于每个region server拥有的region个数对数据进行切割。这个切割策略使用最大store文件基于Min (R^2 “hbase.hregion.memstore.flush.size”, “hbase.hregion.max.filesize”)，R是在一个region server上一个表拥有的region个数。例如，默认的内存存储是128MB，默认的最大store大小是10GB，只有在第一个flush达到128MB时，第一个region才会被切割。随着在region server上的region个数的增长，它将会增加分割大小：512MB, 1152MB, 2GB, 3.2GB, 4.6GB, 6.2GB,等待。当达到9个region时，分割大小将会超过参数 “hbase.hregion.max.filesize”，此时，10GB的切割大小将会被开始使用。对于所有的这些算法，无论何时发生拆分，所使用的拆分点都是对应于“块索引”中最大存储文件的中点的rowkey。 KeyPrefixRegionSplitPolicy is a curious addition to the HBase arsenal. You can configure the length of the prefix for your row keys for grouping them, and this split policy ensures that the regions are not split in the middle of a group of rows having the same prefix. If you have set prefixes for your keys, then you can use this split policy to ensure that rows having the same rowkey prefix always end up in the same region. This grouping of records is sometimes referred to as “Entity Groups” or “Row Groups”. This is a key feature when considering use of the “local transactions” (alternative link) feature in your application design.KeyPrefixRegionSplitPolicy是一个很好的补充。你可以配置你用来分组的row key的前缀的长度，这个分割策略可以确保有相同的前缀的一组数据不会被从中间切割。如果你设置了你的key的前缀，你可以使用这个切割策略来确保你所有有相同前缀的数据不会被分割到不同的region中。这个分组是指实例组或者行组。当在你的应用设计考虑使用本地处理时这是一个关键的功能。 You can configure the default split policy to be used by setting the configuration “hbase.regionserver.region.split.policy”, or by configuring the table descriptor. For you brave souls, you can also implement your own custom split policy, and plug that in at table creation time, or by modifying an existing table:你可以使用配置参数“hbase.regionserver.region.split.policy”来配置你的默认切割策略，或者通过配置表的描述信息。你也可以使用你自己的切割策略，并且在你创建表的时候使用它，或者修改已经存在的表。1234HTableDescriptor tableDesc = new HTableDescriptor(&quot;example-table&quot;);tableDesc.setValue(HTableDescriptor.SPLIT_POLICY, AwesomeSplitPolicy.class.getName());//add columns etcadmin.createTable(tableDesc); If you are doing pre-splitting, and want to manually manage region splits, you can also disable region splits, by setting “hbase.hregion.max.filesize” to a high number and setting the split policy to ConstantSizeRegionSplitPolicy. However, you should use a safeguard value of like 100GB, so that regions does not grow beyond a region server’s capabilities. You can consider disabling automated splitting and rely on the initial set of regions from pre-splitting for example, if you are using uniform hashes for your key prefixes, and you can ensure that the read/write load to each region as well as its size is uniform across the regions in the table.如果你正在做pre-splitting，并且向要手动的管理你的region切割，你可以通过设置“hbase.hregion.max.filesize”一个比较大的值和设置切割策略为ConstantSizeRegionSplitPolicy来禁用它。然后，你应该使用一个比较安全的值如100GB，这样region不会增长到超过一个region server的容量。你可以考虑禁用自动切割和通过pre-splitting的初始设置，例如，如果你对你的key的前缀使用统一的hash，你可以确保对每个区域的读/写负载以及表中各个region的大小都是统一的。 FORCED SPLITS HBase also enables clients to force split an online table from the client side. For example, the HBase shell can be used to split all regions of the table, or split a region, optionally by supplying a split point.12hbase(main):024:0&gt; split &apos;b07d0034cbe72cb040ae9cf66300a10c&apos;, &apos;b&apos;0 row(s) in 0.1620 seconds With careful monitoring of your HBase load distribution, if you see that some regions are getting uneven loads, you may consider manually splitting those regions to even-out the load and improve throughput. Another reason why you might want to do manual splits is when you see that the initial splits for the region turns out to be suboptimal, and you have disabled automated splits. That might happen for example, if the data distribution changes over time.HBase也提供通过客户端去强制分割在线的表。例如，可以使用HBase shell去分割表的所有region或者一个region，也可以通过提供分割点去切割。仔细监控你的HBase的加载分布，如果有些region变的不均匀，你需要考虑手分割这些region来提高图吞吐量。为什么你需要手动分割的另外一个原因是当你发现初始的region变的不标准或者你禁用了自分割。例如，你的数据分布随着时间一直在变化。 HOW REGION SPLITS ARE IMPLEMENTED As write requests are handled by the region server, they accumulate in an in-memory storage system called the “memstore”. Once the memstore fills, its content are written to disk as additional store files. This event is called a “memstore flush”. As store files accumulate, the RegionServer will “compact” them into combined, larger files. After each flush or compaction finishes, a region split request is enqueued if the RegionSplitPolicy decides that the region should be split into two. Since all data files in HBase are immutable, when a split happens, the newly created daughter regions will not rewrite all the data into new files. Instead, they will create small sym-link like files, named Reference files, which point to either top or bottom part of the parent store file according to the split point. The reference file will be used just like a regular data file, but only half of the records. The region can only be split if there are no more references to the immutable data files of the parent region. Those reference files are cleaned gradually by compactions, so that the region will stop referring to its parents files, and can be split further.在HBase中，写的需求是由region server来处理的，他们被存储在内存存储系统“memstore”中。当memstore文件满了，它开始将数据写入存储在磁盘的存储文件中。这个过程叫做“memstore flush”。RegionServer将会把这些存储文件合并成大文件。每次flush或者合并完成后，如果RegionSplitPolicy决定这个region应该被分割成2个region，就会触发一个分割的请求。因为在HBase中所有的数据文件都是不可变的，当需要一个分割时，对于一个region的2个子region，他们不是重新将所有的数据写入新的文件，而是会先创建小的类似软连接的叫做引用文件的文件。一个region只有在没有引用指向它的父文件时才可以被分割。这些引用文件会被compaction清除，这样它就不会执行他们的父文件，然后可以被分割。 Although splitting the region is a local decision made at the RegionServer, the split process itself must coordinate with many actors. The RegionServer notifies the Master before and after the split, updates the .META. table so that clients can discover the new daughter regions, and rearranges the directory structure and data files in HDFS. Split is a multi task process. To enable rollback in case of an error, the RegionServer keeps an in-memory journal about the execution state. The steps taken by the RegionServer to execute the split are illustrated by Figure 1. Each step is labeled with its step number. Actions from RegionServers or Master are shown in red, while actions from the clients are show in green.尽管region分割是被在RegionServer本地处理的问题，但是分割过程要考虑很多动作。RegionServer在分割的前后会通知Master去更新.META.表，这样客户端可以发现新的子region和文件夹结构和HDFS的数据文件。分割是有多个任务的程序。为了再错误时能够回滚，RegionServer将执行的状态保存在内存中。RegionServer执行分割的过程如下图。每一步都标注了号码。RegionServer的操作或者Master操作用红色标注，客户端的操作是绿颜色。 RegionServer decides locally to split the region, and prepares the split. As a first step, it creates a znode in zookeeper under /hbase/region-in-transition/region-name in SPLITTING state.RegionServe决定在本地分割region，并且开始准备分割。第一步，它首先在zookeeper的/hbase/region-in-transition/region-name下创建一个SPLITTING状态的znode。 The Master learns about this znode, since it has a watcher for the parent region-in-transition znode.因为Master会监控父级znode region-in-transition，所以Master将获取到这个znode。 RegionServer creates a sub-directory named “.splits” under the parent’s region directory in HDFS.RegionServer在HDFS上的父region目录下创建一个名为“.splits”的子文件夹 RegionServer closes the parent region, forces a flush of the cache and marks the region as offline in its local data structures. At this point, client requests coming to the parent region will throw NotServingRegionException. The client will retry with some backoff.RegionServer关闭父region，强制执行一个缓存的flush并且在它本地的数据结构中标志这个region为offline。此时，客户端发送到父region的请求将会抛出NotServingRegionException，客户端重试一些回退。 RegionServer create the region directories under .splits directory, for daughter regions A and B, and creates necessary data structures. Then it splits the store files, in the sense that it creates two Reference files per store file in the parent region. Those reference files will point to the parent regions files.RegionServer在.splits文件夹下为子regionA和B创建region文件夹并创建需要的数据结构。然后开始分割存储文件，此时它会为每一个存储文件创建2个引用文件指向父region。这些引用文件将会指向父region的文件。 RegionServer creates the actual region directory in HDFS, and moves the reference files for each daughter.RegionServe在HDFS上创建真正的region文件夹并移动引用文件到每一个子region下。 RegionServer sends a Put request to the .META. table, and sets the parent as offline in the .META. table and adds information about daughter regions. At this point, there won’t be individual entries in .META. for the daughters. Clients will see the parent region is split if they scan .META., but won’t know about the daughters until they appear in .META.. Also, if this Put to .META. succeeds, the parent will be effectively split. If the RegionServer fails before this RPC succeeds, Master and the next region server opening the region will clean dirty state about the region split. After the .META. update, though, the region split will be rolled-forward by Master.RegionServer发送一个Put请求到.META.表，并在.META.表中将父region设置为offline，并添加子region的信息。客户端通过扫描.META.会发现父region正在做分割，但是并不知道子region知道在.META.表中有他们的信息。如果这个PUT请求成功了，父region会被分割。如果RegionServer在这个RPC成功之前失败了，Master和下一个访问region的region server将会清除region分割的垃圾状态。在更新.META.表后，Master会继续向前处理region的分割。 RegionServer opens daughters in parallel to accept writes.RegionServer打开子region并接受向其中写入数据。 RegionServer adds the daughters A and B to .META. together with information that it hosts the regions. After this point, clients can discover the new regions, and issue requests to the new region. Clients cache the .META. entries locally, but when they make requests to the region server or .META., their caches will be invalidated, and they will learn about the new regions from .META..RegionServer添加子region A和B和他们所在节点的信息到.META.。在这个操作完成后，客户端可以发现新的region，并发送请求到新的region。客户端本地缓存.META.，但是当他们发送请求到region server或者 .META.，他们的缓存将会验证，这时他们会更新他们的 .META.。 RegionServer updates znode /hbase/region-in-transition/region-name in zookeeper to state SPLIT, so that the master can learn about it. The balancer can freely re-assign the daughter regions to other region servers if it chooses so.RegionServer在zookeeper中更新znode /hbase/region-in-transition/region-name的状态为SPLIT，这样master就获取到这个状态。balancer可以将子region重新分配给其他的region server。 After the split, meta and HDFS will still contain references to the parent region. Those references will be removed when compactions in daughter regions rewrite the data files. Garbage collection tasks in the master periodically checks whether the daughter regions still refer to parents files. If not, the parent region will be removed.在分割后，HDFS仍旧保留这执行父region的引用文件。这些引用在子region重写数据时将会别移除。Master垃圾回收任务也会检查子region是否还有引用文件执行父region，如果没有父region将会被移除。 REGION MERGES Unlike region splitting, HBase at this point does not provide usable tools for merging regions. Although there are HMerge, and Merge tools, they are not very suited for general usage. There currently is no support for online tables, and auto-merging functionality. However, with issues like OnlineMerge, Master initiated automatic region merges, ZK-based Read/Write locks for table operations, we are working to stabilize region splits and enable better support for region merges. Stay tuned! CONCLUSION As you can see, under-the-hood HBase does a lot of housekeeping to manage regions splits and do automated sharding through regions. However, HBase also provides the necessary tools around region management, so that you can manage the splitting process. You can also control precisely when and how region splits are happening via a RegionSplitPolicy.The number of regions in a table, and how those regions are split are crucial factors in understanding, and tuning your HBase cluster load. If you can estimate your key distribution, you should create the table with pre-splitting to get the optimum initial load performance. You can start with a lower multiple of number of region servers as a starting point for initial number of regions, and let automated splitting take over. If you cannot correctly estimate the initial split points, it is better to just create the table with one region, and start some initial load with automated splitting, and use IncreasingToUpperBoundRegionSplitPolicy. However, keep in mind that, the total number of regions will stabilize over time, and the current set of region split points will be determined from the data that the table has received so far. You may want to monitor the load distribution across the regions at all times, and if the load distribution changes over time, use manual splitting, or set more aggressive region split sizes. Lastly, you can try out the upcoming online merge feature and contribute your use case.","comments":true,"categories":[],"tags":[{"name":"HBase","slug":"HBase","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/HBase/"}]},{"title":"Kerberos 认证流程图","date":"2018-10-23T11:29:50.000Z","path":"2018/10/23/Kerberos/","text":"","comments":true,"categories":[],"tags":[{"name":"Kerberos","slug":"Kerberos","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/Kerberos/"}]},{"title":"Cloudera Manager API的简单实用","date":"2018-10-23T10:08:57.000Z","path":"2018/10/23/Cloudera-Manager-API/","text":"参考： https://www.cloudera.com/documentation/enterprise/5-15-x/topics/cm_intro_api.html Alter 告警Alter是从event中获取的。event的元数据 property type 描述 id id (string) 事件的唯一ID. content content (string) 事件内容描述 timeOccurred timeOccurred (dateTime) 事件发生事件 timeReceived timeReceived (dateTime) Cloudera Manager 获取事件的时间. 事件并不是顺时到达的. category category (apiEventCategory) 事件的分类 – 健康的事件，审计事件或者操作时间等 UNKNOWN：未知分类；HEALTH_EVENT：健康事件；LOG_EVENT：日志事件；AUDIT_EVENT：审计事件；ACTIVITY_EVENT：活动事件；HBASE：HBase事件；SYSTEM：系统事件； severity severity (apiEventSeverity) 事件的严重性 UNKNOWN：未知；INFORMATIONAL：状态修改；IMPORTANT：需要注意的事件；CRITICAL：严重，需要立即解决 alert alert (boolean) 事件是否需要晋级 attributes array of attributes/attributes (apiEventAttribute) 属性列表 查看所有事件http://&lt; cloudera manager host &gt;:7180/api/v19/events查看所有警告事件http://&lt; cloudera manager host &gt;:7180/api/v19/events/?query=alert==true查看严重的警告事件http://&lt; cloudera manager host &gt;:7180/api/v19/events/?query=alert==true;severity==CRITICAL查看一个时间区间范围内的严重警告事件http://&lt; cloudera manager host &gt;:7180/api/v19/events/?query=alert==true;severity==critical;timeReceived=ge=2018-10-16T00:00;timeReceived=lt=2018-10-18T00:10查看指定eventid的事件内容http://&lt; cloudera manager host &gt;:7180/api/v19/events/272baa75-0fe9-4fd9-ab27-f4e333fd524 监控报表tsquery Language http://&lt; cloudera manager host &gt;:7180/api/v19/timeseries/?query=select%20*%20where%20roleType=DATANODE http://&lt; cloudera manager host &gt;:7180/api/v19/timeseries/?query=select%20cpu_user_rate%20where%20roleType=DATANODE http://&lt; cloudera manager host &gt;:7180/api/v19/timeseries/?query=select%20await_time,%20await_read_time,%20await_write_time,%20250%20where%20category=disk 如何获取Cloudera Manager已有报表的数据 打开对应报表所在的页面 点击在报表的右上角上的工具选项 选择弹出框的Open in Chart Builder 复制查询语句 在浏览器或其他restful api工具上执行查询。http://&lt; cloudera manager host &gt;:7180/api/v19/timeseries/?query=select%20cpu_percent_across_hosts%20where%20category%20=%20CLUSTER","comments":true,"categories":[],"tags":[{"name":"CM","slug":"CM","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CM/"}]},{"title":"Cloudera Manager High Availability","date":"2018-10-22T03:28:39.000Z","path":"2018/10/22/Cloudera-Manager-High-Availability/","text":"1. 环境准备1.1 环境操作系统：CentOS Linux release 7.3.1611 (Core)JDK：jdk1.8.0_111服务器：53-58 60-61 共8台 1.2 软件下载HAProxy: http://www.haproxy.org/download/1.8/src/haproxy-1.8.13.tar.gz 1.3 架构 1.4 修改主机名称12345678910127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6172.17.1.53 cms1.test.com cms1172.17.1.54 cms2.test.com cms2172.17.1.55 mgmt1.test.com mgmt1172.17.1.56 mgmt2.test.com mgmt2172.17.1.57 nfs.test.com nfs172.17.1.58 cms.test.com cms172.17.1.60 mgmt.test.com mgmt172.17.1.61 dn1.test.com dn1 2 安装前环境准备2.1 创建主和副主机Cloudera Manager Server and Cloudera Management Service Primary host cms1.test.comCloudera Manager Server and Cloudera Management Service Secondary host cms2.test.com 此外，Cloudera建议： 不要在安装CDH的节点安装Cloudera Manager或者Cloudera Management Service，因为这样会使failover的配置变的复杂。并且覆盖失败的域名可能会造成容错和错误检查的问题。 对主副主机都使用相同的主机配置。用来保证故障转移后性能不会降低。 对主副主机使用分开的主机和网络组件。 主机分配 IP DomainName 功能 角色 172.17.1.53 cms1.test.com cloudera manager 主节点 cms,cma 172.17.1.54 cms2.test.com cloudera manager 备份节点 cms,cma 172.17.1.55 mgmt1.test.com cloudera management service 主节点 cmmg,cma 172.17.1.56 mgmt2.test.com cloudera managerment service 备份节点 cmmg,cma 172.17.1.57 nfs.test.com 挂载存储服务器 nfs 172.17.1.58 cms.test.com cm代理服务器 haproxy 172.17.1.60 mgmt.test.com mgmt代理服务器 haproxy 172.17.1.61 dn1.test.com 数据节点 cma,数据库 2.2 安装配置Load Balancer在cms节点上安装HAProxy1yum install -y haproxy 配置haproxy开机启动1systemctl enable haproxy 配置HAProxy在cms上，编辑/etc/haproxy/haproxy.cfg文件，添加需要代理的端口 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214#---------------------------------------------------------------------# Example configuration for a possible web application. See the# full configuration options online.## http://haproxy.1wt.eu/download/1.4/doc/configuration.txt##---------------------------------------------------------------------#---------------------------------------------------------------------# Global settings#---------------------------------------------------------------------global # to have these messages end up in /var/log/haproxy.log you will # need to: # # 1) configure syslog to accept network log events. This is done # by adding the &apos;-r&apos; option to the SYSLOGD_OPTIONS in # /etc/sysconfig/syslog # # 2) configure local2 events to go to the /var/log/haproxy.log # file. A line like the following can be added to # /etc/sysconfig/syslog # # local2.* /var/log/haproxy.log # log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats#---------------------------------------------------------------------# common defaults that all the &apos;listen&apos; and &apos;backend&apos; sections will# use if not designated in their block#---------------------------------------------------------------------defaults mode http log global option httplog option dontlognull option http-server-close #option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000#---------------------------------------------------------------------# main frontend which proxys to the backends#---------------------------------------------------------------------#frontend main *:5000# acl url_static path_beg -i /static /images /javascript /stylesheets# acl url_static path_end -i .jpg .gif .png .css .js## use_backend static if url_static# default_backend app#---------------------------------------------------------------------# static backend for serving up images, stylesheets and such#---------------------------------------------------------------------#backend static# balance roundrobin# server static 127.0.0.1:4331 check#---------------------------------------------------------------------# round robin balancing between the various backends#---------------------------------------------------------------------#backend app# balance roundrobin# server app1 127.0.0.1:5001 check# server app2 127.0.0.1:5002 check# server app3 127.0.0.1:5003 check# server app4 127.0.0.1:5004 check#---------------------------------------------------------------------# stats port#---------------------------------------------------------------------listen stats bind 0.0.0.0:1080 mode http option httplog maxconn 5000 stats refresh 30s stats uri /stats#---------------------------------------------------------------------# Cloudera Manager#---------------------------------------------------------------------listen cmf mode tcp option tcplog bind 0.0.0.0:7180 server cmfhttp1 cms1.test.com:7180 check server cmfhttp2 cms2.test.com:7180 checklisten cmfavro :7182 mode tcp option tcplog server cmfavro1 cms1.test.com:7182 check server cmfavro2 cms2.test.com:7182 check#ssl pass-through, without terminationlisten cmfhttps :7183 mode tcp option tcplog server cmfhttps1 cms1.test.com:7183 check server cmfhttps2 cms2.test.com:7183 checklisten mgmt1 :5678 mode tcp option tcplog server mgmt1a cms1.test.com check server mgmt1b cms2.test.com checklisten mgmt2 :7184 mode tcp option tcplog server mgmt2a cms1.test.com check server mgmt2b cms2.test.com checklisten mgmt3 :7185 mode tcp option tcplog server mgmt3a cms1.test.com check server mgmt3b cms2.test.com checklisten mgmt4 :7186 mode tcp option tcplog server mgmt4a cms1.test.com check server mgmt4b cms2.test.com checklisten mgmt5 :7187 mode tcp option tcplog server mgmt5a cms1.test.com check server mgmt5b cms2.test.com checklisten mgmt6 :8083 mode tcp option tcplog server mgmt6a cms1.test.com check server mgmt6b cms2.test.com checklisten mgmt7 :8084 mode tcp option tcplog server mgmt7a cms1.test.com check server mgmt7b cms2.test.com checklisten mgmt8 :8086 mode tcp option tcplog server mgmt8a cms1.test.com check server mgmt8b cms2.test.com checklisten mgmt9 :8087 mode tcp option tcplog server mgmt9a cms1.test.com check server mgmt9b cms2.test.com checklisten mgmt10 :8091 mode tcp option tcplog server mgmt10a cms1.test.com check server mgmt10b cms2.test.com checklisten mgmt-agent :9000 mode tcp option tcplog server mgmt-agenta cms1.test.com check server mgmt-agentb cms2.test.com checklisten mgmt11 :9994 mode tcp option tcplog server mgmt11a cms1.test.com check server mgmt11b cms2.test.com checklisten mgmt12 :9995 mode tcp option tcplog server mgmt12a cms1.test.com check server mgmt12b cms2.test.com checklisten mgmt13 :9996 mode tcp option tcplog server mgmt13a cms1.test.com check server mgmt13b cms2.test.com checklisten mgmt14 :9997 mode tcp option tcplog server mgmt14a cms1.test.com check server mgmt14b cms2.test.com checklisten mgmt15 :9998 mode tcp option tcplog server mgmt15a cms1.test.com check server mgmt15b cms2.test.com checklisten mgmt16 :9999 mode tcp option tcplog server mgmt16a cms1.test.com check server mgmt16b cms2.test.com checklisten mgmt17 :10101 mode tcp option tcplog server mgmt17a cms1.test.com check server mgmt17b cms2.test.com check 重启haproxy 1systemctl restart haproxy 2.3 安装配置数据库在dn1节点上安装数据库1yum install -y mariadb mariadb-server 启动数据库1systemctl restart mariadb 初始化1/usr/bin/mysql_secure_installation 配置mariadb的主从 参考在主节点上的/etc/my.cnf添加如下配置并重启1234[mariadb]log-binserver_id=1log-basename=master1 创建replication用户12CREATE USER &apos;replication_user&apos;@&apos;%&apos; IDENTIFIED BY &apos;bigs3cret&apos;;GRANT REPLICATION SLAVE ON *.* TO &apos;replication_user&apos;@&apos;%&apos;; 在从节点上修改/etc/my.cnf文件,并重启1234[mariadb]log-binserver_id=2log-basename=slave1 获取主节点的Binary Log在主节点上执行命令锁住所有的表1FLUSH TABLES WITH READ LOCK 执行命令获取主节点的状态，并记住Position的值123456789MariaDB [(none)]&gt; SHOW MASTER STATUS;+------------------+----------+--------------+------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+------------------+----------+--------------+------------------+| mysql-bin.000004 | 495 | | |+------------------+----------+--------------+------------------+1 row in set (0.00 sec)MariaDB [(none)]&gt; 在从节点执行如下命令：12345678CHANGE MASTER TO MASTER_HOST=&apos;cms1.test.com&apos;, MASTER_USER=&apos;replication_user&apos;, MASTER_PASSWORD=&apos;bigs3cret&apos;, MASTER_PORT=3306, MASTER_LOG_FILE=&apos;mysql-bin.000004&apos;, MASTER_LOG_POS=495, MASTER_CONNECT_RETRY=10; 在主节点执行命令1UNLOCK TABLES; 在从节点执行命令启动slave1START SLAVE; 使用命令查询是否配置争取12345678910111213141516171819202122232425262728293031323334353637383940414243MariaDB [(none)]&gt; show slave status \\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: cms1.test.com Master_User: replication_user Master_Port: 3306 Connect_Retry: 10 Master_Log_File: mysql-bin.000004 Read_Master_Log_Pos: 495 Relay_Log_File: slave1-relay-bin.000005 Relay_Log_Pos: 529 Relay_Master_Log_File: mysql-bin.000004 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 495 Relay_Log_Space: 824 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 11 row in set (0.00 sec) Slave_IO_Running 和 Slave_SQL_Running应该是yes 在主节点上执行如下命令来创建数据库和用户12345678910111213141516171819202122232425262728mysql -u root --password=&apos;123456&apos; -e &apos;create database metastore default character set utf8;&apos;mysql -u root --password=&apos;123456&apos; -e &quot;CREATE USER &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;GRANT ALL PRIVILEGES ON metastore. * TO &apos;hive&apos;@&apos;%&apos;;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;amon&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; mysql -u root --password=&apos;123456&apos; -e &apos;create database amon default character set utf8&apos; mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on amon.* to &apos;amon&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;rman&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; mysql -u root --password=&apos;123456&apos; -e &apos;create database rman default character set utf8&apos; mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on rman.* to &apos;rman&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;sentry&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; mysql -u root --password=&apos;123456&apos; -e &apos;create database sentry default character set utf8&apos; mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on sentry.* to &apos;sentry&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;nav&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; mysql -u root --password=&apos;123456&apos; -e &apos;create database nav default character set utf8&apos; mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on nav.* to &apos;nav&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;navms&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; mysql -u root --password=&apos;123456&apos; -e &apos;create database navms default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on navms.* to &apos;navms&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;cm&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; mysql -u root --password=&apos;123456&apos; -e &apos;create database cm default character set utf8&apos; mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on cm.* to &apos;cm&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;oozie&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; mysql -u root --password=&apos;123456&apos; -e &apos;create database oozie default character set utf8&apos; mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on oozie.* to &apos;oozie&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;hue&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot; mysql -u root --password=&apos;123456&apos; -e &apos;create database hue default character set utf8&apos; mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on hue.* to &apos;hue&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;FLUSH PRIVILEGES;&quot; 2.4 安装配置NFS Server在nfs.test.com节点上安装NFS1yum install nfs-utils nfs-utils-lib 启动nfs和rpcbind123systemctl enable nfssystemctl start rpcbindsystemctl start nfs 创建nfs文件目录1mkdir -p /media/cloudera-scm-server 在文件/etc/exports中添加如下的内容12/media/cloudera-scm-server cms1(rw,sync,no_root_squash,no_subtree_check)/media/cloudera-scm-server cms2(rw,sync,no_root_squash,no_subtree_check) 执行mounts1exportfs -a 在cms1和cms2上创建挂载点12rm -rf /var/lib/cloudera-scm-servermkdir -p /var/lib/cloudera-scm-server 安装nfs工具并执行挂载命令其中nfs-utils是在centos7上有效123yum install nfs-utils-lib yum install nfs-utilsmount -t nfs nfs.test.com:/media/cloudera-scm-server /var/lib/cloudera-scm-server 重启rpcbind1systemctl restart rpcbind 修改/etc/fstab文件，使用其永久有效1nfs.test.com:/media/cloudera-scm-server /var/lib/cloudera-scm-server nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0 查看12345678910cms1:~&gt;df -h文件系统 容量 已用 可用 已用% 挂载点/dev/vda2 90G 6.2G 84G 7% /devtmpfs 7.5G 0 7.5G 0% /devtmpfs 7.5G 0 7.5G 0% /dev/shmtmpfs 7.5G 8.4M 7.5G 1% /runtmpfs 7.5G 0 7.5G 0% /sys/fs/cgrouptmpfs 1.5G 0 1.5G 0% /run/user/0/dev/loop0 4.1G 4.1G 0 100% /mntnfs.test.com:/media/cloudera-scm-server 90G 2.1G 88G 3% /var/lib/cloudera-scm-server 123456789cms2:~&gt;df -h文件系统 容量 已用 可用 已用% 挂载点/dev/vda2 90G 2.1G 88G 3% /devtmpfs 7.5G 0 7.5G 0% /devtmpfs 7.5G 0 7.5G 0% /dev/shmtmpfs 7.5G 8.4M 7.5G 1% /runtmpfs 7.5G 0 7.5G 0% /sys/fs/cgrouptmpfs 1.5G 0 1.5G 0% /run/user/0nfs.test.com:/media/cloudera-scm-server 90G 2.1G 88G 3% /var/lib/cloudera-scm-server 3. Cloudera Manager高可用的安装和配置在主节点上安装（cms1）安装cloudera manager server 服务1yum install -y cloudera-manager-server 配置cloudera manager server所使用的数据库1/usr/share/cmf/schema/scm_prepare_database.sh mysql -h dn1.test.com cm cm 123456 启动cloudera manager server1service cloudera-scm-server start 验证：http://CMS1:7180 在代理服务器上检查代理的状态http://cms.test.com:1080/stats并查看http://cms.test.com:7180/cmf/login 是否可用（通过代理的ip访问） HTTP Referer 配置Cloudera推荐禁用HTTP Referer检查，因为它可能会造成一些代理或者load balancer出错。通过如下步骤手动禁用。 在副节点上安装1yum install -y cloudera-manager-server 复制数据库配置文件1scp root@cms1:/etc/cloudera-scm-server/db.properties /etc/cloudera-scm-server/ 关闭开机启动1systemctl disable cloudera-scm-server 如果配置自动故障转移，需要在主节点也禁用自动开机启动 测试故障转移停止cms1.test.com上的cloudera-scm-server1systemctl stop cloudera-scm-server 等待1分钟左右在cms2.test.com上启动cloudera-scm-server1systemctl start cloudera-scm-server 访问http://cm.test.com:1080/stats和http://cm.test.com:7180/cmf/login 更新cloudera manager agens使其使用load balancer (除了cms1，cms2，mgmt1，mgmt2这几个节点)更新配置文件/etc/cloudera-scm-agent/config.ini，修改server_host1server_host = cms.test.com 重启agent1service cloudera-scm-agent restart 4. Cloudera Management Service的高可用安装和配置4.1 为Cloudera Management Service 设置NFS挂载点在NFS服务器 dn3.test.com上执行命令1234567mkdir -p /media/cloudera-host-monitormkdir -p /media/cloudera-scm-agentmkdir -p /media/cloudera-scm-eventservermkdir -p /media/cloudera-scm-headlampmkdir -p /media/cloudera-service-monitormkdir -p /media/cloudera-scm-navigatormkdir -p /media/etc-cloudera-scm-agent 在NFS服务器的/etc/exports 文件中添加如下内容1234567891011121314/media/cloudera-host-monitor mgmt1(rw,sync,no_root_squash,no_subtree_check)/media/cloudera-scm-agent mgmt1(rw,sync,no_root_squash,no_subtree_check)/media/cloudera-scm-eventserver mgmt1(rw,sync,no_root_squash,no_subtree_check)/media/cloudera-scm-headlamp mgmt1(rw,sync,no_root_squash,no_subtree_check)/media/cloudera-service-monitor mgmt1(rw,sync,no_root_squash,no_subtree_check)/media/cloudera-scm-navigator mgmt1(rw,sync,no_root_squash,no_subtree_check)/media/etc-cloudera-scm-agent mgmt1(rw,sync,no_root_squash,no_subtree_check)/media/cloudera-host-monitor mgmt2(rw,sync,no_root_squash,no_subtree_check)/media/cloudera-scm-agent mgmt2(rw,sync,no_root_squash,no_subtree_check)/media/cloudera-scm-eventserver mgmt2(rw,sync,no_root_squash,no_subtree_check)/media/cloudera-scm-headlamp mgmt2(rw,sync,no_root_squash,no_subtree_check)/media/cloudera-service-monitor mgmt2(rw,sync,no_root_squash,no_subtree_check)/media/cloudera-scm-navigator mgmt2(rw,sync,no_root_squash,no_subtree_check)/media/etc-cloudera-scm-agent mgmt2(rw,sync,no_root_squash,no_subtree_check) 在NFS上执行如下命令导出挂载点1exportfs -a 在MGMT1和MGMT2节点上配置，此处仍旧使用cms1.test.com 和cms2.test.com如果是新的节点，则需要安装nfs-utils1yum install nfs-utils 4.2 在MGMT1和MGMT2上创建挂载点1234567mkdir -p /var/lib/cloudera-host-monitormkdir -p /var/lib/cloudera-scm-agentmkdir -p /var/lib/cloudera-scm-eventservermkdir -p /var/lib/cloudera-scm-headlampmkdir -p /var/lib/cloudera-service-monitormkdir -p /var/lib/cloudera-scm-navigatormkdir -p /etc/cloudera-scm-agent 挂载1234567mount -t nfs nfs.test.com:/media/cloudera-host-monitor /var/lib/cloudera-host-monitormount -t nfs nfs.test.com:/media/cloudera-scm-agent /var/lib/cloudera-scm-agentmount -t nfs nfs.test.com:/media/cloudera-scm-eventserver /var/lib/cloudera-scm-eventservermount -t nfs nfs.test.com:/media/cloudera-scm-headlamp /var/lib/cloudera-scm-headlampmount -t nfs nfs.test.com:/media/cloudera-service-monitor /var/lib/cloudera-service-monitormount -t nfs nfs.test.com:/media/cloudera-scm-navigator /var/lib/cloudera-scm-navigatormount -t nfs nfs.test.com:/media/etc-cloudera-scm-agent /etc/cloudera-scm-agent 设置fstab，添加如下内容在/etc/fstab文件中1234567nfs.test.com:/media/cloudera-host-monitor /var/lib/cloudera-host-monitor nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0nfs.test.com:/media/cloudera-scm-agent /var/lib/cloudera-scm-agent nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0nfs.test.com:/media/cloudera-scm-eventserver /var/lib/cloudera-scm-eventserver nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0nfs.test.com:/media/cloudera-scm-headlamp /var/lib/cloudera-scm-headlamp nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0nfs.test.com:/media/cloudera-service-monitor /var/lib/cloudera-service-monitor nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0nfs.test.com:/media/cloudera-scm-navigator /var/lib/cloudera-scm-navigator nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0nfs.test.com:/media/etc-cloudera-scm-agent /etc/cloudera-scm-agent nfs auto,noatime,nolock,intr,tcp,actimeo=1800 0 0 4.3 在主节点上安装 agent登陆MGMT1，安装cloudera-manager-daemons 和cloudera-manager-agent1yum install -y cloudera-manager-daemons cloudera-manager-agent 配置agent的/etc/cloudera-scm-agent/config.ini12server_host = cms.test.comlistening_hostname = mgmt.test.com 编辑/etc/hosts 文件，添加如下内容1172.17.1.60 dn1.test.com 使用ping检查1234mgmt1:~&gt;ping dn1.test.comPING dn1.test.com (172.17.1.60) 56(84) bytes of data.64 bytes from mgmt1.test.com (172.17.1.60): icmp_seq=1 ttl=64 time=0.092 ms64 bytes from mgmt1.test.com (172.17.1.60): icmp_seq=2 ttl=64 time=0.059 ms 修改文件夹的权限123456chown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-eventserverchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-navigatorchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-service-monitorchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-host-monitorchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-agentchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-headlamp 重启 agent1service cloudera-scm-agent restart 4.4 在副节点上安装agent在cm上停掉所有的Cloudera Management Service服务 停掉主节点的cloudera-scm-agent 服务12mgmt1:~&gt;systemctl stop cloudera-scm-agentmgmt1:~&gt; 在副节点上安装cloudera-manager-agent1yum install -y cloudera-manager-agent 在cm上启动所有的cloudera management service Note: 确保cloudera-scm用户UID和GID在Cloudera Management Service的主副节点上是一致的。启动Cloudera Management Service 5. 问题：节点的SELINUX没有设施为disabled","comments":true,"categories":[],"tags":[{"name":"CDH安装","slug":"CDH安装","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CDH安装/"}]},{"title":"CDH-安装准备","date":"2018-09-21T09:15:19.000Z","path":"2018/09/21/CDH-安装准备/","text":"操作系统镜像文件CentOS7/RedHat7https://wiki.centos.org/Download CenOS6/RedHat6https://wiki.centos.org/Download Cloudera Manager 安装文件CentOS6Cloudera Manager5http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/Cloudera Manager6https://archive.cloudera.com/cm6/6.0.0/redhat6/yum/ CentOS7 Cloudera Manager5http://archive.cloudera.com/cm5/redhat/7/x86_64/cm/Cloudera Manager6https://archive.cloudera.com/cm6/redhat/7/x86_64/cm/ CDH 安装文件CentOS6CDH5http://archive.cloudera.com/cdh5/parcels/CDH6https://archive.cloudera.com/cdh6/6.0.0/parcels/ CentOS7CDH5http://archive.cloudera.com/cdh5/parcels/CDH6https://archive.cloudera.com/cdh6/6.0.0/parcels/ KakfaCentOS6http://archive.cloudera.com/kafka/parcels/ CentOS7http://archive.cloudera.com/kafka/parcels/ Spark 2CentOS6https://archive.cloudera.com/spark2/parcels/ CentOS7https://archive.cloudera.com/spark2/parcels/ Oracle JDKJDK1.7http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.htmlJDK1.8http://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html MySQL/MariaDBMySQLhttps://dev.mysql.com/downloads/mysql/ MariaDBhttps://downloads.mariadb.org MySQL JDBChttps://dev.mysql.com/downloads/connector/j/ Hive ODBC JDBC jarJDBChttps://www.cloudera.com/downloads/connectors/hive/jdbc/2-6-1.html ODBChttps://www.cloudera.com/downloads/connectors/hive/odbc/2-5-25.html Impala ODBC JDBC jarJDBChttps://www.cloudera.com/downloads/connectors/impala/jdbc/2-6-3.html ODBChttps://www.cloudera.com/downloads/connectors/impala/odbc/2-5-42.html 报表软件润乾、帆软、Tableau","comments":true,"categories":[],"tags":[{"name":"CDH安装","slug":"CDH安装","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CDH安装/"}]},{"title":"Install CDH6 On CentOS7","date":"2018-09-13T09:58:15.000Z","path":"2018/09/13/Install-CDH6-on-CentOS7/","text":"1. 安装准备1.1. 环境操作系统: CentOS Linux release 7.3.1611 (Core)JDK: 1.8.0_181CM:6.0.0CDH:6.0.0mariadb:5.5 1.2. 下载信息CM6.0下载地址: https://archive.cloudera.com/cm6/6.0.0/redhat7/yum/RPMS/x86_64/allkeys.asc文件下载地址: https://archive.cloudera.com/cm6/6.0.0/CDH6.0下载地址: https://archive.cloudera.com/cdh6/6.0.0/parcels/mariadb下载地址: https://downloads.mariadb.orgPsycopg下载地址: http://initd.org/psycopg/tarballs/PSYCOPG-2-5/ 1.3. 关闭IPv6在文件/etc/sysctl.conf文件中如下内容 12net.ipv6.conf.all.disable_ipv6 = 1net.ipv6.conf.default.disable_ipv6 = 1 并在命令行执行 1sysctl -p 1.4. 关闭交换内存在文件/etc/sysctl.conf文件中如下内容 1vm.swappiness=0 并在命令行执行 1sysctl -p 1.5. 修改主机名在CentOS7上可以直接执行如下命令修改主机名称1234[root@localhost ~]# hostnamectl set-hostname node1.example.com[root@localhost ~]# hostnamenode1.example.com[root@localhost ~]# 修改/etc/hosts 文件 1234567127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.11.224 node1-c7.example.com node1192.168.11.225 node2-c7.example.com node2192.168.11.226 node3-c7.example.com node3192.168.11.227 node4-c7.example.com node4192.168.11.228 node5-c7.example.com node5 1.6. 关闭防火墙在CentOS7上使用如下命令关闭防火墙并禁止开机启动。 12345[root@localhost java]# systemctl stop firewalld[root@localhost java]# systemctl disable firewalldRemoved symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.Removed symlink /etc/systemd/system/basic.target.wants/firewalld.service.[root@localhost java]# 1.7. 关闭SELINUX修改文件/etc/selinux/config 将其中的SELINUX=enforcing修改为disabled1SELINUX=disabled 1.8. 关闭透明大页在命令行执行 123[root@localhost ~]# echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled[root@localhost ~]# echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag[root@localhost ~]# 为使其永久生效，将其添加到文件/ect/rc.local 123touch /var/lock/subsys/localecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defrag 1.9. ntp服务参考其他网上对ntp的配置 1.10. 创建本地repository参考：https://www.cloudera.com/documentation/enterprise/upgrade/topics/cm_ig_create_local_package_repo.html 1.10.1. 安装Httpd和createrepo执行如下命令安装http和createrepo 1[root@localhost ~]# yum install -y httpd createrepo 修改配置文件/etc/httpd/conf/httpd.conf，修改284行附件将 12AddType application/x-compress .ZAddType application/x-gzip .gz .tgz 修改为 12AddType application/x-compress .ZAddType application/x-gzip .gz .tgz .parcel 启动Http 1systemctl start httpd 在/var/www/html/下创建文件夹repos，在repos下创建文件夹cm6和cdh6 12mkdir -p /var/www/html/repos/cm6mkdir -p /var/www/html/repos/cdh6 将下载的文件分别上传到对应的文件夹 12345678910111213/var/www/html/repos/|-- cdh6| |-- CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel| |-- CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha256| `--manifest.json`-- cm6 |-- allkeys.asc |-- cloudera-manager-agent-6.0.0-530873.el7.x86_64.rpm |-- cloudera-manager-daemons-6.0.0-530873.el7.x86_64.rpm |-- cloudera-manager-server-6.0.0-530873.el7.x86_64.rpm |-- cloudera-manager-server-db-2-6.0.0-530873.el7.x86_64.rpm |-- index.html `-- oracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpm 在repos文件夹下执行命令 12createrepo cm6createrepo cdh6 1.11. 安装数据库使用命令行安装mariadb，因为当前版本的Linux镜像中自带的就是mariadb5.5 1yum install -y mariadb-server 启动mariadb 12systemctl start mariadbsystemctl enable mariadb 创建数据库和用户 12345678910111213141516171819202122232425262728mysql -u root --password=&apos;123456&apos; -e &apos;create database metastore default character set utf8;&apos;mysql -u root --password=&apos;123456&apos; -e &quot;CREATE USER &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;GRANT ALL PRIVILEGES ON metastore. * TO &apos;hive&apos;@&apos;%&apos;;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;amon&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database amon default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on amon .* to &apos;amon&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;rman&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database rman default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on rman .* to &apos;rman&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;sentry&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database sentry default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on sentry.* to &apos;sentry&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;nav&apos;@&apos;%&apos;identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database nav default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on nav.* to &apos;nav&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;navms&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database navms default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on navms.* to &apos;navms&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;cm&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database cm default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on cm.* to &apos;cm&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;oozie&apos;@&apos;%&apos;identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database oozie default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on oozie.* to &apos;oozie&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;hue&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database hue default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on hue.* to &apos;hue&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;FLUSH PRIVILEGES; 1.12. 安装JDK进入jdk的rpm包所在的目录执行命令: 1rpm -ivh jdk-8u181-linux-x64.rpm 修改/etc/profile文件,在最后添加如下内容: 12export JAVA_HOME=/usr/java/latestexport PATH=$JAVA_HOME/bin:$PATH 1.13. 安装psycopg安装依赖包 1yum install -y gcc postgresql postgresql-server postgresql-devel 将之前下载的psycopg文件解压并进入解压的psycopg文件夹执行命令 12python setup.py buildpython setup.py install 2. 安装Cloudera Manager使用yum 命令安装cloudera manager 1yum install -y cloudera-manager-server 安装完成后，配置cm数据库。 12345678[root@node1 java]# /opt/cloudera/cm/schema/scm_prepare_database.sh mysql cm cm 123456JAVA_HOME=/usr/java/latestVerifying that we can write to /etc/cloudera-scm-serverCreating SCM configuration file in /etc/cloudera-scm-serverExecuting: /usr/java/latest/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/java/postgresql-connector-java.jar:/opt/cloudera/cm/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.[ main] DbCommandExecutor INFO Successfully connected to database.All done, your SCM database is configured correctly![root@node1 java]# 3.页面安装CDH打开网页http://cloudera manager host:7180,用户名和密码都是admin同意License选择安装的版本,在CDH6中同CDH5一样,安装的时候可以进行3个版本的选择,Cloudera Express-免费版;Cloudera Enterprise Trial-企业60天试用版; Cloudera Enterprise-企业版.此处选择60天试用版本.进入集群安装界面在CDH6的版本中,建议用户启用TLS,在安装的过程中会出现如下界面,给出启用TLS的步骤,如果需要启用可以按照其步骤进行操作,如果不需要则直接跳过.填入需要安装的主机IP地址在Custom Repository的输入框中填入之前创建的本地CM的yum源地址在CDH and other software的More Options的选项中填入CDH的yum源的地址，当CDH Version自动选择的CDH-6.0才继续下一步如果安装过了JDK，此处不勾选。填入root用户的密码进入安装cloudera agent的步骤进入分发parcels的步骤主机检查，尽量修复所有的警告问题。选择安装的方式,不同的安装方式默认安装的CDH组件不同.配置所有组件需要的数据库连接信息启动安装完成","comments":true,"categories":[],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CDH/"},{"name":"CDH6","slug":"CDH6","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CDH6/"}]},{"title":"在CentOS6上安装CDH6","date":"2018-09-10T01:41:00.000Z","path":"2018/09/10/Install-CDH6-On-CentOS6/","text":"1. 安装准备1.1. 环境操作系统: CentOS release 6.9 (Final)JDK: 1.8.0_121CM:6.0.0CDH:6.0.0MySQL:5.5.59 1.2. 下载信息CM6.0下载地址: https://archive.cloudera.com/cm6/6.0.0/redhat6/yum/RPMS/x86_64/allkeys.asc文件下载地址: https://archive.cloudera.com/cm6/6.0.0/CDH6.0下载地址: https://archive.cloudera.com/cdh6/6.0.0/parcels/MySQL下载地址: https://dev.mysql.com/downloads/mysql/5.5.html#downloadsPython27下载地址: https://www.python.org/ftp/python/2.7.13/Psycopg下载地址: http://initd.org/psycopg/tarballs/PSYCOPG-2-5/ 1.3. 修改主机名执行如下命令对其内容进行修改vim /etc/sysconfig/network做类型如下的修改 12NETWORKING=yesHOSTNAME=node1-c6.example.com 并在命令行执行 1hostname node1-c6.example.com 修改/etc/hosts文件,内容如下: 1234567127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.11.234 node1-c6.example.com node1-c6192.168.11.235 node2-c6.example.com node2-c6192.168.11.236 node3-c6.example.com node3-c6192.168.11.237 node4-c6.example.com node4-c6192.168.11.238 node5-c6.example.com node5-c6 1.4. 关闭防火墙执行命令:service iptables stop并禁止开机启动chkconfig iptables off 1.5. 安装JDK进入jdk的rpm包所在的目录执行命令: 1rpm -ivh jdk-8u121-linux-x64.rpm 修改/etc/profile文件,在最后添加如下内容: 12export JAVA_HOME=/usr/java/latestexport PATH=$JAVA_HOME/bin:$PATH 1.6. 关闭交换内存在文件/etc/sysctl.conf文件中添加如下内容:vm.swappiness=0并执行命令使其生效sysctl -p 1.7. 关闭Selinux修改文件/etc/selinux/config,将其中的SELINUX=enforcing修改为disabled注:此操作需要重启生效,可以使用命令setenforce 1 使其临时生效 1.8. 关闭透明大页在命令行执行如下命令:echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/defrag此命令只是临时生效,如想永久生效,可将其添加到文件/etc/rc.local中去. 1.9. 创建本地yum源仓库 安装httpd服务执行命令 yum install -y httpd 来安装此服务 修改配置文件在配置文件/etc/httpd/conf/httpd.conf的779行附件将如下内容修改为 12AddType application/x-compress .Z AddType application/x-gzip .gz .tgz 修改后: 12AddType application/x-compress .ZAddType application/x-gzip .gz .tgz .parcel 启动httpd 服务service httpd start 将下载的cm和cdh文件分别放置在/var/www/html/repos/cm6/和/var/www/html/repos/cdh6/下,格式类型如下: repos├── cdh6│ ├── 6.0.0│ │ └── parcels│ │ ├── CDH-6.0.0-1.cdh6.0.0.p0.537114-el6.parcel│ │ ├── CDH-6.0.0-1.cdh6.0.0.p0.537114-el6.parcel.sha256│ │ ├── index.html│ │ └── manifest.json└── cm6 ├── 6.0.0 │ ├── cloudera-manager-agent-6.0.0-530873.el6.x86_64.rpm │ ├── cloudera-manager-daemons-6.0.0-530873.el6.x86_64.rpm │ ├── cloudera-manager-server-6.0.0-530873.el6.x86_64.rpm │ ├── cloudera-manager-server-db-2-6.0.0-530873.el6.x86_64.rpm │ ├── index.html │ └── oracle-j2sdk1.8-1.8.0+update141-1.x86_64.rpm ├── allkeys.asc └── repomd.xml 安装createrepo软件yum install -y createrepo 在/var/www/html/repos下执行命令createrepo cm6createrepo cdh6 在浏览器查看cm6: http://192.168.11.234/repos/cm6/6.0.0/cdh6: http://192.168.11.234/repos/cdh6/6.0.0/parcels/ 1.10. MySQL 安装 卸载操作系统上已经存在的mysql-libs文件先用命令查询rpm -qa | grep mysql-libs,查出rpm文件,再使用如下命令进行卸载 1rpm -e mysql-libs-5.1.73-8.el6_8.x86_64 安装MySQL的client,server等rpm包 1234rpm -ivh MySQL-shared-5.5.59-1.el6.x86_64.rpm rpm -ivh MySQL-shared-compat-5.5.59-1.el6.x86_64.rpm rpm -ivh MySQL-client-5.5.59-1.el6.x86_64.rpm rpm -ivh MySQL-server-5.5.59-1.el6.x86_64.rpm 配置执行初始化脚本/usr/bin/mysql_secure_installation,添加root用户的密码和移除root用户的远程登录权限 启动mysqld并设置为开机启动 12service mysqld startchkconfig mysqld on 创建用户和数据库 12345678910111213141516171819202122232425262728mysql -u root --password=&apos;123456&apos; -e &apos;create database metastore default character set utf8;&apos;mysql -u root --password=&apos;123456&apos; -e &quot;CREATE USER &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;GRANT ALL PRIVILEGES ON metastore. * TO &apos;hive&apos;@&apos;%&apos;;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;amon&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database amon default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on amon .* to &apos;amon&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;rman&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database rman default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on rman .* to &apos;rman&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;sentry&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database sentry default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on sentry.* to &apos;sentry&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;nav&apos;@&apos;%&apos;identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database nav default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on nav.* to &apos;nav&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;navms&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database navms default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on navms.* to &apos;navms&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;cm&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database cm default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on cm.* to &apos;cm&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;oozie&apos;@&apos;%&apos;identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database oozie default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on oozie.* to &apos;oozie&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;create user &apos;hue&apos;@&apos;%&apos; identified by &apos;123456&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &apos;create database hue default character set utf8&apos;mysql -u root --password=&apos;123456&apos; -e &quot;grant all privileges on hue.* to &apos;hue&apos;@&apos;%&apos;&quot;mysql -u root --password=&apos;123456&apos; -e &quot;FLUSH PRIVILEGES;&quot; 1.11. 升级Python因为CDH6需要python2.7,而centos6默认安装的是python2.6如果服务器可以联网,则直接执行如下命令: 123yum install centos-release-scl -y yum install scl-utils -y yum install python27 -y 并执行如下语句,且将其加入环境变量中去 1source /opt/rh/python27/enable 如果不能联网,则需要手动进行升级下载Python文件并解压 1tar -zxvf Python-2.7.13.tgz 将解压的文件夹移动到/usr/local下,并重命名为python27 1mv Python-2.7.13 /usr/local/python27 编译安装 123./configure --prefix=/usr/local/python27makemake install 将系统默认的python,从2.6的指向2.7的版本 12mv /usr/bin/python /usr/bin/python_oldln -s /usr/local/python27/python2.7 /usr/bin/python 测试是否修改正确 1python -v 1.12. 安装psycopg安装依赖包 1yum install -y gcc postgresql postgresql-server postgresql-devel 将之前下载的psycopg文件解压并进入解压的psycopg文件夹执行命令 12python setup.py buildpython setup.py install 1.13. 配置ntp参考其他网上对ntp的配置 2. Cloudera Manager安装2.1. 配置CM的yum源在要安装Cloudera Manager的主机上的/etc/yum.repos.d/下,创建文件cloudera-manager.local.repo,并在其中添加如下内容: 1234[cm-local]name = cm-localbaseurl = http://192.168.11.234/repos/cm6/gpgcheck = 0 其中的IP需要做对应的修改 2.2. 安装执行命令: 1yum install -y cloudera-manager-server 2.3. 配置数据库在命令行执行: 1/opt/cloudera/cm/schema/scm_database_functions.sh mysql cm cm 123456 其中,mysql是数据库类型,第一个cm是数据库名称,第二个cm是用户名,123456是密码. 2.4. 启动cloudera manager server执行命令 1service cloudera-scm-server start 并设置为开机启动 1chkconfig cloudera-scm-server on 2.5. 检查执行命令: 1tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log 监控log,当log出现如下内容时,表示启动成功,可以进入下一步操作. 3. CDH安装登录http://cloudera manager server:7180,用户名和密码都是admin同意License协议选择安装的版本,在CDH6中同CDH5一样,安装的时候可以进行3个版本的选择,Cloudera Express-免费版;Cloudera Enterprise Trial-企业60天试用版; Cloudera Enterprise-企业版.此处选择60天试用版本.进入添加集群的安装界面在CDH6的版本中,建议用户启用TLS,在安装的过程中会出现如下界面,给出启用TLS的步骤,如果需要启用可以按照其步骤进行操作,如果不需要则直接跳过.在主机搜索界面,选择要添加的主机,可以是ip也可以是域名.在下一页面中,在CDH and other software选项中,点击more options,在Remote Parcel Repository URLs的配置项中,将默认的配置都删除,添加前面配置的本地源,如下图所示:在Cloudera Manager Agent的 Repository Location选项中选择Custom Repository,并将之前配置的本地cm的源地址填入其中如果自己手动安装过JDK,此处可以不用勾选.填入用户名和密码或者使用公钥文件 进入安装步骤如果出现类型如下错误,说明在cm的yum源下少了allkeys.asc文件进入分发CDH和激活界面在主机检查界面,尽量将所有的红色或者黄色的问题都修复掉,修复后再重新运行主机检查,所有的检查都通过后,再进入下一步 选择安装的方式,不同的安装方式默认安装的CDH组件不同.配置所有组件需要的数据库连接信息 启动安装完成点击完成,会自动跳转到集群的home界面.如下图 4. 验证4.1. HDFS验证执行如下命令,进行验证 12export HADOOP_USER_NAME=hdfshdfs dfs -ls / 4.2. Hive验证123export HADOOP_USER_NAME=hdfsbeeline -u &quot;jdbc:hive2://node1-c6.example.com:10000/default&quot;show databases; 4.3. Impala验证12impala-shell -i node3-c6.example.comshow databases; 4.4. HBase验证123export HADOOP_USER_NAME=hbasehbase shelllist_namespace","comments":true,"categories":[{"name":"CDH安装","slug":"CDH安装","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/categories/CDH安装/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CDH/"},{"name":"CDH6","slug":"CDH6","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CDH6/"}]},{"title":"Test hive-overwrite and delete-target-dir in sqoop","date":"2018-05-16T06:15:39.000Z","path":"2018/05/16/Test-hive-overwrite-and-delete-target-dir-in-sqoop/","text":"测试Sqoop 中hive-overwrite和delete-target-dir是否冲突 指定 –hive-import –hive-table –target-dir –delete-target-dir ,并且target-dir和hive表所在的路径一致。 1sqoop import --hive-import --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --table orders --hive-table orders --target-dir /user/hive/warehouse/orders/ --delete-target-dir --hive-overwrite 指定 –hive-import –hive-table –target-dir –delete-target-dir ,并且target-dir和hive表所在的路径不一致。 1sqoop import --hive-import --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --table orders --hive-table orders --target-dir /user/hive/warehouse/order-1/ --delete-target-dir --hive-overwrite 从结果来看，数据没有导入到—target-dir 指定的HDFS路径上，而是导入到表中去了。 如果不指定target-dir和—delete-target-dir 1sqoop import --hive-import --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --table orders --hive-table orders --hive-overwrite 调整下 –hive-overwrite的位置 1sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --table orders --hive-table orders 原因是默认在当前用户的hdfs home目录下创建一个和表名一致的文件夹，先手动删除这个文件夹然后再执行命令，在执行的Log中有如下的内容，因为不是使用hive进行数据导入的。 检查结果 再次执行，即使没有delete-target-dir 仍能执行成功，并且在执行的过程中，发现确实创建了 /user/training/orders文件夹，然后在任务执行完成后将文件移动到表的路径下，如下面第二张图所示。 测试-query的情况 1sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table orders 没有指定—target-dir 报如下错误 1sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table orders –target-dir /user/training/orders/ 必须指定split-by 1sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table orders --target-dir /user/training/orders-1/ --split-by order_id 在导入的过程中在target-dir生成临时文件，完成后将文件移动到表的路径下。所以此时不需要—delete-target-dir 如果即使用–hive-table –hive-overwrite 、–target-dir、 –delete-target-dir 并且 target-dir和表的路径一致。 1sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table orders --target-dir /user/hive/warehouse/orders/ --split-by order_id --delete-target-dir 报如下错误 1sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table test.orders --target-dir /user/hive/warehouse/test.db/orders/ --split-by order_id --delete-target-dir 即使是加上db name也是一样的错误。 使用如下格式的代码导入也是一样的错误，但是如果是Oracle数据库，则可以执行成功但是没有数据。（暂时无法解释） 1234567891011121314sqoop import -D \\mapreduce.job.queuename=hdfs \\ --hive-import \\ --hive-overwrite \\--connect &quot;jdbc:mysql://lion:3306/retail_db&quot; \\--username retaildba --password retaildba \\ --fetch-size 5000 \\--query &quot;select * from orders WHERE \\$CONDITIONS&quot; \\ --target-dir &apos;/user/hive/warehouse/test.db/orders/&apos; \\ --delete-target-dir \\ --hive-table test.orders \\ --hive-delims-replacement &apos; &apos; \\ --null-string &apos;\\\\N&apos; --null-non-string &apos;\\\\N&apos;\\ --split-by order_id \\ 结论： –target-dir 在有–hive-import的情况下只是一个临时的存储文件夹，而且不要和表的路径设置一样。 –hive-overwrite 首先创建一个表test.order执行下面的sqoop 1sqoop import --hive-import --hive-overwrite --connect &quot;jdbc:mysql://lion:3306/retail_db&quot; --username retaildba --password retaildba --query &quot;select * from orders WHERE \\$CONDITIONS&quot; --hive-table test.orders --target-dir /user/hive/orders/ --split-by order_id --delete-target-dir 再次查询表结构 结论：不要轻易使用 –hive-overwrite","comments":true,"categories":[],"tags":[{"name":"sqoop","slug":"sqoop","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/sqoop/"}]},{"title":"Hive 复杂数据类型","date":"2018-05-13T07:08:12.000Z","path":"2018/05/13/Hive-复杂数据类型/","text":"https://community.hortonworks.com/questions/22262/insert-values-in-array-data-type-hive.html 1 ) INSERT INTO table test_array VALUES (1,array(&apos;a&apos;,&apos;b&apos;)); Error: Error while compiling statement: FAILED: SemanticException [Error 10293]: Unable to create temp file for insert values Expression of type TOK_FUNCTION not supported in insert/values (state=42000,code=10293)2 ) INSERT INTO test_array (col2) VALUES (array(&apos;a&apos;,&apos;b&apos;)) from dummy limit 1; Error: Error while compiling statement: FAILED: ParseException line 1:54 missing EOF at ‘from’ near ‘)’ (state=42000,code=40000) create table test_array_type( arr array&lt;string&gt; ); create table dummy(a string); insert into table dummy values (&apos;a&apos;); INSERT INTO table test_array_type SELECT array(&apos;a&apos;,&apos;b&apos;) from dummy; SELECT * from test_array_type; select arr[1] from test_array_type; select explode(arr) from test_array_type; select * from (select explode(arr) from test_array_type) s join (select &quot;AA&quot;)t on 1 =1 ; create table test_map_type( map1 map&lt;int,string&gt; ); INSERT INTO table test_map_type SELECT map(1,&apos;b&apos;,2,&apos;c&apos;) from dummy; select * from test_map_type; select map1,map_keys(map1),map_values(map1),map1[2] from test_map_type; create table test_struct_type( map1 struct&lt;col1:int,col2:string, col3:int&gt; ); INSERT INTO table test_struct_type SELECT struct(1,&apos;b&apos;,2) from dummy; select * from test_struct_type; create table test_arr_map_type( map1 array&lt;map&lt;int,string&gt;&gt; ); INSERT INTO table test_arr_map_type SELECT array(map(1,&apos;b&apos;,2,&apos;c&apos;),map(3,&apos;e&apos;)) from dummy; select * from test_arr_map_type; select map1,map_keys(map1[0]),map_keys(map1[1]),map1[1][3] from test_arr_map_type;","comments":true,"categories":[],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/Hive/"},{"name":"Data Type","slug":"Data-Type","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/Data-Type/"}]},{"title":"PARQUET FILE FORMAT","date":"2018-05-13T03:59:26.000Z","path":"2018/05/13/Parquet-File-Format/","text":"PARQUET FILE FORMAT来源 https://www.cloudera.com/documentation/enterprise/latest/topics/impala_parquet.html#parquet_ddl Parquet是一个列式存储文件格式，可用于Hadoop生态系统的各个组件之上。 列式存储：对单独一个列的查询可以只查询数据的一部分数据。 灵活的压缩方式：数据可以使用多种压缩方式进行压缩 创新的编码方案：相同的、相似的或相关的数据值的序列可以以节省磁盘空间和内存的方式表示。 大文件存储:Parquet的文件存储格式就是为大文件而设计的，文件范围从M-G。 Parquet文件的压缩对于大多数CDH的组件，Parquet文件默认是没有压缩的，CDH建议启用压缩以减少硬盘的使用和提高文件的读写效率在读取压缩格式的Parquet文件时，你不需要指定压缩类型，但是当你写一个压缩的Parquet文件时，你必须指定压缩格式。 HBase使用ParquetTBD Hive中使用Parquet表使用如下的命令创建一个Parquet格式的Hive表。 1CREATE TABLE parquet_table_name (x INT, y STRING) STORED AS PARQUET; 当你创建完成Parquet格式的hive表， 你可以使用Impala或者Spark去访问它。 如果数据文件是由外部导入的，你也可以创建外部表123456create external table parquet_table_name (x INT, y STRING)ROW FORMAT SERDE &apos;parquet.hive.serde.ParquetHiveSerDe&apos;STORED ASINPUTFORMAT &quot;parquet.hive.DeprecatedParquetInputFormat&quot;OUTPUTFORMAT &quot;parquet.hive.DeprecatedParquetOutputFormat&quot;LOCATION &apos;/test-warehouse/tinytable&apos;; 写入数据时，设置压缩方式。12set parquet.compression=GZIP;INSERT OVERWRITE TABLE tinytable SELECT * FROM texttable; 支持的压缩方式有UNCOMPRESSED、GZIP和SNAPPY。 Impala中使用Parquet表Impala只要在创建表的时候使用STORED AS PARQUET关键字指定存储格式，就能在插入和读写时使用它了。 create table parquet_table (x int, y string) stored as parquet; insert into parquet_table select x, y from some_other_table; Inserted 50000000 rows in 33.52s select y from parquet_table where x between 70 and 100; 使用这种方法创建的表，可以使用Impala或者Hive进行查询。在Impala2.0 默认Parquet文件的大小是256M，低版本是1G。避免使用INSERT … VALUES这样的语法或者使用颗粒度比较大的字段进行分区。那样会降低Parquet的性能。Impala的数据插入是内存密集型操作，因为每个数据文件都需要一些内存区域来保存数据。有时这样的操作可能会超过HDFS同时打开文件的限制，在这种情况下，可以考虑将插入操作拆分为每个分区一insert语句。如果使用Sqoop将RDBMS的数据转换为Parquet文件，请注意 DATE, DATETIME, or TIMESTAMP类型的列，Parquet底层使用INT64表示这些值，Parquet使用毫秒表示这些值，但是Impala编译BIGINT作为秒的为单位的时间，因此如果你有BIGINT类型的Time类型，在转换为Parquet文件时需要除以1000 。 Parquet文件结构使用parquet-tools 查看Parquet文件的内容，在CDH中包含这个命令。cat: 使用标准输出Parquet格式的文件head: 输出文件的前几行schema: 输出文件的schemameta: 输出文件的元数据信息，包括key-value属性，压缩率，编码，压缩方式和行组信息。dump: 输出所有的数据和元数据信息。 Parquet表查询优化Parquet表的查询依赖需要查询的列的个数和where条件。Parquet将数据根据固定块的大小切分成大的文件。例如：下面的命令就是一个高效的查询 select avg(income) from census_data where state = &apos;CA&apos;; 这个查询语句只处理了2列数据，如果表是根据state进行分区的，那将会更加高效，因为它只需要读取分区‘CA’文件夹下文件的部分数据。下面的命令就比较低效 select * from census_data; 如果对表进行大量数据的插入后，执行 COMPUTE STATS 更新meta信息。https://www.cloudera.com/documentation/enterprise/latest/topics/impala_compute_stats.html#compute_stats 12345678910COMPUTE STATS [db_name.]table_nameCOMPUTE INCREMENTAL STATS [db_name.]table_name [PARTITION (partition_spec)]partition_spec ::= partition_col=constant_valuepartition_spec ::= simple_partition_spec | complex_partition_specsimple_partition_spec ::= partition_col=constant_valuecomplex_partition_spec ::= comparison_expression_on_partition_col 注：对于一个表，不要将 COMPUTE STATS 和COMPUTE INCREMENTAL STATS 混合使用，如果想要切换，需要先使用 DROP STATS 和 DROP INCREMENTAL STATS。对于一个比较大的表需要 400 bytes的metedata信息。如果所有表的metedata超过了2G，可能会失败几次。 优化查询的第一步就是对所有的表执行COMPUTE STATS。或者在out-of-memory错误的时候 准确的统计数据有助于Impala构建一个高效的查询计划，用于查询、提高性能和减少内存使用。 准确的统计数据帮助Impala将工作有效地分配到Parquet表中，提高性能和减少内存使用。 准确的统计数据有助于Impala估计每个查询所需的内存，当您使用资源管理特性时，例如权限控制和YARN资源管理框架，这一点非常重要。统计数据帮助Impala实现高并发性，充分利用可用内存，并避免与其他Hadoop组件的工作负载争用。 在CDH 5.10 / Impala 2.8 或者更高的版本, 当您运行计算统计数据或在Parquet表上计算增量统计语句时，Impala会自动应用查询选项设置MT_DOP = 4，以增加在此cpu密集型操作期间内节点并行度的增加。 在CDH 5.10 / Impala 2.8 或者更高的版本,可以使用如下的命令计算多个分区的信息 compute incremental stats int_partitions partition (x &lt; 100); 查询表的状态 show table stats t1; 查询列的状态 show column stats t1; Impala支持的文件格式https://www.cloudera.com/documentation/enterprise/latest/topics/impala_file_formats.html#file_formats 文件类型 结构 压缩方式 Impala能否创建 Impala能否插入 Parquet 有结构的 Snappy, gzip; 默认 Snappy 能 是: 创建表, 插入, Load数据和查询 Text 无结构的 LZO, gzip, bzip2, Snappy 是. 在创建表的时候不带STORED AS 语法, 默认文件格式是没有压缩的text, 使用ASCII 0x01 做分隔符 (通常点表Ctrl-A). 是: 创建表, 插入, Load数据和查询. 如果使用LZO压缩, 你必须在hive里面创建表和load数据. 如果使用了其他的压缩翻身,你必须load数据通过LOAD DATA, Hive,或者手动在 HDFS上操作. Avro 有结构的 Snappy, gzip, deflate, bzip2 是, 在Impala 1.4.0和更高的版本. 之前的版本使用hive创建表 不能, 导入数据时，使用正确格式的数据文件加载数据，或者在Hive中使用INSERT，然后在Impala中使用REFRESH table_name。 RCFile 有结构的 Snappy, gzip, deflate, bzip2 是 不能, 导入数据时，使用正确格式的数据文件加载数据，或者在Hive中使用INSERT，然后在Impala中使用REFRESH table_name。 SequenceFile 有结构的 Snappy, gzip, deflate, bzip2 是 不能,导入数据时，使用正确格式的数据文件加载数据，或者在Hive中使用INSERT，然后在Impala中使用REFRESH table_name。 Impala 只支持以上的文件格式，尤其要指出，Impala不支持ORC格式的文件。 Impala 支持以下的压缩格式 Snappy. 因为其压缩率和解压速度是优先被推荐的格式 Snappy 压缩速度非常快,但是gzip更节省空间. 在 Impala 2.0 和更高的版本支持text使用这种压缩方式。 Gzip. 当需要更高level的压缩时，Gzip是被推荐的 (以为其更节省空间). 在 Impala 2.0 和更高的版本支持text使用这种压缩方式 Deflate. text 文件不支持使用此种压缩方式. Bzip2. 在 Impala 2.0 和更高的版本支持text使用这种压缩方式 LZO, 只能为text文件使用. Impala 只能查询 LZO压缩的 text表,但是还不能创建和插入; 在Hive中去创建表和插入数据。 为一个表选择文件格式 如果您正在处理的文件格式是已经被支持的，请使用与实际的Impala表相同的格式。如果原始格式不产生可接受的查询性能或资源使用情况，可以考虑使用不同的文件格式或压缩特性创建一个新的Impala表，并使用INSERT语句将数据复制到新表进行一次性转换。根据文件格式，您可以在impala - shell中或在Hive中运行INSERT语句。 许多不同的工具都可以很方便的生产文本文件，而且是便于验证和调试的。这些特性就是为什么文本是Impala创建表语句的默认格式。当性能和资源使用是主要考虑因素时，使用另一种文件格式，并考虑使用压缩。一个典型的工作流可能包括将数据复制到一个Impala表中，将CSV或TSV文件复制到适当的数据目录中，然后使用insert…select语法转换到适当的表中 如果您的体系结构需要在内存中存储数据，那么不要压缩数据。由于数据不需要从磁盘移动，所以没有I/O成本，但是要解压数据，需要消耗CPU成本。 Impala数据类型https://www.cloudera.com/documentation/enterprise/latest/topics/impala_datatypes.htmlNote: 当前Impala只支持标量的数据类型，不支持复合和嵌套类型。 ARRAY类型：语法 column_name ARRAY &lt; type &gt; type ::= primitive_type | complex_type","comments":true,"categories":[],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CDH/"},{"name":"Parquet","slug":"Parquet","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/Parquet/"}]},{"title":"Cloudera Navigator","date":"2018-05-13T03:56:26.000Z","path":"2018/05/13/Cloudera-Navigator/","text":"Cloudera NavigatorAnalytics Audit可以查看最活跃的用户 Analytics Dashboard可以查看一些活动的报表结果，如最近一段时间内每天或者每周创建了多少表，每天有多少查询等等。虽然页面上没有但是应该能通过添加配置出每天数据的增量。 Analytics Dashboard2可以看到一些单个文件比较大的文件列表 Analytics Data Exploprer趋势详情，可以点击跳转到Search页面，查看更多的详细信息。 Analytics HDFS对HDFS的存储信息和操作信息进行分组分用户的统计信息。 AuditEvent审计日志，包含用户、IP、操作类型、操作的数据为准、目标服务器等信息。 AuditEvent Filter添加过滤条件的审计日志 Lineage血缘关系：可以查看一段SQL里面的血缘关系。 Search使用的是Solr，可以查询被添加了标签的数据，也可以自己添加标签。 Policies规则或者叫策略 可以给HDFS内的实体自定义标签 定时对文件进行操作、移动、删除等。 管路元数据 对特定的实体执行命令等。 发送JMS消息","comments":true,"categories":[],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CDH/"},{"name":"Navigator","slug":"Navigator","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/Navigator/"}]},{"title":"Hive窗口函数和分析函数","date":"2018-05-13T03:53:26.000Z","path":"2018/05/13/Hive窗口函数和分析函数/","text":"Hive窗口函数和分析函数创建测试表并插入测试数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556create table test_analy_funs (stu_id int,stu_nm varchar(20),class int,course varchar(20),score decimal(6,2));insert into table test_analy_funsselect 1,&apos;Daniel&apos;,201,&apos;maths&apos;,85 union allselect 2,&apos;Tom&apos;,201,&apos;maths&apos;,82 union allselect 3,&apos;Macal&apos;,201,&apos;maths&apos;,26 union allselect 4,&apos;Lily&apos;,201,&apos;maths&apos;,99 union allselect 5,&apos;Onel&apos;,201,&apos;maths&apos;,100 union allselect 6,&apos;Kebe&apos;,201,&apos;maths&apos;,67 union allselect 7,&apos;Kaka&apos;,201,&apos;maths&apos;,89 union allselect 8,&apos;Piye&apos;,201,&apos;maths&apos;,93 union allselect 9,&apos;Luo&apos;,202,&apos;maths&apos;,73 union allselect 10,&apos;Lufi&apos;, 202,&apos;maths&apos;,98 union allselect 11,&apos;Tony&apos;, 202,&apos;maths&apos;,100 union allselect 12,&apos;Miya&apos;, 202,&apos;maths&apos;,100 union allselect 13,&apos;Joy&apos;, 202,&apos;maths&apos;,87 union allselect 14,&apos;Tong&apos;, 202,&apos;maths&apos;,79 union allselect 15,&apos;Roy&apos;, 202,&apos;maths&apos;,84 union allselect 16,&apos;Erw&apos;, 202,&apos;maths&apos;,84 union allselect 1,&apos;Daniel&apos;,201,&apos;english&apos;,82 union allselect 2,&apos;Tom&apos;,201,&apos;english&apos;,26 union allselect 3,&apos;Macal&apos;,201,&apos;english&apos;,99 union allselect 4,&apos;Lily&apos;,201,&apos;english&apos;,100 union allselect 5,&apos;Onel&apos;,201,&apos;english&apos;,77 union allselect 6,&apos;Kebe&apos;,201,&apos;english&apos;,89 union allselect 7,&apos;Kaka&apos;,201,&apos;english&apos;,77 union allselect 8,&apos;Piye&apos;,201,&apos;english&apos;,85 union allselect 9,&apos;Luo&apos;,202,&apos;english&apos;,80 union allselect 10,&apos;Lufi&apos;, 202,&apos;english&apos;,100 union allselect 11,&apos;Tony&apos;, 202,&apos;english&apos;,59 union allselect 12,&apos;Miya&apos;, 202,&apos;english&apos;,84 union allselect 13,&apos;Joy&apos;, 202,&apos;english&apos;,82 union allselect 14,&apos;Tong&apos;, 202,&apos;english&apos;,89 union allselect 15,&apos;Roy&apos;, 202,&apos;english&apos;,90 union allselect 16,&apos;Erw&apos;, 202,&apos;english&apos;,95 union allselect 1,&apos;Daniel&apos;,201,&apos;chinese&apos;,28 union allselect 2,&apos;Tom&apos;,201,&apos;chinese&apos;,99 union allselect 3,&apos;Macal&apos;,201,&apos;chinese&apos;,89 union allselect 4,&apos;Lily&apos;,201,&apos;chinese&apos;,100 union allselect 5,&apos;Onel&apos;,201,&apos;chinese&apos;,88 union allselect 6,&apos;Kebe&apos;,201,&apos;chinese&apos;,83 union allselect 7,&apos;Kaka&apos;,201,&apos;chinese&apos;,92 union allselect 8,&apos;Piye&apos;,201,&apos;chinese&apos;,95 union allselect 9,&apos;Luo&apos;,202,&apos;chinese&apos;,82 union allselect 10,&apos;Lufi&apos;, 202,&apos;chinese&apos;,92 union allselect 11,&apos;Tony&apos;, 202,&apos;chinese&apos;,99 union allselect 12,&apos;Miya&apos;, 202,&apos;chinese&apos;,78 union allselect 13,&apos;Joy&apos;, 202,&apos;chinese&apos;,28 union allselect 14,&apos;Tong&apos;, 202,&apos;chinese&apos;,50 union allselect 15,&apos;Roy&apos;, 202,&apos;chinese&apos;,30 union allselect 16,&apos;Erw&apos;, 202,&apos;chinese&apos;,88 排序dense_rank() OVER([partition_by_clause] order_by_clause)返回从1开始的升序序列，根据不同的order by的字段生成不同的值，如果值一样则排序结果也一样。Returns an ascending sequence of integers, starting with 1. The output sequence produces duplicate integers for duplicate values of the ORDER BY expressions. rank() OVER([partition_by_clause] order_by_clause)返回从1开始的升序序列，根据不同的order by的字段生成不同的值，如果值一样则排序结果也一样。然后序列值会根据实际的数值增加。Returns an ascending sequence of integers, starting with 1. The output sequence produces duplicate integers for duplicate values of the ORDER BY expressions. After generating duplicate output values for the “tied” input values, the function increments the sequence by the number of tied values. row_number() OVER([partition_by_clause] order_by_clause)返回从1开始的升序序列，根据不同的order by的字段生成不同的值，即使值一样，排序结果也不一样。Returns an ascending sequence of integers, starting with 1. Starts the sequence over for each group produced by the PARTITIONED BY clause. The output sequence includes different values for duplicate input values. Therefore, the sequence never contains any duplicates or gaps, regardless of duplicate input values. 1234567891011121314151617181920212223242526272829303132select stu_nm, class, course, score, rank() over(partition by class, course order by score desc) rk1, dense_rank() over(partition by class, course order by score desc) rk2, row_number() over(partition by class, course order by score desc) rk3, percent_rank() over(partition by class, course order by score desc)from test_analy_funs;+---------+--------+----------+--------+------+------+------+----------------------+--+| stu_nm | class | course | score | rk1 | rk2 | rk3 | _wcol3 |+---------+--------+----------+--------+------+------+------+----------------------+--+| Lily | 201 | english | 100 | 1 | 1 | 1 | 0.0 || Macal | 201 | english | 99 | 2 | 2 | 2 | 0.14285714285714285 || Kebe | 201 | english | 89 | 3 | 3 | 3 | 0.2857142857142857 || Piye | 201 | english | 85 | 4 | 4 | 4 | 0.42857142857142855 || Daniel | 201 | english | 82 | 5 | 5 | 5 | 0.5714285714285714 || Onel | 201 | english | 77 | 6 | 6 | 6 | 0.7142857142857143 || Kaka | 201 | english | 77 | 6 | 6 | 7 | 0.7142857142857143 || Tom | 201 | english | 26 | 8 | 7 | 8 | 1.0 || Tony | 202 | maths | 100 | 1 | 1 | 1 | 0.0 || Miya | 202 | maths | 100 | 1 | 1 | 2 | 0.0 || Lufi | 202 | maths | 98 | 3 | 2 | 3 | 0.2857142857142857 || Joy | 202 | maths | 87 | 4 | 3 | 4 | 0.42857142857142855 || Roy | 202 | maths | 84 | 5 | 4 | 5 | 0.5714285714285714 || Erw | 202 | maths | 84 | 5 | 4 | 6 | 0.5714285714285714 || Tong | 202 | maths | 79 | 7 | 5 | 7 | 0.8571428571428571 || Luo | 202 | maths | 73 | 8 | 6 | 8 | 1.0 |+---------+--------+----------+--------+------+------+------+----------------------+--+ 差值lag(expr [, offset] [, default]) OVER ([partition_by_clause] order_by_clause)This function returns the value of an expression using column values from a preceding row. You specify an integer offset, which designates a row position some number of rows previous to the current row. Any column references in the expression argument refer to column values from that prior row. lead(expr [, offset] [, default]) OVER([partition_by_clause] order_by_clause)This function returns the value of an expression using column values from a following row. You specify an integer offset, which designates a row position some number of rows after to the current row. Any column references in the expression argument refer to column values from that later row. 1234567891011121314151617181920212223242526272829select class, course, score, rank() over(partition by class, course order by score desc) rk1, LAG(score,2) over(partition by class, course order by score) as lag, lead(score,2) over(partition by class, course order by score) as leadfrom test_analy_funswhere course=&apos;chinese&apos;;+--------+----------+--------+------+-------+-------+--+| class | course | score | rk1 | lag | lead |+--------+----------+--------+------+-------+-------+--+| 201 | chinese | 28 | 8 | NULL | 88 || 201 | chinese | 83 | 7 | NULL | 89 || 201 | chinese | 88 | 6 | 28 | 92 || 201 | chinese | 89 | 5 | 83 | 95 || 201 | chinese | 92 | 4 | 88 | 99 || 201 | chinese | 95 | 3 | 89 | 100 || 201 | chinese | 99 | 2 | 92 | NULL || 201 | chinese | 100 | 1 | 95 | NULL || 202 | chinese | 28 | 8 | NULL | 50 || 202 | chinese | 30 | 7 | NULL | 78 || 202 | chinese | 50 | 6 | 28 | 82 || 202 | chinese | 78 | 5 | 30 | 88 || 202 | chinese | 82 | 4 | 50 | 92 || 202 | chinese | 88 | 3 | 78 | 99 || 202 | chinese | 92 | 2 | 82 | NULL || 202 | chinese | 99 | 1 | 88 | NULL |+--------+----------+--------+------+-------+-------+--+ 第一个或者最后一个值获取分组集合中的第一个值或者最后一个值，last_value需要加rows between current row and unbounded following first_value(expr) OVER([partition_by_clause] order_by_clause [window_clause])Returns the expression value from the first row in the window. The return value is NULL if the input expression is NULL. last_value(expr) OVER([partition_by_clause] order_by_clause [window_clause])Returns the expression value from the last row in the window. The return value is NULL if the input expression is NULL. 12345678910111213141516171819202122232425262728293031323334select stu_id, class, course, score, rank() over(partition by class, course order by score desc) rk1, first_value(score) over(partition by class, course order by score) first, last_value(score) over(partition by class, course order by score) last, last_value(score) over(partition by class, course order by score rows between current row and unbounded following) lastfrom test_analy_funswhere course=&apos;chinese&apos;order by class, stu_id;+---------+--------+----------+--------+------+--------+-------+-------+--+| stu_id | class | course | score | rk1 | first | last | last |+---------+--------+----------+--------+------+--------+-------+-------+--+| 1 | 201 | chinese | 28 | 8 | 28 | 28 | 100 || 2 | 201 | chinese | 99 | 2 | 28 | 99 | 100 || 3 | 201 | chinese | 89 | 5 | 28 | 89 | 100 || 4 | 201 | chinese | 100 | 1 | 28 | 100 | 100 || 5 | 201 | chinese | 88 | 6 | 28 | 88 | 100 || 6 | 201 | chinese | 83 | 7 | 28 | 83 | 100 || 7 | 201 | chinese | 92 | 4 | 28 | 92 | 100 || 8 | 201 | chinese | 95 | 3 | 28 | 95 | 100 || 9 | 202 | chinese | 82 | 4 | 28 | 82 | 99 || 10 | 202 | chinese | 92 | 2 | 28 | 92 | 99 || 11 | 202 | chinese | 99 | 1 | 28 | 99 | 99 || 12 | 202 | chinese | 78 | 5 | 28 | 78 | 99 || 13 | 202 | chinese | 28 | 8 | 28 | 28 | 99 || 14 | 202 | chinese | 50 | 6 | 28 | 50 | 99 || 15 | 202 | chinese | 30 | 7 | 28 | 30 | 99 || 16 | 202 | chinese | 88 | 3 | 28 | 88 | 99 |+---------+--------+----------+--------+------+--------+-------+-------+--+16 rows selected (25.701 seconds) 分桶ntile() Ntile 是Hive很强大的一个分析函数。可以看成是：它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。语法是： ntile (num) over ([partition_clause] order_by_clause) as your_bucket_num 然后可以根据桶号，选取前或后 n分之几的数据。 123456789101112131415161718192021222324252627select stu_id, class , score, NTILE(5) OVER( partition by class, course order by score desc) AS rn from test_analy_funswhere course=&apos;chinese&apos;;+---------+--------+--------+-----+--+| stu_id | class | score | rn |+---------+--------+--------+-----+--+| 4 | 201 | 100 | 1 || 2 | 201 | 99 | 1 || 8 | 201 | 95 | 2 || 7 | 201 | 92 | 2 || 3 | 201 | 89 | 3 || 5 | 201 | 88 | 3 || 6 | 201 | 83 | 4 || 1 | 201 | 28 | 5 || 11 | 202 | 99 | 1 || 10 | 202 | 92 | 1 || 16 | 202 | 88 | 2 || 9 | 202 | 82 | 2 || 12 | 202 | 78 | 3 || 14 | 202 | 50 | 3 || 15 | 202 | 30 | 4 || 13 | 202 | 28 | 5 |+---------+--------+--------+-----+--+ 值排序占比cume_dist() CUME_DIST 小于等于当前值的行数/分组内总行数 12345678910111213141516171819202122232425262728SELECT class,stu_id,score,CUME_DIST() OVER(partition by class ORDER BY score) AS cm1from test_analy_funswhere course=&apos;chinese&apos;;+--------+---------+--------+--------+--+| class | stu_id | score | cm1 |+--------+---------+--------+--------+--+| 201 | 1 | 28 | 0.125 || 201 | 6 | 83 | 0.25 || 201 | 5 | 88 | 0.375 || 201 | 3 | 89 | 0.5 || 201 | 7 | 92 | 0.625 || 201 | 8 | 95 | 0.75 || 201 | 2 | 99 | 0.875 || 201 | 4 | 100 | 1.0 || 202 | 13 | 28 | 0.125 || 202 | 15 | 30 | 0.25 || 202 | 14 | 50 | 0.375 || 202 | 12 | 78 | 0.5 || 202 | 9 | 82 | 0.625 || 202 | 16 | 88 | 0.75 || 202 | 10 | 92 | 0.875 || 202 | 11 | 99 | 1.0 |+--------+---------+--------+--------+--+","comments":true,"categories":[],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/Hive/"},{"name":"函数","slug":"函数","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/函数/"}]},{"title":"Hive UDFs","date":"2018-05-13T03:50:26.000Z","path":"2018/05/13/Hive UDFs/","text":"Hive UDFs创建自定义UDF首先需要一个类继承hive.udf类。 1234567891011package com.example.hive.udf; import org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text; public final class Lower extends UDF &#123; public Text evaluate(final Text s) &#123; if (s == null) &#123; return null; &#125; return new Text(s.toString().toLowerCase()); &#125;&#125; 编译之后，将jar包放到hive的classpath路径下。 1234567891011## 默认读取当前路径，hiveserver2上的路径。hive&gt; add jar my_jar.jar;Added my_jar.jar to class path## 也可以指定绝对路径hive&gt; add jar /tmp/my_jar.jar;Added /tmp/my_jar.jar to class path## 查看所有的jarshive&gt; list jars;my_jar.jar 创建临时函数并使用 12345678## 创建函数create temporary function my_lower as &apos;com.example.hive.udf.Lower&apos;;## 使用 select my_lower(title), sum(freq) from titles group by my_lower(title); ## 或者使用如下方法创建。CREATE FUNCTION myfunc AS &apos;myclass&apos; USING JAR &apos;hdfs:///path/to/jar&apos;; Hive Auxiliary JARs Directory在CDH上可以将开发的jar放到 HIVE_AUX_JARS_PATH 配置的路径下（这个是本地路径，并非hdfs路径),然后重启，这样所有的用户都能够读取并使用这些jar包。 UDF 开发","comments":true,"categories":[],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/Hive/"},{"name":"Udf","slug":"Udf","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/Udf/"}]},{"title":"Configuration DbVisualize to Connect Hive with Kerberos On CDH","date":"2018-05-13T03:20:26.000Z","path":"2018/05/13/Configuration-DbVisualize-Connect-Hive/","text":"Configuration DbVisualize to Connect Hive with Kerberos On CDHWhat you need DbVisualize Download From Here Kerbers Windows Tool. Download From Here(64) Here(32) Jre Download From Here krb5.conf / krb5.ini This should copy from your kerberos server. jaas.conf Cloudera Hive JDBC Driver Download from Here Make sure your laptop have the UTP access from port 88 to the kdc server. Install dbvis_windows-x64_10_0_jre.exeInstall kfw-4.0.1-amd64.msiConfig jrecp local_policy.jar US_export_policy.jar from jre folder to %DbViisualize_HOME%\\jre\\lib\\security Configuration Envrionment ParametersKRB5_CONFIG=C:\\Windows\\krb5.iniKRB5CCNAME=C:\\Users\\Username\\krb5cache KinitUse Administrator run cmd cd C:\\&quot;Program Files&quot;\\MIT\\Kerberos\\bin kinit -kt hive.keytab -c *cachefilename* hive/h2_server.com@REALM klist -c *cachefilename* cachefilename is KRB5CCNAME jass.confHiveClient { com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=true useTicketCache=true principal=”hive/example@HTSEC.COM” useKeyTab=true keyTab=”C:/Users/Administrator/hive.keytab” client=true;}; Start DbVisualizeUse cmd to start DbVisualize set jvm on DbVisualize -Dsun.security.jgss.debug=true -Dsun.security.krb5.debug=true -Djava.security.krb5.realm=REALM.COM -Djava.security.krb5.kdc=example.com -Djava.security.auth.login.config=C:\\Users\\Users\\jaas.conf -Djava.security.krb5.conf=C:\\Windows\\krb5.ini restart DbVisualize Load Driver Connect Hive2jdbc:hive2://example.com:10000/default;AuthMech=1;KrbRealm=REALM.COM;KrbHostFQDN=h2_server.com;KrbServiceName=hive;","comments":true,"categories":[],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CDH/"},{"name":"Hive","slug":"Hive","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/Hive/"},{"name":"Kerberos","slug":"Kerberos","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/Kerberos/"}]},{"title":"CDH 用户管理","date":"2018-05-13T02:15:26.000Z","path":"2018/05/13/CDH-User-Managerment/","text":"CDH User ManagermentHive创建一个普通用户 hive_test 在OS上创建这个普通用户 hive_test 1234567nodes=`cat /etc/hosts | grep -v ^# | grep htsec | awk &apos;&#123;print $2&#125;&apos;`for target in $nodes; dossh root@$target &lt;&lt;comduseradd hive_testusermod -a -G hive hive_testcomddone 在hdfs上创建用户文件夹如果集群启用了Kerberos，那么需要首先kinit，否则不需要 123kinit -kt hdfs.keytab hdfs/nn1.example.comhdfs dfs -mkdir -p /user/hive_testhdfs dfs -chown -R dev_user:hive_test /user/hive_test 创建role 只有admin用户可以创建和删除roles；hive、hue、impala都是默认的admin用户。 123kinit -kt hive.keytab hive/nn1.example.com beeline -u &quot;jdbc:hive2://nn1.example.com:10000/default;principal=hive/nn1.example.com@EXAMPLE.COM&quot;create role hive_test; //创建用户 删除role drop role hive_test； grant role grant role hive_test to group hive_test; ## 将用户分到一个组，在Sentry里用户必须在组里才能使用。 revoke role revoke role hive_test from group hive_test； 在hive上grant权限 grant权限 123456GRANT &lt;PRIVILEGE&gt; [, &lt;PRIVILEGE&gt; ] ON &lt;OBJECT&gt; &lt;object_name&gt; TO ROLE &lt;roleName&gt; [,ROLE &lt;roleName&gt;]e.g. grant all on server server1 to role hive_test; hive的权限分类 ALL： 所有权限 ALTER：允许修改元数据（modify metadata data of object）—表信息数据 UPDATE： 允许修改物理数据（modify physical data of object）—实际数据 CREATE： 允许进行Create操作 DROP： 允许进行DROP操作 INDEX： 允许建索引（目前还没有实现） LOCK： 当出现并发的使用允许用户进行LOCK和UNLOCK操作 SELECT：允许用户进行SELECT操作 SHOW_DATABASE： 允许用户查看可用的数据库 查询权限 12show roles;show grant role hive_test; revoke 权限 REVOKE &lt;PRIVILEGE&gt; [, &lt;PRIVILEGE&gt; ] ON &lt;OBJECT&gt; &lt;object_name&gt; FROM ROLE &lt;roleName&gt; [,ROLE &lt;roleName&gt;] e.g. REVOKE SELECT(column_name) ON TABLE table_name FROM ROLE REVOKE SELECT(column_name) ON TABLE table_name FROM ROLE role_name; // 将列的查询权限赋值给用户 REVOKE ROLE role_name from GROUP group_name[,GROUP group_name] // 将用户移除一个组 grant URIs GRANT ALL ON URI &apos;hdfs://namenode:XXX/path/to/table&apos; CREATE EXTERNAL TABLE foo LOCATION &apos;namenode:XXX/path/to/table&apos; SET ROLE Statement SET ROLE NONE 使所有的role变成非活动状态。 SET ROLE ALL 使ROLE拥有所有的权限。 SET ROLE 激活一个role，并将分配其的权限激活。 Impalaimpala 的权限和hive一致 Hbase 在OS上创建用户 hive_test （同Hive) 需要添加用户到组hbase usermod -a -G base hive_test 在hdfs上创建用户文件夹 （同Hive) 在Hbase上grant权限 如果集群启用了Kerberos，那么需要首先kinit，否则不需要 12kinit -kt hbase.keytab hbase/nn1.example.comgrant &apos;hive_test&apos; ,&apos;RWXCA&apos;,&apos;@hbase&apos; Hbase权限分类HBase提供的五个权限标识符：RWXCA,分别对应着READ(‘R’), WRITE(‘W’), EXEC(‘X’), CREATE(‘C’), ADMIN(‘A’) HBase提供的五个权限标识符：RWXCA,分别对应着READ(‘R’), WRITE(‘W’), EXEC(‘X’), CREATE(‘C’), ADMIN(‘A’)HBase提供的安全管控级别包括：Superuser：拥有所有权限的超级管理员用户。通过hbase.superuser参数配置Global：全局权限可以作用在集群所有的表上。Namespace ：命名空间级。Table：表级。ColumnFamily：列簇级权限。Cell：单元级。 将namespace权限符给用户 grant &apos;user&apos;, &apos;RWXCA&apos;, &apos;@namespace&apos; 将表的权限赋给用户 grant &apos;user&apos;, &apos;RWXCA&apos;, &apos;tablename&apos; 将表的一个字段的权限赋给用户 grant &apos;user&apos;, &apos;RWXCA&apos;, &apos;TABLE&apos;, &apos;CF&apos;, &apos;CQ&apos; http://hbase.apache.org/book.html 查看用户权限1user_permission &apos;@namespace&apos;","comments":true,"categories":[],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CDH/"},{"name":"User","slug":"User","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/User/"}]},{"title":"CDH 安装环境需求","date":"2018-05-13T02:10:26.000Z","path":"2018/05/13/CDH-POC-Env-requirement/","text":"CDH 安装环境需求https://www.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#concept_mh3_sht_kbb 操作系统Red Hat/CentOS 7.3, 7.2, 7.1,6.9, 6.8, 6.7, 6.5 SUSE 12 SP1, 11 SP4, 11 SP3, 11 SP2 Ubuntu 14.04,12.04 文件系统挂载时不带atime，可以加速读取 12/dev/sdb1 /data1 ext4 defaults,noatime 0mount -o remount /data1 ext3,ext4，XFS 机器内存： &gt; 4G IPv6：不支持，而且必须关闭 端口： 7180，8088,8888,19888(根据实际需求增加） 硬盘空间 /var &gt; 20G /usr &gt; 500M /opt &gt; 10G yum源挂载硬盘镜像并配置本地yum源 Others所有的数据库使用UTF-8编码。 MySQL 5.5,5.6,5.7 MariaDB 5.5, 10.0 PostgreSQL 8.1,8.3,8.4,9.1,9.2,9.3,9.4 Oracle 11g R2,12c R1 JDK 1.7u80,1.7u75,1.7u67,1.7u55,1.8u121,1.8u111，1.8u102，1.8u91，1.8u74，1.8u31 （如果需要安装Spark2.x 则需要JDK 1.8） 浏览器 Chrome，Firefox，Internet Explorer，Safari 注意项 Cloudera Manager对应的CDH版本，el6对应于el6的版本。 CDH的大版本不能比CM的大版本大。 所有的CDH节点必须允许统一版本的操作系统。 Kudu 只可以允许在ext4 和 XFS的文件系统上。 必须修改/etc/sysconfig/network里面的host名和/etc/hosts一致** /etc/hosts 里不允许有大写的域名","comments":true,"categories":[],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CDH/"}]},{"title":"CCAH-131 Test","date":"2018-05-13T00:38:26.000Z","path":"2018/05/13/CCAH-131-Test/","text":"TestBenchmark the cluster operational metrics, test system configuration for operation and efficiency Execute file system commands via HTTPFSUsing the HttpFS Server with curl 例如查看 用户training的Home目录 123[training@elephant ~]$ curl &quot;http://elephant:14000/webhdfs/v1?op=gethomedirectory&amp;user.name=training&quot;&#123;&quot;Path&quot;:&quot;\\/user\\/training&quot;&#125;[training@elephant ~]$ WebHDFS REST API 123curl -i -X PUT &quot;http://&lt;HOST&gt;:&lt;PORT&gt;/webhdfs/v1/&lt;PATH&gt;?op=CREATE [&amp;overwrite=&lt;true|false&gt;][&amp;blocksize=&lt;LONG&gt;][&amp;replication=&lt;SHORT&gt;] [&amp;permission=&lt;OCTAL&gt;][&amp;buffersize=&lt;INT&gt;]&quot; 查询一个目录 12345678910[training@elephant ~]$ curl -i &quot;http://elephant:14000/webhdfs/v1/user?op=LISTSTATUS&amp;user.name=training&quot;HTTP/1.1 200 OKServer: Apache-Coyote/1.1Set-Cookie: hadoop.auth=&quot;u=training&amp;p=training&amp;t=simple-dt&amp;e=1515685732115&amp;s=9cv4wBc8rJe83+6WUg7B5xaeafE=&quot;; Path=/; HttpOnlyContent-Type: application/jsonTransfer-Encoding: chunkedDate: Thu, 11 Jan 2018 05:48:52 GMT&#123;&quot;FileStatuses&quot;:&#123;&quot;FileStatus&quot;:[&#123;&quot;pathSuffix&quot;:&quot;history&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;mapred&quot;,&quot;group&quot;:&quot;hadoop&quot;,&quot;permission&quot;:&quot;777&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515572964146,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;hive&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;hive&quot;,&quot;group&quot;:&quot;hive&quot;,&quot;permission&quot;:&quot;1775&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515575604845,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;hue&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;hue&quot;,&quot;group&quot;:&quot;hue&quot;,&quot;permission&quot;:&quot;775&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515575637482,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;impala&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;impala&quot;,&quot;group&quot;:&quot;impala&quot;,&quot;permission&quot;:&quot;775&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515575938788,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;oozie&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;oozie&quot;,&quot;group&quot;:&quot;oozie&quot;,&quot;permission&quot;:&quot;775&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515645000550,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;spark&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;spark&quot;,&quot;group&quot;:&quot;spark&quot;,&quot;permission&quot;:&quot;751&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515573417601,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;,&#123;&quot;pathSuffix&quot;:&quot;training&quot;,&quot;type&quot;:&quot;DIRECTORY&quot;,&quot;length&quot;:0,&quot;owner&quot;:&quot;training&quot;,&quot;group&quot;:&quot;supergroup&quot;,&quot;permission&quot;:&quot;755&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1515586402387,&quot;blockSize&quot;:0,&quot;replication&quot;:0&#125;]&#125;&#125;[training@elephant ~]$ Efficiently copy data within a cluster/between clusters迁移hdfs数据至新集群 1hadoop distcp -update hdfs://hostname:port/path hdfs://ip:8020/path 迁移Hive数据仓库 可以每次迁移一个表或者一个database。注意： 表的内容小文件不应过多，否则会非常慢。源集群metastore数据备份导出(mysql导出) 1mysqldump -u root -p’密码’--skip-lock-tables -h xxx.xxx.xxx.xxx hive &gt; mysql_hive.sql 新的集群导入metastore数据 1mysql -u root -proot --default-character-set=utf8 hvie &lt; mysql_hive.sql 升级hive内容库(如果hive版本需要升级操作，同版本不需要操作) 12mysql -uroot -proot risk -hxxx.xxx.xxx.xxx &lt; mysqlupgrade-0.13.0-to-0.14.0.mysql.sqlmysql -uroot -proot risk -hxxx.xxx.xxx.xxx &lt; mysqlupgrade-0.14.0-to-1.1.0.mysql.sql 版本要依据版本序列升序升级,不可跨越版本，如当前是hive0.12打算升级到0.14，需要先升级到0.13再升级到0.14 修改 metastore 内容库的集群信息 因为夸集群，hdfs访问的名字可能变化了，所以需要修改下hive库中的表DBS和SDS内容，除非你的集群名字或者HA的名字跟之前的一致这个就不用修改了登录mysql数据库，查看： 123mysql&gt; use hive;mysql&gt; select * from DBS;mysql&gt; select * from SDS; Create/restore a snapshot of an HDFS directory创建一个测试文件夹和文件 123456[training@elephant ~]$ hdfs dfs -mkdir /user/training/snapshottest[training@elephant ~]$ hdfs dfs -put /etc/hosts /user/training/snapshottest/[training@elephant ~]$ hdfs dfs -ls /user/training/snapshottest/Found 1 items-rw-r--r-- 3 training supergroup 251 2018-01-11 14:03 /user/training/snapshottest/hosts[training@elephant ~]$ HOME -&gt; Cluster -&gt; HDFS -&gt; File Browser -&gt; /user/training/snapshottest/ 点击 Enable Snapshot 启用 Take Sanpshot 并命名 在 Snapshots 下可以看到已有的Sanpshots 或者到Hadoop 50070上查看 删除测试文件 123[training@elephant ~]$ hdfs dfs -rm /user/training/snapshottest/hosts18/01/11 14:09:37 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://mycluster/user/training/snapshottest/hosts&apos; to trash at: hdfs://mycluster/user/training/.Trash/Current/user/training/snapshottest/hosts[training@elephant ~]$ 从snapshot恢复 选择使用得分snapshot，并恢复 查看文件是否已经恢复 1234[training@elephant ~]$ hdfs dfs -ls /user/training/snapshottest/Found 1 items-rw-r--r-- 3 hdfs supergroup 251 2018-01-11 14:12 /user/training/snapshottest/hosts[training@elephant ~]$ Get/set ACLs for a file or directory structure12345678910111213141516171819202122232425[training@elephant ~]$ hdfs dfs -ls /user/training/ ## 查看文件夹内内容Found 1 itemsdrwx------ - training supergroup 0 2018-01-11 08:00 /user/training/.Trash[training@elephant ~]$ hdfs dfs -put /etc/hosts /user/training/ ## 上传一个文件[training@elephant ~]$ hdfs dfs -ls /user/training/ ## 查看上传的文件的权限Found 2 itemsdrwx------ - training supergroup 0 2018-01-11 08:00 /user/training/.Trash-rw-r--r-- 3 training supergroup 251 2018-01-11 13:51 /user/training/hosts[training@elephant ~]$ hdfs dfs -getfacl /user/training/hosts ## 使用命令查询文件的ACL # file: /user/training/hosts# owner: training# group: supergroupuser::rw-group::r--other::r--[training@elephant ~]$ hdfs dfs -getfacl /user/training/ ## 查看文件夹的ACL# file: /user/training# owner: training# group: supergroupuser::rwxgroup::r-xother::r-x[training@elephant ~]$ Benchmark the cluster (I/O, CPU, network)","comments":true,"categories":[],"tags":[{"name":"CCAH-131","slug":"CCAH-131","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CCAH-131/"}]},{"title":"CCAH-131 Troubleshoot","date":"2018-05-13T00:37:26.000Z","path":"2018/05/13/CCAH-131-Troubleshoot/","text":"TroubleshootDemonstrate ability to find the root cause of a problem, optimize inefficient execution, and resolve resource contention scenarios Resolve errors/warnings in Cloudera ManagerResolve performance problems/errors in cluster operationDetermine reason for application failureConfigure the Fair Scheduler to resolve application delays","comments":true,"categories":[],"tags":[{"name":"CCAH-131","slug":"CCAH-131","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CCAH-131/"}]},{"title":"CCAH-131 Secure","date":"2018-05-13T00:36:26.000Z","path":"2018/05/13/CCAH-131-Secure/","text":"SecureEnable relevant services and configure the cluster to meet goals defined by security policy; demonstrate knowledge of basic security practices Configure HDFS ACLsHDFS Extended ACLs HOME -&gt; Cluster -&gt; HDFS -Configuration 搜索ACL，勾选启用 Access Control Lists 重启依赖的服务 Install and configure SentryHOME -&gt; Cluster -&gt; Add Services 选择Sentry 选择服务安装的服务器 选择数据库 安装配置并重启 重启完成 安装成功 Hive 启用 Sentry ，在Configuration配置选择中，勾选Sentry Services Impala 启用 Sentry，在Configuration配置选择中，勾选Sentry Services Configure Hue user authorization and authenticationEnable/configure log and query redactionLog and Query Redaction Using Cloudera Navigator Data Management for Data Redaction HOME -&gt; Cluster -&gt; HDFS -&gt; Configuration 搜索redaction 选择要添加的规则并测试 Create encrypted zones in HDFSCreate an encryption key for your zone as the application user that will be using the key. For example, if you are creating an encryption zone for HBase, create the key as the hbase user as follows: 1$ sudo -u hbase hadoop key create &lt;key_name&gt; Create a new empty directory and make it an encryption zone using the key created above. 12$ sudo -u hdfs hadoop fs -mkdir /encryption_zone$ sudo -u hdfs hdfs crypto -createZone -keyName &lt;key_name&gt; -path /encryption_zone You can verify creation of the new encryption zone by running the -listZones command. You should see the encryption zone along with its key listed as follows: 12$ sudo -u hdfs hdfs crypto -listZones/encryption_zone &lt;key_name&gt; Warning: Do not delete an encryption key as long as it is still in use for an encryption zone. This results in loss of access to data in that zone. ###Validate Data EncryptionLogin or su to these users on one of the hosts in your cluster. These directions will help to verify KMS is setup to encrypt files. Create a key and directory. 123su &lt;KEY_ADMIN_USER&gt;hadoop key create mykey1hadoop fs -mkdir /tmp/zone1 1234[training@elephant ~]$ hadoop key create mykey1mykey1 has been successfully created with options Options&#123;cipher=&apos;AES/CTR/NoPadding&apos;, bitLength=128, description=&apos;null&apos;, attributes=null&#125;.KMSClientProvider[http://horse:16000/kms/v1/] has been updated.[training@elephant ~]$ hadoop fs -mkdir /tmp/zone1 Create a zone and link to the key. 12su hdfshdfs crypto -createZone -keyName mykey1 -path /tmp/zone1 1234[training@elephant ~]$ sudo su - hdfs-bash-4.1$ hdfs crypto -createZone -keyName mykey1 -path /tmp/zone1Added encryption zone /tmp/zone1-bash-4.1$ Create a file, put it in your zone and ensure the file can be decrypted. 12345su &lt;KEY_ADMIN_USER&gt;echo &quot;Hello World&quot; &gt; /tmp/helloWorld.txthadoop fs -put /tmp/helloWorld.txt /tmp/zone1hadoop fs -cat /tmp/zone1/helloWorld.txtrm /tmp/helloWorld.txt 123456[training@elephant ~]$ echo &quot;Hello World&quot; &gt; /tmp/helloWorld.txt[training@elephant ~]$ hadoop fs -put /tmp/helloWorld.txt /tmp/zone1[training@elephant ~]$ hadoop fs -cat /tmp/zone1/helloWorld.txtHello World[training@elephant ~]$ rm /tmp/helloWorld.txt[training@elephant ~]$ Ensure the file is stored as encrypted. 123su hdfshadoop fs -cat /.reserved/raw/tmp/zone1/helloWorld.txthadoop fs -rm -R /tmp/zone1 1234567[training@elephant ~]$ hadoop fs -cat /.reserved/raw/tmp/zone1/helloWorld.txtcat: Access denied for user training. Superuser privilege is required[training@elephant ~]$[training@elephant ~]$ sudo -u hdfs hadoop fs -cat /.reserved/raw/tmp/zone1/helloWorld.txt�@*�@��&quot;��[training@elephant ~]$ hadoop fs -rm -R /tmp/zone118/01/10 20:13:22 INFO fs.TrashPolicyDefault: Moved: &apos;hdfs://mycluster/tmp/zone1&apos; to trash at: hdfs://mycluster/user/training/.Trash/Current/tmp/zone1[training@elephant ~]$ By default, non-admin users cannot access any encrypted data. You must create appropriate ACLs before users can access encrypted data. See the Cloudera documentation for more information on managing KMS ACLs. Modify the Advanced Configuration Snippet for ACL file: kms-acls.xml Integrating Key HSM with Key Trustee Server Installing Cloudera Navigator Key HSM Configuring CDH Services for HDFS Encryption","comments":true,"categories":[],"tags":[{"name":"CCAH-131","slug":"CCAH-131","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CCAH-131/"}]},{"title":"CCAH-131 Manage","date":"2018-05-13T00:35:26.000Z","path":"2018/05/13/CCAH-131-Manage/","text":"ManageMaintain and modify the cluster to support day-to-day operations in the enterprise Rebalance the clusterConfiguring and Running the HDFS Balancer Using Cloudera Manager HOME -&gt; Clusters -&gt; HDFS -&gt; Configuration 在 SCOPE 选择 Balancer，在 CATEGORY里选择main。设置Rebalancing Threshold 修改DataNode Advanced Configuration Snippet (Safety Valve) 的值。 输入内容 1234&lt;property&gt; &lt;name&gt;dfs.datanode.balance.max.concurrent.moves&lt;/name&gt; &lt;value&gt;50&lt;/value&gt;&lt;/property&gt; 执行 Rebalance Rebalance log Set up alerting for excessive disk fill配置邮箱Administrator -&gt; Alerts 点击 Edit 修改 Alerts: Mail Message Recipients 或者搜索email进行相关设置修改 修改警告项配置选择一个服务进入configuration 在左侧选择 monitoring ，然后可以修改监控选项的 warning和critial值。 Define and install a rack topology script Install new type of I/O compression library in clusterhttp://archive.cloudera.com/gplextras5/parcels/5.9.0/ Configuring Services to Use the GPL Extras Parcel HOME -&gt; Clusters -&gt; HDFS -&gt; Configuration 搜索 compression点击 + 添加新的压缩类。（Note: 操作系统已经支持此压缩） Revise YARN resource assignment based on user feedbackCommission/decommission a nodeDecommission HOME -&gt; Hosts -&gt; Roles 查看要退役节点上的服务 选择节点，停止上面的所有服务 停止上面所有的服务 查看节点上的服务是否已经都停止 将节点从集群里移除 确认 查看节点，已经没有角色了 退役节点 确认 成功退役 查看节点的服役状态，已经为 Decommission Commission 选择退役的节点，点击重启服役 查看节点的服役状态，已经为 Commission","comments":true,"categories":[],"tags":[{"name":"CCAH-131","slug":"CCAH-131","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CCAH-131/"}]},{"title":"CCAH-131 Configure","date":"2018-05-13T00:34:26.000Z","path":"2018/05/13/CCAH-131-Configure/","text":"Configure Perform basic and advanced configuration needed to effectively administer a Hadoop cluster Configure a service using Cloudera Manager使用Cloudera Manager配置YARN服务 HOME -&gt; CLusters -&gt; YARN(MR2 Included) -&gt; Configuration 到修改配置页面 可以在搜索框中使用模糊搜索或者在左侧的 Filters中选择相应的属性进行过滤 输入更多的条件，过滤到相要修改的属性。修改属性值并保存。 保存后会有重启图标 或者在HOME页面，也可以看到哪些服务受到影响，并且需求重启。 点击其中一个重启，会给出需要修改的配置文件里面修改的内容，红色背景代表去掉这行，绿色背景代表添加这行。点击 Restart Stale Servers进入下一步 勾选 Re-deploy client configuration 并重启 进入重启 重启完成 Create an HDFS user’s home directory12345678910111213[training@elephant ~]$ sudo -u hdfs hdfs dfs -ls /user ## 查看hdfs上 /user目录下当前的文件Found 2 itemsdrwxrwxrwx - mapred hadoop 0 2018-01-10 16:29 /user/historydrwxr-x--x - spark spark 0 2018-01-10 16:36 /user/spark[training@elephant ~]$ sudo -u hdfs hdfs dfs -mkdir /user/training ## 创建training文件夹[training@elephant ~]$ sudo -u hdfs hdfs dfs -chown training /user/training ## 修改其权限[training@elephant ~]$ hdfs dfs -ls /user/training [training@elephant ~]$ hdfs dfs -ls /user/ ## 查看新创建的文件夹Found 3 itemsdrwxrwxrwx - mapred hadoop 0 2018-01-10 16:29 /user/historydrwxr-x--x - spark spark 0 2018-01-10 16:36 /user/sparkdrwxr-xr-x - training supergroup 0 2018-01-10 16:57 /user/training[training@elephant ~]$ Configure NameNode HAHOME -&gt; Cluster -&gt; HDFS -&gt; Actions -&gt; Enable High Availability 点击 填入namespace的名称 选择要添加的新的namenode所在的服务器，并添加奇数个Journal Nodes。 给定Journal Edits所在的文件夹 进入配置重启阶段 因为HDFS在之前是初始化过的，此处初始化会失败。正常就是都会失败。 配置成功后，会提示如果Hive中在HDFS启用HA之前有数据，则需要对Hive执行 Update Hive Metastore NameNodes 查看配置成功后的HDFS 服务， SecondryNamenode被删除，2个Namenode一个是Active 一个是Standby，另外有3个JournalNode Configure ResourceManager HAHOME -&gt; Cluster -&gt; YARN(MR2 Included) -&gt; Instances -&gt; Add Role Instances 选择添加一个Resource Manager 会有红色的配置警告，将Zookeepr勾选为 Zookeeper 服务。 会返回到YARN的实例界面，有重启选项 选择重启，会弹出修改的配置文件内容 红色的为删除的内容，绿色的为添加的内容 选择要重启的服务，因为 Spark、Hive、Oozie、Hue都依赖于YARN，如果YARN配置了HA，这些依赖的服务也要重新更新配置。 所以勾选为需要重启的服务。 重启服务 重启完成 查看服务实例 有2个Resource Manager 一个是Active状态一个是Standby状态。 Configure proxy for Hiveserver2/ImpalaHOME -&gt; Cluster -&gt; Hive -&gt; Configuration 搜索load balance 填写load balance的服务器的域名 添加 高级配置 12Name: hive.server2.support.dynamic.service.discoveryValue: true 停掉一个 hiveserver2 使用如下代码，测试连接 1beeline -u &quot;jdbc:hive2://elephant:2181,horse:2181,tiger:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2&quot; Impala Proxy安装Haproxy Example of Configuring HAProxy Load Balancer for Impala 参考：http://blog.csdn.net/lsb2002/article/details/53843340 http://blog.csdn.net/aa168b/article/details/50372649 下载：haproxy：http://www.haproxy.org/download/1.7/src/haproxy-1.7.1.tar.gz 解压 1tar -xvf haproxy-1.7.1.tar 编译和安装 12345sudo make TARGET=linux31 PREFIX=/usr/local/haproxysudo make install PREFIX=/usr/local/haproxycd /usr/local/haproxy/sudo mkdir -p /usr/haproxy/sudo vim /etc/haproxy/haproxy.cfg 将下面的内容添加到 /etc/haproxy/haproxy.cfg 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586global # To have these messages end up in /var/log/haproxy.log you will # need to: # # 1) configure syslog to accept network log events. This is done # by adding the &apos;-r&apos; option to the SYSLOGD_OPTIONS in # /etc/sysconfig/syslog # # 2) configure local2 events to go to the /var/log/haproxy.log # file. A line like the following can be added to # /etc/sysconfig/syslog # # local2.* /var/log/haproxy.log # log 127.0.0.1 local0 log 127.0.0.1 local1 notice #chroot /usr/local/haproxy pidfile /usr/local/haproxy/logs/haproxy.pid maxconn 4000 #uid 501 #gid 501 daemon # turn on stats unix socket #stats socket /var/lib/haproxy/stats#---------------------------------------------------------------------# common defaults that all the &apos;listen&apos; and &apos;backend&apos; sections will# use if not designated in their block## You might need to adjust timing values to prevent timeouts.#---------------------------------------------------------------------defaults mode http log global option httplog option dontlognull option http-server-close #option forwardfor except 127.0.0.0/8 option redispatch retries 3 maxconn 3000 timeout connect 5000 timeout check 20000 timeout client 50000 timeout server 50000## This sets up the admin page for HA Proxy at port 25002.#listen stats bind 0.0.0.0:25002 balance mode http stats enable stats auth admin:admin# This is the setup for Impala. Impala client connect to load_balancer_host:25003.# HAProxy will balance connections among the list of servers listed below.# The list of Impalad is listening at port 21000 for beeswax (impala-shell) or original ODBC driver.# For JDBC or ODBC version 2.x driver, use port 21050 instead of 21000.listen impala bind 0.0.0.0:25002 mode tcp option tcplog balance leastconn server symbolic_name_1 elephant:21000 server symbolic_name_2 tiger:21000 server symbolic_name_3 monkey:21000 server symbolic_name_4 horse:21000# Setup for Hue or other JDBC-enabled applications.# In particular, Hue requires sticky sessions.# The application connects to load_balancer_host:21051, and HAProxy balances# connections to the associated hosts, where Impala listens for JDBC# requests on port 21050.listen impalajdbc bind 0.0.0.0:21051 mode tcp option tcplog balance source server symbolic_name_5 elephant:21050 server symbolic_name_6 tiger:21050 server symbolic_name_7 monkey:21050 server symbolic_name_8 horse:21050 启动 haproxy 1sudo /usr/local/haproxy/sbin/haproxy -f /etc/haproxy/haproxy.cfg impala 访问 123456[training@elephant ~]$ beeline -d &quot;com.cloudera.impala.jdbc41.Driver&quot; -u &quot;jdbc:impala://elephant:21051&quot;2018-01-12 14:35:57,602 WARN [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present. Continuing without it.Connecting to jdbc:impala://elephant:21051com.cloudera.impala.jdbc41.DriverBeeline version 1.1.0-cdh5.9.0 by Apache Hive0: jdbc:impala://elephant:21051 (closed)&gt; 停掉 elephant上的impala daemon，再次测试 123456[training@elephant ~]$ beeline -d &quot;com.cloudera.impala.jdbc41.Driver&quot; -u &quot;jdbc:impala://elephant:21051&quot;2018-01-12 14:35:57,602 WARN [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present. Continuing without it.Connecting to jdbc:impala://elephant:21051com.cloudera.impala.jdbc41.DriverBeeline version 1.1.0-cdh5.9.0 by Apache Hive0: jdbc:impala://elephant:21051 (closed)&gt;","comments":true,"categories":[],"tags":[{"name":"CCAH-131","slug":"CCAH-131","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CCAH-131/"}]},{"title":"CCAH-131 Install","date":"2018-05-13T00:33:26.000Z","path":"2018/05/13/CCAH-131-Install/","text":"InstallDemonstrate an understanding of the installation process for Cloudera Manager, CDH, and the ecosystem projects. Set up a local CDH repository. 设置CDH 本地Yum源方法1：使用httpd默认的文件路径检查httpd是否安装 ，如果没有安装结果如下： 12[daniel@tiger ~]$ sudo rpm -qa | grep httpd[daniel@tiger ~]$ 如果已经安装则是如下结果 1234[daniel@tiger ~]$ sudo rpm -qa | grep httpdhttpd-tools-2.2.15-60.el6.centos.6.x86_64httpd-2.2.15-60.el6.centos.6.x86_64[daniel@tiger ~]$ 安装 httpd 1sudo yum install -y httpd 启动httpd服务 1234[daniel@tiger ~]$ sudo service httpd startStarting httpd: httpd: Could not reliably determine the server&apos;s fully qualified domain name, using 192.168.25.150 for ServerName [ OK ][daniel@tiger ~]$ 在/var/www/html/ 下创建文件夹 cm 12345[daniel@tiger html]$ sudo mkdir /var/www/html/cm[daniel@tiger html]$ ll /var/www/html/total 4drwxr-xr-x. 2 root root 4096 Jan 5 12:43 cm[daniel@tiger html]$ 将下载的Cloudera Manager 包放到 /var/www/html/的cm文件下 sudo mv CDH-5.9.0-1.cdh5.9.0.p0.23-el5.parcel* /var/www/html/cm 修改cm文件夹得权限123[daniel@tiger cm]$ sudo yum install -y createrepo[daniel@tiger cm]$ sudo createrepo /var/www/html/cm[daniel@tiger cm]$ sudo chmod -R 755 /var/www/html/cm 使用浏览器打开 http://hostname/cm/ 其中 hostname是ip或者服务器的hostname。 配置Yum源在/etc/yum.repo.d/创建文件 cloudera-cm.repo 12345678[daniel@tiger yum.repos.d]$ cat cloudera-cm.repo[cm]name=Cloudera-Managerbaseurl=http://192.168.25.150/cmgpgkey=https://archive.cloudera.com/redhat/cdh/RPM-GPG-KEY-clouderagpgcheck=1enabled=1[daniel@tiger yum.repos.d]$ 配置完成后，更新本地yum的cache，并查询是否配置成功 12345678910111213141516[daniel@tiger yum.repos.d]$ sudo yum clean all &amp;&amp; yum makecache[daniel@tiger yum.repos.d]$ sudo yum search cloudera-managerFailed to set locale, defaulting to CLoaded plugins: fastestmirror, refresh-packagekit, securityLoading mirror speeds from cached hostfile * base: mirrors.shuosc.org * extras: mirrors.cn99.com * updates: mirrors.shuosc.org=================================================================== N/S Matched: cloudera-manager ====================================================================cloudera-manager-agent.x86_64 : The Cloudera Manager Agentcloudera-manager-daemons.x86_64 : Provides daemons for monitoring Hadoop and related tools.cloudera-manager-server.x86_64 : The Cloudera Manager Servercloudera-manager-server-db-2.x86_64 : Embedded database for the Cloudera Manager Server Name and summary matches only, use &quot;search all&quot; for everything.[daniel@tiger yum.repos.d]$ Note: 这个方法中需要在Cloudera Manager Server安装完成后将CDH的parcels文件放到 /opt/cloudera/parcels-repo/路径下。 方法2：使用httpd配置Cloudera Manager和CDH本地源例如： Cloudera Manager的文件在 /home/daniel/software/clouderea-cm5 ,CDH的parcels包文件在/home/daniel/software/cloudera-cdh 使用方法1中安装httpd的方法安装httpd 在配置文件 /etc/httpd/conf/httpd.conf的最后添加 123456789101112&lt;Directory &quot;/home/daniel/software&quot;&gt;Options Indexes FollowSymLinks allowOverride None &lt;/Directory&gt; &lt;VirtualHost tiger:8050&gt; DocumentRoot &quot;/home/daniel/software/cloudera-cm5&quot; ServerName tiger:8050 &lt;/VirtualHost&gt; &lt;VirtualHost *:8000&gt; DocumentRoot /home/daniel/software/cloudera-cdh ServerName tiger:8000 &lt;/VirtualHost&gt; 并将监听的端口从80修改位8000和8050 123#Listen 80Listen 8000Listen 8050 将欢迎页面关闭掉。 1[daniel@tiger ~]$ sudo vim /etc/httpd/conf.d/welcome.conf 注释掉里面的内容 1234#&lt;LocationMatch &quot;^/+$&quot;&gt;# Options -Indexes# ErrorDocument 403 /error/noindex.html#&lt;/LocationMatch&gt; 另外需要注意对于文件所在的文件夹对other用户都有r和x的权限。 12345[daniel@tiger ~]$ ll software/total 8drwxrwxr-x. 2 daniel daniel 4096 Jan 6 05:19 cloudera-cdhdrwxrwxr-x. 3 daniel daniel 4096 Jan 6 14:47 cloudera-cm5[daniel@tiger ~]$ 配置完成后，重启httpd服务，并使用浏览器或者curl检查配置的结果。 12curl tiger:8000curl tiger:8050 ###Perform OS-level configuration for Hadoop installation 关闭防火墙 12sudo service iptables stopsudo chkconfig iptables off 关闭 SELinux 1sudo vim /etc/sysconfig/selinux 将其中的 SELINUX设置为disabled, SELINUXTYPE 设置为targeted 12345678910# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of these two values:# targeted - Targeted processes are protected,# mls - Multi Level Security protection.SELINUXTYPE=targeted 关闭透明大页 因为透明大页（Transparent HugePages ） 存在一些问题： a.在RAC环境下 透明大页（Transparent HugePages ）会导致异常节点重启，和性能问题；b.在单机环境中，透明大页（Transparent HugePages ） 也会导致一些异常的性能问题； 将下面2个命令执行，并写入 /etc/rc.local 文件里 12sudo echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defragsudo echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag 1234567891011[training@lion ~]$ cat /etc/rc.local#!/bin/sh## This script will be executed *after* all the other init scripts.# You can put your own initialization stuff in here if you don&apos;t# want to do the full Sys V style init stuff.touch /var/lock/subsys/localecho never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defragecho never &gt; /sys/kernel/mm/redhat_transparent_hugepage/enabled[training@lion ~]$ 修改使用SWAP分区的优先级在文件 /etc/sysctl.conf 里添加 vm.swappiness = 1 1[training@lion ~]$ vim /etc/sysctl.conf 1vm.swappiness = 1 linux 会使用硬盘的一部分做为SWAP分区，用来进行进程调度–进程是正在运行的程序–把当前不用的进程调成‘等待（standby）‘，甚至‘睡眠（sleep）’，一旦要用，再调成‘活动（active）’，睡眠的进程就躺到SWAP分区睡大觉，把内存空出来让给‘活动’的进程。 如果内存够大，应当告诉 linux 不必太多的使用 SWAP 分区， 可以通过修改 swappiness 的数值。swappiness=0的时候表示最大限度使用物理内存，然后才是 swap空间，swappiness＝100的时候表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间里面。 ###Install Cloudera Manager server and agents 安装Cloudera Manager Server1[daniel@tiger ~]$ sudo yum install -y cloudera-manager-daemons cloudera-manager-server 配置使用数据库执行如下命令：使Cloudera Manager使用配置好的MySQL数据库 1234567[daniel@tiger lib]$ sudo /usr/share/cmf/schema/scm_prepare_database.sh mysql cmserver cmserveruser passwordJAVA_HOME=/usr/lib/javaVerifying that we can write to /etc/cloudera-scm-serverCreating SCM configuration file in /etc/cloudera-scm-serverExecuting: /usr/lib/java/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/cmf/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.[ main] DbCommandExecutor INFO Successfully connected to database.All done, your SCM database is configured correctly! 等待1-2分钟，使用浏览器打开 http://localhost:7180 ###Install CDH using Cloudera Manager安装Cloudear Manager Server 1[training@lion training]$ sudo yum install -y cloudera-manager-daemons cloudera-manager-server 配置Cloudear Manager所使用的数据库 1[training@lion training]$ sudo /usr/share/cmf/schema/scm_prepare_database.sh mysql cmserver cmserveruser password 启动Cloudera Manager， 并使其开机启动,并坚持其状态（TODO：解释1-6） 123456[training@lion training]$ sudo service cloudera-scm-server startStarting cloudera-scm-server: [ OK ][training@lion training]$ sudo chkconfig cloudera-scm-server on[training@lion training]$ sudo chkconfig --list | grep cloudera-scm-servercloudera-scm-server 0:off 1:off 2:on 3:on 4:on 5:on 6:off[training@lion training]$ ###Add a new node to an existing cluster HOME -&gt; Hosts -&gt; All Hosts -&gt; Add New Hosts to Cluster 当前只有4个节点 搜索节点 选择节点 设置本地Cloudera Manager所在的http服务 选择用户并填写密码 添加Host到集群 成功添加Host到集群 分配并激活Parcels 成功激活Parcels 检测服务器 使用模板或者使用默认设置，使用模板可以在添加节点的时候添加和模板一样的服务到新的节点上 添加成功 查看新节点状态 ###Add a service using Cloudera Manager 添加Hive 服务 在集群的Home页，选择下拉框，Add Service选择要添加的服务 选择服务的依赖 选择需要添加的角色 选择要使用的数据库 查看也可修改相关配置 初始化和启动服务 服务启动完成 添加成功","comments":true,"categories":[],"tags":[{"name":"CCAH-131","slug":"CCAH-131","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CCAH-131/"}]},{"title":"Test 随笔","date":"2018-05-13T00:33:26.000Z","path":"2018/05/13/test/","text":"TTTTTeeeeeeeeeeSSSSTTT","comments":true,"categories":[],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/随笔/"}]},{"title":"Hive架构相关","date":"2018-05-12T06:34:44.000Z","path":"2018/05/12/Hive架构相关/","text":"Hive架构相关https://cwiki.apache.org/confluence/display/Hive/Design Hive Architecture（架构） UI – The user interface for users to submit queries and other operations to the system. As of 2011 the system had a command line interface and a web based GUI was being developed. UI – 用户向系统提交查询和其他操作的接口。2011年开始有了命令行接口和一个基于网页的图形化界面接口。 Driver – The component which receives the queries. This component implements the notion of session handles and provides execute and fetch APIs modeled on JDBC/ODBC interfaces. Driver – 用来接收查询的组件。这个组件包含对session的控制并提供通过JDBC/ODBC的接口来执行和获取的API模型。 Compiler – The component that parses the query, does semantic analysis on the different query blocks and query expressions and eventually generates an execution plan with the help of the table and partition metadata looked up from the metastore. Compiler – 对查询语句进行语法分析的组件，对不同的查询块和表达式进行语义上的分析，并根据从元数据存储的表和分区的信息生成最终生成执行计划。 Metastore – The component that stores all the structure information of the various tables and partitions in the warehouse including column and column type information, the serializers and deserializers necessary to read and write data and the corresponding HDFS files where the data is stored. Metastore – 存储所有在仓库里的表和分区的结构化信息的组件，包括列和列的信息，序列化和反序列化的方式对数据进行读写，HDFS文件存储位置的映射关系。 Execution Engine – The component which executes the execution plan created by the compiler. The plan is a DAG of stages. The execution engine manages the dependencies between these different stages of the plan and executes these stages on the appropriate system components. Execution Engine – 用来执行compiler生成的执行计划。执行计划是一个有向无环图。执行引擎管理执行计划间不同层级的依赖关系和这些层级对应的系统组件。 Figure 1 also shows how a typical query flows through the system. The UI calls the execute interface to the Driver (step 1 in Figure 1). The Driver creates a session handle for the query and sends the query to the compiler to generate an execution plan (step 2). The compiler gets the necessary metadata from the metastore (steps 3 and 4). This metadata is used to typecheck the expressions in the query tree as well as to prune partitions based on query predicates. The plan generated by the compiler (step 5) is a DAG of stages with each stage being either a map/reduce job, a metadata operation or an operation on HDFS. For map/reduce stages, the plan contains map operator trees (operator trees that are executed on the mappers) and a reduce operator tree (for operations that need reducers). The execution engine submits these stages to appropriate components (steps 6, 6.1, 6.2 and 6.3). In each task (mapper/reducer) the deserializer associated with the table or intermediate outputs is used to read the rows from HDFS files and these are passed through the associated operator tree. Once the output is generated, it is written to a temporary HDFS file though the serializer (this happens in the mapper in case the operation does not need a reduce). The temporary files are used to provide data to subsequent map/reduce stages of the plan. For DML operations the final temporary file is moved to the table’s location. This scheme is used to ensure that dirty data is not read (file rename being an atomic operation in HDFS). For queries, the contents of the temporary file are read by the execution engine directly from HDFS as part of the fetch call from the Driver (steps 7, 8 and 9).图1展示了一个查询在系统中的流向。UI调用Driver的执行接口（步骤1）。Driver为这个查询创建一个session对象并将查询语句发送给compiler用来生成执行计划（步骤2）。compiler从metastore获取需要的元数据信息（步骤3和4）。这些信息用来检查查询树中的表达式，并根据谓词下推修剪分区。compiler生成的是计划是一个层级的有向无环图，每一个层级都是一个用来操作HDFS或者元数据的map或者reduce任务。对于map/reduce层级，执行计划中包含map操作树（在mappers上执行的操作树）和reduce操作数据（需要reducers的操作）。执行引擎提交这些层级到对应的组件上（步骤6，6.1，6.2和6.3）。在每一个任务中（mapper/reducer）反序列化器将表或者从hdfs文件读取的行的输出和操作树关联起来。一定生成输出，通过序列化器将它写入到HDFS的一个临时文件（因为不需要reduce所以这部分都在mapper里面）。临时文件是为了后面的map/reduce提供数据。对于DML操作最终临时文件会被移动到表所在的位置。这个模式是为了确保垃圾数据不会被读取（在HDFS中会自动将文件重新命名）。对于查询，作为Driver调用的一部分临时文件的内容会被执行引擎直接从HDFS读取到（步骤7，8和9）。 Hive Data ModelData in Hive is organized into: Tables – These are analogous to Tables in Relational Databases. Tables can be filtered, projected, joined and unioned. Additionally all the data of a table is stored in a directory in HDFS. Hive also supports the notion of external tables wherein a table can be created on prexisting files or directories in HDFS by providing the appropriate location to the table creation DDL. The rows in a table are organized into typed columns similar to Relational Databases. 表 – 和关系型数据库中的表类似。表可以被过滤，管理，关联和合并。此外所有的表中的数据都存储在HDFS的文件夹内。Hive支持外部表，表可以通过提供HDFS上已经存在的文件或者文件夹的路径来创建。表中行和列的管理都类似于关系型数据库。 Partitions – Each Table can have one or more partition keys which determine how the data is stored, for example a table T with a date partition column ds had files with data for a particular date stored in the /ds= directory in HDFS. Partitions allow the system to prune data to be inspected based on query predicates, for example a query that is interested in rows from T that satisfy the predicate T.ds = ‘2008-09-01’ would only have to look at files in /ds=2008-09-01/ directory in HDFS. 分区 – 每个表可以有一个或者多个分区键，分区键用来确定数据的存储，例如，一个表T有一个时间分区列ds并且在对应的日期下都有数据存储在HDFS的&lt;表位置&gt;/ds=&lt;日期&gt;文件夹下。分区允许系统根据查询谓词进行修剪，例如从T表中查询T.ds=’2008-09-01’的数据只会查询HDFS中&lt;表位置&gt;/ds=2008-09-01下的文件。 Buckets – Data in each partition may in turn be divided into Buckets based on the hash of a column in the table. Each bucket is stored as a file in the partition directory. Bucketing allows the system to efficiently evaluate queries that depend on a sample of data (these are queries that use the SAMPLE clause on the table). 桶 – 在每个分区内的数据可以再根据表中一个列的哈希进行分桶。每个桶都是在分区文件夹中的一个文件。分桶允许系统根据样例数据（使用SAMPLE的查询）对查询进行有效的评估。 Apart from primitive column types (integers, floating point numbers, generic strings, dates and booleans), Hive also supports arrays and maps. Additionally, users can compose their own types programmatically from any of the primitives, collections or other user-defined types. The typing system is closely tied to the SerDe (Serailization/Deserialization) and object inspector interfaces. User can create their own types by implementing their own object inspectors, and using these object inspectors they can create their own SerDes to serialize and deserialize their data into HDFS files). These two interfaces provide the necessary hooks to extend the capabilities of Hive when it comes to understanding other data formats and richer types. Builtin object inspectors like ListObjectInspector, StructObjectInspector and MapObjectInspector provide the necessary primitives to compose richer types in an extensible manner. For maps (associative arrays) and arrays useful builtin functions like size and index operators are provided. The dotted notation is used to navigate nested types, for example a.b.c = 1 looks at field c of field b of type a and compares that with 1. 除了最原始的列的数据类型（整形，浮点型，普通的字符串，日期和布尔 ），Hive还支持数组和映射类型。此外用户可以根据原始数据类型、集合或者其他用户自定义类型进行自定义类型。类型和SerDe（序列化和反序列化）和接口检查器紧密相连。用户可以通过继承对象接口来创建他们自己的数据类型，并可以使用这些类型创建他们自己的SerDes去序列化和反序列化他们存储在HDFS文件里的数据。这两个接口提供了在理解其他数据格式和较丰富类型时扩展Hive功能的必要钩子。构建对象检查器，如ListObjectInspector、StructObjectInspector和MapObjectInspector提供必要的原语，以可扩展的方式组成更丰富的类型。对maps（联合数组）和数组也提供内置的长度和指针操作。 MetastoreMotivationThe Metastore provides two important but often overlooked features of a data warehouse: data abstraction and data . Without the data abstractions provided in Hive, a user has to provide information about data formats, extractors and loaders along with the query. In Hive, this information is given during table creation and reused every time the table is referenced. This is very similar to the traditional warehousing systems. The second functionality, data discovery, enables users to discover and explore relevant and specific data in the warehouse. Other tools can be built using this metadata to expose and possibly enhance the information about the data and its availability. Hive accomplishes both of these features by providing a metadata repository that is tightly integrated with the Hive query processing system so that data and metadata are in sync.动机Metastore 为数据仓库提供2个非常重要但是经常被忽略的组件：数据抽象和数据发现。如果没有Hive提供的数据抽象，用户不仅要提供查询语句还要提供数据类型、提取器和导入器等信息。在Hive中，每当表创建和重用的时候这些信息都会被提供使用。这点和传统的数据仓库非常类似。第二个功能是数据发现，使用户可以在仓库中发现和浏览相关和特殊的数据。其他工具可以编译使用这些元数据信息用来处理数据。Hive通过提供和Hive查询处理系统紧密相关的元数据仓库来实现这些功能，这样数据和元数据可以保持同步状态。Metadata ObjectsDatabase – is a namespace for tables. It can be used as an administrative unit in the future. The database ‘default’ is used for tables with no user-supplied database name.Table – Metadata for a table contains list of columns, owner, storage and SerDe information. It can also contain any user-supplied key and value data. Storage information includes location of the underlying data, file inout and output formats and bucketing information. SerDe metadata includes the implementation class of serializer and deserializer and any supporting information required by the implementation. All of this information can be provided during creation of the table.Partition – Each partition can have its own columns and SerDe and storage information. This facilitates schema changes without affecting older partitions.元数据对象数据库 – 表的命名空间。它将来可以被用做一个管理单元。默认的数据库’default’被用来存储没有被提供数据库名的表。表 – 表的元数据信息包括列的列表，所有者，存储和SerDe信息。也可以包含用户提供的Key和Value数据。包含数据的底层存储信息，文件输入和输出格式和分桶信息。SerDe元数据包含序列化和反序列化的接口和接口所需要的信息。这些信息在创建表的时候都可以提供。Metastore ArchitectureMetastore is an object store with a database or file backed store. The database backed store is implemented using an object-relational mapping (ORM) solution called the DataNucleus. The prime motivation for storing this in a relational database is queriability of metadata. Some disadvantages of using a separate data store for metadata instead of using HDFS are synchronization and scalability issues. Additionally there is no clear way to implement an object store on top of HDFS due to lack of random updates to files. This, coupled with the advantages of queriability of a relational store, made our approach a sensible one.The metastore can be configured to be used in a couple of ways: remote and embedded. In remote mode, the metastore is a Thrift service. This mode is useful for non-Java clients. In embedded mode, the Hive client directly connects to an underlying metastore using JDBC. This mode is useful because it avoids another system that needs to be maintained and monitored. Both of these modes can co-exist. (Update: Local metastore is a third possibility. See Hive Metastore Administration for details.)元数据存储架构元数据存储是数据或者文件存储对象。数据库支持存储是用一个叫做DataNucleus的对象关系映射（ORM）作为方案的接口。存储在关系型数据库的首要动机就是使这些元数据可被查询。使用一个单独的存储而不使用HDFS作为存储的原因是数据同步和扩展问题。此外，由于缺少文件的随机更新机制，所以不能基于HDFS做对象存储。因此，结合关系型存储的查询优点使我们作出了明智的选择。元数据存储可以使用多种方式进行配置：远程和嵌入的。在远程模式下，元数据存储是一个Thrift服务。这种模式对于非Java客户端非常有用。在嵌入模式下，Hive客户端可以直接使用JDBC连接元数据存储。因为避免了需要管理和监控的其他系统所有它非常有用。所有的模式可以并存。Metastore InterfaceMetastore provides a Thrift interface to manipulate and query Hive metadata. Thrift provides bindings in many popular languages. Third party tools can use this interface to integrate Hive metadata into other business metadata repositories.元数据存储接口元数据存储提供一个Thrift接口用来操作和查询Hive的元数据。Thrift提供多种语言的封装。第三方的工具可以使用这个接口将Hive的元数据集成到其他业务元数据仓库中。 Hive Query LanguageHiveQL is an SQL-like query language for Hive. It mostly mimics SQL syntax for creation of tables, loading data into tables and querying the tables. HiveQL also allows users to embed their custom map-reduce scripts. These scripts can be written in any language using a simple row-based streaming interface – read rows from standard input and write out rows to standard output. This flexibility comes at a cost of a performance hit caused by converting rows from and to strings. However, we have seen that users do not mind this given that they can implement their scripts in the language of their choice. Another feature unique to HiveQL is multi-table insert. In this construct, users can perform multiple queries on the same input data using a single HiveQL query. Hive optimizes these queries to share the scan of the input data, thus increasing the throughput of these queries several orders of magnitude. We omit more details due to lack of space. For a more complete description of the HiveQL language see the language manual. HiveQL是一个类SQL的查询语言。它有类SQL的创建表的语法，导入数据和查询表等。HiveQL也允许用户使用自定义的map-reduce代码。这些代码可以是用任何语言用一个简单的基于行的流接口-从标准输入读取和从标准输出写数据。这个兼容性源于将行和字符串之间做转换的性能花费。然而，用户能使用他们想要使用的语言他们就不在乎这点。一个只在HiveQL中有的特性是多表插入。在这种架构中，用户可以使用一个HiveQL查询同时操作多个查询语句在同一个数据输入上。Hive优化了对输入数据的扫描，因此增加了查询的吞吐。 Compiler Parser – Transform a query string to a parse tree representation. Semantic Analyser – Transform the parse tree to an internal query representation, which is still block based and not an operator tree. As part of this step, the column names are verified and expansions like * are performed. Type-checking and any implicit type conversions are also performed at this stage. If the table under consideration is a partitioned table, which is the common scenario, all the expressions for that table are collected so that they can be later used to prune the partitions which are not needed. If the query has specified sampling, that is also collected to be used later on. Logical Plan Generator – Convert the internal query representation to a logical plan, which consists of a tree of operators. Some of the operators are relational algebra operators like ‘filter’, ‘join’ etc. But some of the operators are Hive specific and are used later on to convert this plan into a series of map-reduce jobs. One such operator is a reduceSink operator which occurs at the map-reduce boundary. This step also includes the optimizer to transform the plan to improve performance – some of those transformations include: converting a series of joins into a single multi-way join, performing a map-side partial aggregation for a group-by, performing a group-by in 2 stages to avoid the scenario when a single reducer can become a bottleneck in presence of skewed data for the grouping key. Each operator comprises a descriptor which is a serializable object. Query Plan Generator – Convert the logical plan to a series of map-reduce tasks. The operator tree is recursively traversed, to be broken up into a series of map-reduce serializable tasks which can be submitted later on to the map-reduce framework for the Hadoop distributed file system. The reduceSink operator is the map-reduce boundary, whose descriptor contains the reduction keys. The reduction keys in the reduceSink descriptor are used as the reduction keys in the map-reduce boundary. The plan consists of the required samples/partitions if the query specified so. The plan is serialized and written to a file. Parser – 语法分析程序将一个查询串转换成一个语法树。 Semantic Analyser – 语义分析，将语法树转换成一个基于块的而非树的内部查询。在这个步骤中，会对列名进行验证并将像*这样的操作展开。同时也会做类型检查和隐性的类型转换。如果要处理的是一个分区表，会收集表的所在后面的修剪不使用的分区的时候会用到的信息。 Logical Plan Generator – 逻辑计划生成器，将内部查询转换成一个操作树的逻辑计划。有些操作是代数关系操作像’filter’，’join’等。但是有些是Hive特有的操作，并用做将这个计划转成成一系列的map-reduce任务。其中就有在map-reduce边界发生的reduceSink操作。在这步中也包括优化器将计划进行优化的转换-这些转换包括将一系列的join转换成一个多表jion，将分区汇总放在map端，将分区放在2个阶段执行，以避免在一个单一的reducer发生由于数据倾斜造成的性能瓶颈。每个操作符包含一个可序列化对象的描述符。 Query Plan Generator – 查询计划生成器 将逻辑计划转换成一系列的map-reduce任务。操作树通过递归遍历将被分解成一系列的用来提交到Hadoop分布式文件系统的mapreduce可序列化的任务。reduceSink操作是map-reduce的边界，它包含Reduce使用到的key。在reduceSink描述符中用的的key也会在map-reduce边界中使用。如果查询指定了sample或者分区，计划只会用他们组成。查询计划会被序列化并写入文件。OptimizerMore plan transformations are performed by the optimizer. The optimizer is an evolving component. As of 2011, it was rule-based and performed the following: column pruning and predicate pushdown. However, the infrastructure was in place, and there was work under progress to include other optimizations like map-side join. (Hive 0.11 added several join optimizations.) The optimizer can be enhanced to be cost-based (see Cost-based optimization in Hive and HIVE-5775). The sorted nature of output tables can also be preserved and used later on to generate better plans. The query can be performed on a small sample of data to guess the data distribution, which can be used to generate a better plan. A correlation optimizer was added in Hive 0.12. The plan is a generic operator tree, and can be easily manipulated. 优化器会做更多的查询计划转换操作。优化器是一个持续更新的组件。从2011年开始，它的基本规则是：列的修剪和谓词下推。然而，基础设施已经改变，现在正在进行的包括其他优化，如map-side join。 优化器可以加强成为基于花费的。输出表的存储可以被后面生成更好的机会使用。可以通过小的抽样数据猜测数据分布，从而生成更好的计划。 Hive APIsHive APIs Overview describes various public-facing APIs that Hive provides.","comments":true,"categories":[],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/Hive/"},{"name":"metastore","slug":"metastore","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/metastore/"}]},{"title":"测试Hive权限级别","date":"2018-05-12T06:34:44.000Z","path":"2018/05/12/测试Hive权限级别/","text":"测试Hive权限级别Hive权限分类 名称 解释 ALL 所有权限 ALTER 允许修改元数据（modify metadata data of object）—表信息数据 UPDATE 允许修改物理数据（modify physical data of object）—实际数据 CREATE 允许进行Create操作 DROP 允许进行DROP操作 INDEX 允许建索引（目前还没有实现） LOCK 当出现并发的使用允许用户进行LOCK和UNLOCK操作 SELECT 允许用户进行SELECT操作 SHOW_DATABASE 允许用户查看可用的数据库 Sentry能管理的权限 SELECT、INSERT、ALL 即Sentry只能分这三种权限进行赋值。 Hive User、Group、RoleRole 可以使用Grant语法，赋权限给Role。 User 操作系统上的用户，Hive权限里无法创建用户 Group 操作系统上的组，Hive权限里如法创建组 关系： 一个用户就是操作系统上的用户 可以把一个用户分配到一个组或者多个组上 一个组就是操作系统上的组 一个组可以有多个角色 角色只能被赋值给组，不能赋值给用户 一个角色可以被赋值多种权限 一个用户，继承它所在的所有组的所有权限 权限管理与分类不同的部门、应用在操作系统上属于不同的组角色和角色的名称应该是有标准的一个部门的用户，在操作系统上属于一个组，用户名不相同 可以有不同的权限 如:部门 IT_DEV 有读取数据库内容的权限Manager 有写数据库的权限如果user1 同时属于2个组，那么user1 具有读写数据库的权限如果user2 只属于ITDEV4组，那么user2 只有读取数据库内容的权限 测试准备创建一个测试库 1create database test; 创建表并插入数据 123456789use test;create table test.test_partition (id int, name string) partitioned by (dt int);insert into test.test_partition partition(dt=20180101) select &apos;1&apos;,&apos;daniel&apos;;select * from test_partition;+--------------------+----------------------+--------------------+--+| test_partition.id | test_partition.name | test_partition.dt |+--------------------+----------------------+--------------------+--+| 1 | daniel | 20180101 |+--------------------+----------------------+--------------------+--+ 创建一个测试用户 1create role test_auto; 在每个操作系统上都创建这个用户添加一个组 1234567groupadd it_dev ## 创建一个用户组useradd test_auto ## 创建一个用户usermod -g it_dev test_auto ## 将这个用户只加入 itdev4 组## 检查用户所属的组[root@dn3 ~]# id test_auto;uid=1008(test_auto) gid=1011(itdev4) groups=1011(itdev4)[root@dn3 ~]# 在Hive用户下，beeline执行，添加角色并分配权限 1234create role reader;grant select on database test to role reader;create role writer;grant insert on database test to role writer; 将只读角色赋到组上 12345678grant role reader to group itdev4;show role grant group itdev4;+---------+---------------+-------------+----------+--+| role | grant_option | grant_time | grantor |+---------+---------------+-------------+----------+--+| reader | false | NULL | -- |+---------+---------------+-------------+----------+--+ 使用用户test_auto连接hive 12beeline -n test_auto -u jdbc:hive2://nn2.htsec.com:10000/testshow tables; ## 执行成功 将写的权限赋到组上 12345678grant role writer to group itdev4;show role grant group itdev4;+---------+---------------+-------------+----------+--+| role | grant_option | grant_time | grantor |+---------+---------------+-------------+----------+--+| reader | false | NULL | -- || writer | false | NULL | -- |+---------+---------------+-------------+----------+--+ 向表中插入数据，执行成功 12345678insert into test_partition partition(dt=20180101) select &apos;2&apos;,&apos;tom&apos;;select * from test_partition;+--------------------+----------------------+--------------------+--+| test_partition.id | test_partition.name | test_partition.dt |+--------------------+----------------------+--------------------+--+| 1 | daniel | 20180101 || 2 | tom | 20180101 |+--------------------+----------------------+--------------------+--+ 创建用户test2，并添加到it_dev 下 123useradd -G it_dev test2[root@nn1 ~]# id test2uid=1009(test2) gid=1010(itdev4) groups=1010(itdev4) 使用beeline连接并测试 123456789101112131415161718beeline -n test2 -u jdbc:hive2://nn2.htsec.com:10000/testselect * from test_partition;+--------------------+----------------------+--------------------+--+| test_partition.id | test_partition.name | test_partition.dt |+--------------------+----------------------+--------------------+--+| 1 | daniel | 20180101 || 2 | tom | 20180101 |+--------------------+----------------------+--------------------+--+insert into test_partition partition(dt=20180101) select &apos;3&apos;,&apos;Monkey&apos;;select * from test_partition;+--------------------+----------------------+--------------------+--+| test_partition.id | test_partition.name | test_partition.dt |+--------------------+----------------------+--------------------+--+| 1 | daniel | 20180101 || 2 | tom | 20180101 || 3 | Monkey | 20180101 |+--------------------+----------------------+--------------------+--+ 测试测试SELECT使用test_auto连接hiveserver2,并测试 1234567891011beeline -n test_auto -u jdbc:hive2://xxx:10000show databases; ##看不到test数据库+----------------+--+| database_name |+----------------+--+| default |+----------------+--+use test; ## 报错，无法切换数据库User test_auto does not have privileges for SWITCHDATABASE 123456beeline -n test_auto -u jdbc:hive2://xxx:10000/testshow tables; ## 无法查看任何表。+-----------+--+| tab_name |+-----------+--++-----------+--+ 在hive用户下，查看test_auto是否在test_auto 组里。 12345678910111213show role grant group test_auto; ## test_auto组下没有任何的role；+-------+---------------+-------------+----------+--+| role | grant_option | grant_time | grantor |+-------+---------------+-------------+----------+--++-------+---------------+-------------+----------+--+grant role test_auto to group test_auto;show role grant group test_auto;+------------+---------------+-------------+----------+--+| role | grant_option | grant_time | grantor |+------------+---------------+-------------+----------+--+| test_auto | false | NULL | -- |+------------+---------------+-------------+----------+--+ 再在test_auto用户下，执行命令 123456789101112131415161718192021show databases;+----------------+--+| database_name |+----------------+--+| default || test |+----------------+--+show tables; ##查看到表+------------------------+--+| tab_name |+------------------------+--+| test_partition |+------------------------+--+select * from test_partition;+--------------------+----------------------+--------------------+--+| test_partition.id | test_partition.name | test_partition.dt |+--------------------+----------------------+--------------------+--+| 1 | daniel | 20180101 |+--------------------+----------------------+--------------------+--+ 查看表的表结构DESCRIBE TABLE 12345678910111213141516desc test_table; ## 报错,因为test_table是一个Kudu的表。 FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.ClassNotFoundException Class not found (state=08S01,code=1)desc test_partition; +--------------------------+-----------------------+-----------------------+--+| col_name | data_type | comment |+--------------------------+-----------------------+-----------------------+--+| id | int | || name | string | || dt | int | || | NULL | NULL || # Partition Information | NULL | NULL || # col_name | data_type | comment || | NULL | NULL || dt | int | |+--------------------------+-----------------------+-----------------------+--+ 查看分区SHOW PARTITIONS 123456show partitions test_partition; ## 有查看表分区的权限+--------------+--+| partition |+--------------+--+| dt=20180101 |+--------------+--+ 创建视图的权限 CREATE VIEW 123create view test_view as select * from test_partition;ser test_auto does not have privileges for CREATEVIEW The required privileges: Server=server1-&gt;Db=test-&gt;action=*; (state=42000,code=40000) 分析表 ANALYZE TABLE 123456analyze table test_partition partition(dt=20180101) COMPUTE STATISTICS; ## 需要Insert 权限Error: Error while compiling statement: FAILED: SemanticException No valid privileges User test_auto does not have privileges for QUERY The required privileges: Server=server1-&gt;Db=test-&gt;Table=test_partition-&gt;action=insert; (state=42000,code=40000) analyze table test_partition compute statistics for columns; ## 执行成功 查看表中的列 SHOW COLUMNS 12345678910show columns in test_partition;+--------+--+| field |+--------+--+| id || name || dt |+--------+--+``` 查看表的统计信息 **SHOW TABLE STATUS** DESCRIBE EXTENDED test_partition; +—————————–+—————————————————-+———————–+–+| col_name | data_type | comment |+—————————–+—————————————————-+———————–+–+| id | int | || name | string | || dt | int | || | NULL | NULL || # Partition Information | NULL | NULL || # col_name | data_type | comment || | NULL | NULL || dt | int | || | NULL | NULL || Detailed Table Information | Table(tableName:test_partition, dbName:test, owner:hive, createTime:1516768367, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:null), FieldSchema(name:name, type:string, comment:null), FieldSchema(name:dt, type:int, comment:null)], location:hdfs://xxx/user/hive/dw/test.db/test_partition, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[FieldSchema(name:dt, type:int, comment:null)], parameters:{numPartitions=1, transient_lastDdlTime=1516768367}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE) | DESCRIBE EXTENDED test_partition.id partition(dt=20180101); +———–+————+——————–+–+| col_name | data_type | comment |+———–+————+——————–+–+| id | int | from deserializer |+———–+————+——————–+–+ describe formatted test_partition.id; ## 执行成功```查看表的属性信息 SHOW TABLE PROPERTIES 1234567891011121314show tblproperties test_partition;+------------------------+-------------+--+| prpt_name | prpt_value |+------------------------+-------------+--+| transient_lastDdlTime | 1516768367 |+------------------------+-------------+--+show tblproperties test_partition(&apos;transient_lastDdlTime&apos;);+-------------+-------------+--+| prpt_name | prpt_value |+-------------+-------------+--+| 1516768367 | NULL |+-------------+-------------+--+ 复制表 CREATE TABLE AS SELECT 1234create table createas_test as select * from test_partition;Error: Error while compiling statement: FAILED: SemanticException No valid privileges User test_auto does not have privileges for CREATETABLE_AS_SELECT The required privileges: Server=server1-&gt;Db=test-&gt;action=*; (state=42000,code=40000) 查看表的创建语句 SHOW CREATE TABLE 1234567891011121314151617181920show create table test_partition;+----------------------------------------------------+--+| createtab_stmt |+----------------------------------------------------+--+| CREATE TABLE `test_partition`( || `id` int, || `name` string) || PARTITIONED BY ( || `dt` int) || ROW FORMAT SERDE || &apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&apos; || STORED AS INPUTFORMAT || &apos;org.apache.hadoop.mapred.TextInputFormat&apos; || OUTPUTFORMAT || &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos; || LOCATION || &apos;hdfs://xxx/user/hive/dw/test.db/test_partition&apos; || TBLPROPERTIES ( || &apos;transient_lastDdlTime&apos;=&apos;1516768367&apos;) |+----------------------------------------------------+--+ 分析 EXPLAIN 1234567891011121314151617181920212223EXPLAIN select * from test_partition;+----------------------------------------------------+--+| Explain |+----------------------------------------------------+--+| STAGE DEPENDENCIES: || Stage-0 is a root stage || || STAGE PLANS: || Stage: Stage-0 || Fetch Operator || limit: -1 || Processor Tree: || TableScan || alias: test_partition || Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE || Select Operator || expressions: id (type: int), name (type: string), dt (type: int) || outputColumnNames: _col0, _col1, _col2 || Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE || ListSink || |+----------------------------------------------------+--+ 测试 INSERT在hive用户下 123456789101112131415161718show grant role test_auto;+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+| database | table | partition | column | principal_name | principal_type | privilege | grant_option | grant_time | grantor |+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+| test | | | | test_auto | ROLE | select | false | 1516762810118000 | -- |+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+## 回收权限revoke select on database test from role test_auto;## 赋予Insert权限grant insert on database test to role test_auto;show grant role test_auto;+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+| database | table | partition | column | principal_name | principal_type | privilege | grant_option | grant_time | grantor |+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+| test | | | | test_auto | ROLE | insert | false | 1516771174805000 | -- |+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+ 在只有INSERT权限，测试如下几个在SELECT命令成功，而现在执行失败或者有问题的仍无法成功 1234567891011121314151617select * from test_partition;Error: Error while compiling statement: FAILED: SemanticException No valid privileges User test_auto does not have privileges for QUERY The required privileges: Server=server1-&gt;Db=test-&gt;Table=test_partition-&gt;Column=dt-&gt;action=select; (state=42000,code=40000) analyze table test_partition partition(dt=20180101) COMPUTE STATISTICS;Error: Error while compiling statement: FAILED: SemanticException No valid privileges User test_auto does not have privileges for QUERY The required privileges: Server=server1-&gt;Db=test-&gt;Table=test_partition-&gt;Column=RAW__DATA__SIZE-&gt;action=select; (state=42000,code=40000)create table createas_test as select * from test_partition;Error: Error while compiling statement: FAILED: SemanticException No valid privileges User test_auto does not have privileges for CREATETABLE_AS_SELECT The required privileges: Server=server1-&gt;Db=test-&gt;Table=test_partition-&gt;Column=dt-&gt;action=select; (state=42000,code=40000) 插入 INSERT 1insert into table test_partition partition(dt=20180102) select &apos;2&apos;,&apos;tom&apos; ; ## 执行成功 导入文件 LOAD 1234load data local inpath &apos;/tmp/hosts&apos; into table test_partition partition(dt=20180103); ## 此时/tmp/hosts文件的所有者是test_auto,但仍旧报错。Error: Error while compiling statement: FAILED: SemanticException No valid privileges User test_auto does not have privileges for LOAD The required privileges: Server=server1-&gt;URI=file:///tmp/hosts-&gt;action=*; (state=42000,code=40000) 更新 UPDATE 12update test_pratition set id =3 where id=1 ;Error: Error while compiling statement: FAILED: SemanticException [Error 10294]: Attempt to do update or delete using transaction manager that does not support these operations. (state=42000,code=10294) 在hive用户下，给用户赋予SELECT 权限测试以上几个失败的命令 1grant select on base test from role test_auto; 12345678910111213select * from test_partition;+--------------------+----------------------+--------------------+--+| test_partition.id | test_partition.name | test_partition.dt |+--------------------+----------------------+--------------------+--+| 1 | daniel | 20180101 || 2 | tom | 20180102 |+--------------------+----------------------+--------------------+--+analyze table test_partition partition(dt=20180101) COMPUTE STATISTICS; ## 执行成功create table createas_test as select * from test_partition; Error: Error while compiling statement: FAILED: SemanticException No valid privileges User test_auto does not have privileges for CREATETABLE_AS_SELECT The required privileges: Server=server1-&gt;Db=test-&gt;action=*; (state=42000,code=40000) 测试UPDATE不支持Update权限的赋值 12345revoke select on database test from role test_auto;revoke insert on database test from role test_auto;grant update on database test to role test_auto;Error: Error while compiling statement: FAILED: SemanticException Sentry does not support privilege: Update (state=42000,code=40000) 测试DELETE不支持Delete权限的赋值 12grant delete on database test to role test_auto;Error: Error while compiling statement: FAILED: SemanticException Sentry does not support privilege: Delete (state=42000,code=40000) URI1234567891011121314151617GRANT ALL ON URI &apos;hdfs://tmp/hosts&apos; to role test_auto; ## 成功load data inpath &apos;/tmp/hosts&apos; into table test_partition partition(dt=20180103); revoke all on uri &apos;hdfs://tmp/hosts&apos; from role test_auto;GRANT select ON URI &apos;hdfs://tmp/hosts&apos; to role test_auto; ## 成功load data inpath &apos;/tmp/hosts&apos; into table test_partition partition(dt=20180103); grant all on uri &apos;hdfs:/tmp/&apos; to role test_auto; insert overwrite directory &quot;/user/test_auto/result/test_partition&quot; row format delimited fields terminated by &quot;\\t&quot; select * from test_partition; insert overwrite directory &quot;/user/test_auto/result/test_partition&quot; select * from test_partition; INSERT OVERWRITE DIRECTORY &apos;/tmp/test_partition&apos; SELECT * FROM test_partition; ## 执行成功需要hive能在这个路径下创建文件夹，并且test_auto也有权限、","comments":true,"categories":[],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/Hive/"}]},{"title":"Sqoop 导入数据","date":"2018-05-12T06:27:44.000Z","path":"2018/05/12/Sqoop-导入数据库到hive/","text":"Sqoop 导入数据sqoop help123456789101112131415Available commands: codegen Generate code to interact with database records create-hive-table Import a table definition into Hive eval Evaluate a SQL statement and display the results export Export an HDFS directory to a database table help List available commands import Import a table from a database to HDFS import-all-tables Import tables from a database to HDFS import-mainframe Import datasets from a mainframe server to HDFS job Work with saved jobs list-databases List available databases on a server list-tables List available tables in a database merge Merge results of incremental imports metastore Run a standalone Sqoop metastore version Display version information sqoop codegencodegen 命令用来生成Java代码例如： 123sqoop codegen --connect jdbc:mysql://quickstart:3306/retail_db \\--username retail_dba --password cloudera -e &apos;select * from orders \\where $CONDITIONS limit 10&apos; sqoop create-hive-table根据源数据库表结构在hive中创建表,可以创建别名的表，但是只能创建textfile格式的表 12sqoop create-hive-table --connect jdbc:mysql://quickstart:3306/retail_db \\ --username retail_dba --password cloudera --table orders --hive-table orders_2 在hive中查看创建的表的结构 1234567891011121314151617181920CREATE TABLE `orders_2`( `order_id` int, `order_date` string, `order_customer_id` int, `order_status` string) COMMENT &apos;Imported by sqoop on 2017/10/21 02:27:38&apos; ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&apos; WITH SERDEPROPERTIES ( &apos;field.delim&apos;=&apos;\\u0001&apos;, &apos;line.delim&apos;=&apos;\\n&apos;, &apos;serialization.format&apos;=&apos;\\u0001&apos;) STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos; OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos; LOCATION &apos;hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders_2&apos; TBLPROPERTIES ( &apos;transient_lastDdlTime&apos;=&apos;1508578063&apos;) sqoop eval执行一段sql 12sqoop eval --connect jdbc:mysql://quickstart:3306/retail_db \\--username retail_dba --password cloudera --query &apos;select count(1) from orders&apos; 结果 12345------------------------ count(1) ------------------------ 68883 ------------------------ sqoop export将hive文件夹中的数据导入到数据库表中 Parquet 表 123sqoop export --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba \\ --password cloudera --hcatalog-table &quot;products&quot; --hcatalog-database &quot;default&quot; \\ --table products_1 -m 1 Text表 123sqoop export --connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba \\--password cloudera --export-dir /user/hive/warehouse/products_2 \\ --input-fields-terminated-by &apos;\\001&apos; --table products_2 -m 1 Sqoop import导入一张表到HDFS 123sqoop import --connect jdbc:mysql://quickstart:3306/retail_db \\--username=retail_dba --password=cloudera \\--target-dir /usr/hive/warehouse/orders --table orders --hive-import Hive 表结构 12345678910111213141516171819202122232425CREATE TABLE `orders`( `order_id` int, `order_date` string, `order_customer_id` int, `order_status` string) COMMENT &apos;Imported by sqoop on 2017/10/26 05:50:14&apos; ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&apos; WITH SERDEPROPERTIES ( &apos;field.delim&apos;=&apos;\\u0001&apos;, &apos;line.delim&apos;=&apos;\\n&apos;, &apos;serialization.format&apos;=&apos;\\u0001&apos;) STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos; OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos; LOCATION &apos;hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders&apos; TBLPROPERTIES ( &apos;COLUMN_STATS_ACCURATE&apos;=&apos;true&apos;, &apos;numFiles&apos;=&apos;1&apos;, &apos;numRows&apos;=&apos;0&apos;, &apos;rawDataSize&apos;=&apos;0&apos;, &apos;totalSize&apos;=&apos;2999944&apos;, &apos;transient_lastDdlTime&apos;=&apos;1509022219&apos;) HDFS 文件 1234[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/ordersFound 2 items-rw-r--r-- 1 cloudera supergroup 0 2017-10-26 06:20 /user/hive/warehouse/orders/_SUCCESS-rwxr-xr-x 1 cloudera supergroup 2999944 2017-10-26 06:20 /user/hive/warehouse/orders/part-m-00000 导入parquet格式表 123sqoop import --connect jdbc:mysql://quickstart:3306/retail_db \\--username=retail_dba --password=cloudera \\--target-dir /usr/hive/warehouse/orders --table orders --hive-import --as-parquetfile HDFS文件1234567[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders/Found 5 itemsdrwxr-xr-x - cloudera supergroup 0 2017-10-26 05:53 /user/hive/warehouse/orders/.metadatadrwxr-xr-x - cloudera supergroup 0 2017-10-26 05:54 /user/hive/warehouse/orders/.signals-rw-r--r-- 1 cloudera supergroup 488257 2017-10-26 05:54 /user/hive/warehouse/orders/5e708d3e-a273-48de-8a3d-37efcdaf8d62.parquet-rw-r--r-- 1 cloudera supergroup 0 2017-10-26 05:50 /user/hive/warehouse/orders/_SUCCESS-rwxr-xr-x 1 cloudera supergroup 2999944 2017-10-26 05:50 /user/hive/warehouse/orders/part-m-00000 加上Snappy 压缩 12345[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/orders/Found 3 itemsdrwxr-xr-x - cloudera supergroup 0 2017-10-26 06:04 /user/hive/warehouse/orders/.metadatadrwxr-xr-x - cloudera supergroup 0 2017-10-26 06:05 /user/hive/warehouse/orders/.signals-rw-r--r-- 1 cloudera supergroup 488257 2017-10-26 06:05 /user/hive/warehouse/orders/bfa9265e-2711-494e-a924-bd8c6591954e.parquet Sqoop import-all-tables导入数据库所有表到hdfs：创建表并将数据导入HDFS 123sqoop import-all-tables --connect jdbc:mysql://quickstart:3306/retail_db \\--username=retail_dba --password=cloudera --compress-codec=snappy \\ --as-parquetfile --warehouse-dir=/user/hive/warehouse --hive-import 检查其中一张表到hdfs文件 12345[cloudera@quickstart ~]$ hdfs dfs -ls /user/hive/warehouse/ordersFound 3 itemsdrwxr-xr-x - cloudera supergroup 0 2017-10-20 18:57 /user/hive/warehouse/orders/.metadatadrwxr-xr-x - cloudera supergroup 0 2017-10-20 18:58 /user/hive/warehouse/orders/.signals-rw-r--r-- 1 cloudera supergroup 488257 2017-10-20 18:58 /user/hive/warehouse/orders/a851e80b-1238-465d-93b7-a8ac11c53697.parquet 检查hive表结构 1show create table orders 12345678910111213141516171819202122CREATE TABLE `orders`( `order_id` int, `order_date` bigint, `order_customer_id` int, `order_status` string) ROW FORMAT SERDE &apos;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe&apos; STORED AS INPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat&apos; OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat&apos; LOCATION &apos;hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders&apos; TBLPROPERTIES ( &apos;COLUMN_STATS_ACCURATE&apos;=&apos;false&apos;, &apos;avro.schema.url&apos;=&apos;hdfs://quickstart.cloudera:8020/user/hive/warehouse/orders/.metadata/schemas/1.avsc&apos;, &apos;kite.compression.type&apos;=&apos;snappy&apos;, &apos;numFiles&apos;=&apos;0&apos;, &apos;numRows&apos;=&apos;-1&apos;, &apos;rawDataSize&apos;=&apos;-1&apos;, &apos;totalSize&apos;=&apos;0&apos;, &apos;transient_lastDdlTime&apos;=&apos;1508551065&apos;) Sqoop list-databases查看数据库列表 12sqoop list-databases --connect jdbc:mysql://quickstart:3306 \\--username retail_dba --password cloudera 结果 12information_schemaretail_db Sqoop list-tables查看数据库中表到列表 12sqoop list-tables --connect jdbc:mysql://quickstart:3306/retail_db \\--username retail_dba --password cloudera 结果： 123456categoriescustomersdepartmentsorder_itemsordersproducts","comments":true,"categories":[],"tags":[{"name":"sqoop","slug":"sqoop","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/sqoop/"}]},{"title":"CCAH-131 Required Skills （考试要求）","date":"2018-05-12T04:03:44.000Z","path":"2018/05/12/CCAH-131-Required-Skills/","text":"Required Skills （技能）Install （安装）Demonstrate an understanding of the installation process for Cloudera Manager, CDH, and the ecosystem projects. （熟悉Cloudera Manager、CDH和其他生态系统内的项目的安装过程） Set up a local CDH repository （设置一个本地的CDH仓库） Perform OS-level configuration for Hadoop installation （操作系统级别的配置准备） Install Cloudera Manager server and agents （安装 Cloudera Manager 服务和代理） Install CDH using Cloudera Manager （使用Cloudera Manager 安装CDH） Add a new node to an existing cluster （在集群上添加一个新的节点） Add a service using Cloudera Manager （使用Cloudera Manager添加一个新的服务） Configure （配置）Perform basic and advanced configuration needed to effectively administer a Hadoop cluster （为了高效管理集群的基础和高级配置） Configure a service using Cloudera Manager（使用Cloudera Manager 配置一个集群） Create an HDFS user’s home directory （创建用户的HDFS home文件夹） Configure NameNode HA （配置Hadoop NameNode的高可用） Configure ResourceManager HA （配置Yarn的ResourceManager的高可用） Configure proxy for Hiveserver2/Impala（配置HiveServer2或者Impala的代理） Manage （管理）Maintain and modify the cluster to support day-to-day operations in the enterprise (维护或者修改集群已满足企业的日常使用） Rebalance the cluster（重新平衡集群） Set up alerting for excessive disk fill （设置磁盘使用预警） Define and install a rack topology script （定义和安装机架拓扑） Install new type of I/O compression library in cluster（在集群中安装新的读写压缩包） Revise YARN resource assignment based on user feedback（根据用户的反馈配置YARN的资源） Commission/decommission a node（服役/退役一个节点） Secure （安全）Enable relevant services and configure the cluster to meet goals defined by security policy; demonstrate knowledge of basic security practices（可以根据安全要求进行服务安全配置） Configure HDFS ACLs （配置HDFS的ACLs） Install and configure Sentry（安装配置Sentry） Configure Hue user authorization and authentication（配置Hue的用户认证） Enable/configure log and query redaction（启用或者配置log和查询的修改） Create encrypted zones in HDFS（在HDFS上创建加密区间） Test （测试）Benchmark the cluster operational metrics, test system configuration for operation and efficiency（对集群的性能进行基准测试） Execute file system commands via HTTPFS （通过HTTPFS执行文件系统命令） Efficiently copy data within a cluster/between clusters （集群间的数据复制） Create/restore a snapshot of an HDFS directory（创建或者恢复HDFS文件夹得快照） Get/set ACLs for a file or directory structure（获取/设置一个文件或者文件夹的ALCs） Benchmark the cluster (I/O, CPU, network)（测试集群的读写、CPU和网络性能） Troubleshoot （定位错误）Demonstrate ability to find the root cause of a problem, optimize inefficient execution, and resolve resource contention scenarios（证明有找到问题根本错误的能力，优化慢的执行任务和解决实际问题的能了） Resolve errors/warnings in Cloudera Manager （解决Cloudera Manager上的错误或者警告） Resolve performance problems/errors in cluster operation（解决集群操作上性能问题） Determine reason for application failure（定位应用失败的原因） Configure the Fair Scheduler to resolve application delays（配置公平调度用以解决应用的提交延迟）","comments":true,"categories":[],"tags":[{"name":"CCAH-131","slug":"CCAH-131","permalink":"https://github.com/FrommyMind/FrommyMind.github.io/tags/CCAH-131/"}]},{"title":"Hello World","date":"2017-12-23T05:53:16.740Z","path":"2017/12/23/help/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","comments":true,"categories":[],"tags":[]}]