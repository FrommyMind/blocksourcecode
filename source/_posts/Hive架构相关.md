---
title: Hive架构相关
date: 2018-05-12 14:34:44
tags: 
    - Hive
    - metastore
---
#  Hive架构相关

https://cwiki.apache.org/confluence/display/Hive/Design


![Alt text](https://cwiki.apache.org/confluence/download/attachments/27362072/system_architecture.png?version=1&modificationDate=1414560669000&api=v2)

## Hive Architecture（架构）

<!-- more -->

-  UI – The user interface for users to submit queries and other operations to the system. As of 2011 the system had a command line interface and a web based GUI was being developed.
-  UI – 用户向系统提交查询和其他操作的接口。2011年开始有了命令行接口和一个基于网页的图形化界面接口。
- Driver – The component which receives the queries. This component implements the notion of session handles and provides execute and fetch APIs modeled on JDBC/ODBC interfaces.
- Driver – 用来接收查询的组件。这个组件包含对session的控制并提供通过JDBC/ODBC的接口来执行和获取的API模型。
- Compiler – The component that parses the query, does semantic analysis on the different query blocks and query expressions and eventually generates an execution plan with the help of the table and partition metadata looked up from the metastore.
- Compiler – 对查询语句进行语法分析的组件，对不同的查询块和表达式进行语义上的分析，并根据从元数据存储的表和分区的信息生成最终生成执行计划。
- Metastore – The component that stores all the structure information of the various tables and partitions in the warehouse including column and column type information, the serializers and deserializers necessary to read and write data and the corresponding HDFS files where the data is stored.
- Metastore – 存储所有在仓库里的表和分区的结构化信息的组件，包括列和列的信息，序列化和反序列化的方式对数据进行读写，HDFS文件存储位置的映射关系。
- Execution Engine – The component which executes the execution plan created by the compiler. The plan is a DAG of stages. The execution engine manages the dependencies between these different stages of the plan and executes these stages on the appropriate system components.
- Execution Engine – 用来执行compiler生成的执行计划。执行计划是一个有向无环图。执行引擎管理执行计划间不同层级的依赖关系和这些层级对应的系统组件。

Figure 1 also shows how a typical query flows through the system. The UI calls the execute interface to the Driver (step 1 in Figure 1). The Driver creates a session handle for the query and sends the query to the compiler to generate an execution plan (step 2). The compiler gets the necessary metadata from the metastore (steps 3 and 4). This metadata is used to typecheck the expressions in the query tree as well as to prune partitions based on query predicates. The plan generated by the compiler (step 5) is a DAG of stages with each stage being either a map/reduce job, a metadata operation or an operation on HDFS. For map/reduce stages, the plan contains map operator trees (operator trees that are executed on the mappers) and a reduce operator tree (for operations that need reducers). The execution engine submits these stages to appropriate components (steps 6, 6.1, 6.2 and 6.3). In each task (mapper/reducer) the deserializer associated with the table or intermediate outputs is used to read the rows from HDFS files and these are passed through the associated operator tree. Once the output is generated, it is written to a temporary HDFS file though the serializer (this happens in the mapper in case the operation does not need a reduce). The temporary files are used to provide data to subsequent map/reduce stages of the plan. For DML operations the final temporary file is moved to the table's location. This scheme is used to ensure that dirty data is not read (file rename being an atomic operation in HDFS). For queries, the contents of the temporary file are read by the execution engine directly from HDFS as part of the fetch call from the Driver (steps 7, 8 and 9).
图1展示了一个查询在系统中的流向。UI调用Driver的执行接口（步骤1）。Driver为这个查询创建一个session对象并将查询语句发送给compiler用来生成执行计划（步骤2）。compiler从metastore获取需要的元数据信息（步骤3和4）。这些信息用来检查查询树中的表达式，并根据谓词下推修剪分区。compiler生成的是计划是一个层级的有向无环图，每一个层级都是一个用来操作HDFS或者元数据的map或者reduce任务。对于map/reduce层级，执行计划中包含map操作树（在mappers上执行的操作树）和reduce操作数据（需要reducers的操作）。执行引擎提交这些层级到对应的组件上（步骤6，6.1，6.2和6.3）。在每一个任务中（mapper/reducer）反序列化器将表或者从hdfs文件读取的行的输出和操作树关联起来。一定生成输出，通过序列化器将它写入到HDFS的一个临时文件（因为不需要reduce所以这部分都在mapper里面）。临时文件是为了后面的map/reduce提供数据。对于DML操作最终临时文件会被移动到表所在的位置。这个模式是为了确保垃圾数据不会被读取（在HDFS中会自动将文件重新命名）。对于查询，作为Driver调用的一部分临时文件的内容会被执行引擎直接从HDFS读取到（步骤7，8和9）。
## Hive Data Model
Data in Hive is organized into:
- Tables – These are analogous to Tables in Relational Databases. Tables can be filtered, projected, joined and unioned. Additionally all the data of a table is stored in a directory in HDFS. Hive also supports the notion of external tables wherein a table can be created on prexisting files or directories in HDFS by providing the appropriate location to the table creation DDL. The rows in a table are organized into typed columns similar to Relational Databases.
- 表 – 和关系型数据库中的表类似。表可以被过滤，管理，关联和合并。此外所有的表中的数据都存储在HDFS的文件夹内。Hive支持外部表，表可以通过提供HDFS上已经存在的文件或者文件夹的路径来创建。表中行和列的管理都类似于关系型数据库。
- Partitions – Each Table can have one or more partition keys which determine how the data is stored, for example a table T with a date partition column ds had files with data for a particular date stored in the <table location>/ds=<date> directory in HDFS. Partitions allow the system to prune data to be inspected based on query predicates, for example a query that is interested in rows from T that satisfy the predicate T.ds = '2008-09-01' would only have to look at files in <table location>/ds=2008-09-01/ directory in HDFS.
- 分区 – 每个表可以有一个或者多个分区键，分区键用来确定数据的存储，例如，一个表T有一个时间分区列ds并且在对应的日期下都有数据存储在HDFS的<表位置>/ds=<日期>文件夹下。分区允许系统根据查询谓词进行修剪，例如从T表中查询T.ds='2008-09-01'的数据只会查询HDFS中<表位置>/ds=2008-09-01下的文件。
- Buckets – Data in each partition may in turn be divided into Buckets based on the hash of a column in the table. Each bucket is stored as a file in the partition directory. Bucketing allows the system to efficiently evaluate queries that depend on a sample of data (these are queries that use the SAMPLE clause on the table).
- 桶 – 在每个分区内的数据可以再根据表中一个列的哈希进行分桶。每个桶都是在分区文件夹中的一个文件。分桶允许系统根据样例数据（使用SAMPLE的查询）对查询进行有效的评估。

Apart from primitive column types (integers, floating point numbers, generic strings, dates and booleans), Hive also supports arrays and maps. Additionally, users can compose their own types programmatically from any of the primitives, collections or other user-defined types. The typing system is closely tied to the SerDe (Serailization/Deserialization) and object inspector interfaces. User can create their own types by implementing their own object inspectors, and using these object inspectors they can create their own SerDes to serialize and deserialize their data into HDFS files). These two interfaces provide the necessary hooks to extend the capabilities of Hive when it comes to understanding other data formats and richer types. Builtin object inspectors like ListObjectInspector, StructObjectInspector and MapObjectInspector provide the necessary primitives to compose richer types in an extensible manner. For maps (associative arrays) and arrays useful builtin functions like size and index operators are provided. The dotted notation is used to navigate nested types, for example a.b.c = 1 looks at field c of field b of type a and compares that with 1.

除了最原始的列的数据类型（整形，浮点型，普通的字符串，日期和布尔 ），Hive还支持数组和映射类型。此外用户可以根据原始数据类型、集合或者其他用户自定义类型进行自定义类型。类型和SerDe（序列化和反序列化）和接口检查器紧密相连。用户可以通过继承对象接口来创建他们自己的数据类型，并可以使用这些类型创建他们自己的SerDes去序列化和反序列化他们存储在HDFS文件里的数据。这两个接口提供了在理解其他数据格式和较丰富类型时扩展Hive功能的必要钩子。构建对象检查器，如ListObjectInspector、StructObjectInspector和MapObjectInspector提供必要的原语，以可扩展的方式组成更丰富的类型。对maps（联合数组）和数组也提供内置的长度和指针操作。
## Metastore

**Motivation**
The Metastore provides two important but often overlooked features of a data warehouse: data abstraction and data . Without the data abstractions provided in Hive, a user has to provide information about data formats, extractors and loaders along with the query. In Hive, this information is given during table creation and reused every time the table is referenced. This is very similar to the traditional warehousing systems. The second functionality, data discovery, enables users to discover and explore relevant and specific data in the warehouse. Other tools can be built using this metadata to expose and possibly enhance the information about the data and its availability. Hive accomplishes both of these features by providing a metadata repository that is tightly integrated with the Hive query processing system so that data and metadata are in sync.
**动机**
Metastore 为数据仓库提供2个非常重要但是经常被忽略的组件：数据抽象和数据发现。如果没有Hive提供的数据抽象，用户不仅要提供查询语句还要提供数据类型、提取器和导入器等信息。在Hive中，每当表创建和重用的时候这些信息都会被提供使用。这点和传统的数据仓库非常类似。第二个功能是数据发现，使用户可以在仓库中发现和浏览相关和特殊的数据。其他工具可以编译使用这些元数据信息用来处理数据。Hive通过提供和Hive查询处理系统紧密相关的元数据仓库来实现这些功能，这样数据和元数据可以保持同步状态。
**Metadata Objects**
Database – is a namespace for tables. It can be used as an administrative unit in the future. The database 'default' is used for tables with no user-supplied database name.
Table – Metadata for a table contains list of columns, owner, storage and SerDe information. It can also contain any user-supplied key and value data. Storage information includes location of the underlying data, file inout and output formats and bucketing information. SerDe metadata includes the implementation class of serializer and deserializer and any supporting information required by the implementation. All of this information can be provided during creation of the table.
Partition – Each partition can have its own columns and SerDe and storage information. This facilitates schema changes without affecting older partitions.
**元数据对象**
数据库 – 表的命名空间。它将来可以被用做一个管理单元。默认的数据库'default'被用来存储没有被提供数据库名的表。
表 – 表的元数据信息包括列的列表，所有者，存储和SerDe信息。也可以包含用户提供的Key和Value数据。包含数据的底层存储信息，文件输入和输出格式和分桶信息。SerDe元数据包含序列化和反序列化的接口和接口所需要的信息。这些信息在创建表的时候都可以提供。
**Metastore Architecture**
Metastore is an object store with a database or file backed store. The database backed store is implemented using an object-relational mapping (ORM) solution called the DataNucleus. The prime motivation for storing this in a relational database is queriability of metadata. Some disadvantages of using a separate data store for metadata instead of using HDFS are synchronization and scalability issues. Additionally there is no clear way to implement an object store on top of HDFS due to lack of random updates to files. This, coupled with the advantages of queriability of a relational store, made our approach a sensible one.
The metastore can be configured to be used in a couple of ways: remote and embedded. In remote mode, the metastore is a Thrift service. This mode is useful for non-Java clients. In embedded mode, the Hive client directly connects to an underlying metastore using JDBC. This mode is useful because it avoids another system that needs to be maintained and monitored. Both of these modes can co-exist. (Update: Local metastore is a third possibility. See Hive Metastore Administration for details.)
**元数据存储架构**
元数据存储是数据或者文件存储对象。数据库支持存储是用一个叫做DataNucleus的对象关系映射（ORM）作为方案的接口。存储在关系型数据库的首要动机就是使这些元数据可被查询。使用一个单独的存储而不使用HDFS作为存储的原因是数据同步和扩展问题。此外，由于缺少文件的随机更新机制，所以不能基于HDFS做对象存储。因此，结合关系型存储的查询优点使我们作出了明智的选择。
元数据存储可以使用多种方式进行配置：远程和嵌入的。在远程模式下，元数据存储是一个Thrift服务。这种模式对于非Java客户端非常有用。在嵌入模式下，Hive客户端可以直接使用JDBC连接元数据存储。因为避免了需要管理和监控的其他系统所有它非常有用。所有的模式可以并存。
**Metastore Interface**
Metastore provides a Thrift interface to manipulate and query Hive metadata. Thrift provides bindings in many popular languages. Third party tools can use this interface to integrate Hive metadata into other business metadata repositories.
**元数据存储接口**
元数据存储提供一个Thrift接口用来操作和查询Hive的元数据。Thrift提供多种语言的封装。第三方的工具可以使用这个接口将Hive的元数据集成到其他业务元数据仓库中。

## Hive Query Language
HiveQL is an SQL-like query language for Hive. It mostly mimics SQL syntax for creation of tables, loading data into tables and querying the tables. HiveQL also allows users to embed their custom map-reduce scripts. These scripts can be written in any language using a simple row-based streaming interface – read rows from standard input and write out rows to standard output. This flexibility comes at a cost of a performance hit caused by converting rows from and to strings. However, we have seen that users do not mind this given that they can implement their scripts in the language of their choice. Another feature unique to HiveQL is multi-table insert. In this construct, users can perform multiple queries on the same input data using a single HiveQL query. Hive optimizes these queries to share the scan of the input data, thus increasing the throughput of these queries several orders of magnitude. We omit more details due to lack of space. For a more complete description of the HiveQL language see the [language manual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual).

HiveQL是一个类SQL的查询语言。它有类SQL的创建表的语法，导入数据和查询表等。HiveQL也允许用户使用自定义的map-reduce代码。这些代码可以是用任何语言用一个简单的基于行的流接口-从标准输入读取和从标准输出写数据。这个兼容性源于将行和字符串之间做转换的性能花费。然而，用户能使用他们想要使用的语言他们就不在乎这点。一个只在HiveQL中有的特性是多表插入。在这种架构中，用户可以使用一个HiveQL查询同时操作多个查询语句在同一个数据输入上。Hive优化了对输入数据的扫描，因此增加了查询的吞吐。
## Compiler
- Parser – Transform a query string to a parse tree representation.
- Semantic Analyser – Transform the parse tree to an internal query representation, which is still block based and not an operator tree. As part of this step, the column names are verified and expansions like * are performed. Type-checking and any implicit type conversions are also performed at this stage. If the table under consideration is a partitioned table, which is the common scenario, all the expressions for that table are collected so that they can be later used to prune the partitions which are not needed. If the query has specified sampling, that is also collected to be used later on.
- Logical Plan Generator – Convert the internal query representation to a logical plan, which consists of a tree of operators. Some of the operators are relational algebra operators like 'filter', 'join' etc. But some of the operators are Hive specific and are used later on to convert this plan into a series of map-reduce jobs. One such operator is a reduceSink operator which occurs at the map-reduce boundary. This step also includes the optimizer to transform the plan to improve performance – some of those transformations include: converting a series of joins into a single multi-way join, performing a map-side partial aggregation for a group-by, performing a group-by in 2 stages to avoid the scenario when a single reducer can become a bottleneck in presence of skewed data for the grouping key. Each operator comprises a descriptor which is a serializable object.
- Query Plan Generator – Convert the logical plan to a series of map-reduce tasks. The operator tree is recursively traversed, to be broken up into a series of map-reduce serializable tasks which can be submitted later on to the map-reduce framework for the Hadoop distributed file system. The reduceSink operator is the map-reduce boundary, whose descriptor contains the reduction keys. The reduction keys in the reduceSink descriptor are used as the reduction keys in the map-reduce boundary. The plan consists of the required samples/partitions if the query specified so. The plan is serialized and written to a file.

- Parser – 语法分析程序将一个查询串转换成一个语法树。
- Semantic Analyser – 语义分析，将语法树转换成一个基于块的而非树的内部查询。在这个步骤中，会对列名进行验证并将像*这样的操作展开。同时也会做类型检查和隐性的类型转换。如果要处理的是一个分区表，会收集表的所在后面的修剪不使用的分区的时候会用到的信息。
- Logical Plan Generator – 逻辑计划生成器，将内部查询转换成一个操作树的逻辑计划。有些操作是代数关系操作像'filter'，'join'等。但是有些是Hive特有的操作，并用做将这个计划转成成一系列的map-reduce任务。其中就有在map-reduce边界发生的reduceSink操作。在这步中也包括优化器将计划进行优化的转换-这些转换包括将一系列的join转换成一个多表jion，将分区汇总放在map端，将分区放在2个阶段执行，以避免在一个单一的reducer发生由于数据倾斜造成的性能瓶颈。每个操作符包含一个可序列化对象的描述符。
- Query Plan Generator – 查询计划生成器 将逻辑计划转换成一系列的map-reduce任务。操作树通过递归遍历将被分解成一系列的用来提交到Hadoop分布式文件系统的mapreduce可序列化的任务。reduceSink操作是map-reduce的边界，它包含Reduce使用到的key。在reduceSink描述符中用的的key也会在map-reduce边界中使用。如果查询指定了sample或者分区，计划只会用他们组成。查询计划会被序列化并写入文件。
## Optimizer
More plan transformations are performed by the optimizer. The optimizer is an evolving component. As of 2011, it was rule-based and performed the following: column pruning and predicate pushdown. However, the infrastructure was in place, and there was work under progress to include other optimizations like map-side join. (Hive 0.11 added several [join optimizations](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization).)

The optimizer can be enhanced to be cost-based (see Cost-based optimization in Hive and HIVE-5775). The sorted nature of output tables can also be preserved and used later on to generate better plans. The query can be performed on a small sample of data to guess the data distribution, which can be used to generate a better plan.

A [correlation optimizer](https://cwiki.apache.org/confluence/display/Hive/Correlation+Optimizer) was added in Hive 0.12.

The plan is a generic operator tree, and can be easily manipulated.

优化器会做更多的查询计划转换操作。优化器是一个持续更新的组件。从2011年开始，它的基本规则是：列的修剪和谓词下推。然而，基础设施已经改变，现在正在进行的包括其他优化，如map-side join。

优化器可以加强成为基于花费的。输出表的存储可以被后面生成更好的机会使用。可以通过小的抽样数据猜测数据分布，从而生成更好的计划。


## Hive APIs
[Hive APIs Overview](https://cwiki.apache.org/confluence/display/Hive/Hive+APIs+Overview) describes various public-facing APIs that Hive provides.
